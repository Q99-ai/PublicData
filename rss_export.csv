ID,title,link,description,authors,published
0,"Zero Point Cryogenics, Qubic Technology, and Team Win $3 Million Award for Quantum Sensing",https://quantumcomputingreport.com/zero-point-cryogenics-qubic-technology-and-team-win-3-million-award-for-quantum-sensing/,"<p>Zero Point Cryogenics (ZPC) has secured funding from the Government of Canada through the Innovation for Defence Excellence and Security (IDEaS) program, in collaboration with the University of Waterloo. This initiative, aimed at advancing quantum technology research for defence and security applications, highlights Canada's commitment to technological innovation. ZPC's involvement in the 'Microwave Quantum Radar' [...]</p>
<p>The post <a href=\"https://quantumcomputingreport.com/zero-point-cryogenics-qubic-technology-and-team-win-3-million-award-for-quantum-sensing/\">Zero Point Cryogenics, Qubic Technology, and Team Win $3 Million Award for Quantum Sensing</a> appeared first on <a href=\"https://quantumcomputingreport.com\">Quantum Computing Report</a>.</p>",[{'name': 'Mohamed Abdel-Kareem'}],"Thu, 07 Mar 2024 19:06:45 +0000"
1,Tabor Electronics and FormFactor Unveil Full Stack 5-Qubit Quantum Computer Project Powered by QuantWare,https://quantumcomputingreport.com/tabor-electronics-and-formfactor-unveil-full-stack-5-qubit-quantum-computer-project-powered-by-quantware/,"<p>Tabor Electronics, in collaboration with FormFactor, is demonstrating the first phase of the Echo-5Q project, a full stack 5-Qubit Quantum Computer powered by QuantWare's Quantum Processing Unit (QPU). This achievement showcases a significant 250% enhancement in T1 times compared to similar systems, ensuring higher qubit fidelity and increased quantum operations per cycle. The Echo-5Q project [...]</p>
<p>The post <a href=\"https://quantumcomputingreport.com/tabor-electronics-and-formfactor-unveil-full-stack-5-qubit-quantum-computer-project-powered-by-quantware/\">Tabor Electronics and FormFactor Unveil Full Stack 5-Qubit Quantum Computer Project Powered by QuantWare</a> appeared first on <a href=\"https://quantumcomputingreport.com\">Quantum Computing Report</a>.</p>",[{'name': 'Mohamed Abdel-Kareem'}],"Thu, 07 Mar 2024 18:43:20 +0000"
2,ConScience AB Partners with Quantum Microwave to Distribute Quantum Devices in North America,https://quantumcomputingreport.com/conscience-ab-partners-with-quantum-microwave-to-distribute-quantum-devices-in-north-america/,"<p>ConScience AB, based in Gothenburg, Sweden, has entered into a distribution agreement with Quantum Microwave, a US-based company, covering the USA and Canada. This agreement follows ConScience AB's launch of its first Quantum device, the QiB0, in Q1 2024. Quantum Microwave, known for distributing components in the quantum computing market, is pleased to include ConScience [...]</p>
<p>The post <a href=\"https://quantumcomputingreport.com/conscience-ab-partners-with-quantum-microwave-to-distribute-quantum-devices-in-north-america/\">ConScience AB Partners with Quantum Microwave to Distribute Quantum Devices in North America</a> appeared first on <a href=\"https://quantumcomputingreport.com\">Quantum Computing Report</a>.</p>",[{'name': 'Mohamed Abdel-Kareem'}],"Thu, 07 Mar 2024 18:27:46 +0000"
3,"Qblox, Maybell, and QuantWare Assembled a Full-Stack Quantum System at APS March Meeting 2024",https://quantumcomputingreport.com/qblox-maybell-and-quantware-assembled-a-full-stack-quantum-system-at-aps-march-meeting-2024/,"<p>Qblox, Maybell, and QuantWare collaborated to unveil a fully functional superconducting quantum computer at the APS March Meeting 2024, a global first for a conference setting. The system, assembled over a weekend, integrates quantum devices from QuantWare, a 10mK cryostat from Maybell, and control electronics from Qblox for live qubit operations. Featuring the Maybell Fridge, [...]</p>
<p>The post <a href=\"https://quantumcomputingreport.com/qblox-maybell-and-quantware-assembled-a-full-stack-quantum-system-at-aps-march-meeting-2024/\">Qblox, Maybell, and QuantWare Assembled a Full-Stack Quantum System at APS March Meeting 2024</a> appeared first on <a href=\"https://quantumcomputingreport.com\">Quantum Computing Report</a>.</p>",[{'name': 'Mohamed Abdel-Kareem'}],"Thu, 07 Mar 2024 00:48:25 +0000"
4,Podcast with Anca Albu of QAI Ventures,https://quantumcomputingreport.com/podcast-with-anca-albu-of-qai-ventures/,"<p>QAI Ventures just celebrated its first anniversary as a $50 million, early-stage venture fund and startup accelerator. Anca Albu is the chief marketing officer at QAI Ventures and works with entrepreneurs to help them craft compelling stories about their work. She helped build the largest and most active angel investor network in Switzerland, so she [...]</p>
<p>The post <a href=\"https://quantumcomputingreport.com/podcast-with-anca-albu-of-qai-ventures/\">Podcast with Anca Albu of QAI Ventures</a> appeared first on <a href=\"https://quantumcomputingreport.com\">Quantum Computing Report</a>.</p>",[{'name': 'dougfinke'}],"Thu, 07 Mar 2024 00:23:41 +0000"
5,SCALINQ Unveils LINQER600: Largest Commercial Quantum Device Packaging Solution,https://quantumcomputingreport.com/scalinq-unveils-linqer600-largest-commercial-quantum-device-packaging-solution/,"<p>SCALINQ’s latest product, LINQER600, is a complete packaging solution with 600 lines. SCALINQ introduces LINQER600, the largest commercially available quantum device packaging solution, enabling control of superconducting or spin qubit based quantum devices with hundreds of qubits. The LINQER600 includes a sample holder for the qubit chip and provides a means of routing up to [...]</p>
<p>The post <a href=\"https://quantumcomputingreport.com/scalinq-unveils-linqer600-largest-commercial-quantum-device-packaging-solution/\">SCALINQ Unveils LINQER600: Largest Commercial Quantum Device Packaging Solution</a> appeared first on <a href=\"https://quantumcomputingreport.com\">Quantum Computing Report</a>.</p>",[{'name': 'Mohamed Abdel-Kareem'}],"Wed, 06 Mar 2024 03:56:16 +0000"
6,$5 Million XPrize Competition Announced to Develop Real World Applications Quantum Applications,https://quantumcomputingreport.com/5-million-xprize-competition-announced-to-develop-real-world-applications-quantum-applications/,"<p>Google Quantum AI along with additional support from the Geneva Science Diplomacy Anticipator (GESDA) Foundation is sponsoring a 3 year, $5 million prize competition to encourage development of novel algorithms, new applications, or ways of implementing enhanced performance of quantum solutions that can help solve real-world challenges. The contest will be administered by the XPRIZE [...]</p>
<p>The post <a href=\"https://quantumcomputingreport.com/5-million-xprize-competition-announced-to-develop-real-world-applications-quantum-applications/\">$5 Million XPrize Competition Announced to Develop Real World Applications Quantum Applications</a> appeared first on <a href=\"https://quantumcomputingreport.com\">Quantum Computing Report</a>.</p>",[{'name': 'dougfinke'}],"Tue, 05 Mar 2024 21:26:03 +0000"
7,Multiverse Computing Raises €25 Million ($27M USD) in Oversubscribed Series A Funding Round,https://quantumcomputingreport.com/multiverse-computing-raises-e25-million-27m-usd-in-oversubscribed-series-a-funding-round/,"<p>Multiverse Computing, a leading European quantum software company, has successfully raised €25 million ($27M USD) in Series A funding. The investment round, oversubscribed, was led by Columbus Venture Partners and Quantonation Ventures, with contributions from various other investors including the European Innovation Council Fund and Redstone QAI Quantum Fund. This funding will be utilized to [...]</p>
<p>The post <a href=\"https://quantumcomputingreport.com/multiverse-computing-raises-e25-million-27m-usd-in-oversubscribed-series-a-funding-round/\">Multiverse Computing Raises €25 Million ($27M USD) in Oversubscribed Series A Funding Round</a> appeared first on <a href=\"https://quantumcomputingreport.com\">Quantum Computing Report</a>.</p>",[{'name': 'Mohamed Abdel-Kareem'}],"Tue, 05 Mar 2024 18:55:19 +0000"
8,Maybell Quantum Secures $25M Series A Funding Led by Cerberus Ventures,https://quantumcomputingreport.com/maybell-quantum-secures-25m-series-a-funding-led-by-cerberus-ventures/,"<p>Maybell Quantum, a quantum infrastructure company, has secured $25M in Series A funding led by Cerberus Ventures. The funding will enable Maybell to scale manufacturing and open a new production facility in 2024. Maybell, founded in 2021, provides quantum infrastructure products such as their cryogenic platforms, the Fridge and the Big Fridge. In 2023, they [...]</p>
<p>The post <a href=\"https://quantumcomputingreport.com/maybell-quantum-secures-25m-series-a-funding-led-by-cerberus-ventures/\">Maybell Quantum Secures $25M Series A Funding Led by Cerberus Ventures</a> appeared first on <a href=\"https://quantumcomputingreport.com\">Quantum Computing Report</a>.</p>",[{'name': 'Mohamed Abdel-Kareem'}],"Tue, 05 Mar 2024 05:51:45 +0000"
9,Quantum Computing Inc. Introduces Its Dirac-3 Entropy Quantum Computer for Optimization Applications,https://quantumcomputingreport.com/quantum-computing-inc-its-dirac-3-entropy-quantum-computer-for-optimization-applications/,"<p>Picture of the Direc-3 Entropy Quantum Computer. Credit: QCI Quantum Computing Inc. (QCi) has launched Dirac-3, a quantum optimization platform designed to solve complex problems using nonlinear quantum optics. The system encodes a problem into its photonic architecture and modulates the interaction between variables, ultimately finding an optimal solution. Unique to Dirac is its ability [...]</p>
<p>The post <a href=\"https://quantumcomputingreport.com/quantum-computing-inc-its-dirac-3-entropy-quantum-computer-for-optimization-applications/\">Quantum Computing Inc. Introduces Its Dirac-3 Entropy Quantum Computer for Optimization Applications</a> appeared first on <a href=\"https://quantumcomputingreport.com\">Quantum Computing Report</a>.</p>",[{'name': 'dougfinke'}],"Sat, 02 Mar 2024 01:21:59 +0000"
10,Vescent Announces $5 Million in Series Seed Funding Led by Corporate Fuel,https://thequantuminsider.com/2024/03/07/vescent-announces-5-million-in-series-seed-funding-led-by-corporate-fuel/,"<a href=\"https://thequantuminsider.com/2024/03/07/vescent-announces-5-million-in-series-seed-funding-led-by-corporate-fuel/\" rel=\"nofollow\" title=\"Vescent Announces $5 Million in Series Seed Funding Led by Corporate Fuel\"><img alt=\"Vescent\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"405\" src=\"https://thequantuminsider.com/wp-content/uploads/2024/03/Clean-room-1.jpg\" style=\"display: block; margin: auto; margin-bottom: 10px;\" width=\"608\" /></a>Insider Brief Vescent closed a $5 million Series Seed funding round. The funding round was led by Corporate Fuel, with additional participation from Caruso Ventures. The funds are expected to accelerate new product introduction, scales manufacturing to meet customer demand, and expands its market reach. PRESS RELEASE &#8212; Vescent, the leader in Quantum 2.0 Technologies [&#8230;]",[{'name': 'Matt Swayne'}],"Thu, 07 Mar 2024 21:30:30 +0000"
11,QuantrolOx And Zurich Instruments Unite For Quantum Advantage,https://thequantuminsider.com/2024/03/07/quantrolox-and-zurich-instruments-unite-for-quantum-advantage/,"<a href=\"https://thequantuminsider.com/2024/03/07/quantrolox-and-zurich-instruments-unite-for-quantum-advantage/\" rel=\"nofollow\" title=\"QuantrolOx And Zurich Instruments Unite For Quantum Advantage\"><img alt=\"Zurich Instruments Quantrolox\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"1600\" src=\"https://thequantuminsider.com/wp-content/uploads/2024/03/quantrolox_zhinst_unite_pr_banner_1200x800px_2024_final.png\" style=\"display: block; margin: auto; margin-bottom: 10px;\" width=\"2400\" /></a>Insider Brief QuantrolOx and Zurich Instruments announced a partnership. Zurich Instruments’  Quantum Computing Control System (QCCS) is now integrated into the QuantrolOx’s Quantum EDGE. The integration empowers users to obtain outstanding results, reduces complexity and enhances performance of quantum computing experiments. PRESS RELEASE &#8212; QuantrolOx (Espoo, Finland) and Zurich Instruments (Zurich, Switzerland) are pleased to [&#8230;]",[{'name': 'Matt Swayne'}],"Thu, 07 Mar 2024 20:58:33 +0000"
12,Researchers Say New Algorithm Outperforms Existing AI Compressors in Accuracy and Reduction of Computational Demands of GPT-2,https://thequantuminsider.com/2024/03/07/researchers-say-new-algorithm-outperforms-existing-ai-compressors-in-accuracy-and-reduction-of-computational-demands-of-gpt-2/,"<a href=\"https://thequantuminsider.com/2024/03/07/researchers-say-new-algorithm-outperforms-existing-ai-compressors-in-accuracy-and-reduction-of-computational-demands-of-gpt-2/\" rel=\"nofollow\" title=\"Researchers Say New Algorithm Outperforms Existing AI Compressors in Accuracy and Reduction of Computational Demands of GPT-2\"><img alt=\"a computer chip with the word gat printed on it\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"901\" src=\"https://thequantuminsider.com/wp-content/uploads/2024/03/fc1gbkmv-dw.jpg\" style=\"display: block; margin: auto; margin-bottom: 10px;\" width=\"1600\" /></a>Insider Brief Terra Quantum introduced TQCompressor, an algorithm that shrinks large language models (LLMs) while maintaining comparable performance. The study demonstrates the compression of GPT-2 small, achieving a 35% reduction in the number of parameters. The significant time, computation and energy resources continue to rise for these large NLP models, particularly for training. Terra Quantum, [&#8230;]",[{'name': 'Matt Swayne'}],"Thu, 07 Mar 2024 19:28:40 +0000"
13,D-Wave-led Research Team Reports on Quantum Advantage in Quantum Simulation Task,https://thequantuminsider.com/2024/03/07/d-wave-led-research-team-reports-on-quantum-advantage-in-quantum-simulation-task/,"<a href=\"https://thequantuminsider.com/2024/03/07/d-wave-led-research-team-reports-on-quantum-advantage-in-quantum-simulation-task/\" rel=\"nofollow\" title=\"D-Wave-led Research Team Reports on Quantum Advantage in Quantum Simulation Task\"><img alt=\"D-Wave\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"732\" src=\"https://thequantuminsider.com/wp-content/uploads/2024/03/Screenshot-2024-03-06-at-8.26.55-AM.png\" style=\"display: block; margin: auto; margin-bottom: 10px;\" width=\"684\" /></a>Insider Brief A team of D-Wave-led researchers report on strides toward quantum advantage. In a study, the scientists said superconducting quantum annealing processors can efficiently simulate the nonequilibrium dynamics of a magnetic spin system undergoing a quantum phase transition. The researchers suggest that a classical computer would taken millions of years to complete complex versions [&#8230;]",[{'name': 'Matt Swayne'}],"Thu, 07 Mar 2024 15:04:46 +0000"
14,Kvantify Launches Its First Product to Improve Drug Discovery,https://thequantuminsider.com/2024/03/07/kvantify-launches-its-first-product-to-improve-drug-discovery/,"<a href=\"https://thequantuminsider.com/2024/03/07/kvantify-launches-its-first-product-to-improve-drug-discovery/\" rel=\"nofollow\" title=\"Kvantify Launches Its First Product to Improve Drug Discovery\"><img alt=\"Pharmaceutical Research, Scientist holding multi well plate\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"2000\" src=\"https://thequantuminsider.com/wp-content/uploads/2024/02/pharmaceutical-research-scientist-holding-multi-well-plate.jpg\" style=\"display: block; margin: auto; margin-bottom: 10px;\" width=\"1500\" /></a>Insider Brief Danish quantum company Kvantify announced the launch of its first product, Kvantify Koffee. The product aims to complement and reduce costly and time-consuming laboratory experiment. Kvantify Koffee calculates Unbinding Kinetics, a critical parameter in screening and selecting new drug candidates. PRESS RELEASE &#8212; In a groundbreaking advancement for computational drug discovery, Danish quantum [&#8230;]",[{'name': 'Matt Swayne'}],"Thu, 07 Mar 2024 14:29:15 +0000"
15,Study: Quantum Computing Will Radically Alter Copyright Law Application,https://thequantuminsider.com/2024/03/07/study-quantum-computing-will-radically-alter-copyright-law-application/,"<a href=\"https://thequantuminsider.com/2024/03/07/study-quantum-computing-will-radically-alter-copyright-law-application/\" rel=\"nofollow\" title=\"Study: Quantum Computing Will Radically Alter Copyright Law Application\"><img alt=\"Copyright Infringement\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"1238\" src=\"https://thequantuminsider.com/wp-content/uploads/2024/03/copyright-infringement.jpg\" style=\"display: block; margin: auto; margin-bottom: 10px;\" width=\"2000\" /></a>Insider Brief A new study suggests that quantum computing could challenge long-held notions of copyright. Faster computing will bring exponentially greater possibilities in the tracking and tracing of the legal owners of art, music, culture and books. However, according to the researcher, faster computers will also be able to potentially break and get around certain [&#8230;]",[{'name': 'Matt Swayne'}],"Thu, 07 Mar 2024 11:24:40 +0000"
16,France 2030: Progress Update Three Years After the Launch of the National Quantum Technologies Strategy and the Launch of the PROQCIMA Program,https://thequantuminsider.com/2024/03/07/france-2030-progress-update-three-years-after-the-launch-of-the-national-quantum-technologies-strategy-and-the-launch-of-the-proqcima-program/,"<a href=\"https://thequantuminsider.com/2024/03/07/france-2030-progress-update-three-years-after-the-launch-of-the-national-quantum-technologies-strategy-and-the-launch-of-the-proqcima-program/\" rel=\"nofollow\" title=\"France 2030: Progress Update Three Years After the Launch of the National Quantum Technologies Strategy and the Launch of the PROQCIMA Program\"><img alt=\"flag of France\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"1333\" src=\"https://thequantuminsider.com/wp-content/uploads/2024/03/flag-of-france.jpg\" style=\"display: block; margin: auto; margin-bottom: 10px;\" width=\"2000\" /></a>Insider Brief Members of the French quantum ecosystem reviewed the results of the national quantum strategy, three years after its launch by President Emmanuel Macron. They also announced the launch of PROQCIMA, which aims to have two prototypes of universal quantum computers of French design by 2030. The plan is structured around strategic objectives that [&#8230;]",[{'name': 'Matt Swayne'}],"Thu, 07 Mar 2024 10:42:05 +0000"
17,"Nokia Leads the Charge in Quantum-Safe Networking, Says VP James Watt at #MWC24",https://thequantuminsider.com/2024/03/06/nokia-leads-the-charge-in-quantum-safe-networking-says-vp-james-watt-at-mwc24/,"<a href=\"https://thequantuminsider.com/2024/03/06/nokia-leads-the-charge-in-quantum-safe-networking-says-vp-james-watt-at-mwc24/\" rel=\"nofollow\" title=\"Nokia Leads the Charge in Quantum-Safe Networking, Says VP James Watt at #MWC24\"><img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"973\" src=\"https://thequantuminsider.com/wp-content/uploads/2024/03/Screenshot-1489.png\" style=\"display: block; margin: auto; margin-bottom: 10px;\" width=\"1920\" /></a>In an enlightening conversation at #MWC24, James Watt, Vice President of Optical Networks at Nokia, discussed the burgeoning field of quantum computing and its implications for network security. As quantum computing continues to evolve, its potential to transform the communications industry is undeniable, promising unprecedented computational power but also posing significant security challenges. “Quantum computing [&#8230;]",[{'name': 'James Dargan'}],"Wed, 06 Mar 2024 12:05:40 +0000"
18,The Open Quantum Institute Launches Its Pilot Phase at CERN,https://thequantuminsider.com/2024/03/06/the-open-quantum-institute-launches-its-pilot-phase-at-cern/,"<a href=\"https://thequantuminsider.com/2024/03/06/the-open-quantum-institute-launches-its-pilot-phase-at-cern/\" rel=\"nofollow\" title=\"The Open Quantum Institute Launches Its Pilot Phase at CERN\"><img alt=\"Open Quantum Initiative\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"959\" src=\"https://thequantuminsider.com/wp-content/uploads/2024/03/thumbnail_Sony-2062.jpeg\" style=\"display: block; margin: auto; margin-bottom: 10px;\" width=\"1440\" /></a>Insider Brief The Open Quantum Institute (OQI) at CERN is now operational. Proposed, designed, and incubated through GESDA, in collaboration with some 180 experts from all over the world, the OQI is a multilateral science diplomacy initiative. The institute&#8217;s goal is to &#8220;help unleash the full power of quantum computing for the benefit of all.&#8221; [&#8230;]",[{'name': 'Matt Swayne'}],"Wed, 06 Mar 2024 11:53:06 +0000"
19,SCALINQ Launches Large-Scale Packaging Solution That Can Control Quantum Devices With Hundreds of Qubits,https://thequantuminsider.com/2024/03/06/scalinq-launches-large-scale-packaging-solution-that-can-control-quantum-devices-with-hundreds-of-qubits/,"<a href=\"https://thequantuminsider.com/2024/03/06/scalinq-launches-large-scale-packaging-solution-that-can-control-quantum-devices-with-hundreds-of-qubits/\" rel=\"nofollow\" title=\"SCALINQ Launches Large-Scale Packaging Solution That Can Control Quantum Devices With Hundreds of Qubits\"><img alt=\"LINQER\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"319\" src=\"https://thequantuminsider.com/wp-content/uploads/2024/03/Screenshot_2024-03-05_at_1.47.45_PM_35.webp\" style=\"display: block; margin: auto; margin-bottom: 10px;\" width=\"512\" /></a>Insider Brief SCALINQ announces the release of a large-scale packaging solution capable of controlling quantum devices with hundreds of qubits. The company reports the solution is the biggest commercially available of its kind. The system was developed in close cooperation with IQM. PRESS RELEASE &#8212; SCALINQ, a rapidly growing provider of quantum computing hardware, proudly [&#8230;]",[{'name': 'Matt Swayne'}],"Wed, 06 Mar 2024 10:05:40 +0000"
20,More than half of American Indian youth may have abnormal or high cholesterol,https://www.sciencedaily.com/releases/2024/03/240306150704.htm,"More than 70% of American Indian young adults aged 20-39 and 50% of American Indian teens have cholesterol levels or elevated fat in the blood that put them at risk for cardiovascular disease, a new study suggests. In some cases, these levels -- specifically high low-density lipoprotein (LDL) cholesterol, often thought of as 'bad cholesterol,' -- were linked to plaque buildup and cardiovascular events, such as heart attack and stroke.",[],"Wed, 06 Mar 2024 15:07:04 EST"
21,Finding new physics in debris from colliding neutron stars,https://www.sciencedaily.com/releases/2024/03/240306150618.htm,"Neutron star mergers are a treasure trove for new physics signals, with implications for determining the true nature of dark matter, according to physicists.",[],"Wed, 06 Mar 2024 15:06:18 EST"
22,Astronomers spot oldest 'dead' galaxy yet observed,https://www.sciencedaily.com/releases/2024/03/240306150612.htm,"A galaxy that suddenly stopped forming new stars more than 13 billion years ago has been observed by astronomers. Using the James Webb Space Telescope, astronomers have spotted a 'dead' galaxy when the universe was just 700 million years old, the oldest such galaxy ever observed.",[],"Wed, 06 Mar 2024 15:06:12 EST"
23,Schizophrenia and aging may share a common biological basis,https://www.sciencedaily.com/releases/2024/03/240306150610.htm,Researchers have uncovered a strikingly similar suite of changes in gene activity in brain tissue from people with schizophrenia and from older adults. These changes suggest a common biological basis for the cognitive impairment often seen in people with schizophrenia and in the elderly.,[],"Wed, 06 Mar 2024 15:06:10 EST"
24,Synthetic gene helps explain the mysteries of transcription across species,https://www.sciencedaily.com/releases/2024/03/240306150549.htm,"'Random DNA' is naturally active in the one-celled fungi yeast, while such DNA is turned off as its natural state in mammalian cells, despite their having a common ancestor a billion years ago and the same basic molecular machinery, a new study finds.",[],"Wed, 06 Mar 2024 15:05:49 EST"
25,"Compact chips advance precision timing for communications, navigation and other applications",https://www.sciencedaily.com/releases/2024/03/240306150545.htm,"Precision timing and synchronization are crucial for navigation, communication and radar systems. Scientists have built compact chips capable of converting light into microwaves, which could improve these systems. This technology shrinks a tabletop system into a chip-sized format, reducing power usage and making it more applicable for use in everyday devices.",[],"Wed, 06 Mar 2024 15:05:45 EST"
26,Decoding the language of epigenetic modifications,https://www.sciencedaily.com/releases/2024/03/240306150542.htm,"Epigenetic changes play important roles in cancer, metabolic and aging-related diseases, but also during loss of resilience as they cause the genetic material to be incorrectly interpreted in affected cells. A major study now provides important new insights into how complex epigenetic modification signatures regulate the genome. This study will pave the way for new treatments of diseases caused by faulty epigenetic machineries.",[],"Wed, 06 Mar 2024 15:05:42 EST"
27,Psychosocial stressors linked to higher inflammation in Black pregnant women,https://www.sciencedaily.com/releases/2024/03/240306150529.htm,"Living in neighborhoods with more white residents and greater lifetime experiences of racial discrimination are linked to increased systemic inflammation during pregnancy among Black women, according to new research.",[],"Wed, 06 Mar 2024 15:05:29 EST"
28,Amyloid blood levels associated with brain changes in Alzheimer's study,https://www.sciencedaily.com/releases/2024/03/240306150527.htm,A link between abnormal blood levels of amyloid -- a protein associated with Alzheimer's disease -- and subtle changes in brain microstructures on a type of MRI could lead to a new way to detect Alzheimer's earlier in people with no clinical signs.,[],"Wed, 06 Mar 2024 15:05:27 EST"
29,Study reveals genetic clusters and biological pathways that may explain differences in type 2 diabetes risk,https://www.sciencedaily.com/releases/2024/03/240306150525.htm,A recent study analyzed individuals from diverse backgrounds and identified various genetic clusters involved in a broad range of biological mechanisms that may help explain ancestry-associated differences in type 2 diabetes clinical presentations.,[],"Wed, 06 Mar 2024 15:05:25 EST"
30,"Herbivores, displaced by ocean warming, threaten subtropical seagrass meadows",https://www.sciencedaily.com/releases/2024/03/240306150523.htm,"The findings suggest that subtropical seagrasses are less resilient to heavy grazing from marine herbivores, in part because they receive less sunlight relative to their tropical counterparts. As tropical herbivores move into subtropical waters, overgrazing may prevent subtropical seagrass meadows from persisting in these environments.",[],"Wed, 06 Mar 2024 15:05:23 EST"
31,A step towards clinic-ready patient-derived organoids,https://www.sciencedaily.com/releases/2024/03/240306150518.htm,"A recent study provides critical insights into the reliability and robustness of patient-derived organoids as a clinical model of pancreatic cancer. The findings reveal that organoids' gene expression and drug responses are not affected by the brand of extracellular matrix used in the cell culture. However, one commercial product did increase the growth rate of pancreatic tumor organoids, making it particularly well-suited for the fast pace of pancreatic cancer treatment protocols.",[],"Wed, 06 Mar 2024 15:05:18 EST"
32,Discovery tests theory on cooling of white dwarf stars,https://www.sciencedaily.com/releases/2024/03/240306150510.htm,Open any astronomy textbook to the section on white dwarf stars and you'll likely learn that they are 'dead stars' that continuously cool down over time. Astronomers are challenging this theory after discovering a population of white dwarf stars that stopped cooling for more than eight billion years.,[],"Wed, 06 Mar 2024 15:05:10 EST"
33,Invasive plant time bombs: A hidden ecological threat,https://www.sciencedaily.com/releases/2024/03/240306150501.htm,"Invasive plants can stay dormant for decades or even centuries before rapidly expanding and wreaking ecological havoc, finds a new study.",[],"Wed, 06 Mar 2024 15:05:01 EST"
34,"Older adults want to express themselves with emojis, they just don't understand how to",https://www.sciencedaily.com/releases/2024/03/240306150459.htm,"A new study is providing insight into how different generations are interpreting the use of emojis in their communications, with older adults unsure of how to use them as frequently as younger adults despite understanding their meaning.",[],"Wed, 06 Mar 2024 15:04:59 EST"
35,Consuming refined carbs might be linked to perceived facial attractiveness,https://www.sciencedaily.com/releases/2024/03/240306150457.htm,"In a new study, participants' levels of consumption of refined carbohydrates were statistically linked with their facial attractiveness as rated by heterosexual volunteers of the opposite sex.",[],"Wed, 06 Mar 2024 15:04:57 EST"
36,"COVID-19 physiological impacts vary by sex, wearable technology reveals",https://www.sciencedaily.com/releases/2024/03/240306150455.htm,"Males and females have differential physiological responses to COVID-19 infections, with males having larger increases in skin temperature, breathing rate and heart rate during an acute infection, according to a new study that used data from wearable devices.",[],"Wed, 06 Mar 2024 15:04:55 EST"
37,Factors associated with age-related hearing loss differ between males and females,https://www.sciencedaily.com/releases/2024/03/240306150450.htm,"Certain factors associated with developing age-related hearing loss differ by sex, including weight, smoking behavior, and hormone exposure, according to a new study.",[],"Wed, 06 Mar 2024 15:04:50 EST"
38,This injectable hydrogel mitigates damage to the right ventricle of the heart,https://www.sciencedaily.com/releases/2024/03/240306150441.htm,"An injectable hydrogel can mitigate damage to the right ventricle of the heart with chronic pressure overload, according to a new study. In 2019, this same hydrogel was shown to be safe in humans through an FDA-approved Phase 1 trial in people who suffered a heart attack. As a result of the new preclinical study, the FDA approved an investigational new drug application to start a clinical trial with the hydrogel in pediatric patients in the coming months, once institutional approvals are received.",[],"Wed, 06 Mar 2024 15:04:41 EST"
39,New deep-sea worm discovered at methane seep off Costa Rica,https://www.sciencedaily.com/releases/2024/03/240306150437.htm,Marine biologists have discovered a new species of deep-sea worm living near a methane seep some 50 kilometers (30 miles) off the Pacific coast of Costa Rica.,[],"Wed, 06 Mar 2024 15:04:37 EST"
40,Genetic mutation in a quarter of all Labradors hard-wires them for obesity,https://www.sciencedaily.com/releases/2024/03/240306150433.htm,New research finds around a quarter of Labrador retriever dogs face a double-whammy of feeling hungry all the time and burning fewer calories due to a genetic mutation.,[],"Wed, 06 Mar 2024 15:04:33 EST"
41,New type of nanoparticle makes vaccines more powerful,https://www.sciencedaily.com/releases/2024/03/240306150424.htm,A type of nanoparticle called a metal organic framework (MOF) could be used to deliver vaccines and act as an adjuvant. Researchers find these particles provoke a strong immune response by activating the innate immune system through cell proteins called toll-like receptors.,[],"Wed, 06 Mar 2024 15:04:24 EST"
42,Advanced noise suppression technology for improved search and rescue drones,https://www.sciencedaily.com/releases/2024/03/240306145036.htm,"Unmanned Aerial Vehicles (UAVs) are beneficial in search and rescue missions during natural disasters like earthquakes. However, current UAVs depend on visual information and cannot detect victims trapped under rubble. While some studies have used sound for detection, the noise from UAV propellers can drown out human sounds. To address this issue, researchers have developed a novel artificial intelligence-based system that effectively suppresses UAV noise and amplifies human sounds.",[],"Wed, 06 Mar 2024 14:50:36 EST"
43,Do some electric fish sense the world through comrades' auras?,https://www.sciencedaily.com/releases/2024/03/240306144830.htm,"It would be a game-changer if all members of a basketball team could see out of each other's eyes in addition to their own. Biologists have found evidence that this kind of collective sensing occurs in close-knit groups of African weakly electric fish, also known as elephantnose fish. This instantaneous sharing of sensory intelligence could help the fish locate food, friends and foes.",[],"Wed, 06 Mar 2024 14:48:30 EST"
44,Sinking land increases risk for thousands of coastal residents by 2050,https://www.sciencedaily.com/releases/2024/03/240306144815.htm,"A new study provides a new comprehensive look at the potential for flooding in a combined 32 cities along the Atlantic, Pacific, and Gulf coasts. It predicts as many as 500,000 people will be affected by flooding alongside 1 in 35 privately owned properties within the next three decades, and it highlights the racial and socioeconomic demographics of those potentially affected.",[],"Wed, 06 Mar 2024 14:48:15 EST"
45,AI can speed design of health software,https://www.sciencedaily.com/releases/2024/03/240306144729.htm,"Artificial intelligence helped clinicians to accelerate the design of diabetes prevention software, a new study finds.",[],"Wed, 06 Mar 2024 14:47:29 EST"
46,Can you tell AI-generated people from real ones?,https://www.sciencedaily.com/releases/2024/03/240306003456.htm,"If you recently had trouble figuring out if an image of a person is real or generated through artificial intelligence (AI), you're not alone. A new study found that people had more difficulty than was expected distinguishing who is a real person and who is artificially generated.",[],"Wed, 06 Mar 2024 00:34:56 EST"
47,Key advance toward removing common herbicide from groundwater,https://www.sciencedaily.com/releases/2024/03/240305204445.htm,Chemists are closing in on a new tool for tackling the global problem of weedkiller-tainted groundwater.,[],"Tue, 05 Mar 2024 20:44:45 EST"
48,Research explores the cooling effects of 'scuba-diving' in lizards,https://www.sciencedaily.com/releases/2024/03/240305204443.htm,"Anoles are the scuba-diving champions of the lizard world, able to stay underwater for more than 16 minutes. For animals whose body temperature depends on the environment, time spent in a cool running stream can have some tradeoffs, according to new research.",[],"Tue, 05 Mar 2024 20:44:43 EST"
49,Aluminum nanoparticles make tunable green catalysts,https://www.sciencedaily.com/releases/2024/03/240305204424.htm,A nanotechnology pioneer has uncovered a transformative approach to harnessing the catalytic power of aluminum nanoparticles by annealing them in various gas atmospheres at high temperatures.,[],"Tue, 05 Mar 2024 20:44:24 EST"
50,Tiny worms tolerate Chornobyl radiation,https://www.sciencedaily.com/releases/2024/03/240305165928.htm,"A new study finds that exposure to chronic radiation from Chornobyl has not damaged the genomes of microscopic worms living there today -- which doesn't mean that the region is safe, the scientists caution, but suggests that these worms are exceptionally resilient.",[],"Tue, 05 Mar 2024 16:59:28 EST"
51,Having self-control leads to power,https://www.sciencedaily.com/releases/2024/03/240305165925.htm,Out-of-control behavior by CEOs and other powerful people constantly makes headlines -- so much so that some might consider impulsivity a pathway to power. New research finds that having self-control is often what leads to power.,[],"Tue, 05 Mar 2024 16:59:25 EST"
52,Harmful 'forever chemicals' removed from water with new electrocatalysis method,https://www.sciencedaily.com/releases/2024/03/240305165911.htm,"Scientists have developed new electrochemical approaches to clean up pollution from 'forever chemicals' found in clothing, food packaging, firefighting foams, and a wide array of other products. A new study describes nanocatalysts developed to remediate per- and polyfluoroalkyl substances, known as PFAS.",[],"Tue, 05 Mar 2024 16:59:11 EST"
53,"For a healthy fruit snack, what would you choose?",https://www.sciencedaily.com/releases/2024/03/240305165908.htm,"Next time you're packing lunch for your kid or reaching for a healthy afternoon bite, consider this: only three types of packaged fruit snacks -- dried fruit, fruit puree and canned fruit with juice -- meet the latest recommendations for high-nutrition snacks set by federal dietary guidelines, according to food scientists.",[],"Tue, 05 Mar 2024 16:59:08 EST"
54,Evolving hydrogen-storage technology: Guidelines developed for the design of anti-evaporation catalysts,https://www.sciencedaily.com/releases/2024/03/240305165709.htm,A research team has identified materials capable of catalyzing the conversion of ortho-hydrogen to para-hydrogen. These catalysts should be essential to the spread of mass-transportation/storage of liquid hydrogen.,[],"Tue, 05 Mar 2024 16:57:09 EST"
55,A smart molecule beats the mutation behind most pancreatic cancer,https://www.sciencedaily.com/releases/2024/03/240305134402.htm,"Researchers have designed a candidate drug that could help make pancreatic cancer, which is almost always fatal, a treatable, perhaps even curable, condition.",[],"Tue, 05 Mar 2024 13:44:02 EST"
56,Using light to precisely control single-molecule devices,https://www.sciencedaily.com/releases/2024/03/240305134359.htm,Researchers flip the switch at the nanoscale by applying light to induce bonding for single-molecule device switching.,[],"Tue, 05 Mar 2024 13:43:59 EST"
57,"Lab-grown liver organoid to speed up turtle research, making useful traits easier to harness",https://www.sciencedaily.com/releases/2024/03/240305134355.htm,"Researchers developed protocols for growing organoids that mimic a turtle liver, the first organoids developed for a turtle and only the second for any reptile. The discovery will aid deeper study of turtle genetics, including the cause of traits with potential medical applications for humans such as the ability to survive weeks without oxygen.",[],"Tue, 05 Mar 2024 13:43:55 EST"
58,New method measures the 3D position of individual atoms,https://www.sciencedaily.com/releases/2024/03/240305134352.htm,"Since more than a decade it has been possible for physicists to accurately measure the location of individual atoms to a precision of smaller than one thousandth of a millimeter using a special type of microscope. However, this method has so far only provided the x and y coordinates. Information on the vertical position of the atom -- i.e., the distance between the atom and the microscope objective -- is lacking. A new method has now been developed that can determine all three spatial coordinates of an atom with one single image.",[],"Tue, 05 Mar 2024 13:43:52 EST"
59,Toward understanding sperm quality,https://www.sciencedaily.com/releases/2024/03/240305134346.htm,A novel screening system enables researchers to investigate sperm cell development and health at the molecular level. The new approach promises breakthroughs in male contraception and infertility treatments.,[],"Tue, 05 Mar 2024 13:43:46 EST"
60,"Researchers closing in on genetic treatments for hereditary lung disease, vision loss",https://www.sciencedaily.com/releases/2024/03/240305134330.htm,"Researchers who work with tiny drug carriers known as lipid nanoparticles have developed a new type of material capable of reaching the lungs and the eyes, an important step toward genetic therapy for hereditary conditions like cystic fibrosis and inherited vision loss.",[],"Tue, 05 Mar 2024 13:43:30 EST"
61,Gen Z's climate anxiety is real and needs action -- for everyone's wellbeing,https://www.sciencedaily.com/releases/2024/03/240305134319.htm,"Young people have major concerns about climate change, which is having a significant impact on their lives and could have broader consequences decades into the future.",[],"Tue, 05 Mar 2024 13:43:19 EST"
62,Shortcut to Success: Toward fast and robust quantum control through accelerating adiabatic passage,https://www.sciencedaily.com/releases/2024/03/240305134315.htm,"Researchers achieved the acceleration of adiabatic evolution of a single spin qubit in gate-defined quantum dots. After the pulse optimization to suppress quasistatic noises, the spin flip fidelity can be as high as 97.5% in GaAs quantum dots. This work may be useful to achieve fast and high-fidelity quantum computing.",[],"Tue, 05 Mar 2024 13:43:15 EST"
63,Smart optimization paves the way for improved silicon solar cell performance,https://www.sciencedaily.com/releases/2024/03/240305134312.htm,"Bayesian optimization (BO) can help determine the optimal deposition conditions for high-performance passivation films in solar cells. However, simple implementations tend to suggest excessively thick films or infeasible deposition conditions. Recently, researchers from Japan developed a novel constrained BO scheme that combines several prediction models to restrict suggested deposition conditions to realistic values. Their strategy could help minimize trial and error and improve material performance in various fields.",[],"Tue, 05 Mar 2024 13:43:12 EST"
64,Sweetened drinks linked to atrial fibrillation risk,https://www.sciencedaily.com/releases/2024/03/240305134303.htm,"An analysis of health data in the UK Biobank found a 20% higher risk of irregular heart rhythm, known as atrial fibrillation, among people who said they drank two liters or more per week (about 67 ounces) of artificially sweetened drinks. The risk was 10% higher among people who said they drank similar amounts of sugar-sweetened beverages.",[],"Tue, 05 Mar 2024 13:43:03 EST"
65,'Like a lab in your pocket' -- new test strips raise game in gene-based diagnostics,https://www.sciencedaily.com/releases/2024/03/240305134300.htm,Biosensing technology developed by engineers has made it possible to create gene test strips that rival conventional lab-based tests in quality.,[],"Tue, 05 Mar 2024 13:43:00 EST"
66,Fossils of giant sea lizard with dagger-like teeth show how our oceans have fundamentally changed since the dinosaur era,https://www.sciencedaily.com/releases/2024/03/240305134258.htm,"Fossils of a strange new species of marine lizard with dagger-like teeth that lived 66 million years ago, show a dramatically more biodiverse ocean ecosystem to what we see today.",[],"Tue, 05 Mar 2024 13:42:58 EST"
67,Running performance helped by mathematical research,https://www.sciencedaily.com/releases/2024/03/240305134255.htm,"A new mathematical model has shown, with great precision, the impact that physiological and psychological parameters have on running performance and provides tips for optimized training.",[],"Tue, 05 Mar 2024 13:42:55 EST"
68,Waist-to-height ratio detects fat obesity in children and adolescents significantly better than BMI,https://www.sciencedaily.com/releases/2024/03/240305134243.htm,An inexpensive measure of obesity in children and adolescents that could replace body mass index (BMI) has been identified in a new study as waist circumference-to-height ratio. This measure detected excess fat mass and distinguished fat mass from muscle mass in children and adolescents more accurately than BMI.,[],"Tue, 05 Mar 2024 13:42:43 EST"
69,Groundbreaking survey reveals secrets of planet birth around dozens of stars,https://www.sciencedaily.com/releases/2024/03/240305134240.htm,"A team of astronomers has shed new light on the fascinating and complex process of planet formation. The research brings together observations of more than 80 young stars that might have planets forming around them, providing astronomers with a wealth of data and unique insights into how planets arise in different regions of our galaxy.",[],"Tue, 05 Mar 2024 13:42:40 EST"
70,One way to improve a fusion reaction: Use weaknesses as strengths,https://www.sciencedaily.com/releases/2024/03/240305134238.htm,"Scientists are embracing imperfection, using less-than-ideal magnetic fields to make the plasma more manageable.",[],"Tue, 05 Mar 2024 13:42:38 EST"
71,Possible 'Trojan Horse' found for treating stubborn bacterial infections,https://www.sciencedaily.com/releases/2024/03/240305134236.htm,"Bacteria can be tricked into sending death signals to stop the growth of their slimy, protective homes that lead to deadly infections, a new study demonstrates. The discovery could someday be harnessed as an alternative to antibiotics for treating difficult infections. The researchers used the messengers, which they named death extracellular vesicles (D-EVs), to reduce growth of the bacterial communities by up to 99.99% in laboratory experiments.",[],"Tue, 05 Mar 2024 13:42:36 EST"
72,Conservation actions for SA's white shark population now a matter of urgency,https://www.sciencedaily.com/releases/2024/03/240305134233.htm,"A group of marine biologists specializing in shark ecology, genetics and fisheries have challenged the findings from a recent study suggesting that South Africa's white shark population has not decreased, but simply redistributed eastwards to flee predation from orcas.",[],"Tue, 05 Mar 2024 13:42:33 EST"
73,"Smoking during pregnancy may increase the risk of behavioral disorders in newborns, predicts AI",https://www.sciencedaily.com/releases/2024/03/240305134228.htm,"Although several studies have linked smoking during pregnancy with neurodevelopmental disorders, the results of behavioral experiments in mice prenatally exposed to nicotine have been inconsistent. In a recent study, scientists from Japan developed a deep learning-based framework to automatically observe and classify mice behavior in such experiments, producing more accurate and unbiased results. They show that prenatal exposure to nicotine could increase the risk of autism spectrum- and attention deficit/hyperactivity disorders in newborns.",[],"Tue, 05 Mar 2024 13:42:28 EST"
74,What makes black holes grow and new stars form? Machine learning helps solve the mystery,https://www.sciencedaily.com/releases/2024/03/240305134225.htm,It takes more than a galaxy merger to make a black hole grow and new stars form: machine learning shows cold gas is needed too to initiate rapid growth -- new research finds.,[],"Tue, 05 Mar 2024 13:42:25 EST"
75,"Sprinting 'like a jet' will produce Premier League strikers of tomorrow, study shows",https://www.sciencedaily.com/releases/2024/03/240305134220.htm,"Sprinting 'like a jet plane taking off' will help produce Premier League star strikers of tomorrow, new research has revealed. A new study of Tottenham Hotspur's academy has shown that just a few words can instantly boost sprinting speed by 3 per cent over 20 meters. It would normally take weeks of targeted training to achieve such a large increase.",[],"Tue, 05 Mar 2024 13:42:20 EST"
76,"Juno spacecraft measures oxygen production on Jupiter's moon, Europa",https://www.sciencedaily.com/releases/2024/03/240305134217.htm,"NASA's Juno spacecraft has directly measured charged oxygen and hydrogen molecules from the atmosphere of one of Jupiter's largest moons, Europa. These observations provide key constraints on the potential oxygenation of its subsurface ocean.",[],"Tue, 05 Mar 2024 13:42:17 EST"
77,"After decades of Arctic sea ice getting faster and more hazardous for transport, models suggest a dramatic reversal is coming",https://www.sciencedaily.com/releases/2024/03/240305134215.htm,"Will ice floating in the Arctic Ocean move faster or slower over the coming decades? The answer to this question will tell us whether marine transportation can be expected to get more or less hazardous. It might also have important implications for the rate of ice cover loss, which is hugely consequential for Northern Indigenous communities, ecosystems, and the global climate system. While observational data suggest the trend has been towards faster sea ice speeds, climate models project that those speeds will slow down during the summer season. This contrast has led to some questions around the plausibility of the model projections.",[],"Tue, 05 Mar 2024 13:42:15 EST"
78,New cardiovascular imaging approach provides a better view of dangerous plaques,https://www.sciencedaily.com/releases/2024/03/240305134213.htm,"Researchers have developed a new catheter-based device that combines two powerful optical techniques to image the dangerous plaques that can build up inside the arteries that supply blood to the heart. By providing new details about plaque, the device could help clinicians and researchers improve treatments for preventing heart attacks and strokes.",[],"Tue, 05 Mar 2024 13:42:13 EST"
79,Robotic-assisted surgery for gallbladder cancer as effective as traditional surgery,https://www.sciencedaily.com/releases/2024/03/240305134210.htm,"Each year, approximately 2,000 people die annually of gallbladder cancer (GBC) in the U.S., with only one in five cases diagnosed at an early stage. With GBC rated as the first biliary tract cancer and the 17th most deadly cancer worldwide, pressing attention for proper management of disease must be addressed. For patients diagnosed, surgery is the most promising curative treatment. While there has been increasing adoption of minimally invasive surgical techniques in gastrointestinal malignancies, including utilization of laparoscopic and robotic surgery, there are reservations in utilizing minimally invasive surgery for gallbladder cancer. A new study has found that robotic-assisted surgery for GBC is as effective as traditional open and laparoscopic methods, with added benefits in precision and quicker post-operative recovery.",[],"Tue, 05 Mar 2024 13:42:10 EST"
80,"Responsible computing and accelerating scientific discovery across HPC, AI, and Quantum",https://cloudblogs.microsoft.com/quantum/2024/03/05/responsible-computing-and-accelerating-scientific-discovery-across-hpc-ai-and-quantum/,"<p>The technological landscape can evolve quickly, and early adoption of governance and risk mitigation measures sets a critical foundation for future innovation.</p>
<p>The post <a href=\"https://cloudblogs.microsoft.com/quantum/2024/03/05/responsible-computing-and-accelerating-scientific-discovery-across-hpc-ai-and-quantum/\">Responsible computing and accelerating scientific discovery across HPC, AI, and Quantum</a> appeared first on <a href=\"https://cloudblogs.microsoft.com/quantum\">Microsoft Azure Quantum Blog</a>.</p>",[{'name': 'Matthias Troyer'}],"Tue, 05 Mar 2024 20:00:00 +0000"
81,Microsoft and 1910 Genetics partner to turbocharge R&D productivity for the pharmaceutical industry,https://cloudblogs.microsoft.com/quantum/2024/02/29/microsoft-and-1910-genetics-partner-to-turbocharge-rd-productivity-for-the-pharmaceutical-industry/,"<p>Unprecedented collaboration will build the most powerful, fully integrated, AI-driven drug discovery and development platform to dramatically improve pharmaceutical research and development (R&#38;D) productivity and bring novel therapeutics to patients faster and more cost-effectively than traditional approaches.</p>
<p>The post <a href=\"https://cloudblogs.microsoft.com/quantum/2024/02/29/microsoft-and-1910-genetics-partner-to-turbocharge-rd-productivity-for-the-pharmaceutical-industry/\">Microsoft and 1910 Genetics partner to turbocharge R&amp;D productivity for the pharmaceutical industry</a> appeared first on <a href=\"https://cloudblogs.microsoft.com/quantum\">Microsoft Azure Quantum Blog</a>.</p>",[{'name': 'Zulfi Alam'}],"Thu, 29 Feb 2024 14:00:00 +0000"
82,DARPA selects Microsoft to continue the development of a utility-scale quantum computer,https://cloudblogs.microsoft.com/quantum/2024/02/08/darpa-selects-microsoft-to-continue-the-development-of-a-utility-scale-quantum-computer/,"<p>United States Government renews funding to Microsoft Azure Quantum based on recent results and detailed plans for building a quantum computer.</p>
<p>The post <a href=\"https://cloudblogs.microsoft.com/quantum/2024/02/08/darpa-selects-microsoft-to-continue-the-development-of-a-utility-scale-quantum-computer/\">DARPA selects Microsoft to continue the development of a utility-scale quantum computer </a> appeared first on <a href=\"https://cloudblogs.microsoft.com/quantum\">Microsoft Azure Quantum Blog</a>.</p>",[{'name': 'David Bohn'}],"Thu, 08 Feb 2024 16:00:00 +0000"
83,Unlocking a new era for scientific discovery with AI: How Microsoft’s AI screened over 32 million candidates to find a better battery,https://cloudblogs.microsoft.com/quantum/2024/01/09/unlocking-a-new-era-for-scientific-discovery-with-ai-how-microsofts-ai-screened-over-32-million-candidates-to-find-a-better-battery/,"<p>Announcing the Microsoft Quantum team achieved a major milestone toward that vision, using advanced AI to screen over 32 million candidates to discover and synthesize a new material that holds the potential for better batteriesthe first real-life example of many that will be achieved in a new era of scientific discovery driven by AI.</p>
<p>The post <a href=\"https://cloudblogs.microsoft.com/quantum/2024/01/09/unlocking-a-new-era-for-scientific-discovery-with-ai-how-microsofts-ai-screened-over-32-million-candidates-to-find-a-better-battery/\">Unlocking a new era for scientific discovery with AI: How Microsoft’s AI screened over 32 million candidates to find a better battery</a> appeared first on <a href=\"https://cloudblogs.microsoft.com/quantum\">Microsoft Azure Quantum Blog</a>.</p>",[{'name': 'Dr. Nathan Baker'}],"Tue, 09 Jan 2024 16:00:00 +0000"
84,Increasing research and development productivity with Copilot in Azure Quantum Elements,https://cloudblogs.microsoft.com/quantum/2023/12/12/increasing-research-and-development-productivity-with-copilot-in-azure-quantum-elements/,"<p>At Microsoft, we see opportunities to accelerate research and development productivity and usher in a new era of scientific discovery by harnessing the power of today's new generation of AI. </p>
<p>The post <a href=\"https://cloudblogs.microsoft.com/quantum/2023/12/12/increasing-research-and-development-productivity-with-copilot-in-azure-quantum-elements/\">Increasing research and development productivity with Copilot in Azure Quantum Elements</a> appeared first on <a href=\"https://cloudblogs.microsoft.com/quantum\">Microsoft Azure Quantum Blog</a>.</p>",[{'name': 'Dr. Chi Chen and Yousif Almulla'}],"Tue, 12 Dec 2023 16:00:00 +0000"
85,Defining logical qubits: criteria for Resilient Quantum Computation,https://aka.ms/AQ/LogicalQubit/Blog,"<p>As an industry, we are all collectively committed to bringing scaled quantum computing to fruition. That's why in June 2023, we offered how quantum computing must graduate through three implementation levels to achieve utility scale: Level 1 Foundational, Level 2 Resilient, Level 3 Scale.</p>
<p>The post <a href=\"https://aka.ms/AQ/LogicalQubit/Blog\">Defining logical qubits: criteria for Resilient Quantum Computation</a> appeared first on <a href=\"https://cloudblogs.microsoft.com/quantum\">Microsoft Azure Quantum Blog</a>.</p>",[{'name': 'Krysta Svore'}],"Tue, 28 Nov 2023 17:00:00 +0000"
86,Microsoft and Photonic join forces on the path to quantum at scale,https://cloudblogs.microsoft.com/quantum/2023/11/08/microsoft-and-photonic-join-forces-on-the-path-to-quantum-at-scale/,"<p>We are excited to announce a strategic co-innovation collaboration with Photonic Inc., a company focused on building scalable, fault tolerant, and distributed quantum technologies.</p>
<p>The post <a href=\"https://cloudblogs.microsoft.com/quantum/2023/11/08/microsoft-and-photonic-join-forces-on-the-path-to-quantum-at-scale/\">Microsoft and Photonic join forces on the path to quantum at scale</a> appeared first on <a href=\"https://cloudblogs.microsoft.com/quantum\">Microsoft Azure Quantum Blog</a>.</p>",[{'name': 'Dennis Tom'}],"Wed, 08 Nov 2023 14:00:00 +0000"
87,Quantum networking: A roadmap to a quantum internet,https://cloudblogs.microsoft.com/quantum/2023/11/01/quantum-networking-a-roadmap-to-a-quantum-internet/,"<p>Quantum computing has the potential to tackle some of our most pressing global issues, from climate change to food security. We're dedicated to building a full-scale, fault-tolerant quantum computer that can help solve these challenges.</p>
<p>The post <a href=\"https://cloudblogs.microsoft.com/quantum/2023/11/01/quantum-networking-a-roadmap-to-a-quantum-internet/\">Quantum networking: A roadmap to a quantum internet</a> appeared first on <a href=\"https://cloudblogs.microsoft.com/quantum\">Microsoft Azure Quantum Blog</a>.</p>",[{'name': 'Brad Lackey'}],"Wed, 01 Nov 2023 16:00:00 +0000"
88,Azure Quantum learning resources enable getting ready for a quantum supercomputer,https://cloudblogs.microsoft.com/quantum/2023/09/18/azure-quantum-learning-resources-enable-getting-ready-for-a-quantum-supercomputer/,"<p>As an industry, we are all collectively committed to bringing scaled quantum computing to fruition. However, it can get hard to separate the signal from the noise. To help, we have offered quantum computing implementation levels to the industry to help frame the roadmap towards achieving this shared goal.  </p>
<p>The post <a href=\"https://cloudblogs.microsoft.com/quantum/2023/09/18/azure-quantum-learning-resources-enable-getting-ready-for-a-quantum-supercomputer/\">Azure Quantum learning resources enable getting ready for a quantum supercomputer </a> appeared first on <a href=\"https://cloudblogs.microsoft.com/quantum\">Microsoft Azure Quantum Blog</a>.</p>",[{'name': 'Krysta Svore'}],"Mon, 18 Sep 2023 17:00:00 +0000"
89,Announcing season 2 of the Microsoft Quantum Innovator Series,https://cloudblogs.microsoft.com/quantum/2023/08/23/announcing-season-2-of-the-microsoft-quantum-innovator-series/,"<p>In this series, you will hear directly from the Microsoft Azure Quantum scientists and leaders about the path to quantum at scale and how you can get involved today.</p>
<p>The post <a href=\"https://cloudblogs.microsoft.com/quantum/2023/08/23/announcing-season-2-of-the-microsoft-quantum-innovator-series/\">Announcing season 2 of the Microsoft Quantum Innovator Series</a> appeared first on <a href=\"https://cloudblogs.microsoft.com/quantum\">Microsoft Azure Quantum Blog</a>.</p>",[{'name': 'Microsoft Azure Quantum Team'}],"Wed, 23 Aug 2023 16:00:00 +0000"
90,"Electrons become fractions of themselves in graphene, study finds",https://news.mit.edu/2024/electrons-become-fractions-graphene-study-finds-0221,An exotic electronic state observed by MIT physicists could enable more robust forms of quantum computing.,[{'name': 'Jennifer Chu | MIT News'}],"Wed, 21 Feb 2024 11:00:00 -0500"
91,Technique could improve the sensitivity of quantum sensing devices,https://news.mit.edu/2024/technique-could-improve-sensitivity-quantum-sensing-devices-0208,"The method lets researchers identify and control larger numbers of atomic-scale defects, to build a bigger system of qubits.",[{'name': 'Adam Zewe | MIT News'}],"Thu, 08 Feb 2024 00:00:00 -0500"
92,New MIT.nano equipment to accelerate innovation in “tough tech” sectors,https://news.mit.edu/2024/new-mit-nano-equipment-accelerate-innovation-0130,The advanced fabrication tools will enable the next generation of microelectronics and microsystems while bridging the gap from the lab to commercialization.,[{'name': 'Zach Winn | MIT News'}],"Tue, 30 Jan 2024 13:00:00 -0500"
93,"With a quantum “squeeze,” clocks could keep even more precise time, MIT researchers propose",https://news.mit.edu/2023/quantum-squeeze-clocks-more-precise-time-1130,"More stable clocks could measure quantum phenomena, including the presence of dark matter.",[{'name': 'Jennifer Chu | MIT News'}],"Thu, 30 Nov 2023 00:00:00 -0500"
94,Celebrating five years of MIT.nano,https://news.mit.edu/2023/celebrating-five-years-mit-nano-summit-1127,The Nano Summit highlights nanoscale research across multiple disciplines at MIT.,[{'name': 'Amanda Stoll DiCristofaro | MIT.nano'}],"Mon, 27 Nov 2023 15:15:00 -0500"
95,Physicists trap electrons in a 3D crystal for the first time,https://news.mit.edu/2023/physicists-trap-electrons-3d-crystal-first-time-1108,The results open the door to exploring superconductivity and other exotic electronic states in three-dimensional materials.,[{'name': 'Jennifer Chu | MIT News'}],"Wed, 08 Nov 2023 11:00:00 -0500"
96,MIT receives major National Science Foundation grant for quantum science,https://news.mit.edu/2023/mit-receives-major-nsf-grant-quantum-science-1018,Center for Ultracold Atoms gets funding boost to “punch through tough scientific barriers and see what's on the other side.”,[{'name': 'Sandi Miller | Department of Physics'}],"Wed, 18 Oct 2023 15:10:00 -0400"
97,"From a five-layer graphene sandwich, a rare electronic state emerges",https://news.mit.edu/2023/five-layer-graphene-sandwich-rare-electronic-behavior-1018,A newly discovered type of electronic behavior could help with packing more data into magnetic memory devices.,[{'name': 'Jennifer Chu | MIT News'}],"Wed, 18 Oct 2023 11:00:00 -0400"
98,Quantum repeaters use defects in diamond to interconnect quantum systems,https://news.mit.edu/2023/quantum-repeaters-use-defects-diamond-interconnect-quantum-systems-0927,This technology for storing and transmitting quantum information over lossy links could provide the foundation for scalable quantum networking.,[{'name': 'Ariana Tantillo | MIT Lincoln Laboratory'}],"Wed, 27 Sep 2023 16:35:00 -0400"
99,New qubit circuit enables quantum operations with higher accuracy,https://news.mit.edu/2023/new-qubit-circuit-enables-quantum-operations-higher-accuracy-0925,The advance brings quantum error correction a step closer to reality.,[{'name': 'Adam Zewe | MIT News'}],"Mon, 25 Sep 2023 11:00:00 -0400"
100,Four Lincoln Laboratory technologies win five 2023 R&D 100 awards,https://news.mit.edu/2023/lincoln-laboratory-technologies-win-rd-world-awards-0919,"Inventions in medical imaging, aircrew scheduling, data security, and quantum networking are named among the year’s most innovative new products.",[{'name': 'Kylie Foy | MIT Lincoln Laboratory'}],"Tue, 19 Sep 2023 16:35:34 -0400"
101,Canceling noise to improve quantum devices,https://news.mit.edu/2023/canceling-noise-improve-quantum-devices-0906,MIT researchers develop a protocol to extend the life of quantum coherence.,[{'name': 'Peter Reuell | Department of Nuclear Science and Engineering'}],"Wed, 06 Sep 2023 17:15:00 -0400"
102,"Simple superconducting device could dramatically cut energy use in computing, other applications",https://news.mit.edu/2023/simple-superconducting-device-could-dramatically-cut-energy-use-computing-other-important-0815,The ultrasmall “switch” could be easily scaled.,[{'name': 'Elizabeth Thomson | Materials Research Laboratory'}],"Tue, 15 Aug 2023 16:00:00 -0400"
103,Wolfgang Ketterle receives Vannevar Bush Faculty Fellowship,https://news.mit.edu/2023/wolfgang-ketterle-vannevar-bush-faculty-fellowship-0807,Professor of physics will use US Department of Defense fellowship to study quantum science with ultracold atoms.,[{'name': 'Sandi Miller | Department of Physics'}],"Mon, 07 Aug 2023 13:35:00 -0400"
104,Sensing and controlling microscopic spin density in materials,https://news.mit.edu/2023/sensing-microscopic-spin-density-materials-0802,"By fine-tuning the spin density in some materials, researchers may be able to develop new quantum sensors or quantum simulations.",[{'name': 'David L. Chandler | MIT News'}],"Wed, 02 Aug 2023 09:00:00 -0400"
105,Researchers grow precise arrays of nanoLEDs,https://news.mit.edu/2023/researchers-grow-precise-arrays-nanoleds-0706,"A new technique produces perovskite nanocrystals right where they’re needed, so the exceedingly delicate materials can be integrated into nanoscale devices.",[{'name': 'Adam Zewe | MIT News Office'}],"Thu, 06 Jul 2023 00:00:00 -0400"
106,Superconducting qubit foundry accelerates progress in quantum research,https://news.mit.edu/2023/superconducting-qubit-foundry-accelerates-progress-quantum-research-0705,The foundry gives the wider research community access to Lincoln Laboratory’s expertise in fabricating quantum circuits.,[{'name': 'Kylie Foy | Haley Wahl | MIT Lincoln Laboratory'}],"Wed, 05 Jul 2023 16:45:00 -0400"
107,A new mathematical “blueprint” is accelerating fusion device development,https://news.mit.edu/2023/mathematical-blueprint-accelerating-fusion-device-development-0622,New research explores how Dyson maps are putting quantum computers to work in designing fusion energy devices.,[{'name': 'Plasma Science and Fusion Center'}],"Thu, 22 Jun 2023 09:50:00 -0400"
108,Researchers develop a new source of quantum light,https://news.mit.edu/2023/researchers-develop-new-source-quantum-light-0622,The device emits a stream of single photons and could provide a basis for optical quantum computers.,[{'name': 'David L. Chandler | MIT News Office'}],"Thu, 22 Jun 2023 00:00:00 -0400"
109,Three Spanish MIT physics postdocs receive Botton Foundation fellowships,https://news.mit.edu/2023/botton-foundation-fellowships-spanish-mit-physics-postdocs-0609,"Recipients Luis Antonio Benítez, Carolina Cuesta-Lazaro, and Fernando Romero López receive support for their scientific research.",[{'name': 'Sandi Miller | Department of Physics'}],"Fri, 09 Jun 2023 17:00:00 -0400"
110,Life in a hologram,https://news.mit.edu/2023/daniel-harlow-life-hologram-0607,Physicist Daniel Harlow explores an alternate quantum reality in search of fundamental truths to our physical universe.,[{'name': 'Jennifer Chu | MIT News Office'}],"Wed, 07 Jun 2023 00:00:00 -0400"
111,Fueled by problem-solving,https://news.mit.edu/2023/fueled-by-problem-solving-thomas-bergamaschi-0530,Undergraduate research helped feed physics and EECS major Thomas Bergamaschi’s post-MIT interest in tackling challenges.,[{'name': 'Sandi Miller | Department of Physics'}],"Tue, 30 May 2023 14:20:00 -0400"
112,Success at the intersection of technology and finance,https://news.mit.edu/2023/success-intersection-technology-and-finance-0510,"Citadel founder and CEO Ken Griffin visits MIT, discusses how technology will continue to transform trading and investing.",[{'name': 'MIT Career Advising and Professional Development'}],"Wed, 10 May 2023 14:00:00 -0400"
113,MIT physicists predict exotic new phenomena and give “recipe” for realizing them,https://news.mit.edu/2023/mit-physicists-predict-exotic-new-phenomena-0317,Work with skyrmions could have applications in future computers and more.,[{'name': 'Elizabeth A. Thomson | Materials Research Laboratory'}],"Fri, 17 Mar 2023 16:10:00 -0400"
114,"It’s a weird, weird quantum world",https://news.mit.edu/2023/weird-weird-quantum-world-peter-shor-killian-lecture-0310,"In MIT’s 2023 Killian Lecture, Peter Shor shares a brief history of quantum computing from a personal viewpoint.",[{'name': 'Jennifer Chu | MIT News Office'}],"Fri, 10 Mar 2023 16:25:00 -0500"
115,QuARC 2023 explores the leading edge in quantum information and science,https://news.mit.edu/2023/quarc-2023-explores-leading-edge-quantum-information-and-science-0303,The second annual student-industry conference was held in-person for the first time.,[{'name': 'Center for Quantum Engineering'}],"Fri, 03 Mar 2023 12:30:00 -0500"
116,A new way for quantum computing systems to keep their cool,https://news.mit.edu/2023/new-way-quantum-computing-systems-keep-their-cool-0221,A wireless technique enables a super-cold quantum computer to send and receive data without generating too much error-causing heat.,[{'name': 'Adam Zewe | MIT News Office'}],"Tue, 21 Feb 2023 00:00:00 -0500"
117,Scientists boost quantum signals while reducing noise,https://news.mit.edu/2023/boost-quantum-signals-squeezing-noise-0209,“Squeezing” noise over a broad frequency bandwidth in a quantum system could lead to faster and more accurate quantum measurements.,[{'name': 'Adam Zewe | MIT News Office'}],"Thu, 09 Feb 2023 11:00:00 -0500"
118,Physicists observe rare resonance in molecules for the first time,https://news.mit.edu/2023/rare-resonance-ultracold-molecules-0201,The findings could provide a new way to control chemical reactions.,[{'name': 'Jennifer Chu | MIT News Office'}],"Wed, 01 Feb 2023 11:00:00 -0500"
119,Can you trust your quantum simulator?,https://news.mit.edu/2023/quantum-simulator-randomness-0118,A new technique helps verify the accuracy of experiments that probe the strange behavior of atomic-scale systems.,[{'name': 'Jennifer Chu | MIT News Office'}],"Wed, 18 Jan 2023 11:00:00 -0500"
120,New quantum computing architecture could be used to connect large-scale devices,https://news.mit.edu/2023/quantum-interconnects-photon-emission-0105,"Researchers have demonstrated directional photon emission, the first step toward extensible quantum interconnects.",[{'name': 'Adam Zewe | MIT News Office'}],"Thu, 05 Jan 2023 11:00:00 -0500"
121,MIT researchers use quantum computing to observe entanglement,https://news.mit.edu/2022/mit-researchers-use-quantum-computing-observe-entanglement-1201,Researchers at the Center for Theoretical Physics lead work on testing quantum gravity on a quantum processor.,[{'name': 'Julia C. Keller | School of Science'}],"Thu, 01 Dec 2022 07:45:00 -0500"
122,The task of magnetic classification suddenly looks easier,https://news.mit.edu/2022/task-magnetic-classification-suddenly-looks-easier-1128,"MIT undergraduate researchers Helena Merker, Harry Heiberger, and Linh Nguyen, and PhD student Tongtong Liu, exploit machine-learning techniques to determine the magnetic structure of materials.",[{'name': 'Steve Nadis | Department of Nuclear Science and Engineering'}],"Mon, 28 Nov 2022 15:25:00 -0500"
123,Liang Fu and Patrick Lee receive Larkin Awards in Theoretical Physics,https://news.mit.edu/2022/liang-fu-patrick-lee-receive-larkin-awards-theoretical-physics-1031,Inaugural award goes to MIT condensed matter theory professors of physics.,[{'name': 'Sandi Miller | Department of Physics'}],"Mon, 31 Oct 2022 16:45:00 -0400"
124,A faster experiment to find and study topological materials,https://news.mit.edu/2022/faster-topological-materials-quantum-1026,"Using machine learning and simple X-ray spectra, researchers can uncover compounds that might enable next-generation computer chips or quantum devices.",[{'name': 'David L. Chandler | MIT News Office'}],"Wed, 26 Oct 2022 09:00:00 -0400"
125,Making quantum computers more accurate,https://news.mit.edu/2022/alex-greene-quantum-computers-1013,PhD student Alex Greene studies superconducting quantum computing systems while rounding out their busy schedule with water sanitation projects.,[{'name': 'Rachel Yang | MIT News correspondent'}],"Thu, 13 Oct 2022 00:00:00 -0400"
126,Professor Danna Freedman receives 2022 MacArthur Fellowship,https://news.mit.edu/2022/danna-freedman-macarthur-fellowship-1012,"MIT chemist designs novel molecules that could be used for quantum sensing and communication; visiting scholar Moriba Jah is also awarded, for work on space sustainability.",[{'name': 'Anne Trafton | MIT News Office'}],"Wed, 12 Oct 2022 12:00:00 -0400"
127,Peter Shor wins Breakthrough Prize in Fundamental Physics,https://news.mit.edu/2022/shor-spielman-breakthrough-prize-0922,MIT professor to share $3 million prize with three others; Daniel Spielman PhD ’95 wins Breakthrough Prize in Mathematics.,[{'name': 'Jennifer Chu | MIT News Office'}],"Thu, 22 Sep 2022 09:00:00 -0400"
128,Between two universes,https://news.mit.edu/2022/between-two-universes-mohammad-javad-khojasteh-0920,"Mohammad Javad Khojasteh, a postdoc at MIT LIDS, uses both classical and quantum physics to improve state-of-the-art capabilities in communication, sensing, and computation.",[{'name': 'Raleigh McElvery | MIT Laboratory for Information and Decision Systems'}],"Tue, 20 Sep 2022 17:10:00 -0400"
129,"For Danna Freedman, an impasse is an invitation",https://news.mit.edu/2022/danna-freedman-chemistry-0731,The chemistry professor embraces the most challenging moments of her work to design molecules for quantum information science.,[{'name': 'Michaela Jarvis | MIT News correspondent'}],"Sun, 31 Jul 2022 00:00:00 -0400"
130,"Physicists discover a “family” of robust, superconducting graphene structures",https://news.mit.edu/2022/superconducting-graphene-family-0708,The findings could inform the design of practical superconducting devices.,[{'name': 'Jennifer Chu | MIT News Office'}],"Fri, 08 Jul 2022 00:00:00 -0400"
131,Quantum sensor can detect electromagnetic signals of any frequency,https://news.mit.edu/2022/quantum-sensor-frequency-0621,"MIT engineers expand the capabilities of these ultrasensitive nanoscale detectors, with potential uses for quantum computing and biological sensing.",[{'name': 'David L. Chandler | MIT News Office'}],"Tue, 21 Jun 2022 00:00:00 -0400"
132,Mining valuable insights from diamonds,https://news.mit.edu/2022/mining-valuable-insights-diamonds-changhao-li-0531,A drive to understand natural science phenomena ignited MIT graduate student Changhao Li’s love of quantum physics.,[{'name': 'Poornima Apte | Department of Nuclear Science and Engineering'}],"Tue, 31 May 2022 00:00:00 -0400"
133,Peter Shor receives 2022-2023 Killian Award,https://news.mit.edu/2022/peter-shor-receives-2022-2023-killian-award-0511,"The MIT professor is honored for extraordinary accomplishments in mathematics, computer science, and quantum physics.",[{'name': 'Jennifer Chu | MIT News Office'}],"Wed, 11 May 2022 16:00:00 -0400"
134,Bringing together the next generation of quantum coders,https://news.mit.edu/2022/bringing-together-next-generation-quantum-coders-0405,MIT iQuHACK gave teams from across the country time and experience on real quantum computers.,[{'name': 'Rachel Yang | Department of Electrical Engineering and Computer Science'}],"Tue, 05 Apr 2022 15:10:00 -0400"
135,School of Engineering welcomes Thomas Tull as visiting innovation scholar,https://news.mit.edu/2022/school-engineering-welcomes-thomas-tull-visiting-innovation-scholar-0404,"Primary focus will be to advance and promote technology, innovation, and entrepreneurship across the school.",[{'name': 'Lori LoTurco | School of Engineering'}],"Mon, 04 Apr 2022 15:40:00 -0400"
136,Seeing an elusive magnetic effect through the lens of machine learning,https://news.mit.edu/2022/seeing-elusive-magnetic-effect-through-lens-machine-learning-0324,An MIT team incorporates AI to facilitate the detection of an intriguing materials phenomenon that can lead to electronics without energy dissipation.,[{'name': 'Steve Nadis | Department of Nuclear Science and Engineering'}],"Thu, 24 Mar 2022 13:40:00 -0400"
137,Making quantum circuits more robust,https://news.mit.edu/2022/quantum-circuits-robust-noise-0321,"Researchers have developed a technique for making quantum computing more resilient to noise, which boosts performance.",[{'name': 'Adam Zewe | MIT News Office'}],"Mon, 21 Mar 2022 00:00:00 -0400"
138,MIT.nano receives grant to acquire focused ion beam scanning electron microscope,https://news.mit.edu/2022/mitnano-receives-grant-acquire-focused-ion-beam-scanning-electron-microscope-0209,National Science Foundation award will allow the VELION FIB-SEM to become a permanent instrument in MIT.nano’s characterization facility.,[{'name': 'MIT.nano'}],"Wed, 09 Feb 2022 16:20:00 -0500"
139,Tiny materials lead to a big advance in quantum computing,https://news.mit.edu/2022/tiny-materials-quibits-quantum-computing-0128,Using ultrathin materials to reduce the size of superconducting qubits may pave the way for personal-sized quantum devices.,[{'name': 'Adam Zewe | MIT News Office'}],"Thu, 27 Jan 2022 11:00:00 -0500"
140,Method rapidly verifies that a robot will avoid collisions,https://news.mit.edu/2024/method-rapidly-verifies-robot-will-avoid-collisions-0307,"Faster and more accurate than some alternatives, this approach could be useful for robots that interact with humans or work in tight spaces.",[{'name': 'Adam Zewe | MIT News'}],"Thu, 07 Mar 2024 00:00:00 -0500"
141,Study determines the original orientations of rocks drilled on Mars,https://news.mit.edu/2024/study-determines-original-orientations-rocks-drilled-mars-0304,"The “oriented” samples, the first of their kind from any planet, could shed light on Mars’ ancient magnetic field.",[{'name': 'Jennifer Chu | MIT News'}],"Mon, 04 Mar 2024 00:00:00 -0500"
142,“We offer another place for knowledge”,https://news.mit.edu/2024/we-offer-another-place-knowledge-0226,"After acquiring data science and AI skills from MIT, Jospin Hassan shared them with his community in the Dzaleka Refugee Camp in Malawi and built pathways for talented learners.",[{'name': 'Katherine Ouellette | MIT Open Learning'}],"Mon, 26 Feb 2024 14:35:00 -0500"
143,Smart glove teaches new physical skills,https://news.mit.edu/2024/smart-glove-teaches-new-physical-skills-0220,"Adaptive smart glove from MIT CSAIL researchers can send tactile feedback to teach users new skills, guide robots with more precise manipulation, and help train surgeons and pilots.",[{'name': 'Alex Shipps | MIT CSAIL'}],"Tue, 20 Feb 2024 11:50:00 -0500"
144,Six MIT students selected as spring 2024 MIT-Pillar AI Collective Fellows,https://news.mit.edu/2024/mit-pillar-ai-collective-fellows-0206,"The graduate students will aim to commercialize innovations in AI, machine learning, and data science.",[{'name': 'School of Engineering'}],"Tue, 06 Feb 2024 16:50:00 -0500"
145,"Professor Emeritus Igor Paul, an expert in product design and safety, dies at 87",https://news.mit.edu/2024/professor-emeritus-igor-paul-dies-0131,"Longtime professor helped develop the Department of Mechanical Engineering’s design and manufacturing curriculum, contributed to artificial joints as well as NASA inertial guidance systems.",[{'name': 'Anne Wilson | Department of Mechanical Engineering'}],"Wed, 31 Jan 2024 16:35:00 -0500"
146,Baran Mensah: Savoring college life in a new country,https://news.mit.edu/2024/baran-mensah-savoring-college-life-new-country-0119,"From robotics to dance, the MIT senior has made it his mission to explore as many new experiences as possible at the Institute.",[{'name': 'Angelina Parrillo | MIT News correspondent'}],"Fri, 19 Jan 2024 00:00:00 -0500"
147,Reasoning and reliability in AI,https://news.mit.edu/2024/reasoning-and-reliability-in-ai-0118,PhD students interning with the MIT-IBM Watson AI Lab look to improve natural language usage.,[{'name': 'Lauren Hinkel | MIT-IBM Watson AI Lab'}],"Thu, 18 Jan 2024 13:00:00 -0500"
148,"Richard Wiesman, professor of the practice in mechanical engineering, dies at age 69",https://news.mit.edu/2024/meche-professor-practice-richard-wiesman-dies-0110,"A highly respected educator and mentor with a distinguished industry career, Wiesman inspired generations of mechanical engineering students.",[{'name': 'Department of Mechanical Engineering'}],"Wed, 10 Jan 2024 14:20:00 -0500"
149,Multiple AI models help robots execute complex plans more transparently,https://news.mit.edu/2024/multiple-ai-models-help-robots-execute-complex-plans-more-transparently-0108,"A multimodal system uses models trained on language, vision, and action data to help robots develop and execute plans for household, construction, and manufacturing tasks.",[{'name': 'Alex Shipps | MIT CSAIL'}],"Mon, 08 Jan 2024 15:15:00 -0500"
150,MIT Generative AI Week fosters dialogue across disciplines,https://news.mit.edu/2023/mit-generative-ai-week-fosters-dialogue-across-disciplines-1211,"During the last week of November, MIT hosted symposia and events aimed at examining the implications and possibilities of generative AI.",[{'name': 'Mary Beth Gallagher | School of Engineering'}],"Mon, 11 Dec 2023 16:25:00 -0500"
151,MIT engineers design a robotic replica of the heart’s right chamber,https://news.mit.edu/2023/mit-engineers-design-robotic-replica-hearts-right-chamber-1208,The realistic model could aid the development of better heart implants and shed light on understudied heart disorders.,[{'name': 'Jennifer Chu | MIT News'}],"Fri, 08 Dec 2023 05:00:00 -0500"
152,A new optimization framework for robot motion planning,https://news.mit.edu/2023/new-optimization-framework-robot-motion-planning-1130,"MIT CSAIL researchers established new connections between combinatorial and continuous optimization, which can find global solutions for complex motion-planning puzzles.",[{'name': 'Alex Shipps | MIT CSAIL'}],"Thu, 30 Nov 2023 09:00:00 -0500"
153,Using language to give robots a better grasp of an open-ended world,https://news.mit.edu/2023/using-language-give-robots-better-grasp-open-ended-world-1102,"By blending 2D images with foundation models to build 3D feature fields, a new MIT method helps robots understand and manipulate nearby objects with open-ended language prompts.",[{'name': 'Alex Shipps | MIT CSAIL'}],"Thu, 02 Nov 2023 16:25:00 -0400"
154,New technique helps robots pack objects into a tight space,https://news.mit.edu/2023/new-technique-helps-robots-pack-objects-tight-space-1017,Researchers coaxed a family of generative AI models to work together to solve multistep robot manipulation problems.,[{'name': 'Adam Zewe | MIT News'}],"Tue, 17 Oct 2023 00:00:00 -0400"
155,A method to interpret AI might not be so interpretable after all,https://news.mit.edu/2023/method-interpret-ai-might-not-be-so-interpretable-after-all-1016,Some researchers see formal specifications as a way for autonomous systems to \"explain themselves\" to humans. But a new study finds that we aren't understanding.,[{'name': 'Kylie Foy | MIT Lincoln Laboratory'}],"Mon, 16 Oct 2023 16:25:00 -0400"
156,Finger-shaped sensor enables more dexterous robots,https://news.mit.edu/2023/finger-shaped-sensor-enables-more-dexterous-robots-1004,"MIT engineers develop a long, curved touch sensor that could enable a robot to grasp and manipulate objects in multiple ways.",[{'name': 'Adam Zewe | MIT News'}],"Wed, 04 Oct 2023 00:00:00 -0400"
157,AI copilot enhances human precision for safer aviation,https://news.mit.edu/2023/ai-co-pilot-enhances-human-precision-safer-aviation-1003,"Designed to ensure safer skies, “Air-Guardian” blends human intuition with machine precision, creating a more symbiotic relationship between pilot and aircraft.",[{'name': 'Rachel Gordon | MIT CSAIL'}],"Tue, 03 Oct 2023 14:55:00 -0400"
158,2023-2024 Accenture Fellows advance technology at the crossroads of business and society,https://news.mit.edu/2023/accenture-fellows-advance-technology-crossroads-business-society-0919,The MIT and Accenture Convergence Initiative for Industry and Technology announces new graduate fellows.,[{'name': 'School of Engineering'}],"Tue, 19 Sep 2023 16:45:00 -0400"
159,Mechanical engineering with a twist: Pursuing a passion for robotics with customized major,https://news.mit.edu/2023/mechanical-engineering-twist-pursuing-passion-robotics-customized-major-0918,"Sharmi Shah ’23 pursued Course 2-A/6, a customizable degree path that combines mechanical engineering with computer science and electrical engineering.",[{'name': 'Department of Mechanical Engineering'}],"Mon, 18 Sep 2023 14:45:00 -0400"
160,A. Michael West: Advancing human-robot interactions in health care,https://news.mit.edu/2023/michael-west-advancing-human-robot-interactions-0913,"When he isn’t investigating human motor control, the graduate student gives back by volunteering with programs that helped him grow as a researcher.",[{'name': 'Michaela Jarvis | School of Engineering'}],"Wed, 13 Sep 2023 00:00:00 -0400"
161,Making life friendlier with personal robots,https://news.mit.edu/2023/making-life-friendlier-personal-robots-sharifa-alghowinem-0910,"Sharifa Alghowinem, a research scientist at the Media Lab, explores personal robot technology that explains emotions in English and Arabic.",[{'name': 'Dorothy Hanna | Department of Mechanical Engineering'}],"Sun, 10 Sep 2023 00:00:00 -0400"
162,AI helps robots manipulate objects with their whole bodies,https://news.mit.edu/2023/ai-technique-robots-manipulate-objects-whole-bodies-0824,"With a new technique, a robot can reason efficiently about moving objects using more than just its fingertips.",[{'name': 'Adam Zewe | MIT News'}],"Thu, 24 Aug 2023 00:00:00 -0400"
163,"SMART launches research group to advance AI, automation, and the future of work",https://news.mit.edu/2023/smart-launches-m3s-research-group-advance-ai-automation-future-work-0823,"Mens, Manus and Machina (M3S) will design technology, training programs, and institutions for successful human-machine collaboration.",[{'name': 'Singapore-MIT Alliance for Research and Technology'}],"Wed, 23 Aug 2023 14:00:00 -0400"
164,"MIT engineers use kirigami to make ultrastrong, lightweight structures",https://news.mit.edu/2023/using-kirigami-ultrastrong-lightweight-structures-0822,"Produced with techniques borrowed from Japanese paper-cutting, the strong metal lattices are lighter than cork and have customizable mechanical properties.",[{'name': 'Adam Zewe | MIT News'}],"Tue, 22 Aug 2023 00:00:00 -0400"
165,A simpler method for learning to control a robot,https://news.mit.edu/2023/simpler-method-learning-control-robot-0726,"Researchers develop a machine-learning technique that can efficiently learn to control a robot, leading to better performance with fewer data.",[{'name': 'Adam Zewe | MIT News Office'}],"Wed, 26 Jul 2023 00:00:00 -0400"
166,New quantum magnet unleashes electronics potential,https://news.mit.edu/2023/new-quantum-magnet-unleashes-electronics-potential-0725,"Researchers discover how to control the anomalous Hall effect and Berry curvature to create flexible quantum magnets for use in computers, robotics, and sensors.",[{'name': 'Julianna Mullen | Plasma Science and Fusion Center'}],"Tue, 25 Jul 2023 17:20:00 -0400"
167,A faster way to teach a robot,https://news.mit.edu/2023/faster-way-teach-robot-technique-0718,"A new technique helps a nontechnical user understand why a robot failed, and then fine-tune it with minimal effort to perform a task effectively.",[{'name': 'Adam Zewe | MIT News Office'}],"Tue, 18 Jul 2023 00:00:00 -0400"
168,AI helps household robots cut planning time in half,https://news.mit.edu/2023/ai-helps-household-robots-cut-planning-time-half-0714,"PIGINet leverages machine learning to streamline and enhance household robots' task and motion planning, by assessing and filtering feasible solutions in complex environments.",[{'name': 'Rachel Gordon | MIT CSAIL'}],"Fri, 14 Jul 2023 11:00:00 -0400"
169,3 Questions: Honing robot perception and mapping,https://news.mit.edu/2023/honing-robot-perception-mapping-0710,Luca Carlone and Jonathan How of MIT LIDS discuss how future robots might perceive and interact with their environment.,[{'name': 'Madeleine Turner | MIT Laboratory for Information and Decision Systems'}],"Mon, 10 Jul 2023 14:30:00 -0400"
170,"Magnetic robots walk, crawl, and swim",https://news.mit.edu/2023/magnetic-robots-walk-crawl-swim-0707,New soft-bodied robots that can be controlled by a simple magnetic field are well suited to work in confined spaces.,[{'name': 'Jennifer Michalowski | McGovern Institute for Brain Research'}],"Fri, 07 Jul 2023 14:20:00 -0400"
171,The chore of packing just got faster and easier,https://news.mit.edu/2023/chore-packing-just-got-faster-and-easier-0706,A new computational method facilitates the dense placement of objects inside a rigid container.<br />,[{'name': 'Steve Nadis | MIT CSAIL'}],"Thu, 06 Jul 2023 00:00:00 -0400"
172,Defining the public interest in new technologies,https://news.mit.edu/2023/defining-public-interest-new-technologies-0613,"New online journal seeks to bring together the MIT community to discuss the social responsibilities of individuals who design, implement, and evaluate technologies.",[{'name': 'Department of Urban Studies and Planning'}],"Tue, 13 Jun 2023 16:45:00 -0400"
173,A step toward safe and reliable autopilots for flying,https://news.mit.edu/2023/safe-and-reliable-autopilots-flying-0612,A new AI-based approach for controlling autonomous robots satisfies the often-conflicting goals of safety and stability.,[{'name': 'Adam Zewe | MIT News Office'}],"Mon, 12 Jun 2023 00:00:00 -0400"
174,Bioinspired robotics class offers intriguing surprises,https://news.mit.edu/2023/bioinspired-robotics-class-offers-intriguing-surprises-0609,"Students learn about the complexity behind simple, everyday movement before experimenting with mechanical models.",[{'name': 'Michaela Jarvis | Department of Mechanical Engineering'}],"Fri, 09 Jun 2023 00:00:00 -0400"
175,Helping robots handle fluids,https://news.mit.edu/2023/helping-robots-handle-fluids-0524,Researchers create a new simulation tool for robots to manipulate complex fluids in a step toward helping them more effortlessly assist with daily tasks.,[{'name': 'Rachel Gordon | MIT CSAIL'}],"Wed, 24 May 2023 10:00:00 -0400"
176,2.007 Forever!,https://news.mit.edu/2023/2007-forever-annual-robot-competition-0518,"With winches, spinners, and telescoping contraptions, bots go head to head in student robot competition inspired by “Black Panther: Wakanda Forever.”",[{'name': 'Jennifer Chu | MIT News Office'}],"Thu, 18 May 2023 13:00:00 -0400"
177,Four researchers with MIT ties earn 2023 Schmidt Science Fellowships,https://news.mit.edu/2023/four-researchers-mit-ties-earn-schmidt-science-fellowships-0512,The illustrious prize supports early-career scientists and engineers as they pursue interdisciplinary work.,[{'name': 'Danielle Randall Doughty | Department of Chemistry'}],"Fri, 12 May 2023 14:00:00 -0400"
178,Open-source platform simulates wildlife for soft robotics designers,https://news.mit.edu/2023/softzoo-simulates-wildlife-soft-robotics-for-designers-0502,SoftZoo is a soft robot co-design platform that can test optimal shapes and sizes for robotic performance in different environments.,[{'name': 'Alex Shipps | MIT CSAIL'}],"Tue, 02 May 2023 12:25:00 -0400"
179,Speedy robo-gripper reflexively organizes cluttered spaces,https://news.mit.edu/2023/speedy-robo-gripper-reflexively-organizes-spaces-0427,"Rather than start from scratch after a failed attempt, the pick-and-place robot adapts in the moment to get a better hold.",[{'name': 'Jennifer Chu | MIT News Office'}],"Thu, 27 Apr 2023 00:00:00 -0400"
180,Miniscule device could help preserve the battery life of tiny sensors,https://news.mit.edu/2023/miniscule-device-could-help-preserve-battery-life-0424,Researchers demonstrate a low-power “wake-up” receiver one-tenth the size of other devices.,[{'name': 'Adam Zewe | MIT News Office'}],"Mon, 24 Apr 2023 00:00:00 -0400"
181,Drones navigate unseen environments with liquid neural networks,https://news.mit.edu/2023/drones-navigate-unseen-environments-liquid-neural-networks-0419,"MIT researchers exhibit a new advancement in autonomous drone navigation, using brain-inspired liquid neural networks that excel in out-of-distribution scenarios.",[{'name': 'Rachel Gordon | MIT CSAIL'}],"Wed, 19 Apr 2023 14:15:00 -0400"
182,Robotic hand can identify objects with just one grasp,https://news.mit.edu/2023/robotic-hand-can-identify-objects-just-one-grasp-0403,The three-fingered robotic gripper can “feel” with great sensitivity along the full length of each finger – not just at the tips.,[{'name': 'Adam Zewe | MIT News Office'}],"Mon, 03 Apr 2023 00:00:00 -0400"
183,A four-legged robotic system for playing soccer on various terrains,https://news.mit.edu/2023/legged-robotic-system-playing-soccer-various-terrains-0403,"“DribbleBot” can maneuver a soccer ball on landscapes such as sand, gravel, mud, and snow, using reinforcement learning to adapt to varying ball dynamics.",[{'name': 'Rachel Gordon | MIT CSAIL'}],"Mon, 03 Apr 2023 00:00:00 -0400"
184,A portfolio that’s out of this world,https://news.mit.edu/2023/portfolio-out-of-this-world-ezinne-uzo-okoro-0327,"Following an influential career at NASA, Ezinne Uzo-Okoro SM ’20, PhD ’22 now shapes space policy as a top White House advisor.",[{'name': 'Leda Zimmerman | Department of Aeronautics and Astronautics'}],"Mon, 27 Mar 2023 16:45:00 -0400"
185,3 Questions: How automation and good jobs can co-exist,https://news.mit.edu/2023/3-questions-automation-and-good-jobs-can-co-exist-0317,Work of the Future Initiative co-directors Julie Shah and Ben Armstrong describe their vision of “positive-sum automation.”,[{'name': 'Peter Dizikes | MIT News Office'}],"Fri, 17 Mar 2023 00:00:00 -0400"
186,Resilient bug-sized robots keep flying even after wing damage,https://news.mit.edu/2023/resilient-bug-sized-robots-wing-damage-0315,New repair techniques enable microscale robots to recover flight performance after suffering severe damage to the artificial muscles that power their wings.,[{'name': 'Adam Zewe | MIT News Office'}],"Wed, 15 Mar 2023 14:00:00 -0400"
187,Mix-and-match kit could enable astronauts to build a menagerie of lunar exploration bots,https://news.mit.edu/2023/mixed-robot-kit-lunar-exploration-0314,Robotic parts could be assembled into nimble spider bots for exploring lava tubes or heavy-duty elephant bots for transporting solar panels.,[{'name': 'Jennifer Chu | MIT News Office'}],"Tue, 14 Mar 2023 00:00:00 -0400"
188,New “traffic cop” algorithm helps a drone swarm stay on task,https://news.mit.edu/2023/new-traffic-cop-algorithm-drone-swarm-wireless-0313,"By keeping data fresh, the system could help robots inspect buildings or search disaster zones.",[{'name': 'Jennifer Chu | MIT News Office'}],"Mon, 13 Mar 2023 00:00:00 -0400"
189,"Custom, 3D-printed heart replicas look and pump just like the real thing",https://news.mit.edu/2023/custom-3d-printed-heart-replicas-patient-specific-0222,The soft robotic models are patient-specific and could help clinicians zero in on the best implant for an individual.,[{'name': 'Jennifer Chu | MIT News Office'}],"Wed, 22 Feb 2023 14:00:00 -0500"
190,MIT scientists use a new type of nanoparticle to make vaccines more powerful,https://news.mit.edu/2024/mit-scientists-use-new-nanoparticle-more-powerful-vaccines-0306,Study shows metal-organic particles can both deliver vaccines and act as an adjuvant to generate a strong immune response at a lower dose.,[{'name': 'Anne Trafton | MIT News'}],"Wed, 06 Mar 2024 14:00:00 -0500"
191,Brain surgery training from an avatar,https://news.mit.edu/2024/brain-surgery-training-avatar-0229,MIT.nano Immersion Lab works with AR/VR startup to create transcontinental medical instruction.,[{'name': 'Becky Ham | MIT.nano'}],"Thu, 29 Feb 2024 16:30:00 -0500"
192,Study unlocks nanoscale secrets for designing next-generation solar cells,https://news.mit.edu/2024/study-unlocks-nanoscale-secrets-tuning-perovskites-0228,"The work will help researchers tune surface properties of perovskites, a promising alternative and supplement to silicon, for more efficient photovoltaics.",[{'name': 'David L. Chandler | MIT News'}],"Wed, 28 Feb 2024 05:00:00 -0500"
193,Researchers harness 2D magnetic materials for energy-efficient computing,https://news.mit.edu/2024/researchers-harness-2d-magnetic-materials-energy-efficient-computing-0222,"An MIT team precisely controlled an ultrathin magnet at room temperature, which could enable faster, more efficient processors and computer memories.",[{'name': 'Adam Zewe | MIT News'}],"Thu, 22 Feb 2024 00:00:00 -0500"
194,"Play it again, Spirio",https://news.mit.edu/2024/play-it-again-spirio-0221,A piano that captures the data of live performance offers the MIT community new possibilities for studying and experimenting with music.<br />,[{'name': 'Nicole Estvanik Taylor | Arts at MIT'}],"Wed, 21 Feb 2024 00:00:00 -0500"
195,Technique could improve the sensitivity of quantum sensing devices,https://news.mit.edu/2024/technique-could-improve-sensitivity-quantum-sensing-devices-0208,"The method lets researchers identify and control larger numbers of atomic-scale defects, to build a bigger system of qubits.",[{'name': 'Adam Zewe | MIT News'}],"Thu, 08 Feb 2024 00:00:00 -0500"
196,MIT-led team receives funding to pursue new treatments for metabolic disease,https://news.mit.edu/2024/mit-led-team-receives-funding-new-treatments-metabolic-disease-0205,"Awarded $65.67 million from ARPA-H, the researchers will work to develop ingestible capsules that deliver mRNA and electric stimuli to treat metabolic disorders such as diabetes.",[{'name': 'Anne Trafton | MIT News'}],"Mon, 05 Feb 2024 15:30:00 -0500"
197,New MIT.nano equipment to accelerate innovation in “tough tech” sectors,https://news.mit.edu/2024/new-mit-nano-equipment-accelerate-innovation-0130,The advanced fabrication tools will enable the next generation of microelectronics and microsystems while bridging the gap from the lab to commercialization.,[{'name': 'Zach Winn | MIT News'}],"Tue, 30 Jan 2024 13:00:00 -0500"
198,"MIT, Applied Materials, and the Northeast Microelectronics Coalition Hub to bring 200mm advanced research capabilities to MIT.nano",https://news.mit.edu/2024/mit-applied-materials-nemc-hub-advanced-research-capabilities-mit-nano-0130,"State-of-the-art toolset will bridge academic innovations and industry pathways to scale for semiconductors, microelectronics, and other critical technologies.",[{'name': 'MIT.nano'}],"Tue, 30 Jan 2024 13:00:00 -0500"
199,DNA particles that mimic viruses hold promise as vaccines,https://news.mit.edu/2024/dna-particles-mimic-viruses-hold-promise-vaccines-0130,"Using a DNA-based scaffold carrying viral proteins, researchers created a vaccine that provokes a strong antibody response against SARS-CoV-2.",[{'name': 'Anne Trafton | MIT News'}],"Tue, 30 Jan 2024 05:00:00 -0500"
200,"Middle-school students meet a beam of electrons, and excitement results",https://news.mit.edu/2024/middle-school-students-meet-beam-electrons-excitement-results-0129,EMERGE program ignites interest in science through hands-on electron microscopy.,[{'name': 'Lauren Paul | Department of Materials Science and Engineering'}],"Mon, 29 Jan 2024 17:00:00 -0500"
201,Benchtop test quickly identifies extremely impact-resistant materials,https://news.mit.edu/2024/benchtop-test-identifies-extremely-impact-resistant-materials-0129,"High-speed experiments can help identify lightweight, protective “metamaterials” for spacecraft, vehicles, helmets, or other objects.",[{'name': 'Jennifer Chu | MIT News'}],"Mon, 29 Jan 2024 15:00:00 -0500"
202,Capsid of HIV-1 behaves like cell’s cargo receptor to enter the nucleus,https://news.mit.edu/2024/capsid-hiv-1-behaves-like-cell-cargo-receptor-to-enter-nucleus-0124,Biologists demonstrate that HIV-1 capsid acts like a Trojan horse to pass viral cargo across the nuclear pore.,[{'name': 'Lillian Eden | Department of Biology'}],"Wed, 24 Jan 2024 13:45:00 -0500"
203,Researchers improve blood tests’ ability to detect and monitor cancer,https://news.mit.edu/2024/researchers-improve-blood-tests-ability-detect-monitor-cancer-0118,"The advance makes it easier to detect circulating tumor DNA in blood samples, which could enable earlier cancer diagnosis and help guide treatment.",[{'name': 'Anne Trafton | MIT News'}],"Thu, 18 Jan 2024 14:00:00 -0500"
204,Shell joins MIT.nano Consortium,https://news.mit.edu/2024/shell-joins-mitnano-consortium-0109,International energy company becomes sustaining member of industry group.,[{'name': 'MIT.nano'}],"Tue, 09 Jan 2024 09:00:00 -0500"
205,MIT’s tiny technologies go to Washington,https://news.mit.edu/2023/mit-tiny-technologies-go-to-washington-1218,Cancer nanomedicine was on display at the 2023 White House Demo Day.,[{'name': 'Koch Institute'}],"Mon, 18 Dec 2023 12:45:00 -0500"
206,Nanoparticle-delivered RNA reduces neuroinflammation in lab tests,https://news.mit.edu/2023/nanoparticle-delivered-rna-reduces-neuroinflammation-lab-tests-1215,"MIT researchers find that in mice and human cell cultures, lipid nanoparticles can deliver a potential therapy for inflammation in the brain, a prominent symptom in Alzheimer’s.",[{'name': 'David Orenstein | Picower Institute'}],"Fri, 15 Dec 2023 15:45:00 -0500"
207,Angela Belcher delivers 2023 Dresselhaus Lecture on evolving organisms for new nanomaterials,https://news.mit.edu/2023/angela-belcher-delivers-dresselhaus-lecture-1213,"MIT professor combines nanoscience and viruses to develop solutions in energy, environment, and medicine.",[{'name': 'Amanda Stoll DiCristofaro | MIT.nano'}],"Wed, 13 Dec 2023 16:40:00 -0500"
208,Closing the design-to-manufacturing gap for optical devices,https://news.mit.edu/2023/closing-design-manufacturing-gap-optical-devices-1213,"A new method enables optical devices that more closely match their design specifications, boosting accuracy and efficiency.",[{'name': 'Adam Zewe | MIT News'}],"Wed, 13 Dec 2023 00:00:00 -0500"
209,3 Questions: Darrell Irvine on making HIV vaccines more powerful,https://news.mit.edu/2023/darrell-irvine-making-hiv-vaccines-more-powerful-1212,"Human volunteers will soon begin receiving an HIV vaccine that contains an adjuvant developed in Irvine’s lab, which helps to boost B cell responses to the vaccine.",[{'name': 'Anne Trafton | MIT News'}],"Tue, 12 Dec 2023 00:00:00 -0500"
210,Scientists 3D print self-heating microfluidic devices,https://news.mit.edu/2023/scientists-3d-print-self-heating-microfluidic-devices-1211,The one-step fabrication process rapidly produces miniature chemical reactors that could be used to detect diseases or analyze substances.,[{'name': 'Adam Zewe | MIT News'}],"Mon, 11 Dec 2023 00:00:00 -0500"
211,Researchers safely integrate fragile 2D materials into devices,https://news.mit.edu/2023/researchers-safely-integrate-fragile-2d-materials-devices-1208,The advance opens a path to next-generation devices with unique optical and electronic properties.,[{'name': 'Adam Zewe | MIT News'}],"Fri, 08 Dec 2023 05:00:00 -0500"
212,"Remembering Professor Judy Hoyt, a pioneer in semiconductor research",https://news.mit.edu/2023/remembering-professor-judy-hoyt-1208,Her demonstration of incorporating lattice strain as a means to enhance performance in scaled silicon devices has informed virtually every high-performance chip manufactured today.,[{'name': 'Jane Halpern | Department of Electrical Engineering and Computer Science'}],"Thu, 07 Dec 2023 15:00:00 -0500"
213,A new way to deliver drugs more efficiently,https://news.mit.edu/2023/new-way-deliver-drugs-more-efficiently-1128,Core-shell structures made of hydrogel could enable more efficient uptake in the body.,[{'name': 'Department of Chemical Engineering'}],"Tue, 28 Nov 2023 11:00:00 -0500"
214,Celebrating five years of MIT.nano,https://news.mit.edu/2023/celebrating-five-years-mit-nano-summit-1127,The Nano Summit highlights nanoscale research across multiple disciplines at MIT.,[{'name': 'Amanda Stoll DiCristofaro | MIT.nano'}],"Mon, 27 Nov 2023 15:15:00 -0500"
215,Team engineers nanoparticles using ion irradiation to advance clean energy and fuel conversion,https://news.mit.edu/2023/team-engineers-nanoparticles-advance-clean-energy-fuel-conversion-1127,The work demonstrates control over key properties leading to better performance.,[{'name': 'Elizabeth Thomson | Materials Research Laboratory'}],"Mon, 27 Nov 2023 13:45:00 -0500"
216,New laser setup probes metamaterial structures with ultrafast pulses,https://news.mit.edu/2023/new-laser-setup-probes-metamaterial-structures-ultrafast-pulses-1115,"The LIRAS technique could speed up the development of acoustic lenses, impact-resistant films, and other futuristic materials.",[{'name': 'Jennifer Chu | MIT News'}],"Wed, 15 Nov 2023 11:00:00 -0500"
217,MIT physicists turn pencil lead into “gold”,https://news.mit.edu/2023/mit-physicists-turn-pencil-lead-into-gold-1114,Thin flakes of graphite can be tuned to exhibit three important properties.,[{'name': 'Elizabeth A. Thomson | Materials Research Laboratory'}],"Tue, 14 Nov 2023 15:10:00 -0500"
218,Using AI to optimize for rapid neural imaging,https://news.mit.edu/2023/using-ai-optimize-rapid-neural-imaging-1106,"MIT CSAIL researchers combine AI and electron microscopy to expedite detailed brain network mapping, aiming to enhance connectomics research and clinical pathology.",[{'name': 'Rachel Gordon | MIT CSAIL'}],"Mon, 06 Nov 2023 13:00:00 -0500"
219,"In a surprising finding, light can make water evaporate without heat",https://news.mit.edu/2023/surprising-finding-light-makes-water-evaporate-without-heat-1031,A newly identified process could explain a variety of natural phenomena and enable new approaches to desalination.,[{'name': 'David L. Chandler | MIT News'}],"Tue, 31 Oct 2023 15:20:00 -0400"
220,"From a five-layer graphene sandwich, a rare electronic state emerges",https://news.mit.edu/2023/five-layer-graphene-sandwich-rare-electronic-behavior-1018,A newly discovered type of electronic behavior could help with packing more data into magnetic memory devices.,[{'name': 'Jennifer Chu | MIT News'}],"Wed, 18 Oct 2023 11:00:00 -0400"
221,Photos: Moungi Bawendi’s first day as a Nobel laureate,https://news.mit.edu/2023/photos-moungi-bawendi-first-day-as-nobel-laureate-1004,A look at how the MIT professor spent his day after learning he had won the Nobel Prize in Chemistry.,[{'name': 'Maia Weinstock | MIT News'}],"Wed, 04 Oct 2023 18:40:00 -0400"
222,MIT Professor Moungi Bawendi shares Nobel Prize in Chemistry,https://news.mit.edu/2023/mit-chemist-moungi-bawendi-shares-nobel-prize-chemistry-1004,"For his work on techniques to generate quantum dots of uniform size and color, Bawendi is honored along with Louis Brus and Alexei Ekimov.",[{'name': 'Anne Trafton | MIT News'}],"Wed, 04 Oct 2023 06:00:00 -0400"
223,Quantum repeaters use defects in diamond to interconnect quantum systems,https://news.mit.edu/2023/quantum-repeaters-use-defects-diamond-interconnect-quantum-systems-0927,This technology for storing and transmitting quantum information over lossy links could provide the foundation for scalable quantum networking.,[{'name': 'Ariana Tantillo | MIT Lincoln Laboratory'}],"Wed, 27 Sep 2023 16:35:00 -0400"
224,MIT.nano Family Day invites those at home to come to work,https://news.mit.edu/2023/mitnano-family-day-invites-those-home-come-work-0918,Staff share nano experience — and ice cream — with their families.,[{'name': 'MIT.nano'}],"Mon, 18 Sep 2023 14:00:00 -0400"
225,Pixel-by-pixel analysis yields insights into lithium-ion batteries,https://news.mit.edu/2023/pixel-analysis-yields-insights-lithium-ion-batteries-0913,"In a first, researchers have observed how lithium ions flow through a battery interface, which could help engineers optimize the material’s design.",[{'name': 'Anne Trafton | MIT News'}],"Wed, 13 Sep 2023 11:00:00 -0400"
226,MIT engineers design more powerful RNA vaccines,https://news.mit.edu/2023/mit-engineers-design-more-powerful-rna-vaccines-0907,The new approach could lead to intranasal vaccines for Covid-19 and other respiratory diseases.,[{'name': 'Anne Trafton | MIT News'}],"Thu, 07 Sep 2023 11:00:00 -0400"
227,How to prevent biofilms in space,https://news.mit.edu/2023/preventing-biofilms-space-0907,"Microbial or fungal biofilms on spacecraft can clog hoses and filters, or make astronauts sick. Space Station tests show that a surface treatment can help.",[{'name': 'David L. Chandler | MIT News'}],"Thu, 07 Sep 2023 00:00:00 -0400"
228,Deciphering the cellular mechanisms behind ALS,https://news.mit.edu/2024/ernest-fraenkel-deciphering-cellular-mechanisms-behind-als-0306,"Professor Ernest Fraenkel has decoded fundamental aspects of Huntington’s disease and glioblastoma, and is now using computation to better understand amyotrophic lateral sclerosis.",[{'name': 'Michaela Jarvis | School of Engineering'}],"Wed, 06 Mar 2024 16:00:00 -0500"
229,A noninvasive treatment for “chemo brain”,https://news.mit.edu/2024/noninvasive-treatment-chemo-brain-0306,Stimulating gamma brain waves may protect cancer patients from memory impairment and other cognitive effects of chemotherapy.,[{'name': 'Anne Trafton | MIT News'}],"Wed, 06 Mar 2024 14:00:00 -0500"
230,“This MIT Bootcamp shook everything upside down and has given me the spirit of innovation”,https://news.mit.edu/2024/mit-bootcamp-tackling-substance-use-disorder-biomedical-device-innovation-0305,New MIT offering brings a multidisciplinary approach to tackling substance use disorder through biomedical device innovation.,[{'name': 'Mariah Rawding | MIT Bootcamps'}],"Tue, 05 Mar 2024 16:15:00 -0500"
231,Brain surgery training from an avatar,https://news.mit.edu/2024/brain-surgery-training-avatar-0229,MIT.nano Immersion Lab works with AR/VR startup to create transcontinental medical instruction.,[{'name': 'Becky Ham | MIT.nano'}],"Thu, 29 Feb 2024 16:30:00 -0500"
232,What can super-healing species teach us about regeneration?,https://news.mit.edu/2024/what-can-super-healing-species-teach-us-about-regeneration-albert-almada-0221,"Albert Almada PhD ’13 studies the mechanics of how stem cells rebuild tissues. “Digging deep into the science is what MIT taught me,” he says.",[{'name': 'Lillian Eden | Department of Biology'}],"Wed, 21 Feb 2024 15:50:00 -0500"
233,MIT Solve announces 2024 Global Challenges and Indigenous Communities Fellowship,https://news.mit.edu/2024/mit-solve-announces-global-challenges-indigenous-communities-fellowship-0220,Over $1 million in prize funding available for tech-enabled solutions to the 2024 Global Challenges.,[{'name': 'Maya Bingman | MIT Solve'}],"Tue, 20 Feb 2024 12:30:00 -0500"
234,New model identifies drugs that shouldn’t be taken together,https://news.mit.edu/2024/new-model-identifies-drugs-shouldnt-be-taken-together-0220,"Using a machine-learning algorithm, researchers can predict interactions that could interfere with a drug’s effectiveness.",[{'name': 'Anne Trafton | MIT News'}],"Tue, 20 Feb 2024 05:00:00 -0500"
235,"MIT course aids social connection, better relationships, and happiness",https://news.mit.edu/2024/mit-course-aids-social-connection-better-relationships-happiness-0215,"New PE.0539 course supports students who want to meet new people, strengthen friendships, build communities, and make MIT a more welcoming place.",[{'name': 'Sarah Foote | Division of Student Life'}],"Thu, 15 Feb 2024 16:55:00 -0500"
236,Hitchhiking cancer vaccine makes progress in the clinic,https://news.mit.edu/2024/hitchhiking-cancer-vaccine-makes-progress-in-clinic-0215,MIT spinout Elicio developed a vaccine based on a lymph node-targeting approach first developed at the Koch Institute. Phase 1 solid tumor clinical trial results are promising so far.,[{'name': 'Bendta Schroeder | Koch Institute'}],"Thu, 15 Feb 2024 15:30:00 -0500"
237,A new test could predict how heart attack patients will respond to mechanical pumps,https://news.mit.edu/2024/new-test-predict-heart-attack-patients-response-mechanical-pumps-0214,Performing this test could help doctors prevent dysfunction that can occur when the right and left ventricles of the heart become imbalanced.,[{'name': 'Anne Trafton | MIT News'}],"Wed, 14 Feb 2024 14:00:00 -0500"
238,Annie Liau: Infinite caring for the MIT community,https://news.mit.edu/2024/annie-liau-infinite-caring-mit-community-0208,"A longtime physician reflects on her journey from Thailand to MIT, and her four decades of service at MIT Health.",[{'name': 'Maia Weinstock | MIT News'}],"Thu, 08 Feb 2024 15:00:00 -0500"
239,Scientists develop a low-cost device to make cell therapy safer,https://news.mit.edu/2024/scientists-develop-low-cost-device-safer-cell-therapy-0207,A plastic microfluidic chip can remove some risky cells that could potentially become tumors before they are implanted in a patient.,[{'name': 'Adam Zewe | MIT News'}],"Wed, 07 Feb 2024 00:00:00 -0500"
240,MIT-led team receives funding to pursue new treatments for metabolic disease,https://news.mit.edu/2024/mit-led-team-receives-funding-new-treatments-metabolic-disease-0205,"Awarded $65.67 million from ARPA-H, the researchers will work to develop ingestible capsules that deliver mRNA and electric stimuli to treat metabolic disorders such as diabetes.",[{'name': 'Anne Trafton | MIT News'}],"Mon, 05 Feb 2024 15:30:00 -0500"
241,Doctors have more difficulty diagnosing disease when looking at images of darker skin,https://news.mit.edu/2024/doctors-more-difficulty-diagnosing-diseases-images-darker-skin-0205,"Dermatologists and general practitioners are somewhat less accurate in diagnosing disease in darker skin, a new study finds. Used correctly, AI may be able to help.",[{'name': 'Anne Trafton | MIT News'}],"Mon, 05 Feb 2024 05:00:00 -0500"
242,Blood cell family trees trace how production changes with aging,https://news.mit.edu/2024/blood-cell-family-trees-trace-how-production-changes-aging-0131,"Jonathan Weissman and collaborators developed a tool to reconstruct human cell family trees, revealing how blood cell production changes in old age.",[{'name': 'Greta Friar | Whitehead Institute'}],"Wed, 31 Jan 2024 15:10:00 -0500"
243,School of Engineering fourth quarter 2023 awards,https://news.mit.edu/2024/school-engineering-fourth-quarter-awards-0126,"Faculty and researchers across MIT’s School of Engineering receive many awards in recognition of their scholarship, service, and overall excellence.",[{'name': 'School of Engineering'}],"Fri, 26 Jan 2024 14:20:00 -0500"
244,MIT Faculty Founder Initiative announces finalists for second competition,https://news.mit.edu/2024/mit-faculty-founder-initiative-announces-finalists-second-competition-0124,Twelve researchers selected as finalists for 2023-24 MIT-Royalty Pharma Prize Competition to support female entrepreneurs in biotech.,[{'name': 'Mary Beth Gallagher | School of Engineering'}],"Wed, 24 Jan 2024 16:15:00 -0500"
245,What to do about AI in health?,https://news.mit.edu/2024/what-to-do-about-ai-in-health-0123,"Although artificial intelligence in health has shown great promise, pressure is mounting for regulators around the world to act, as AI tools demonstrate potentially harmful outcomes.",[{'name': 'Alex Ouyang | Abdul Latif Jameel Clinic for Machine Learning in Health'}],"Tue, 23 Jan 2024 16:25:00 -0500"
246,"Getfit, MIT Health’s winter exercise challenge, turns 20 in 2024",https://news.mit.edu/2024/getfit-winter-exercise-challenge-turns-20-0119,The program encourages everyone on campus to get moving during the cold months.,[{'name': 'Denise Brehm | Health and Safety Office'}],"Fri, 19 Jan 2024 13:15:00 -0500"
247,Researchers improve blood tests’ ability to detect and monitor cancer,https://news.mit.edu/2024/researchers-improve-blood-tests-ability-detect-monitor-cancer-0118,"The advance makes it easier to detect circulating tumor DNA in blood samples, which could enable earlier cancer diagnosis and help guide treatment.",[{'name': 'Anne Trafton | MIT News'}],"Thu, 18 Jan 2024 14:00:00 -0500"
248,New hope for early pancreatic cancer intervention via AI-based risk prediction,https://news.mit.edu/2024/new-hope-early-pancreatic-cancer-intervention-ai-based-risk-prediction-0118,MIT CSAIL researchers develop advanced machine-learning models that outperform current methods in detecting pancreatic ductal adenocarcinoma.,[{'name': 'Rachel Gordon | MIT CSAIL'}],"Thu, 18 Jan 2024 14:00:00 -0500"
249,MedLinks volunteers aid students in residence halls with minor medical issues,https://news.mit.edu/2024/medlinks-volunteers-aid-students-minor-medical-issues-0118,Some 150 MIT students participate in the 30-year-old program.,[{'name': 'Sarah Foote | Division of Student Life'}],"Thu, 18 Jan 2024 12:00:00 -0500"
250,Stratospheric safety standards: How aviation could steer regulation of AI in health,https://news.mit.edu/2024/stratospheric-safety-standards-how-aviation-could-steer-ai-health-regulation-0117,An interdisciplinary team of researchers thinks health AI could benefit from some of the aviation industry’s long history of hard-won lessons that have created one of the safest activities today.,[{'name': 'Alex Ouyang | Abdul Latif Jameel Clinic for Machine Learning in Health'}],"Wed, 17 Jan 2024 12:50:00 -0500"
251,K. Lisa Yang Global Engineering and Research Center will prioritize innovations for resource-constrained communities,https://news.mit.edu/2024/k-lisa-yang-global-engineering-research-gear-center-0117,"Collaborative hub founded by philanthropist Lisa Yang will catalyze academic innovation and result in real-world, global impact.",[{'name': 'Department of Mechanical Engineering'}],"Wed, 17 Jan 2024 09:00:00 -0500"
252,Turning history of science into a comic adventure,https://news.mit.edu/2024/turning-science-history-comic-adventure-0111,Associate Professor Lydia Bourouiba and artist Argha Manna take readers through a series of discoveries in infectious disease.,[{'name': 'Zach Winn | MIT News'}],"Thu, 11 Jan 2024 00:00:00 -0500"
253,Inhalable sensors could enable early lung cancer detection,https://news.mit.edu/2024/inhalable-sensors-early-lung-cancer-detection-0105,"The diagnostic, which requires only a simple urine test to read the results, could make lung cancer screening more accessible worldwide.",[{'name': 'Anne Trafton | MIT News'}],"Fri, 05 Jan 2024 14:00:00 -0500"
254,Does “food as medicine” make a big dent in diabetes?,https://news.mit.edu/2023/food-medicine-diabetes-study-1227,"Study of rigorous trial shows mixed results, suggests need to keep examining how nutrition can combat a pervasive disease.",[{'name': 'Peter Dizikes | MIT News'}],"Wed, 27 Dec 2023 00:00:00 -0500"
255,"Engineers develop a vibrating, ingestible capsule that might help treat obesity",https://news.mit.edu/2023/engineers-develop-vibrating-ingestible-capsule-1222,"Swallowing the device before a meal could create a sense of fullness, tricking the brain into thinking it’s time to stop eating.",[{'name': 'Anne Trafton | MIT News'}],"Fri, 22 Dec 2023 14:00:00 -0500"
256,MIT in the media: 2023 in review,https://news.mit.edu/2023/mit-media-year-in-review-1221,MIT community members made headlines with key research advances and their efforts to tackle pressing challenges.,[{'name': 'MIT News'}],"Thu, 21 Dec 2023 00:00:00 -0500"
257,"Using AI, MIT researchers identify a new class of antibiotic candidates",https://news.mit.edu/2023/using-ai-mit-researchers-identify-antibiotic-candidates-1220,"These compounds can kill methicillin-resistant Staphylococcus aureus (MRSA), a bacterium that causes deadly infections.",[{'name': 'Anne Trafton | MIT News'}],"Wed, 20 Dec 2023 11:00:00 -0500"
258,Study: Colon cancer screenings are more effective than previously understood,https://news.mit.edu/2023/study-colon-cancer-screenings-are-more-effective-1219,"By reevaluating existing data, researchers find the procedure is even more valuable than consensus had indicated.",[{'name': 'Peter Dizikes | MIT News'}],"Tue, 19 Dec 2023 00:00:00 -0500"
259,New exhibits showcase trailblazing MIT women,https://news.mit.edu/2024/new-exhibits-showcase-trailblazing-mit-women-0306,Materials from MIT’s Distinctive Collections reveal stories of women at the Institute.,[{'name': 'Brigham Fay | MIT Libraries'}],"Wed, 06 Mar 2024 10:00:00 -0500"
260,School of Science announces 2024 Infinite Expansion Awards,https://news.mit.edu/2024/school-science-infinite-expansion-awards-0304,Nine postdocs and research scientists honored for contributions to the Institute.,[{'name': 'School of Science'}],"Mon, 04 Mar 2024 17:20:00 -0500"
261,Eight from MIT named 2024 Sloan Research Fellows,https://news.mit.edu/2024/sloan-research-fellows-0229,"Fellows honored for creativity, innovation, and research accomplishments.",[{'name': 'School of Science'}],"Thu, 29 Feb 2024 16:50:00 -0500"
262,Study unlocks nanoscale secrets for designing next-generation solar cells,https://news.mit.edu/2024/study-unlocks-nanoscale-secrets-tuning-perovskites-0228,"The work will help researchers tune surface properties of perovskites, a promising alternative and supplement to silicon, for more efficient photovoltaics.",[{'name': 'David L. Chandler | MIT News'}],"Wed, 28 Feb 2024 05:00:00 -0500"
263,"With just a little electricity, MIT researchers boost common catalytic reactions",https://news.mit.edu/2024/mit-researchers-boost-common-catalytic-reactions-with-electricity-0215,"Applying a small voltage to a catalyst can increase the rates of reactions used in petrochemical processing, pharmaceutical manufacture, and many other processes.",[{'name': 'David L. Chandler | MIT News'}],"Thu, 15 Feb 2024 14:00:00 -0500"
264,MIT community members honored with 2024 Franklin Institute Awards,https://news.mit.edu/2024/mit-community-members-honored-franklin-institute-awards-0209,"Two professors and three additional alumni recognized for “dreaming up solutions to global challenges — advancing health, sustainability, and human connection.”",[{'name': 'Department of Chemical Engineering'}],"Fri, 09 Feb 2024 16:30:00 -0500"
265,Researchers discover new channels to excite magnetic waves with terahertz light,https://news.mit.edu/2024/new-channels-excite-magnetic-waves-terahertz-light-0206,"The finding provides new insights into the ultrafast control of magnetic materials, with potential to enable next-generation information processing technologies.",[{'name': 'Department of Chemistry'}],"Tue, 06 Feb 2024 17:25:00 -0500"
266,"Middle-school students meet a beam of electrons, and excitement results",https://news.mit.edu/2024/middle-school-students-meet-beam-electrons-excitement-results-0129,EMERGE program ignites interest in science through hands-on electron microscopy.,[{'name': 'Lauren Paul | Department of Materials Science and Engineering'}],"Mon, 29 Jan 2024 17:00:00 -0500"
267,Susan Solomon wins VinFuture Award for Female Innovators,https://news.mit.edu/2024/susan-solomon-vinfuture-award-female-innovators-0126,The award recognizes Solomon’s contributions to understanding ozone depletion and the creation of the Montreal Protocol.,[{'name': 'Paige Colley | EAPS'}],"Fri, 26 Jan 2024 10:00:00 -0500"
268,Performance art and science collide as students experience “Blue Man Group”,https://news.mit.edu/2024/performance-art-science-collide-blue-man-group-0125,Students from Course 5.111 (Principles of Chemical Science) were treated to a performance that brought to life the chemical structures and crystal field theory concepts covered in class.,[{'name': 'Danielle Randall Doughty | Department of Chemistry'}],"Thu, 25 Jan 2024 13:10:00 -0500"
269,A new drug candidate can shrink kidney cysts,https://news.mit.edu/2024/new-drug-candidate-can-shrink-kidney-cysts-0122,"A compound originally developed to treat cancer could be repurposed to treat polycystic kidney disease, an inherited condition that can lead to kidney failure.",[{'name': 'Anne Trafton | MIT News'}],"Mon, 22 Jan 2024 00:00:00 -0500"
270,Cobalt-free batteries could power cars of the future,https://news.mit.edu/2024/cobalt-free-batteries-could-power-future-cars-0118,"MIT chemists developed a battery cathode based on organic materials, which could reduce the EV industry’s reliance on scarce metals.",[{'name': 'Anne Trafton | MIT News'}],"Thu, 18 Jan 2024 08:00:00 -0500"
271,Study reveals a reaction at the heart of many renewable energy technologies,https://news.mit.edu/2024/study-reveals-electrode-reaction-renewable-energy-technologies-0116,New insights into how proton-coupled electron transfers occur at an electrode could help researchers design more efficient fuel cells and electrolyzers.,[{'name': 'Anne Trafton | MIT News'}],"Tue, 16 Jan 2024 05:00:00 -0500"
272,MIT community members elected to the National Academy of Inventors for 2023,https://news.mit.edu/2024/mit-community-members-elected-national-academy-inventors-0103,"MIT Koch Institute researchers Daniel Anderson and Ana Jaklenec, plus 11 MIT alumni, are honored for inventions that have made a tangible impact on society.",[{'name': 'Bendta Schroeder | Koch Institute'}],"Wed, 03 Jan 2024 15:30:00 -0500"
273,Computational model captures the elusive transition states of chemical reactions,https://news.mit.edu/2023/computational-model-captures-elusive-transition-states-1215,"Using generative AI, MIT chemical engineers and chemists created a model that can predict the structures formed when a chemical reaction reaches its point of no return.",[{'name': 'Anne Trafton | MIT News'}],"Fri, 15 Dec 2023 05:00:00 -0500"
274,Moungi Bawendi honored during Nobel Week in Stockholm,https://news.mit.edu/2023/moungi-bawendi-honored-during-nobel-week-stockholm-1211,"The professor of chemistry participated in various festivities, culminating in the Nobel Prize ceremony on Dec. 10.",[{'name': 'MIT News'}],"Mon, 11 Dec 2023 16:03:00 -0500"
275,MIT engineers develop a way to determine how the surfaces of materials behave,https://news.mit.edu/2023/mit-engineers-how-surfaces-materials-behave-1207,"Using machine learning, the computational method can provide details of how materials work as catalysts, semiconductors, or battery components.",[{'name': 'David L. Chandler | MIT News'}],"Thu, 07 Dec 2023 05:00:00 -0500"
276,Chemists create organic molecules in a rainbow of colors,https://news.mit.edu/2023/chemists-create-organic-molecules-rainbow-colors-1205,"The molecules, known as acenes, could be useful as organic light-emitting diodes or solar cells, among other possible applications.",[{'name': 'Anne Trafton | MIT News'}],"Tue, 05 Dec 2023 05:00:00 -0500"
277,Explained: The sugar coating of life,https://news.mit.edu/2023/explained-glycoscience-carbohydrates-1201,"Researchers are working to advance the field of glycoscience, illuminating the essential role of carbohydrates for human health and disease.",[{'name': 'Leah Campbell | School of Science'}],"Fri, 01 Dec 2023 11:00:00 -0500"
278,Celebrating five years of MIT.nano,https://news.mit.edu/2023/celebrating-five-years-mit-nano-summit-1127,The Nano Summit highlights nanoscale research across multiple disciplines at MIT.,[{'name': 'Amanda Stoll DiCristofaro | MIT.nano'}],"Mon, 27 Nov 2023 15:15:00 -0500"
279,GlycoMIT Symposium celebrates advancements in glycobiology,https://news.mit.edu/2023/glycomit-symposium-celebrates-advancements-glycobiology-1107,Glyco enthusiasts from MIT and beyond recently gathered in Bartos Theatre to enjoy presentations of the latest advancements in glycobiology research.,[{'name': 'Danielle Randall Doughty | Department of Chemistry'}],"Tue, 07 Nov 2023 17:00:00 -0500"
280,Bringing the environment to the forefront of engineering,https://news.mit.edu/2023/desiree-plata-sustainability-engineering-1024,Desirée Plata is on a lifelong mission to make sustainability a bigger factor in design decisions.,[{'name': 'Zach Winn | MIT News'}],"Tue, 24 Oct 2023 00:00:00 -0400"
281,Targeting a coronavirus ion channel could yield new Covid-19 drugs,https://news.mit.edu/2023/targeting-coronavirus-ion-channel-could-yield-new-covid-19-drugs-1013,"Chemists discover the structures of open and closed states of the channel, which could help the development of antiviral drugs to reduce inflammation.",[{'name': 'Anne Trafton | MIT News'}],"Fri, 13 Oct 2023 14:00:00 -0400"
282,Photos: Moungi Bawendi’s first day as a Nobel laureate,https://news.mit.edu/2023/photos-moungi-bawendi-first-day-as-nobel-laureate-1004,A look at how the MIT professor spent his day after learning he had won the Nobel Prize in Chemistry.,[{'name': 'Maia Weinstock | MIT News'}],"Wed, 04 Oct 2023 18:40:00 -0400"
283,MIT Professor Moungi Bawendi shares Nobel Prize in Chemistry,https://news.mit.edu/2023/mit-chemist-moungi-bawendi-shares-nobel-prize-chemistry-1004,"For his work on techniques to generate quantum dots of uniform size and color, Bawendi is honored along with Louis Brus and Alexei Ekimov.",[{'name': 'Anne Trafton | MIT News'}],"Wed, 04 Oct 2023 06:00:00 -0400"
284,School of Science welcomes new faculty in 2023,https://news.mit.edu/2023/school-science-welcomes-new-faculty-0925,"Sixteen professors join the departments of Biology; Chemistry; Earth, Atmospheric and Planetary Sciences; Mathematics; and Physics.",[{'name': 'School of Science'}],"Mon, 25 Sep 2023 15:55:00 -0400"
285,Future science at the molecular level,https://news.mit.edu/2023/future-science-molecular-level-brad-pentelute-0915,Brad Pentelute and his lab compel the anthrax delivery system to deliver antibody and peptide variants into cells to treat cancer.,[{'name': 'Daniel de Wolff | MIT Industrial Liaison Program'}],"Fri, 15 Sep 2023 16:15:00 -0400"
286,Study suggests energy-efficient route to capturing and converting CO2,https://news.mit.edu/2023/study-suggests-energy-efficient-route-capturing-and-converting-co2-0906,"The findings, based on a single electrochemical process, could help cut emissions from the hardest-to-decarbonize industries, such as steel and cement.",[{'name': 'Jennifer Chu | MIT News'}],"Wed, 06 Sep 2023 14:15:00 -0400"
287,Arrays of quantum rods could enhance TVs or virtual reality devices,https://news.mit.edu/2023/arrays-quantum-rods-could-enhance-tv-virtual-reality-devices-0811,"MIT engineers developed a new way to create these arrays, by scaffolding quantum rods onto patterned DNA.",[{'name': 'Anne Trafton | MIT News'}],"Fri, 11 Aug 2023 14:00:00 -0400"
288,Fourteen MIT School of Science professors receive tenure for 2022 and 2023,https://news.mit.edu/2023/mit-school-science-professors-receive-tenure-0808,"Faculty members were recently granted tenure in the departments of Biology, Brain and Cognitive Sciences, Chemistry, EAPS, and Physics.",[{'name': 'School of Science'}],"Tue, 08 Aug 2023 17:00:00 -0400"
289,Helping to fill in gaps in urology research for female patients,https://news.mit.edu/2023/helping-fill-gaps-urology-research-female-patients-nicole-de-nisco-0802,"Biologist Nicole De Nisco ’07, PhD ’13 draws on her love of problem-solving and interdisciplinary skills honed as a student at MIT.",[{'name': 'Lillian Eden | Department of Biology'}],"Wed, 02 Aug 2023 13:20:00 -0400"
290,Probe expands understanding of oral cavity homeostasis,https://news.mit.edu/2023/probe-expands-understanding-oral-cavity-homeostasis-0718,A new approach opens the door to a greater understanding of protein-microbe interactions.,[{'name': 'Lillian Eden | Department of Biology'}],"Tue, 18 Jul 2023 16:15:00 -0400"
291,How Tau tangles form in the brain,https://news.mit.edu/2023/how-tau-tangles-form-brain-0714,A new study shows that truncated versions of the Tau protein are more likely to form the sticky filaments seen in the brains of people with Alzheimer’s disease.,[{'name': 'Anne Trafton | MIT News Office'}],"Fri, 14 Jul 2023 14:00:00 -0400"
292,Scientists pinpoint where thousands of individual proteins are made in intact tissue and single cells,https://news.mit.edu/2023/scientists-pinpoint-where-thousands-individual-proteins-are-made-0714,A new technology called RIBOmap can give researchers valuable insight into how protein production in animal and human tissue is altered in disease.,[{'name': 'Sarah C.P. Williams | Broad Institute'}],"Fri, 14 Jul 2023 10:00:00 -0400"
293,Learning the language of molecules to predict their properties,https://news.mit.edu/2023/learning-language-molecules-predict-properties-0707,"This AI system only needs a small amount of data to predict molecular properties, which could speed up drug discovery and material development.",[{'name': 'Adam Zewe | MIT News Office'}],"Fri, 07 Jul 2023 00:00:00 -0400"
294,Researchers grow precise arrays of nanoLEDs,https://news.mit.edu/2023/researchers-grow-precise-arrays-nanoleds-0706,"A new technique produces perovskite nanocrystals right where they’re needed, so the exceedingly delicate materials can be integrated into nanoscale devices.",[{'name': 'Adam Zewe | MIT News Office'}],"Thu, 06 Jul 2023 00:00:00 -0400"
295,Chemists discover why photosynthetic light-harvesting is so efficient,https://news.mit.edu/2023/chemists-discover-photosynthetic-light-harvesting-efficiency-0703,The disorganized arrangement of the proteins in light-harvesting complexes is the key to their extreme efficiency.,[{'name': 'Anne Trafton | MIT News Office'}],"Mon, 03 Jul 2023 15:00:00 -0400"
296,QS ranks MIT the world’s No. 1 university for 2023-24,https://news.mit.edu/2023/qs-ranks-mit-1-university-23-24-0627,"Ranking at the top for the 12th year in a row, the Institute also places first in 11 subject areas.",[{'name': 'MIT News Office'}],"Tue, 27 Jun 2023 16:00:00 -0400"
297,Surprise! Weaker bonds can make polymers stronger,https://news.mit.edu/2023/weaker-bonds-can-make-polymers-stronger-0622,"By adding weak linkers to a polymer network, chemists dramatically enhanced the material’s resistance to tearing.",[{'name': 'Anne Trafton | MIT News Office'}],"Thu, 22 Jun 2023 00:00:00 -0400"
298,Researchers develop a new source of quantum light,https://news.mit.edu/2023/researchers-develop-new-source-quantum-light-0622,The device emits a stream of single photons and could provide a basis for optical quantum computers.,[{'name': 'David L. Chandler | MIT News Office'}],"Thu, 22 Jun 2023 00:00:00 -0400"
299,Charlie Farquhar: Forger of chemical and social bonds,https://news.mit.edu/2023/charlie-farquhar-chemical-social-bonds-0621,"While developing targeted drug-delivery methods, the PhD student advocates for inclusion, belonging, and collaboration.",[{'name': 'Laura Rosado | MIT News correspondent'}],"Wed, 21 Jun 2023 00:00:00 -0400"
300,Tiny diamond rotor could improve protein studies,https://news.mit.edu/2023/tiny-diamond-rotor-could-improve-protein-studies-0524,A new way of machining microscale rotors from diamond crystal can enable ultrasensitive NMR devices for probing proteins and other materials.,[{'name': 'David L. Chandler | MIT News Office'}],"Wed, 24 May 2023 00:00:00 -0400"
301,J-WAFS announces 2023 seed grant recipients,https://news.mit.edu/2023/j-wafs-announces-seed-grant-recipients-0517,Fifteen principal investigators from across MIT will conduct early work to solve issues ranging from water contamination to aquaculture monitoring and management.,[{'name': 'Maria Paula Acosta Bello | Abdul Latif Jameel Water and Food Systems Lab'}],"Wed, 17 May 2023 14:45:00 -0400"
302,Four researchers with MIT ties earn 2023 Schmidt Science Fellowships,https://news.mit.edu/2023/four-researchers-mit-ties-earn-schmidt-science-fellowships-0512,The illustrious prize supports early-career scientists and engineers as they pursue interdisciplinary work.,[{'name': 'Danielle Randall Doughty | Department of Chemistry'}],"Fri, 12 May 2023 14:00:00 -0400"
303,Study reveals new ways for exotic quasiparticles to “relax”,https://news.mit.edu/2023/study-reveals-new-ways-exotic-quasiparticles-relax-0512,A perovskite-based device that combines aspects of electronics and photonics may open doors to new kinds of computer chips or quantum qubits.,[{'name': 'David L. Chandler | MIT News Office'}],"Fri, 12 May 2023 00:00:00 -0400"
304,Five MIT faculty elected to the National Academy of Sciences for 2023,https://news.mit.edu/2023/five-mit-elected-national-academy-sciences-2023-0511,"Joshua Angrist, Gang Chen, Catherine Drennan, Dina Katabi, Gregory Stephanopoulos, and seven additional alumni are recognized by their peers for their outstanding contributions to research.",[{'name': 'Mary Beth Gallagher | School of Engineering'}],"Thu, 11 May 2023 16:00:00 -0400"
305,Inaugural J-WAFS Grand Challenge aims to develop enhanced crop variants and move them from lab to land,https://news.mit.edu/2023/inaugural-j-wafs-grand-challenge-aims-develop-enhanced-crop-variants-0510,Matt Shoulders will lead an interdisciplinary team to improve RuBisCO — the photosynthesis enzyme thought to be the holy grail for improving agricultural yield.,[{'name': 'Carolyn Blais | Abdul Latif Jameel Water and Food Systems Lab'}],"Wed, 10 May 2023 13:00:00 -0400"
306,Chemists’ technique reveals whether antibodies neutralize SARS-CoV-2,https://news.mit.edu/2023/chemists-technique-neutralizing-antibodies-0510,The method could enable a rapid test to determine whether individuals are producing antibodies that help protect against Covid-19.,[{'name': 'Anne Trafton | MIT News Office'}],"Wed, 10 May 2023 09:00:00 -0400"
307,Two-component system could offer a new way to halt internal bleeding,https://news.mit.edu/2023/two-component-system-halt-internal-bleeding-0425,"The technology, which mimics the body’s natural clotting process, could help keep severely injured people alive until they are treated at a hospital.",[{'name': 'Anne Trafton | MIT News Office'}],"Tue, 25 Apr 2023 00:00:00 -0400"
308,Nanoparticles provoke immune response against tumors but avoid side effects,https://news.mit.edu/2023/nanoparticles-provoke-immune-response-against-tumors-0419,"In a new study, immunostimulatory drugs slowed tumor growth without producing systemic inflammation.",[{'name': 'Anne Trafton | MIT News Office'}],"Wed, 19 Apr 2023 14:00:00 -0400"
309,How the brain coordinates speaking and breathing,https://news.mit.edu/2024/how-brain-coordinates-speaking-and-breathing-0307,MIT neuroscientists have discovered a circuit that controls vocalization and makes sure that breathing is prioritized over speaking.,[{'name': 'Anne Trafton | MIT News'}],"Thu, 07 Mar 2024 14:00:00 -0500"
310,Deciphering the cellular mechanisms behind ALS,https://news.mit.edu/2024/ernest-fraenkel-deciphering-cellular-mechanisms-behind-als-0306,"Professor Ernest Fraenkel has decoded fundamental aspects of Huntington’s disease and glioblastoma, and is now using computation to better understand amyotrophic lateral sclerosis.",[{'name': 'Michaela Jarvis | School of Engineering'}],"Wed, 06 Mar 2024 16:00:00 -0500"
311,A noninvasive treatment for “chemo brain”,https://news.mit.edu/2024/noninvasive-treatment-chemo-brain-0306,Stimulating gamma brain waves may protect cancer patients from memory impairment and other cognitive effects of chemotherapy.,[{'name': 'Anne Trafton | MIT News'}],"Wed, 06 Mar 2024 14:00:00 -0500"
312,School of Science announces 2024 Infinite Expansion Awards,https://news.mit.edu/2024/school-science-infinite-expansion-awards-0304,Nine postdocs and research scientists honored for contributions to the Institute.,[{'name': 'School of Science'}],"Mon, 04 Mar 2024 17:20:00 -0500"
313,Exposure to different kinds of music influences how the brain interprets rhythm,https://news.mit.edu/2024/exposure-different-kinds-music-influences-how-brain-interprets-rhythm-0304,"A study of people in 15 countries reveals that while everyone favors rhythms with simple integer ratios, biases can vary quite a bit across societies.",[{'name': 'Anne Trafton | MIT News'}],"Mon, 04 Mar 2024 05:00:00 -0500"
314,How cognition changes before dementia hits,https://news.mit.edu/2024/how-cognition-changes-before-dementia-0229,Study finds language-processing difficulties are an indicator — in addition to memory loss — of amnestic mild cognitive impairment.,[{'name': 'Peter Dizikes | MIT News'}],"Thu, 29 Feb 2024 00:00:00 -0500"
315,Imaging method reveals new cells and structures in human brain tissue,https://news.mit.edu/2024/imaging-method-reveals-new-cells-structures-human-brain-tissue-0131,A new microscopy technique that enables high-resolution imaging could one day help doctors diagnose and treat brain tumors.,[{'name': 'Anne Trafton | MIT News'}],"Wed, 31 Jan 2024 14:00:00 -0500"
316,"Simons Center’s collaborative approach propels autism research, at MIT and beyond",https://news.mit.edu/2024/simons-center-collaborative-approach-propels-autism-research-0130,"Team-based targeted projects, multi-mentor fellowships ensure that scientists studying social cognition, behavior, and autism integrate multiple perspectives and approaches to pressing questions.",[{'name': 'David Orenstein | Simons Center for the Social Brain'}],"Tue, 30 Jan 2024 16:35:00 -0500"
317,Creating new skills and new connections with MIT’s Quantitative Methods Workshop,https://news.mit.edu/2024/quantitative-methods-workshop-0130,More than 80 students and faculty from a dozen collaborating institutions became immersed at the intersection of computation and life sciences and forged new ties to MIT and each other.,[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Tue, 30 Jan 2024 15:45:00 -0500"
318,"Professor Emeritus Peter Schiller, a pioneer researcher of the visual system, dies at 92",https://news.mit.edu/2024/professor-emeritus-peter-schiller-dies-0123,His wide-ranging and influential career included fundamental discoveries about how visual scenes and stimuli are processed from the retina through the cortical visual system.,[{'name': 'Department of Brain and Cognitive Sciences'}],"Tue, 23 Jan 2024 15:45:00 -0500"
319,How the brain responds to reward is linked to socioeconomic background,https://news.mit.edu/2024/how-brain-responds-reward-socioeconomic-background-0122,An MIT study finds the brains of children who grow up in less affluent households are less responsive to rewarding experiences.,[{'name': 'Anne Trafton | MIT News'}],"Mon, 22 Jan 2024 13:00:00 -0500"
320,Evidence that gamma rhythm stimulation can treat neurological disorders is emerging,https://news.mit.edu/2024/evidence-gamma-rhythm-stimulation-can-treat-neurological-disorders-emerging-0118,"Researchers survey a broadening landscape of studies showing what’s known, and what remains to be found, about the therapeutic potential of noninvasive sensory, electrical, or magnetic stimulation of gamma brain rhythms.",[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Thu, 18 Jan 2024 12:50:00 -0500"
321,Study reveals a universal pattern of brain wave frequencies,https://news.mit.edu/2024/study-reveals-universal-pattern-brain-wave-frequencies-0118,"Across mammalian species, brain waves are slower in deep cortical layers, while superficial layers generate faster rhythms.",[{'name': 'Anne Trafton | MIT News'}],"Thu, 18 Jan 2024 05:00:00 -0500"
322,Three honored with 2023 School of Science teaching prizes,https://news.mit.edu/2024/three-honored-school-science-teaching-prizes-0110,"Roger Levy, Pulin Li, and David McGee were nominated by peers and students for their exceptional instruction.",[{'name': 'School of Science'}],"Wed, 10 Jan 2024 15:55:00 -0500"
323,Juana De La O: Food for thought,https://news.mit.edu/2024/food-for-thought-juana-de-la-o-0110,This biology graduate student is building connections through her thesis work in mouse development and her passion for cooking and baking.,[{'name': 'Lillian Eden | Department of Biology'}],"Wed, 10 Jan 2024 15:00:00 -0500"
324,"Complex, unfamiliar sentences make the brain’s language network work harder",https://news.mit.edu/2024/complex-unfamiliar-sentences-brains-language-network-0103,"A new study finds that language regions in the left hemisphere light up when reading uncommon sentences, while straightforward sentences elicit little response.",[{'name': 'Anne Trafton | MIT News'}],"Wed, 03 Jan 2024 05:00:00 -0500"
325,Nanoparticle-delivered RNA reduces neuroinflammation in lab tests,https://news.mit.edu/2023/nanoparticle-delivered-rna-reduces-neuroinflammation-lab-tests-1215,"MIT researchers find that in mice and human cell cultures, lipid nanoparticles can deliver a potential therapy for inflammation in the brain, a prominent symptom in Alzheimer’s.",[{'name': 'David Orenstein | Picower Institute'}],"Fri, 15 Dec 2023 15:45:00 -0500"
326,Deep neural networks show promise as models of human hearing,https://news.mit.edu/2023/deep-neural-nets-show-promise-models-of-human-hearing-1213,Study shows computational models trained to perform auditory tasks display an internal organization similar to that of the human auditory cortex.,[{'name': 'Anne Trafton | MIT News'}],"Wed, 13 Dec 2023 14:00:00 -0500"
327,How a mutation in microglia elevates Alzheimer’s risk,https://news.mit.edu/2023/how-mutation-microglia-elevates-alzheimers-risk-1204,"A new study finds that microglia with mutant TREM2 protein reduce brain circuit connections, promote inflammation, and contribute to Alzheimer’s pathology in other ways.",[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Mon, 04 Dec 2023 16:00:00 -0500"
328,Elly Nedivi receives 2023 Kreig Cortical Kudos Discoverer Award,https://news.mit.edu/2023/award-honors-elly-nedivis-research-cortical-plasticity-1129,The neuroscientist is recognized for her ongoing work to understand molecular and cellular mechanisms that enable the brain to adapt to experience.,[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Wed, 29 Nov 2023 11:00:00 -0500"
329,A new way to see the activity inside a living cell,https://news.mit.edu/2023/new-method-fluorescent-labels-living-cell-1128,"Using fluorescent labels that switch on and off, MIT engineers can study how molecules in a cell interact to control the cell’s behavior.",[{'name': 'Anne Trafton | MIT News'}],"Tue, 28 Nov 2023 11:00:00 -0500"
330,Search algorithm reveals nearly 200 new kinds of CRISPR systems,https://news.mit.edu/2023/search-algorithm-reveals-nearly-200-new-kinds-crispr-systems-1123,"By analyzing bacterial data, researchers have discovered thousands of rare new CRISPR systems that have a range of functions and could enable gene editing, diagnostics, and more.",[{'name': 'Allessandra DiCorato | Broad Institute'}],"Thu, 23 Nov 2023 14:00:00 -0500"
331,Three MIT affiliates receive Schmidt awards,https://news.mit.edu/2023/schmidt-awards-1120,"Jörn Dunkel and Surya Ganguli ’98, MNG ’98 receive Science Polymath awards; Josh Tenenbaum is named AI2050 Senior Fellow.",[{'name': 'Sandi Miller | Department of Mathematics'}],"Mon, 20 Nov 2023 13:40:00 -0500"
332,Mark Bear wins Society for Neuroscience Julius Axelrod Prize,https://news.mit.edu/2023/mark-bear-wins-julius-axelrod-prize-1117,"Award recognizes professor's synaptic plasticity research, its translation to potential amblyopia and autism treatments, and his career of mentorship.",[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Fri, 17 Nov 2023 13:20:00 -0500"
333,Rewarding excellence in open data,https://news.mit.edu/2023/rewarding-excellence-in-open-data-1116,MIT researchers who share their data recognized at second annual awards celebration.,[{'name': 'Brigham Fay | MIT Libraries'}],"Thu, 16 Nov 2023 16:30:00 -0500"
334,Aging Brain Initiative symposium showcases “cutting-edge” research across MIT,https://news.mit.edu/2023/aging-brain-initiative-symposium-1116,"Seed projects, posters represent a wide range of labs working on technologies, therapeutic strategies, and fundamental research to advance understanding of age-related neurodegenerative disease.",[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Thu, 16 Nov 2023 16:20:00 -0500"
335,A new wave of treatment for Alzheimer’s disease,https://news.mit.edu/2023/new-wave-treatment-alzheimers-disease-li-huei-tsai-1115,Professor Li-Huei Tsai studies how brain waves can be used to treat neurodegenerative diseases such as Alzheimer’s.,[{'name': 'Eric Bender | MIT Industrial Liaison Program'}],"Wed, 15 Nov 2023 12:55:00 -0500"
336,Anesthesia technology precisely controls unconsciousness in animal tests,https://news.mit.edu/2023/anesthesia-technology-precisely-controls-unconsciousness-animal-tests-1107,An advanced closed-loop anesthesia delivery system that monitors brain state to tailor propofol dose and achieve exactly the desired level of unconsciousness could reduce post-op side effects.,[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Tue, 07 Nov 2023 09:50:00 -0500"
337,Using AI to optimize for rapid neural imaging,https://news.mit.edu/2023/using-ai-optimize-rapid-neural-imaging-1106,"MIT CSAIL researchers combine AI and electron microscopy to expedite detailed brain network mapping, aiming to enhance connectomics research and clinical pathology.",[{'name': 'Rachel Gordon | MIT CSAIL'}],"Mon, 06 Nov 2023 13:00:00 -0500"
338,How “blue” and “green” appeared in a language that didn’t have words for them,https://news.mit.edu/2023/how-blue-and-green-appeared-language-1102,"People of a remote Amazonian society who learned Spanish as a second language began to interpret colors in a new way, an MIT study has found.",[{'name': 'Anne Trafton | MIT News'}],"Thu, 02 Nov 2023 00:00:00 -0400"
339,The brain may learn about the world the same way some computational models do,https://news.mit.edu/2023/brain-self-supervised-computational-models-1030,"Two studies find “self-supervised” models, which learn about their environment from unlabeled data, can show activity patterns similar to those of the mammalian brain.",[{'name': 'Anne Trafton | MIT News'}],"Mon, 30 Oct 2023 00:00:00 -0400"
340,How adults understand what kids are saying,https://news.mit.edu/2023/how-adults-understand-what-kids-are-saying-1026,"It’s not easy to parse young children’s words, but adults’ beliefs about what children want to communicate helps make it possible, a new study finds.",[{'name': 'Anne Trafton | MIT News'}],"Thu, 26 Oct 2023 11:00:00 -0400"
341,New technique helps robots pack objects into a tight space,https://news.mit.edu/2023/new-technique-helps-robots-pack-objects-tight-space-1017,Researchers coaxed a family of generative AI models to work together to solve multistep robot manipulation problems.,[{'name': 'Adam Zewe | MIT News'}],"Tue, 17 Oct 2023 00:00:00 -0400"
342,Study: Deep neural networks don’t see the world the way we do,https://news.mit.edu/2023/study-deep-neural-networks-perception-1016,Images that humans perceive as completely unrelated can be classified as the same by computational models.,[{'name': 'Anne Trafton | MIT News'}],"Mon, 16 Oct 2023 11:00:00 -0400"
343,Using generative AI to improve software testing,https://news.mit.edu/2024/using-generative-ai-improve-software-testing-datacebo-0305,MIT spinout DataCebo helps companies bolster their datasets by creating synthetic data that mimic the real thing.,[{'name': 'Zach Winn | MIT News'}],"Tue, 05 Mar 2024 00:00:00 -0500"
344,Startup accelerates progress toward light-speed computing,https://news.mit.edu/2024/startup-lightmatter-accelerates-progress-toward-light-speed-computing-0301,"Lightmatter, founded by three MIT alumni, is using photonic technologies to reinvent how chips communicate and calculate.",[{'name': 'Zach Winn | MIT News'}],"Fri, 01 Mar 2024 00:00:00 -0500"
345,Dealing with the limitations of our noisy world,https://news.mit.edu/2024/tamara-broderick-quantifying-uncertainty-0301,Tamara Broderick uses statistical approaches to understand and quantify the uncertainty that can affect study results.,[{'name': 'Adam Zewe | MIT News'}],"Fri, 01 Mar 2024 00:00:00 -0500"
346,Brain surgery training from an avatar,https://news.mit.edu/2024/brain-surgery-training-avatar-0229,MIT.nano Immersion Lab works with AR/VR startup to create transcontinental medical instruction.,[{'name': 'Becky Ham | MIT.nano'}],"Thu, 29 Feb 2024 16:30:00 -0500"
347,3 Questions: Shaping the future of work in an age of AI,https://news.mit.edu/2024/3-questions-shaping-future-work-age-of-ai-0228,"Daron Acemoglu, David Autor, and Simon Johnson, faculty co-directors of the new MIT Shaping the Future of Work Initiative, describe why the work matters and what they hope to achieve.",[{'name': 'Blueprint Labs'}],"Wed, 28 Feb 2024 16:40:00 -0500"
348,Sadhana Lolla named 2024 Gates Cambridge Scholar,https://news.mit.edu/2024/sadhana-lolla-gates-cambridge-scholar-0227,The MIT senior will pursue graduate studies in technology policy at Cambridge University.,[{'name': 'Julia Mongo | Office of Distinguished Fellowships'}],"Tue, 27 Feb 2024 16:10:00 -0500"
349,New AI model could streamline operations in a robotic warehouse,https://news.mit.edu/2024/new-ai-model-could-streamline-operations-robotic-warehouse-0227,"By breaking an intractable problem into smaller chunks, a deep-learning technique identifies the optimal areas for thinning out traffic in a warehouse.",[{'name': 'Adam Zewe | MIT News'}],"Tue, 27 Feb 2024 00:00:00 -0500"
350,“We offer another place for knowledge”,https://news.mit.edu/2024/we-offer-another-place-knowledge-0226,"After acquiring data science and AI skills from MIT, Jospin Hassan shared them with his community in the Dzaleka Refugee Camp in Malawi and built pathways for talented learners.",[{'name': 'Katherine Ouellette | MIT Open Learning'}],"Mon, 26 Feb 2024 14:35:00 -0500"
351,Generative AI for smart grid modeling,https://news.mit.edu/2024/generative-ai-smart-grid-modeling-0226,MIT LIDS awarded funding from the Appalachian Regional Commission as part of a multi-state collaborative project to model and test new smart grid technologies for use in rural areas.,[{'name': 'MIT Laboratory for Information and Decision Systems'}],"Mon, 26 Feb 2024 14:30:00 -0500"
352,Putting AI into the hands of people with problems to solve,https://news.mit.edu/2024/pienso-putting-user-friendly-ai-problem-solving-0226,Alumni-founded Pienso has developed a user-friendly AI builder so domain experts can build solutions without writing any code.,[{'name': 'Zach Winn | MIT News'}],"Mon, 26 Feb 2024 00:00:00 -0500"
353,New model identifies drugs that shouldn’t be taken together,https://news.mit.edu/2024/new-model-identifies-drugs-shouldnt-be-taken-together-0220,"Using a machine-learning algorithm, researchers can predict interactions that could interfere with a drug’s effectiveness.",[{'name': 'Anne Trafton | MIT News'}],"Tue, 20 Feb 2024 05:00:00 -0500"
354,"This tiny, tamper-proof ID tag can authenticate almost anything",https://news.mit.edu/2024/tiny-tamper-proof-id-tag-can-authenticate-almost-anything-0218,MIT engineers developed a tag that can reveal with near-perfect accuracy whether an item is real or fake. The key is in the glue on the back of the tag.,[{'name': 'Adam Zewe | MIT News'}],"Sun, 18 Feb 2024 00:00:00 -0500"
355,Using AI to discover stiff and tough microstructures,https://news.mit.edu/2024/using-ai-discover-stiff-tough-microstructures-0214,Innovative AI system from MIT CSAIL melds simulations and physical testing to forge materials with newfound durability and flexibility for diverse engineering uses.,[{'name': 'Rachel Gordon | MIT CSAIL'}],"Wed, 14 Feb 2024 11:40:00 -0500"
356,A new way to let AI chatbots converse all day without crashing,https://news.mit.edu/2024/new-way-let-ai-chatbots-converse-all-day-without-crashing-0213,Researchers developed a simple yet effective solution for a puzzling problem that can worsen the performance of large language models such as ChatGPT.,[{'name': 'Adam Zewe | MIT News'}],"Tue, 13 Feb 2024 00:00:00 -0500"
357,Safer skies with self-flying helicopters,https://news.mit.edu/2024/safer-skies-self-flying-helicopters-rotor-technologies-0209,"Autonomous helicopters made by Rotor Technologies, a startup led by MIT alumni, take the human out of risky commercial missions.",[{'name': 'Zach Winn | MIT News'}],"Fri, 09 Feb 2024 00:00:00 -0500"
358,Six MIT students selected as spring 2024 MIT-Pillar AI Collective Fellows,https://news.mit.edu/2024/mit-pillar-ai-collective-fellows-0206,"The graduate students will aim to commercialize innovations in AI, machine learning, and data science.",[{'name': 'School of Engineering'}],"Tue, 06 Feb 2024 16:50:00 -0500"
359,How symmetry can come to the aid of machine learning,https://news.mit.edu/2024/how-symmetry-can-aid-machine-learning-0205,"Exploiting the symmetry within datasets, MIT researchers show, can decrease the amount of data needed for training neural networks.",[{'name': 'Steve Nadis | MIT CSAIL'}],"Mon, 05 Feb 2024 10:10:00 -0500"
360,Doctors have more difficulty diagnosing disease when looking at images of darker skin,https://news.mit.edu/2024/doctors-more-difficulty-diagnosing-diseases-images-darker-skin-0205,"Dermatologists and general practitioners are somewhat less accurate in diagnosing disease in darker skin, a new study finds. Used correctly, AI may be able to help.",[{'name': 'Anne Trafton | MIT News'}],"Mon, 05 Feb 2024 05:00:00 -0500"
361,Generating the policy of tomorrow,https://news.mit.edu/2024/generating-policy-of-tomorrow-0124,"Hundreds of participants from around the world joined the sixth annual MIT Policy Hackathon to develop data-informed policy solutions to challenges in health, housing, and more.","[{'name': 'Kaitlin Provencher | Institute for Data, Systems, and Society'}]","Wed, 24 Jan 2024 15:25:00 -0500"
362,Q&A: A blueprint for sustainable innovation,https://news.mit.edu/2024/qa-atacama-biomaterials-blueprint-sustainable-innovation-0124,"Atacama Biomaterials, co-founded by Paloma Gonzalez-Rojas SM ’15, PhD ’21, combines architecture, machine learning, and chemical engineering to create eco-friendly materials.",[{'name': 'Adelaide Zollinger | MIT Morningside Academy for Design'}],"Wed, 24 Jan 2024 14:00:00 -0500"
363,What to do about AI in health?,https://news.mit.edu/2024/what-to-do-about-ai-in-health-0123,"Although artificial intelligence in health has shown great promise, pressure is mounting for regulators around the world to act, as AI tools demonstrate potentially harmful outcomes.",[{'name': 'Alex Ouyang | Abdul Latif Jameel Clinic for Machine Learning in Health'}],"Tue, 23 Jan 2024 16:25:00 -0500"
364,New hope for early pancreatic cancer intervention via AI-based risk prediction,https://news.mit.edu/2024/new-hope-early-pancreatic-cancer-intervention-ai-based-risk-prediction-0118,MIT CSAIL researchers develop advanced machine-learning models that outperform current methods in detecting pancreatic ductal adenocarcinoma.,[{'name': 'Rachel Gordon | MIT CSAIL'}],"Thu, 18 Jan 2024 14:00:00 -0500"
365,Reasoning and reliability in AI,https://news.mit.edu/2024/reasoning-and-reliability-in-ai-0118,PhD students interning with the MIT-IBM Watson AI Lab look to improve natural language usage.,[{'name': 'Lauren Hinkel | MIT-IBM Watson AI Lab'}],"Thu, 18 Jan 2024 13:00:00 -0500"
366,Stratospheric safety standards: How aviation could steer regulation of AI in health,https://news.mit.edu/2024/stratospheric-safety-standards-how-aviation-could-steer-ai-health-regulation-0117,An interdisciplinary team of researchers thinks health AI could benefit from some of the aviation industry’s long history of hard-won lessons that have created one of the safest activities today.,[{'name': 'Alex Ouyang | Abdul Latif Jameel Clinic for Machine Learning in Health'}],"Wed, 17 Jan 2024 12:50:00 -0500"
367,Multiple AI models help robots execute complex plans more transparently,https://news.mit.edu/2024/multiple-ai-models-help-robots-execute-complex-plans-more-transparently-0108,"A multimodal system uses models trained on language, vision, and action data to help robots develop and execute plans for household, construction, and manufacturing tasks.",[{'name': 'Alex Shipps | MIT CSAIL'}],"Mon, 08 Jan 2024 15:15:00 -0500"
368,Technique could efficiently solve partial differential equations for numerous applications,https://news.mit.edu/2024/peds-technique-could-efficiently-solve-partial-differential-equations-0108,"MIT researchers propose “PEDS” method for developing models of complex physical systems in mechanics, optics, thermal transport, fluid dynamics, physical chemistry, climate, and more.",[{'name': 'Sandi Miller | Department of Mathematics'}],"Mon, 08 Jan 2024 13:30:00 -0500"
369,AI agents help explain other AI systems,https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103,MIT researchers introduce a method that uses artificial intelligence to automate the explanation of complex neural networks.,[{'name': 'Rachel Gordon | MIT CSAIL'}],"Wed, 03 Jan 2024 15:10:00 -0500"
370,"Complex, unfamiliar sentences make the brain’s language network work harder",https://news.mit.edu/2024/complex-unfamiliar-sentences-brains-language-network-0103,"A new study finds that language regions in the left hemisphere light up when reading uncommon sentences, while straightforward sentences elicit little response.",[{'name': 'Anne Trafton | MIT News'}],"Wed, 03 Jan 2024 05:00:00 -0500"
371,The creative future of generative AI,https://news.mit.edu/2024/creative-future-generative-ai-0102,An MIT panel charts how artificial intelligence will impact art and design.,[{'name': 'Ken Shulman | Arts at MIT'}],"Tue, 02 Jan 2024 15:15:00 -0500"
372,Leveraging language to understand machines,https://news.mit.edu/2023/leveraging-language-understand-machines-1222,Master’s students Irene Terpstra ’23 and Rujul Gandhi ’22 use language to design new integrated circuits and make it understandable to robots.,[{'name': 'Lauren Hinkel | MIT-IBM Watson AI Lab'}],"Fri, 22 Dec 2023 12:45:00 -0500"
373,MIT in the media: 2023 in review,https://news.mit.edu/2023/mit-media-year-in-review-1221,MIT community members made headlines with key research advances and their efforts to tackle pressing challenges.,[{'name': 'MIT News'}],"Thu, 21 Dec 2023 00:00:00 -0500"
374,"Using AI, MIT researchers identify a new class of antibiotic candidates",https://news.mit.edu/2023/using-ai-mit-researchers-identify-antibiotic-candidates-1220,"These compounds can kill methicillin-resistant Staphylococcus aureus (MRSA), a bacterium that causes deadly infections.",[{'name': 'Anne Trafton | MIT News'}],"Wed, 20 Dec 2023 11:00:00 -0500"
375,A flexible solution to help artists improve animation,https://news.mit.edu/2023/flexible-solution-help-artists-improve-animation-1220,This new method draws on 200-year-old geometric foundations to give artists control over the appearance of animated characters.,[{'name': 'Adam Zewe | MIT News'}],"Wed, 20 Dec 2023 00:00:00 -0500"
376,Image recognition accuracy: An unseen challenge confounding today’s AI,https://news.mit.edu/2023/image-recognition-accuracy-minimum-viewing-time-metric-1215,“Minimum viewing time” benchmark gauges image recognition complexity for AI systems by measuring the time needed for accurate human identification.,[{'name': 'Rachel Gordon | MIT CSAIL'}],"Fri, 15 Dec 2023 12:35:00 -0500"
377,Computational model captures the elusive transition states of chemical reactions,https://news.mit.edu/2023/computational-model-captures-elusive-transition-states-1215,"Using generative AI, MIT chemical engineers and chemists created a model that can predict the structures formed when a chemical reaction reaches its point of no return.",[{'name': 'Anne Trafton | MIT News'}],"Fri, 15 Dec 2023 05:00:00 -0500"
378,AI meets climate: MIT Energy and Climate Hack 2023,https://news.mit.edu/2023/ai-meets-climate-mit-energy-climate-hack-1214,The Energy and Climate Hack presented opportunities for students and companies to collaborate and develop innovative solutions.,[{'name': 'Johnathon Horn | Division of Student Life'}],"Thu, 14 Dec 2023 16:55:00 -0500"
379,Three MIT students selected as inaugural MIT-Pillar AI Collective Fellows,https://news.mit.edu/2023/inaugural-mit-pillar-ai-collective-fellows-1213,"The graduate students will aim to commercialize innovations in AI, machine learning, and data science.",[{'name': 'School of Engineering'}],"Wed, 13 Dec 2023 17:00:00 -0500"
380,Deep neural networks show promise as models of human hearing,https://news.mit.edu/2023/deep-neural-nets-show-promise-models-of-human-hearing-1213,Study shows computational models trained to perform auditory tasks display an internal organization similar to that of the human auditory cortex.,[{'name': 'Anne Trafton | MIT News'}],"Wed, 13 Dec 2023 14:00:00 -0500"
381,Closing the design-to-manufacturing gap for optical devices,https://news.mit.edu/2023/closing-design-manufacturing-gap-optical-devices-1213,"A new method enables optical devices that more closely match their design specifications, boosting accuracy and efficiency.",[{'name': 'Adam Zewe | MIT News'}],"Wed, 13 Dec 2023 00:00:00 -0500"
382,A computer scientist pushes the boundaries of geometry,https://news.mit.edu/2023/justin-solomon-pushing-geometric-boundaries-1212,"Justin Solomon applies modern geometric techniques to solve problems in computer vision, machine learning, statistics, and beyond.",[{'name': 'Adam Zewe | MIT News'}],"Tue, 12 Dec 2023 14:05:00 -0500"
383,MIT Generative AI Week fosters dialogue across disciplines,https://news.mit.edu/2023/mit-generative-ai-week-fosters-dialogue-across-disciplines-1211,"During the last week of November, MIT hosted symposia and events aimed at examining the implications and possibilities of generative AI.",[{'name': 'Mary Beth Gallagher | School of Engineering'}],"Mon, 11 Dec 2023 16:25:00 -0500"
384,MIT group releases white papers on governance of AI,https://news.mit.edu/2023/mit-group-releases-white-papers-governance-ai-1211,The series aims to help policymakers create better oversight of AI in society.,[{'name': 'Peter Dizikes | MIT News'}],"Mon, 11 Dec 2023 00:00:00 -0500"
385,Automated system teaches users when to collaborate with an AI assistant,https://news.mit.edu/2023/automated-system-teaches-collaborate-ai-assistant-1208,MIT researchers develop a customized onboarding process that helps a human learn when a model’s advice is trustworthy.,[{'name': 'Adam Zewe | MIT News'}],"Fri, 08 Dec 2023 00:00:00 -0500"
386,MIT engineers develop a way to determine how the surfaces of materials behave,https://news.mit.edu/2023/mit-engineers-how-surfaces-materials-behave-1207,"Using machine learning, the computational method can provide details of how materials work as catalysts, semiconductors, or battery components.",[{'name': 'David L. Chandler | MIT News'}],"Thu, 07 Dec 2023 05:00:00 -0500"
387,Eric Evans to step down as director of MIT Lincoln Laboratory,https://news.mit.edu/2023/eric-evans-steps-down-director-lincoln-laboratory-1206,"During 18 years of leadership, Evans established new R&amp;D mission areas, strengthened ties to the MIT community, and increased inclusion and education efforts.",[{'name': 'Zach Winn | MIT News'}],"Wed, 06 Dec 2023 12:30:00 -0500"
388,AI accelerates problem-solving in complex scenarios,https://news.mit.edu/2023/ai-accelerates-problem-solving-complex-scenarios-1205,"A new, data-driven approach could lead to better solutions for tricky optimization problems like global package routing or power grid operation.",[{'name': 'Adam Zewe | MIT News'}],"Tue, 05 Dec 2023 00:00:00 -0500"
389,What does the future hold for generative AI?,https://news.mit.edu/2023/what-does-future-hold-generative-ai-1129,"Rodney Brooks, co-founder of iRobot, kicks off an MIT symposium on the promise and potential pitfalls of increasingly powerful AI tools like ChatGPT.",[{'name': 'Adam Zewe | MIT News'}],"Wed, 29 Nov 2023 16:00:00 -0500"
390,New method uses crowdsourced feedback to help train robots,https://news.mit.edu/2023/method-uses-crowdsourced-feedback-help-train-robots-1127,"Human Guided Exploration (HuGE) enables AI agents to learn quickly with some help from humans, even if the humans make mistakes.",[{'name': 'Adam Zewe | MIT News'}],"Mon, 27 Nov 2023 00:00:00 -0500"
391,Search algorithm reveals nearly 200 new kinds of CRISPR systems,https://news.mit.edu/2023/search-algorithm-reveals-nearly-200-new-kinds-crispr-systems-1123,"By analyzing bacterial data, researchers have discovered thousands of rare new CRISPR systems that have a range of functions and could enable gene editing, diagnostics, and more.",[{'name': 'Allessandra DiCorato | Broad Institute'}],"Thu, 23 Nov 2023 14:00:00 -0500"
392,Technique enables AI on edge devices to keep learning over time,https://news.mit.edu/2023/technique-enables-ai-edge-devices-keep-learning-over-time,"With the PockEngine training method, machine-learning models can efficiently and continuously learn from user data on edge devices like smartphones.",[{'name': 'Adam Zewe | MIT News'}],"Thu, 16 Nov 2023 00:00:00 -0500"
393,How the brain coordinates speaking and breathing,https://news.mit.edu/2024/how-brain-coordinates-speaking-and-breathing-0307,MIT neuroscientists have discovered a circuit that controls vocalization and makes sure that breathing is prioritized over speaking.,[{'name': 'Anne Trafton | MIT News'}],"Thu, 07 Mar 2024 14:00:00 -0500"
394,Deciphering the cellular mechanisms behind ALS,https://news.mit.edu/2024/ernest-fraenkel-deciphering-cellular-mechanisms-behind-als-0306,"Professor Ernest Fraenkel has decoded fundamental aspects of Huntington’s disease and glioblastoma, and is now using computation to better understand amyotrophic lateral sclerosis.",[{'name': 'Michaela Jarvis | School of Engineering'}],"Wed, 06 Mar 2024 16:00:00 -0500"
395,A noninvasive treatment for “chemo brain”,https://news.mit.edu/2024/noninvasive-treatment-chemo-brain-0306,Stimulating gamma brain waves may protect cancer patients from memory impairment and other cognitive effects of chemotherapy.,[{'name': 'Anne Trafton | MIT News'}],"Wed, 06 Mar 2024 14:00:00 -0500"
396,School of Science announces 2024 Infinite Expansion Awards,https://news.mit.edu/2024/school-science-infinite-expansion-awards-0304,Nine postdocs and research scientists honored for contributions to the Institute.,[{'name': 'School of Science'}],"Mon, 04 Mar 2024 17:20:00 -0500"
397,Exposure to different kinds of music influences how the brain interprets rhythm,https://news.mit.edu/2024/exposure-different-kinds-music-influences-how-brain-interprets-rhythm-0304,"A study of people in 15 countries reveals that while everyone favors rhythms with simple integer ratios, biases can vary quite a bit across societies.",[{'name': 'Anne Trafton | MIT News'}],"Mon, 04 Mar 2024 05:00:00 -0500"
398,How cognition changes before dementia hits,https://news.mit.edu/2024/how-cognition-changes-before-dementia-0229,Study finds language-processing difficulties are an indicator — in addition to memory loss — of amnestic mild cognitive impairment.,[{'name': 'Peter Dizikes | MIT News'}],"Thu, 29 Feb 2024 00:00:00 -0500"
399,Imaging method reveals new cells and structures in human brain tissue,https://news.mit.edu/2024/imaging-method-reveals-new-cells-structures-human-brain-tissue-0131,A new microscopy technique that enables high-resolution imaging could one day help doctors diagnose and treat brain tumors.,[{'name': 'Anne Trafton | MIT News'}],"Wed, 31 Jan 2024 14:00:00 -0500"
400,"Simons Center’s collaborative approach propels autism research, at MIT and beyond",https://news.mit.edu/2024/simons-center-collaborative-approach-propels-autism-research-0130,"Team-based targeted projects, multi-mentor fellowships ensure that scientists studying social cognition, behavior, and autism integrate multiple perspectives and approaches to pressing questions.",[{'name': 'David Orenstein | Simons Center for the Social Brain'}],"Tue, 30 Jan 2024 16:35:00 -0500"
401,Creating new skills and new connections with MIT’s Quantitative Methods Workshop,https://news.mit.edu/2024/quantitative-methods-workshop-0130,More than 80 students and faculty from a dozen collaborating institutions became immersed at the intersection of computation and life sciences and forged new ties to MIT and each other.,[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Tue, 30 Jan 2024 15:45:00 -0500"
402,"Professor Emeritus Peter Schiller, a pioneer researcher of the visual system, dies at 92",https://news.mit.edu/2024/professor-emeritus-peter-schiller-dies-0123,His wide-ranging and influential career included fundamental discoveries about how visual scenes and stimuli are processed from the retina through the cortical visual system.,[{'name': 'Department of Brain and Cognitive Sciences'}],"Tue, 23 Jan 2024 15:45:00 -0500"
403,How the brain responds to reward is linked to socioeconomic background,https://news.mit.edu/2024/how-brain-responds-reward-socioeconomic-background-0122,An MIT study finds the brains of children who grow up in less affluent households are less responsive to rewarding experiences.,[{'name': 'Anne Trafton | MIT News'}],"Mon, 22 Jan 2024 13:00:00 -0500"
404,Evidence that gamma rhythm stimulation can treat neurological disorders is emerging,https://news.mit.edu/2024/evidence-gamma-rhythm-stimulation-can-treat-neurological-disorders-emerging-0118,"Researchers survey a broadening landscape of studies showing what’s known, and what remains to be found, about the therapeutic potential of noninvasive sensory, electrical, or magnetic stimulation of gamma brain rhythms.",[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Thu, 18 Jan 2024 12:50:00 -0500"
405,Study reveals a universal pattern of brain wave frequencies,https://news.mit.edu/2024/study-reveals-universal-pattern-brain-wave-frequencies-0118,"Across mammalian species, brain waves are slower in deep cortical layers, while superficial layers generate faster rhythms.",[{'name': 'Anne Trafton | MIT News'}],"Thu, 18 Jan 2024 05:00:00 -0500"
406,Three honored with 2023 School of Science teaching prizes,https://news.mit.edu/2024/three-honored-school-science-teaching-prizes-0110,"Roger Levy, Pulin Li, and David McGee were nominated by peers and students for their exceptional instruction.",[{'name': 'School of Science'}],"Wed, 10 Jan 2024 15:55:00 -0500"
407,Juana De La O: Food for thought,https://news.mit.edu/2024/food-for-thought-juana-de-la-o-0110,This biology graduate student is building connections through her thesis work in mouse development and her passion for cooking and baking.,[{'name': 'Lillian Eden | Department of Biology'}],"Wed, 10 Jan 2024 15:00:00 -0500"
408,"Complex, unfamiliar sentences make the brain’s language network work harder",https://news.mit.edu/2024/complex-unfamiliar-sentences-brains-language-network-0103,"A new study finds that language regions in the left hemisphere light up when reading uncommon sentences, while straightforward sentences elicit little response.",[{'name': 'Anne Trafton | MIT News'}],"Wed, 03 Jan 2024 05:00:00 -0500"
409,Nanoparticle-delivered RNA reduces neuroinflammation in lab tests,https://news.mit.edu/2023/nanoparticle-delivered-rna-reduces-neuroinflammation-lab-tests-1215,"MIT researchers find that in mice and human cell cultures, lipid nanoparticles can deliver a potential therapy for inflammation in the brain, a prominent symptom in Alzheimer’s.",[{'name': 'David Orenstein | Picower Institute'}],"Fri, 15 Dec 2023 15:45:00 -0500"
410,Deep neural networks show promise as models of human hearing,https://news.mit.edu/2023/deep-neural-nets-show-promise-models-of-human-hearing-1213,Study shows computational models trained to perform auditory tasks display an internal organization similar to that of the human auditory cortex.,[{'name': 'Anne Trafton | MIT News'}],"Wed, 13 Dec 2023 14:00:00 -0500"
411,How a mutation in microglia elevates Alzheimer’s risk,https://news.mit.edu/2023/how-mutation-microglia-elevates-alzheimers-risk-1204,"A new study finds that microglia with mutant TREM2 protein reduce brain circuit connections, promote inflammation, and contribute to Alzheimer’s pathology in other ways.",[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Mon, 04 Dec 2023 16:00:00 -0500"
412,Elly Nedivi receives 2023 Kreig Cortical Kudos Discoverer Award,https://news.mit.edu/2023/award-honors-elly-nedivis-research-cortical-plasticity-1129,The neuroscientist is recognized for her ongoing work to understand molecular and cellular mechanisms that enable the brain to adapt to experience.,[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Wed, 29 Nov 2023 11:00:00 -0500"
413,A new way to see the activity inside a living cell,https://news.mit.edu/2023/new-method-fluorescent-labels-living-cell-1128,"Using fluorescent labels that switch on and off, MIT engineers can study how molecules in a cell interact to control the cell’s behavior.",[{'name': 'Anne Trafton | MIT News'}],"Tue, 28 Nov 2023 11:00:00 -0500"
414,Search algorithm reveals nearly 200 new kinds of CRISPR systems,https://news.mit.edu/2023/search-algorithm-reveals-nearly-200-new-kinds-crispr-systems-1123,"By analyzing bacterial data, researchers have discovered thousands of rare new CRISPR systems that have a range of functions and could enable gene editing, diagnostics, and more.",[{'name': 'Allessandra DiCorato | Broad Institute'}],"Thu, 23 Nov 2023 14:00:00 -0500"
415,Three MIT affiliates receive Schmidt awards,https://news.mit.edu/2023/schmidt-awards-1120,"Jörn Dunkel and Surya Ganguli ’98, MNG ’98 receive Science Polymath awards; Josh Tenenbaum is named AI2050 Senior Fellow.",[{'name': 'Sandi Miller | Department of Mathematics'}],"Mon, 20 Nov 2023 13:40:00 -0500"
416,Mark Bear wins Society for Neuroscience Julius Axelrod Prize,https://news.mit.edu/2023/mark-bear-wins-julius-axelrod-prize-1117,"Award recognizes professor's synaptic plasticity research, its translation to potential amblyopia and autism treatments, and his career of mentorship.",[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Fri, 17 Nov 2023 13:20:00 -0500"
417,Rewarding excellence in open data,https://news.mit.edu/2023/rewarding-excellence-in-open-data-1116,MIT researchers who share their data recognized at second annual awards celebration.,[{'name': 'Brigham Fay | MIT Libraries'}],"Thu, 16 Nov 2023 16:30:00 -0500"
418,Aging Brain Initiative symposium showcases “cutting-edge” research across MIT,https://news.mit.edu/2023/aging-brain-initiative-symposium-1116,"Seed projects, posters represent a wide range of labs working on technologies, therapeutic strategies, and fundamental research to advance understanding of age-related neurodegenerative disease.",[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Thu, 16 Nov 2023 16:20:00 -0500"
419,A new wave of treatment for Alzheimer’s disease,https://news.mit.edu/2023/new-wave-treatment-alzheimers-disease-li-huei-tsai-1115,Professor Li-Huei Tsai studies how brain waves can be used to treat neurodegenerative diseases such as Alzheimer’s.,[{'name': 'Eric Bender | MIT Industrial Liaison Program'}],"Wed, 15 Nov 2023 12:55:00 -0500"
420,Anesthesia technology precisely controls unconsciousness in animal tests,https://news.mit.edu/2023/anesthesia-technology-precisely-controls-unconsciousness-animal-tests-1107,An advanced closed-loop anesthesia delivery system that monitors brain state to tailor propofol dose and achieve exactly the desired level of unconsciousness could reduce post-op side effects.,[{'name': 'David Orenstein | The Picower Institute for Learning and Memory'}],"Tue, 07 Nov 2023 09:50:00 -0500"
421,Using AI to optimize for rapid neural imaging,https://news.mit.edu/2023/using-ai-optimize-rapid-neural-imaging-1106,"MIT CSAIL researchers combine AI and electron microscopy to expedite detailed brain network mapping, aiming to enhance connectomics research and clinical pathology.",[{'name': 'Rachel Gordon | MIT CSAIL'}],"Mon, 06 Nov 2023 13:00:00 -0500"
422,How “blue” and “green” appeared in a language that didn’t have words for them,https://news.mit.edu/2023/how-blue-and-green-appeared-language-1102,"People of a remote Amazonian society who learned Spanish as a second language began to interpret colors in a new way, an MIT study has found.",[{'name': 'Anne Trafton | MIT News'}],"Thu, 02 Nov 2023 00:00:00 -0400"
423,The brain may learn about the world the same way some computational models do,https://news.mit.edu/2023/brain-self-supervised-computational-models-1030,"Two studies find “self-supervised” models, which learn about their environment from unlabeled data, can show activity patterns similar to those of the mammalian brain.",[{'name': 'Anne Trafton | MIT News'}],"Mon, 30 Oct 2023 00:00:00 -0400"
424,How adults understand what kids are saying,https://news.mit.edu/2023/how-adults-understand-what-kids-are-saying-1026,"It’s not easy to parse young children’s words, but adults’ beliefs about what children want to communicate helps make it possible, a new study finds.",[{'name': 'Anne Trafton | MIT News'}],"Thu, 26 Oct 2023 11:00:00 -0400"
425,New technique helps robots pack objects into a tight space,https://news.mit.edu/2023/new-technique-helps-robots-pack-objects-tight-space-1017,Researchers coaxed a family of generative AI models to work together to solve multistep robot manipulation problems.,[{'name': 'Adam Zewe | MIT News'}],"Tue, 17 Oct 2023 00:00:00 -0400"
426,Study: Deep neural networks don’t see the world the way we do,https://news.mit.edu/2023/study-deep-neural-networks-perception-1016,Images that humans perceive as completely unrelated can be classified as the same by computational models.,[{'name': 'Anne Trafton | MIT News'}],"Mon, 16 Oct 2023 11:00:00 -0400"
427,What Is Industrial Generative AI?,https://zapata.ai/what-is-industrial-generative-ai/,<p>The post <a href=\"https://zapata.ai/what-is-industrial-generative-ai/\" rel=\"nofollow\">What Is Industrial Generative AI?</a> appeared first on <a href=\"https://zapata.ai\" rel=\"nofollow\">Zapata AI</a>.</p>,[{'name': 'Rob Kerstens'}],"Fri, 15 Dec 2023 21:34:53 +0000"
428,"Introducing BenchQ, the Result of Our Work With DARPA",https://zapata.ai/darpa-quantum-benchmarking-phase-i-results/,"<p>The post <a href=\"https://zapata.ai/darpa-quantum-benchmarking-phase-i-results/\" rel=\"nofollow\">Introducing BenchQ, the Result of Our Work With DARPA</a> appeared first on <a href=\"https://zapata.ai\" rel=\"nofollow\">Zapata AI</a>.</p>",[{'name': 'Rob Kerstens'}],"Mon, 11 Dec 2023 21:21:53 +0000"
429,How Quantum Science Enhances Generative AI,https://zapata.ai/how-quantum-science-enhances-generative-ai/,<p>The post <a href=\"https://zapata.ai/how-quantum-science-enhances-generative-ai/\" rel=\"nofollow\">How Quantum Science Enhances Generative AI</a> appeared first on <a href=\"https://zapata.ai\" rel=\"nofollow\">Zapata AI</a>.</p>,[{'name': 'Rob Kerstens'}],"Mon, 27 Nov 2023 17:22:31 +0000"
430,Podcast Recap: Zapata AI CEO Answers Potential Investor Questions on Industrial Generative AI,https://zapata.ai/podcast-recap-industrial-generative-ai/,<p>The post <a href=\"https://zapata.ai/podcast-recap-industrial-generative-ai/\" rel=\"nofollow\">Podcast Recap: Zapata AI CEO Answers Potential Investor Questions on Industrial Generative AI</a> appeared first on <a href=\"https://zapata.ai\" rel=\"nofollow\">Zapata AI</a>.</p>,[{'name': 'Rob Kerstens'}],"Fri, 27 Oct 2023 14:40:28 +0000"
431,ICYMI: Zapata’s Generative AI Keynote at Quantum.Tech Boston,https://zapata.ai/zapata-generative-ai-keynote/,<p>The post <a href=\"https://zapata.ai/zapata-generative-ai-keynote/\" rel=\"nofollow\">ICYMI: Zapata’s Generative AI Keynote at Quantum.Tech Boston</a> appeared first on <a href=\"https://zapata.ai\" rel=\"nofollow\">Zapata AI</a>.</p>,[{'name': 'Rob Kerstens'}],"Wed, 26 Jul 2023 13:33:25 +0000"
432,How Generative AI Is Impacting Analytics,https://zapata.ai/how-generative-ai-is-impacting-analytics/,<p>The post <a href=\"https://zapata.ai/how-generative-ai-is-impacting-analytics/\" rel=\"nofollow\">How Generative AI Is Impacting Analytics</a> appeared first on <a href=\"https://zapata.ai\" rel=\"nofollow\">Zapata AI</a>.</p>,[{'name': 'Rob Kerstens'}],"Thu, 29 Jun 2023 11:00:25 +0000"
433,How We Harness Industrial Generative AI for Optimization,https://zapata.ai/industrial-generative-ai-for-optimization/,<p>The post <a href=\"https://zapata.ai/industrial-generative-ai-for-optimization/\" rel=\"nofollow\">How We Harness Industrial Generative AI for Optimization</a> appeared first on <a href=\"https://zapata.ai\" rel=\"nofollow\">Zapata AI</a>.</p>,[{'name': 'Rob Kerstens'}],"Fri, 16 Jun 2023 14:42:10 +0000"
434,New Research Shows How Quantum Generative Models Can Outperform Classical Models,https://zapata.ai/new-research-shows-how-quantum-generative-models-can-outperform-classical-models/,<p>The post <a href=\"https://zapata.ai/new-research-shows-how-quantum-generative-models-can-outperform-classical-models/\" rel=\"nofollow\">New Research Shows How Quantum Generative Models Can Outperform Classical Models</a> appeared first on <a href=\"https://zapata.ai\" rel=\"nofollow\">Zapata AI</a>.</p>,[{'name': 'Rob Kerstens'}],"Tue, 25 Apr 2023 11:08:29 +0000"
435,Quantum Computing for Sustainability: A Practical Guide,https://zapata.ai/quantum-computing-for-sustainability-a-practical-guide/,<p>The post <a href=\"https://zapata.ai/quantum-computing-for-sustainability-a-practical-guide/\" rel=\"nofollow\">Quantum Computing for Sustainability: A Practical Guide</a> appeared first on <a href=\"https://zapata.ai\" rel=\"nofollow\">Zapata AI</a>.</p>,[{'name': 'Rob Kerstens'}],"Wed, 01 Mar 2023 12:47:03 +0000"
436,Can Quantum Computing Be Part of the Solution to Climate Change?,https://zapata.ai/sustainable-quantum-computing-applications-climate-change/,<p>The post <a href=\"https://zapata.ai/sustainable-quantum-computing-applications-climate-change/\" rel=\"nofollow\">Can Quantum Computing Be Part of the Solution to Climate Change?</a> appeared first on <a href=\"https://zapata.ai\" rel=\"nofollow\">Zapata AI</a>.</p>,[{'name': 'Rob Kerstens'}],"Mon, 27 Feb 2023 21:54:11 +0000"
437,Core Scientific to Provide CoreWeave up to 16 MW of Data Center Infrastructure for AI and HPC,https://insidehpc.com/2024/03/core-scientific-to-provide-coreweave-up-to-16-mw-of-data-center-infrastructure-for-ai-and-hpc/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"99\" src=\"https://insidehpc.com/wp-content/uploads/2023/06/CoreWeave-logo-0623-150x99.jpg\" style=\"float: right; margin-left: 5px;\" width=\"150\" /><p>AUSTIN, Texas&#8211; Core Scientific, Inc. (Nasdaq: CORZ), a bitcoin mining and digital infrastructure company, and CoreWeave, a GPU cloud provider, today announced a multi-year contract for Core Scientific to supply up to 16 MW of data center infrastructure to CoreWeave. The total potential revenue associated with the contract is more than $100 million. Under the terms of the [&#8230;]</p>
<p>The post <a href=\"https://insidehpc.com/2024/03/core-scientific-to-provide-coreweave-up-to-16-mw-of-data-center-infrastructure-for-ai-and-hpc/\">Core Scientific to Provide CoreWeave up to 16 MW of Data Center Infrastructure for AI and HPC</a> appeared first on <a href=\"https://insidehpc.com\">High-Performance Computing News Analysis | insideHPC</a>.</p>",[{'name': 'staff'}],"Thu, 07 Mar 2024 21:01:31 +0000"
438,Vultr Expands Cloud Nvidia H100 GPU Capacity,https://insidehpc.com/2024/03/vultr-expands-cloud-nvidia-h100-gpu-capacity/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"85\" src=\"https://insidehpc.com/wp-content/uploads/2023/05/generic-HPC-supercomputing-shutterstock-1107716192-150x85.jpg\" style=\"float: right; margin-left: 5px;\" width=\"150\" /><p>March 5, 2024 &#8212; Cloud hosting and cloud servers company Vultr announced the expansion of our Seattle cloud data center region at Sabey Data Centers&#8217; SDC Columbia location in East Wenatchee, Washington. This expansion includes an increase in NVIDIA HGX H100 clusters, which are now available on demand and via reserved contracts. Vultr&#8217;s decision to [&#8230;]</p>
<p>The post <a href=\"https://insidehpc.com/2024/03/vultr-expands-cloud-nvidia-h100-gpu-capacity/\">Vultr Expands Cloud Nvidia H100 GPU Capacity</a> appeared first on <a href=\"https://insidehpc.com\">High-Performance Computing News Analysis | insideHPC</a>.</p>",[{'name': 'staff'}],"Thu, 07 Mar 2024 20:56:56 +0000"
439,Gates and CoolIT Systems Announce Liquid Cooling Collaboration,https://insidehpc.com/2024/03/gates-and-coolit-systems-announce-liquid-cooling-collaboration/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"211\" src=\"https://insidehpc.com/wp-content/uploads/2024/03/CoolIT-Systems-logo-2-1-324.png\" style=\"float: right; margin-left: 5px;\" width=\"422\" /><p>DENVER, March 6, 2024 &#8211; Gates (NYSE: GTES), a manufacturer of power transmission and fluid power solutions, announced its collaboration in data center cooling market with CoolIT Systems. CoolIT Systems is the world leader in scalable liquid cooling technology for the world’s most demanding computing environments. To meet the surging demand in the AI, high-performance computing (HPC) [&#8230;]</p>
<p>The post <a href=\"https://insidehpc.com/2024/03/gates-and-coolit-systems-announce-liquid-cooling-collaboration/\">Gates and CoolIT Systems Announce Liquid Cooling Collaboration</a> appeared first on <a href=\"https://insidehpc.com\">High-Performance Computing News Analysis | insideHPC</a>.</p>",[{'name': 'staff'}],"Thu, 07 Mar 2024 19:48:08 +0000"
440,NVIDIA CUDA-X to Be Integrated with HP AI Workstations,https://insidehpc.com/2024/03/nvidia-cuda-x-to-be-integrated-with-hp-ai-workstations/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"212\" src=\"https://insidehpc.com/wp-content/uploads/2024/02/Nvidia-logo-2-1-0224.png\" style=\"float: right; margin-left: 5px;\" width=\"426\" /><p>March 7, 2024 — NVIDIA and HP Inc. today announced that NVIDIA CUDA-X data processing libraries will be integrated with HP AI workstation solutions to turbocharge the data preparation and processing work that forms the foundation of generative AI development. Built on the NVIDIA CUDA compute platform, CUDA-X libraries speed data processing for a broad range of data [&#8230;]</p>
<p>The post <a href=\"https://insidehpc.com/2024/03/nvidia-cuda-x-to-be-integrated-with-hp-ai-workstations/\">NVIDIA CUDA-X to Be Integrated with HP AI Workstations</a> appeared first on <a href=\"https://insidehpc.com\">High-Performance Computing News Analysis | insideHPC</a>.</p>",[{'name': 'staff'}],"Thu, 07 Mar 2024 19:32:31 +0000"
441,Qumulo Launches a Cloud Native Solution for Cold File Data,https://insidehpc.com/2024/03/qumulo-launches-a-cloud-native-solution-for-cold-file-data/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"225\" src=\"https://insidehpc.com/wp-content/uploads/2024/03/Qumulo-logo-2-1-0324.png\" style=\"float: right; margin-left: 5px;\" width=\"457\" /><p>SEATTLE– March 5th, 2024 – High performance data management and storage company Qumulo has announced the availability of Azure Native Qumulo Cold (ANQ Cold), part of Qumulo’s Scale Anywhere platform for the hybrid enterprise. ANQ Cold is the world’s first cloud-native service to cost-effectively store and retrieve infrequently accessed “cold” file data at any scale. At [&#8230;]</p>
<p>The post <a href=\"https://insidehpc.com/2024/03/qumulo-launches-a-cloud-native-solution-for-cold-file-data/\">Qumulo Launches a Cloud Native Solution for Cold File Data</a> appeared first on <a href=\"https://insidehpc.com\">High-Performance Computing News Analysis | insideHPC</a>.</p>",[{'name': 'staff'}],"Thu, 07 Mar 2024 18:54:26 +0000"
442,DCS Corporation Names Brian Wood Vice President of Robotic Systems Division,https://insidehpc.com/2024/03/dcs-corporation-names-brian-wood-vice-president-of-robotic-systems-division/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"150\" src=\"https://insidehpc.com/wp-content/uploads/2022/09/generic-technology-data-systems-shutterstock-1435925441--150x150.jpg\" style=\"float: right; margin-left: 5px;\" width=\"150\" /><p>ALEXANDRIA, Va., March 7, 2024 &#8212; DCS Corporation is pleased to announce the appointment of Brian Wood to vice president and division manager of the Robotic Systems Division. Wood brings 24 years of experience in software development, network communications, graphical user interfaces, streaming video, systems engineering and program management. Since joining DCS in 1999, he [&#8230;]</p>
<p>The post <a href=\"https://insidehpc.com/2024/03/dcs-corporation-names-brian-wood-vice-president-of-robotic-systems-division/\">DCS Corporation Names Brian Wood Vice President of Robotic Systems Division</a> appeared first on <a href=\"https://insidehpc.com\">High-Performance Computing News Analysis | insideHPC</a>.</p>",[{'name': 'staff'}],"Thu, 07 Mar 2024 18:41:16 +0000"
443,"Jérôme Denigot Named EVP, Sensing & Monitoring, at CGG",https://insidehpc.com/2024/03/jerome-denigot-named-evp-sensing-monitoring-at-cgg/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"300\" src=\"https://insidehpc.com/wp-content/uploads/2023/11/sc23-cgg.png\" style=\"float: right; margin-left: 5px;\" width=\"600\" /><p>CGG announced today the appointment of Jérôme Denigot as Executive Vice President, Sensing &#38; Monitoring, effective February 26, 2024. In this new position, he will lead CGG’s global Sensing &#38; Monitoring business line (SMO), marketed under the Sercel brand name. Denigot joined CGG in 2003, holding various positions of increasing responsibility in SMO. In 2017 he [&#8230;]</p>
<p>The post <a href=\"https://insidehpc.com/2024/03/jerome-denigot-named-evp-sensing-monitoring-at-cgg/\">Jérôme Denigot Named EVP, Sensing &#038; Monitoring, at CGG</a> appeared first on <a href=\"https://insidehpc.com\">High-Performance Computing News Analysis | insideHPC</a>.</p>",[{'name': 'staff'}],"Thu, 07 Mar 2024 17:54:07 +0000"
444,Argonne: ATPESC Training Application Deadline Extended to March 10,https://insidehpc.com/2024/03/argonne-atpesc-training-application-deadline-extended-to-march-10/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"348\" src=\"https://insidehpc.com/wp-content/uploads/2024/01/ATPESC-2024.png\" style=\"float: right; margin-left: 5px;\" width=\"624\" /><p>The application deadline for the Argonne Training Program on Extreme-Scale Computing (ATPESC) program has been extended to Sunday, March 10. This year marks the 12th  year for ATPESC, which  provides intensive, two-week training on the key skills, approaches, and tools to design, implement, and execute computational science and engineering applications on current high-end computing systems [&#8230;]</p>
<p>The post <a href=\"https://insidehpc.com/2024/03/argonne-atpesc-training-application-deadline-extended-to-march-10/\">Argonne: ATPESC Training Application Deadline Extended to March 10</a> appeared first on <a href=\"https://insidehpc.com\">High-Performance Computing News Analysis | insideHPC</a>.</p>",[{'name': 'staff'}],"Wed, 06 Mar 2024 21:40:00 +0000"
445,Tenstorrent Founder and Engineers Say Taalas AI Chip Outperforms a Small GPU Data Center,https://insidehpc.com/2024/03/tenstorrent-founder-and-engineers-say-their-taalas-ai-chip-outperforms-a-small-gpu-data-center/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"334\" src=\"https://insidehpc.com/wp-content/uploads/2024/03/Taalas-logo-2-1-0324.png\" style=\"float: right; margin-left: 5px;\" width=\"646\" /><p>TORONTO, March 5, 2024 &#8212; AI chip startup Taalas Inc., has announced it has exited stealth mode and raised $50 million over two rounds of funding led by Pierre Lamond and Quiet Capital. Taalas was founded by Tenstorrent founder Ljubisa Bajic, along with early Tenstorrent engineering leaders Drago Ignjatovic, and Lejla Bajic. The three have worked on AI processors, [&#8230;]</p>
<p>The post <a href=\"https://insidehpc.com/2024/03/tenstorrent-founder-and-engineers-say-their-taalas-ai-chip-outperforms-a-small-gpu-data-center/\">Tenstorrent Founder and Engineers Say Taalas AI Chip Outperforms a Small GPU Data Center</a> appeared first on <a href=\"https://insidehpc.com\">High-Performance Computing News Analysis | insideHPC</a>.</p>",[{'name': 'staff'}],"Wed, 06 Mar 2024 21:06:39 +0000"
446,AI Startup Partners with Princeton on DARPA In-Memory AI Chip Project,https://insidehpc.com/2024/03/ai-startup-partners-with-princeton-on-darpa-funded-in-memory-ai-chip-project/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"308\" src=\"https://insidehpc.com/wp-content/uploads/2024/03/DARPA-logo-2-1-0324.png\" style=\"float: right; margin-left: 5px;\" width=\"616\" /><p>An AI startup co-founded by a Princeton University professor has won an $18.6 million DOD grant to develop an in-memory chip built to deliver faster, more efficient  AI inference processing. AI technology company EnCharge AI has announced a partnership....</p>
<p>The post <a href=\"https://insidehpc.com/2024/03/ai-startup-partners-with-princeton-on-darpa-funded-in-memory-ai-chip-project/\">AI Startup Partners with Princeton on DARPA In-Memory AI Chip Project</a> appeared first on <a href=\"https://insidehpc.com\">High-Performance Computing News Analysis | insideHPC</a>.</p>",[{'name': 'Doug Black'}],"Wed, 06 Mar 2024 15:00:10 +0000"
447,Get Ready for Gemma: The Game-Changing 2B 7B 6Trillion Token Platform,https://www.franksworld.com/2024/03/04/get-ready-for-gemma-the-game-changing-2b-7b-6trillion-token-platform/?utm_source=rss&utm_medium=rss&utm_campaign=get-ready-for-gemma-the-game-changing-2b-7b-6trillion-token-platform,This video is from Sam Witteveen.,[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:48:01 +0000"
448,Nintendo just picked a fight with open-source project Yuzu,https://www.franksworld.com/2024/03/04/nintendo-just-picked-a-fight-with-open-source-project-yuzu/?utm_source=rss&utm_medium=rss&utm_campaign=nintendo-just-picked-a-fight-with-open-source-project-yuzu,This video is from Fireship. Nintendo recently sued an open-source Nintendo Switch emulator called Yuzu. Let&#8217;s take a look at the hacking techniques used to exploit the Switch and find out what this lawsuit means for game developers.,[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:45:36 +0000"
449,The NSA has Picked these Programming Languages,https://www.franksworld.com/2024/03/04/the-nsa-has-picked-these-programming-languages/?utm_source=rss&utm_medium=rss&utm_campaign=the-nsa-has-picked-these-programming-languages,"The NSA has recommended a few programming languages, including Rust, C#, Java, Go and others. https://media.defense.gov/2022/Nov/10/2003112742/-1/-1/0/CSI_SOFTWARE_MEMORY_SAFETY.PDF This video is from Stefan Mischook.",[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:43:59 +0000"
450,Unveiling Microsoft’s Revolutionary 1-bit LLMs in AI Research,https://www.franksworld.com/2024/03/04/unveiling-microsofts-revolutionary-1-bit-llms-in-ai-research/?utm_source=rss&utm_medium=rss&utm_campaign=unveiling-microsofts-revolutionary-1-bit-llms-in-ai-research,"In this video from AI Papers Academy.dive into a recent research paper by Microsoft: &#8220;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&#8221;. This paper introduce an interesting and exciting architecture for large language models, called BitNet b1.58, which significantly reduces LLMs memory consumption, and speeds-up LLMs inference latency. All of [&#8230;]",[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:43:03 +0000"
451,40 APIs Every Developer Should Use (in 12 minutes),https://www.franksworld.com/2024/03/04/40-apis-every-developer-should-use-in-12-minutes/?utm_source=rss&utm_medium=rss&utm_campaign=40-apis-every-developer-should-use-in-12-minutes,"In this video, Coding with Lewis introduces you to 40 APIs that every developer should know and use. From social media to finance to weather, these APIs will help you create amazing applications. Most of these APIs are free without an API key, but might require some sort of sign up or a trial to [&#8230;]",[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:41:03 +0000"
452,Do we create reality with our mind? A physicist’s reply.,https://www.franksworld.com/2024/03/04/do-we-create-reality-with-our-mind-a-physicists-reply/?utm_source=rss&utm_medium=rss&utm_campaign=do-we-create-reality-with-our-mind-a-physicists-reply,"This video is from Sabine Hossenfelder. Do we create reality with our minds? I got this question on twitter the other day and after rolling my eyes about it for some while, I decided it’s actually a good question. You might think the answer is obviously “no”. But it’s not that simple. Let me explain.",[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:40:07 +0000"
453,How to Master Machine Learning: A Beginner’s Guide for 2024,https://www.franksworld.com/2024/03/04/how-to-master-machine-learning-a-beginners-guide-for-2024/?utm_source=rss&utm_medium=rss&utm_campaign=how-to-master-machine-learning-a-beginners-guide-for-2024,"This video is from freeCodeCamp.org. This machine learning course is created for beginners who are learning in 2024. The course begins with a Machine Learning Roadmap for 2024, emphasizing career paths and beginner-friendly theory. Then it the course moves on to hands-on practical applications and a comprehensive end-to-end project using Python. ✏️ Course created by [&#8230;]",[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:37:51 +0000"
454,Maximize Your Editing Efficiency: 7 Must-Have AI Tools in Premiere Pro,https://www.franksworld.com/2024/03/04/maximize-your-editing-efficiency-7-must-have-ai-tools-in-premiere-pro/?utm_source=rss&utm_medium=rss&utm_campaign=maximize-your-editing-efficiency-7-must-have-ai-tools-in-premiere-pro,This video from Lila takes you through the innovations in Adobe Premiere that take full advantage of AI.,[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:36:53 +0000"
455,How to Learn the Gemini API and Create a Multi-Turn Chat Bot (1 Hour Crash Course),https://www.franksworld.com/2024/03/04/how-to-learn-the-gemini-api-and-create-a-multi-turn-chat-bot-1-hour-crash-course/?utm_source=rss&utm_medium=rss&utm_campaign=how-to-learn-the-gemini-api-and-create-a-multi-turn-chat-bot-1-hour-crash-course,This video is from Code with Ania Kubów. Get the video api code here: https://bit.ly/stream-code,[{'name': 'Frank'}],"Mon, 04 Mar 2024 15:26:57 +0000"
456,Combining Rust and Python: The Best of Both Worlds?,https://www.franksworld.com/2024/03/03/combining-rust-and-python-the-best-of-both-worlds/?utm_source=rss&utm_medium=rss&utm_campaign=combining-rust-and-python-the-best-of-both-worlds,"In this video, ArjanCodes shows you how to seamlessly integrate Rust with Python using Pyo3. This library allows you to write Python modules with Rust. This means that we get the speed and safety of Rust along with Python&#8217;s easy-to-use features!",[{'name': 'Frank'}],"Sun, 03 Mar 2024 13:53:09 +0000"
457,Breaking Down the Fastest AI Chip in the World: What You Need to Know,https://www.franksworld.com/2024/03/01/breaking-down-the-fastest-ai-chip-in-the-world-what-you-need-to-know/?utm_source=rss&utm_medium=rss&utm_campaign=breaking-down-the-fastest-ai-chip-in-the-world-what-you-need-to-know,"This video from Anastasi In Tech  discusses how it works, benchmarks, how it compares to other AI accelerators and the future outlook!",[{'name': 'Frank'}],"Fri, 01 Mar 2024 17:57:07 +0000"
458,Elevate Your AI Capabilities with Red Hat OpenShift AI: A Demo,https://www.franksworld.com/2024/03/01/elevate-your-ai-capabilities-with-red-hat-openshift-ai-a-demo/?utm_source=rss&utm_medium=rss&utm_campaign=elevate-your-ai-capabilities-with-red-hat-openshift-ai-a-demo,"In this video, Chris Chase demonstrates a typical workflow that includes creating a project, launching a Jupyter notebook with appropriate cluster resources and training a foundation model from Hugging Face with one’s own data. Once the model is fine-tuned, Chris automates the build using a data science pipeline and serves the model for use in [&#8230;]",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:53:02 +0000"
459,Unveiling the Matrix: A Deep Dive into its Philosophical Themes,https://www.franksworld.com/2024/03/01/unveiling-the-matrix-a-deep-dive-into-its-philosophical-themes/?utm_source=rss&utm_medium=rss&utm_campaign=unveiling-the-matrix-a-deep-dive-into-its-philosophical-themes,"In this video from Einzelgänger take a deeper look into the Matrix. The Matrix, a science fiction film created by the Wachowskis, is probably one of the most influential movies ever made. The story starts when computer programmer Thomas Anderson, operating as a hacker under the alias “Neo,” discovers the truth about the world he’s [&#8230;]",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:48:06 +0000"
460,Navigating the AI Landscape: Lessons from the Butlerian Jihad,https://www.franksworld.com/2024/03/01/navigating-the-ai-landscape-lessons-from-the-butlerian-jihad/?utm_source=rss&utm_medium=rss&utm_campaign=navigating-the-ai-landscape-lessons-from-the-butlerian-jihad,"The following was originally published as part of my LinkedIn newsletter: Frank Digs Data, with a special shout out to all the Dune fans out there. In the realm of popular science fiction, few narratives resonate as powerfully with contemporary technological and ethical debates as the Butlerian Jihad from Frank Herbert&#8217;s &#8220;Dune,&#8221; where robotics and [&#8230;]",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:43:15 +0000"
461,Separating Fact from Fiction: Exploring AI Concerns with Dan Hendrycks on Win-Win,https://www.franksworld.com/2024/03/01/separating-fact-from-fiction-exploring-ai-concerns-with-dan-hendrycks-on-win-win/?utm_source=rss&utm_medium=rss&utm_campaign=separating-fact-from-fiction-exploring-ai-concerns-with-dan-hendrycks-on-win-win,"The rate of AI progress is accelerating, so how can we minimize the risks of this incredible technology, while maximizing the rewards?",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:38:52 +0000"
462,Data Engineering in the Age of AI: Data Intelligence Platforms,https://www.franksworld.com/2024/03/01/data-engineering-in-the-age-of-ai-data-intelligence-platforms/?utm_source=rss&utm_medium=rss&utm_campaign=data-engineering-in-the-age-of-ai-data-intelligence-platforms,"Learn about the Data Intelligence Platform and how Data Engineers benefit in their work from AI infused into every layer of the Databricks Lakehouse. This live demo walks you through the classic data engineering tasks, code generation, fixing, and documentation with the Databricks Assistant, automatic documentation with Unity Catalog, and making use of LLMs such [&#8230;]",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:36:56 +0000"
463,How to Give a Great Data Presentation,https://www.franksworld.com/2024/03/01/how-to-give-a-great-data-presentation/?utm_source=rss&utm_medium=rss&utm_campaign=how-to-give-a-great-data-presentation,This video is from DATAcated. Very excited to host Christopher Chin &#8211; to talk about How to Give a Great Data Presentation!!!,[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:35:40 +0000"
464,Uncovering the Fundamentals of SQL Server and Azure SQL DB Security | Data Exposed,https://www.franksworld.com/2024/03/01/uncovering-the-fundamentals-of-sql-server-and-azure-sql-db-security-data-exposed/?utm_source=rss&utm_medium=rss&utm_campaign=uncovering-the-fundamentals-of-sql-server-and-azure-sql-db-security-data-exposed,"In this video is from Microsoft Developer, learn about SQL Server and Azure SQL Database security fundamentals you won&#8217;t want to miss.",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:32:47 +0000"
465,Uncovering the Truth: The Alarming Rise of Fraud in the Scientific Community,https://www.franksworld.com/2024/03/01/uncovering-the-truth-the-alarming-rise-of-fraud-in-the-scientific-community/?utm_source=rss&utm_medium=rss&utm_campaign=uncovering-the-truth-the-alarming-rise-of-fraud-in-the-scientific-community,"This video is from Sabine Hossenfelder. Science has a big problem and it’s been getting rapidly worse in the past two years or so, to no small part because of recent advances in artificial intelligence. Fraudulent papers are getting published more than ever, and the fraudsters are getting increasingly aggressive. In this episode I want [&#8230;]",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:31:54 +0000"
466,Unleashing Creativity: DeepMind’s New AI Generates Games From Scratch,https://www.franksworld.com/2024/03/01/unleashing-creativity-deepminds-new-ai-generates-games-from-scratch/?utm_source=rss&utm_medium=rss&utm_campaign=unleashing-creativity-deepminds-new-ai-generates-games-from-scratch,This video from Two Minute Papers covers the paper &#8220;Genie: Generative Interactive Environments.&#8221; See? Not all hope for Google is lost.,[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:30:17 +0000"
467,Drizzle ORM in 100 Seconds,https://www.franksworld.com/2024/03/01/drizzle-orm-in-100-seconds/?utm_source=rss&utm_medium=rss&utm_campaign=drizzle-orm-in-100-seconds,"Fireship explains Drizzle in 100 seconds. Drizzle is a serverless TypeScript ORM designed for PostgreSQL, MySQL and SQLite.",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:28:35 +0000"
468,The Rise of AGI: How to Prepare for the Next Technological Revolution,https://www.franksworld.com/2024/02/29/the-rise-of-agi-how-to-prepare-for-the-next-technological-revolution/?utm_source=rss&utm_medium=rss&utm_campaign=the-rise-of-agi-how-to-prepare-for-the-next-technological-revolution,"This video is from David Ondrej. Soon AGI will be created, yet most people are grossly unprepared. In this AI course I will show you exactly what you need to do to thrive in the post-AGI world.",[{'name': 'Frank'}],"Thu, 29 Feb 2024 18:32:58 +0000"
469,Is it too late to invest in Nvidia?,https://www.franksworld.com/2024/02/29/is-it-too-late-to-invest-in-nvidia/?utm_source=rss&utm_medium=rss&utm_campaign=is-it-too-late-to-invest-in-nvidia,"On this edition of Quartz Smart Investing, is it too late to invest in Nvidia? Eric Beiley, executive managing director of The Beiley Group at Steward Partners, tells Quartz whether he thinks there&#8217;s still room to run for the AI chip stock.",[{'name': 'Frank'}],"Thu, 29 Feb 2024 18:30:15 +0000"
470,OpenAI Sora: A Closer Look!,https://www.franksworld.com/2024/02/29/openai-sora-a-closer-look/?utm_source=rss&utm_medium=rss&utm_campaign=openai-sora-a-closer-look,This video is from Two Minute Papers. 📝 Sora: https://openai.com/research/video-generation-models-as-world-simulators,[{'name': 'Frank'}],"Thu, 29 Feb 2024 18:29:04 +0000"
471,KubeCon EU 2024 Preview: Exploring the Clouds with Aparna Subramanian,https://www.franksworld.com/2024/02/29/kubecon-eu-2024-preview-exploring-the-clouds-with-aparna-subramanian/?utm_source=rss&utm_medium=rss&utm_campaign=kubecon-eu-2024-preview-exploring-the-clouds-with-aparna-subramanian,"With KubeCon EU in Paris around the corner (19th to 22nd March), what better way to get some insight on what is expected than by speaking to people directly involved in the event. Aparna Subramanian is a technologist and cloud-native enthusiast. She started her career as a Software Engineer and has spent most part of [&#8230;]",[{'name': 'Frank'}],"Thu, 29 Feb 2024 18:27:22 +0000"
472,More than half of American Indian youth may have abnormal or high cholesterol,https://www.sciencedaily.com/releases/2024/03/240306150704.htm,"More than 70% of American Indian young adults aged 20-39 and 50% of American Indian teens have cholesterol levels or elevated fat in the blood that put them at risk for cardiovascular disease, a new study suggests. In some cases, these levels -- specifically high low-density lipoprotein (LDL) cholesterol, often thought of as 'bad cholesterol,' -- were linked to plaque buildup and cardiovascular events, such as heart attack and stroke.",[],"Wed, 06 Mar 2024 15:07:04 EST"
473,Finding new physics in debris from colliding neutron stars,https://www.sciencedaily.com/releases/2024/03/240306150618.htm,"Neutron star mergers are a treasure trove for new physics signals, with implications for determining the true nature of dark matter, according to physicists.",[],"Wed, 06 Mar 2024 15:06:18 EST"
474,Astronomers spot oldest 'dead' galaxy yet observed,https://www.sciencedaily.com/releases/2024/03/240306150612.htm,"A galaxy that suddenly stopped forming new stars more than 13 billion years ago has been observed by astronomers. Using the James Webb Space Telescope, astronomers have spotted a 'dead' galaxy when the universe was just 700 million years old, the oldest such galaxy ever observed.",[],"Wed, 06 Mar 2024 15:06:12 EST"
475,Schizophrenia and aging may share a common biological basis,https://www.sciencedaily.com/releases/2024/03/240306150610.htm,Researchers have uncovered a strikingly similar suite of changes in gene activity in brain tissue from people with schizophrenia and from older adults. These changes suggest a common biological basis for the cognitive impairment often seen in people with schizophrenia and in the elderly.,[],"Wed, 06 Mar 2024 15:06:10 EST"
476,Synthetic gene helps explain the mysteries of transcription across species,https://www.sciencedaily.com/releases/2024/03/240306150549.htm,"'Random DNA' is naturally active in the one-celled fungi yeast, while such DNA is turned off as its natural state in mammalian cells, despite their having a common ancestor a billion years ago and the same basic molecular machinery, a new study finds.",[],"Wed, 06 Mar 2024 15:05:49 EST"
477,"Compact chips advance precision timing for communications, navigation and other applications",https://www.sciencedaily.com/releases/2024/03/240306150545.htm,"Precision timing and synchronization are crucial for navigation, communication and radar systems. Scientists have built compact chips capable of converting light into microwaves, which could improve these systems. This technology shrinks a tabletop system into a chip-sized format, reducing power usage and making it more applicable for use in everyday devices.",[],"Wed, 06 Mar 2024 15:05:45 EST"
478,Decoding the language of epigenetic modifications,https://www.sciencedaily.com/releases/2024/03/240306150542.htm,"Epigenetic changes play important roles in cancer, metabolic and aging-related diseases, but also during loss of resilience as they cause the genetic material to be incorrectly interpreted in affected cells. A major study now provides important new insights into how complex epigenetic modification signatures regulate the genome. This study will pave the way for new treatments of diseases caused by faulty epigenetic machineries.",[],"Wed, 06 Mar 2024 15:05:42 EST"
479,Psychosocial stressors linked to higher inflammation in Black pregnant women,https://www.sciencedaily.com/releases/2024/03/240306150529.htm,"Living in neighborhoods with more white residents and greater lifetime experiences of racial discrimination are linked to increased systemic inflammation during pregnancy among Black women, according to new research.",[],"Wed, 06 Mar 2024 15:05:29 EST"
480,Amyloid blood levels associated with brain changes in Alzheimer's study,https://www.sciencedaily.com/releases/2024/03/240306150527.htm,A link between abnormal blood levels of amyloid -- a protein associated with Alzheimer's disease -- and subtle changes in brain microstructures on a type of MRI could lead to a new way to detect Alzheimer's earlier in people with no clinical signs.,[],"Wed, 06 Mar 2024 15:05:27 EST"
481,Study reveals genetic clusters and biological pathways that may explain differences in type 2 diabetes risk,https://www.sciencedaily.com/releases/2024/03/240306150525.htm,A recent study analyzed individuals from diverse backgrounds and identified various genetic clusters involved in a broad range of biological mechanisms that may help explain ancestry-associated differences in type 2 diabetes clinical presentations.,[],"Wed, 06 Mar 2024 15:05:25 EST"
482,"Herbivores, displaced by ocean warming, threaten subtropical seagrass meadows",https://www.sciencedaily.com/releases/2024/03/240306150523.htm,"The findings suggest that subtropical seagrasses are less resilient to heavy grazing from marine herbivores, in part because they receive less sunlight relative to their tropical counterparts. As tropical herbivores move into subtropical waters, overgrazing may prevent subtropical seagrass meadows from persisting in these environments.",[],"Wed, 06 Mar 2024 15:05:23 EST"
483,A step towards clinic-ready patient-derived organoids,https://www.sciencedaily.com/releases/2024/03/240306150518.htm,"A recent study provides critical insights into the reliability and robustness of patient-derived organoids as a clinical model of pancreatic cancer. The findings reveal that organoids' gene expression and drug responses are not affected by the brand of extracellular matrix used in the cell culture. However, one commercial product did increase the growth rate of pancreatic tumor organoids, making it particularly well-suited for the fast pace of pancreatic cancer treatment protocols.",[],"Wed, 06 Mar 2024 15:05:18 EST"
484,Discovery tests theory on cooling of white dwarf stars,https://www.sciencedaily.com/releases/2024/03/240306150510.htm,Open any astronomy textbook to the section on white dwarf stars and you'll likely learn that they are 'dead stars' that continuously cool down over time. Astronomers are challenging this theory after discovering a population of white dwarf stars that stopped cooling for more than eight billion years.,[],"Wed, 06 Mar 2024 15:05:10 EST"
485,Invasive plant time bombs: A hidden ecological threat,https://www.sciencedaily.com/releases/2024/03/240306150501.htm,"Invasive plants can stay dormant for decades or even centuries before rapidly expanding and wreaking ecological havoc, finds a new study.",[],"Wed, 06 Mar 2024 15:05:01 EST"
486,"Older adults want to express themselves with emojis, they just don't understand how to",https://www.sciencedaily.com/releases/2024/03/240306150459.htm,"A new study is providing insight into how different generations are interpreting the use of emojis in their communications, with older adults unsure of how to use them as frequently as younger adults despite understanding their meaning.",[],"Wed, 06 Mar 2024 15:04:59 EST"
487,Consuming refined carbs might be linked to perceived facial attractiveness,https://www.sciencedaily.com/releases/2024/03/240306150457.htm,"In a new study, participants' levels of consumption of refined carbohydrates were statistically linked with their facial attractiveness as rated by heterosexual volunteers of the opposite sex.",[],"Wed, 06 Mar 2024 15:04:57 EST"
488,"COVID-19 physiological impacts vary by sex, wearable technology reveals",https://www.sciencedaily.com/releases/2024/03/240306150455.htm,"Males and females have differential physiological responses to COVID-19 infections, with males having larger increases in skin temperature, breathing rate and heart rate during an acute infection, according to a new study that used data from wearable devices.",[],"Wed, 06 Mar 2024 15:04:55 EST"
489,Factors associated with age-related hearing loss differ between males and females,https://www.sciencedaily.com/releases/2024/03/240306150450.htm,"Certain factors associated with developing age-related hearing loss differ by sex, including weight, smoking behavior, and hormone exposure, according to a new study.",[],"Wed, 06 Mar 2024 15:04:50 EST"
490,This injectable hydrogel mitigates damage to the right ventricle of the heart,https://www.sciencedaily.com/releases/2024/03/240306150441.htm,"An injectable hydrogel can mitigate damage to the right ventricle of the heart with chronic pressure overload, according to a new study. In 2019, this same hydrogel was shown to be safe in humans through an FDA-approved Phase 1 trial in people who suffered a heart attack. As a result of the new preclinical study, the FDA approved an investigational new drug application to start a clinical trial with the hydrogel in pediatric patients in the coming months, once institutional approvals are received.",[],"Wed, 06 Mar 2024 15:04:41 EST"
491,New deep-sea worm discovered at methane seep off Costa Rica,https://www.sciencedaily.com/releases/2024/03/240306150437.htm,Marine biologists have discovered a new species of deep-sea worm living near a methane seep some 50 kilometers (30 miles) off the Pacific coast of Costa Rica.,[],"Wed, 06 Mar 2024 15:04:37 EST"
492,Genetic mutation in a quarter of all Labradors hard-wires them for obesity,https://www.sciencedaily.com/releases/2024/03/240306150433.htm,New research finds around a quarter of Labrador retriever dogs face a double-whammy of feeling hungry all the time and burning fewer calories due to a genetic mutation.,[],"Wed, 06 Mar 2024 15:04:33 EST"
493,New type of nanoparticle makes vaccines more powerful,https://www.sciencedaily.com/releases/2024/03/240306150424.htm,A type of nanoparticle called a metal organic framework (MOF) could be used to deliver vaccines and act as an adjuvant. Researchers find these particles provoke a strong immune response by activating the innate immune system through cell proteins called toll-like receptors.,[],"Wed, 06 Mar 2024 15:04:24 EST"
494,Advanced noise suppression technology for improved search and rescue drones,https://www.sciencedaily.com/releases/2024/03/240306145036.htm,"Unmanned Aerial Vehicles (UAVs) are beneficial in search and rescue missions during natural disasters like earthquakes. However, current UAVs depend on visual information and cannot detect victims trapped under rubble. While some studies have used sound for detection, the noise from UAV propellers can drown out human sounds. To address this issue, researchers have developed a novel artificial intelligence-based system that effectively suppresses UAV noise and amplifies human sounds.",[],"Wed, 06 Mar 2024 14:50:36 EST"
495,Do some electric fish sense the world through comrades' auras?,https://www.sciencedaily.com/releases/2024/03/240306144830.htm,"It would be a game-changer if all members of a basketball team could see out of each other's eyes in addition to their own. Biologists have found evidence that this kind of collective sensing occurs in close-knit groups of African weakly electric fish, also known as elephantnose fish. This instantaneous sharing of sensory intelligence could help the fish locate food, friends and foes.",[],"Wed, 06 Mar 2024 14:48:30 EST"
496,Sinking land increases risk for thousands of coastal residents by 2050,https://www.sciencedaily.com/releases/2024/03/240306144815.htm,"A new study provides a new comprehensive look at the potential for flooding in a combined 32 cities along the Atlantic, Pacific, and Gulf coasts. It predicts as many as 500,000 people will be affected by flooding alongside 1 in 35 privately owned properties within the next three decades, and it highlights the racial and socioeconomic demographics of those potentially affected.",[],"Wed, 06 Mar 2024 14:48:15 EST"
497,AI can speed design of health software,https://www.sciencedaily.com/releases/2024/03/240306144729.htm,"Artificial intelligence helped clinicians to accelerate the design of diabetes prevention software, a new study finds.",[],"Wed, 06 Mar 2024 14:47:29 EST"
498,Can you tell AI-generated people from real ones?,https://www.sciencedaily.com/releases/2024/03/240306003456.htm,"If you recently had trouble figuring out if an image of a person is real or generated through artificial intelligence (AI), you're not alone. A new study found that people had more difficulty than was expected distinguishing who is a real person and who is artificially generated.",[],"Wed, 06 Mar 2024 00:34:56 EST"
499,Key advance toward removing common herbicide from groundwater,https://www.sciencedaily.com/releases/2024/03/240305204445.htm,Chemists are closing in on a new tool for tackling the global problem of weedkiller-tainted groundwater.,[],"Tue, 05 Mar 2024 20:44:45 EST"
500,Research explores the cooling effects of 'scuba-diving' in lizards,https://www.sciencedaily.com/releases/2024/03/240305204443.htm,"Anoles are the scuba-diving champions of the lizard world, able to stay underwater for more than 16 minutes. For animals whose body temperature depends on the environment, time spent in a cool running stream can have some tradeoffs, according to new research.",[],"Tue, 05 Mar 2024 20:44:43 EST"
501,Aluminum nanoparticles make tunable green catalysts,https://www.sciencedaily.com/releases/2024/03/240305204424.htm,A nanotechnology pioneer has uncovered a transformative approach to harnessing the catalytic power of aluminum nanoparticles by annealing them in various gas atmospheres at high temperatures.,[],"Tue, 05 Mar 2024 20:44:24 EST"
502,Tiny worms tolerate Chornobyl radiation,https://www.sciencedaily.com/releases/2024/03/240305165928.htm,"A new study finds that exposure to chronic radiation from Chornobyl has not damaged the genomes of microscopic worms living there today -- which doesn't mean that the region is safe, the scientists caution, but suggests that these worms are exceptionally resilient.",[],"Tue, 05 Mar 2024 16:59:28 EST"
503,Having self-control leads to power,https://www.sciencedaily.com/releases/2024/03/240305165925.htm,Out-of-control behavior by CEOs and other powerful people constantly makes headlines -- so much so that some might consider impulsivity a pathway to power. New research finds that having self-control is often what leads to power.,[],"Tue, 05 Mar 2024 16:59:25 EST"
504,Harmful 'forever chemicals' removed from water with new electrocatalysis method,https://www.sciencedaily.com/releases/2024/03/240305165911.htm,"Scientists have developed new electrochemical approaches to clean up pollution from 'forever chemicals' found in clothing, food packaging, firefighting foams, and a wide array of other products. A new study describes nanocatalysts developed to remediate per- and polyfluoroalkyl substances, known as PFAS.",[],"Tue, 05 Mar 2024 16:59:11 EST"
505,"For a healthy fruit snack, what would you choose?",https://www.sciencedaily.com/releases/2024/03/240305165908.htm,"Next time you're packing lunch for your kid or reaching for a healthy afternoon bite, consider this: only three types of packaged fruit snacks -- dried fruit, fruit puree and canned fruit with juice -- meet the latest recommendations for high-nutrition snacks set by federal dietary guidelines, according to food scientists.",[],"Tue, 05 Mar 2024 16:59:08 EST"
506,Evolving hydrogen-storage technology: Guidelines developed for the design of anti-evaporation catalysts,https://www.sciencedaily.com/releases/2024/03/240305165709.htm,A research team has identified materials capable of catalyzing the conversion of ortho-hydrogen to para-hydrogen. These catalysts should be essential to the spread of mass-transportation/storage of liquid hydrogen.,[],"Tue, 05 Mar 2024 16:57:09 EST"
507,A smart molecule beats the mutation behind most pancreatic cancer,https://www.sciencedaily.com/releases/2024/03/240305134402.htm,"Researchers have designed a candidate drug that could help make pancreatic cancer, which is almost always fatal, a treatable, perhaps even curable, condition.",[],"Tue, 05 Mar 2024 13:44:02 EST"
508,Using light to precisely control single-molecule devices,https://www.sciencedaily.com/releases/2024/03/240305134359.htm,Researchers flip the switch at the nanoscale by applying light to induce bonding for single-molecule device switching.,[],"Tue, 05 Mar 2024 13:43:59 EST"
509,"Lab-grown liver organoid to speed up turtle research, making useful traits easier to harness",https://www.sciencedaily.com/releases/2024/03/240305134355.htm,"Researchers developed protocols for growing organoids that mimic a turtle liver, the first organoids developed for a turtle and only the second for any reptile. The discovery will aid deeper study of turtle genetics, including the cause of traits with potential medical applications for humans such as the ability to survive weeks without oxygen.",[],"Tue, 05 Mar 2024 13:43:55 EST"
510,New method measures the 3D position of individual atoms,https://www.sciencedaily.com/releases/2024/03/240305134352.htm,"Since more than a decade it has been possible for physicists to accurately measure the location of individual atoms to a precision of smaller than one thousandth of a millimeter using a special type of microscope. However, this method has so far only provided the x and y coordinates. Information on the vertical position of the atom -- i.e., the distance between the atom and the microscope objective -- is lacking. A new method has now been developed that can determine all three spatial coordinates of an atom with one single image.",[],"Tue, 05 Mar 2024 13:43:52 EST"
511,Toward understanding sperm quality,https://www.sciencedaily.com/releases/2024/03/240305134346.htm,A novel screening system enables researchers to investigate sperm cell development and health at the molecular level. The new approach promises breakthroughs in male contraception and infertility treatments.,[],"Tue, 05 Mar 2024 13:43:46 EST"
512,"Researchers closing in on genetic treatments for hereditary lung disease, vision loss",https://www.sciencedaily.com/releases/2024/03/240305134330.htm,"Researchers who work with tiny drug carriers known as lipid nanoparticles have developed a new type of material capable of reaching the lungs and the eyes, an important step toward genetic therapy for hereditary conditions like cystic fibrosis and inherited vision loss.",[],"Tue, 05 Mar 2024 13:43:30 EST"
513,Gen Z's climate anxiety is real and needs action -- for everyone's wellbeing,https://www.sciencedaily.com/releases/2024/03/240305134319.htm,"Young people have major concerns about climate change, which is having a significant impact on their lives and could have broader consequences decades into the future.",[],"Tue, 05 Mar 2024 13:43:19 EST"
514,Shortcut to Success: Toward fast and robust quantum control through accelerating adiabatic passage,https://www.sciencedaily.com/releases/2024/03/240305134315.htm,"Researchers achieved the acceleration of adiabatic evolution of a single spin qubit in gate-defined quantum dots. After the pulse optimization to suppress quasistatic noises, the spin flip fidelity can be as high as 97.5% in GaAs quantum dots. This work may be useful to achieve fast and high-fidelity quantum computing.",[],"Tue, 05 Mar 2024 13:43:15 EST"
515,Smart optimization paves the way for improved silicon solar cell performance,https://www.sciencedaily.com/releases/2024/03/240305134312.htm,"Bayesian optimization (BO) can help determine the optimal deposition conditions for high-performance passivation films in solar cells. However, simple implementations tend to suggest excessively thick films or infeasible deposition conditions. Recently, researchers from Japan developed a novel constrained BO scheme that combines several prediction models to restrict suggested deposition conditions to realistic values. Their strategy could help minimize trial and error and improve material performance in various fields.",[],"Tue, 05 Mar 2024 13:43:12 EST"
516,Sweetened drinks linked to atrial fibrillation risk,https://www.sciencedaily.com/releases/2024/03/240305134303.htm,"An analysis of health data in the UK Biobank found a 20% higher risk of irregular heart rhythm, known as atrial fibrillation, among people who said they drank two liters or more per week (about 67 ounces) of artificially sweetened drinks. The risk was 10% higher among people who said they drank similar amounts of sugar-sweetened beverages.",[],"Tue, 05 Mar 2024 13:43:03 EST"
517,'Like a lab in your pocket' -- new test strips raise game in gene-based diagnostics,https://www.sciencedaily.com/releases/2024/03/240305134300.htm,Biosensing technology developed by engineers has made it possible to create gene test strips that rival conventional lab-based tests in quality.,[],"Tue, 05 Mar 2024 13:43:00 EST"
518,Fossils of giant sea lizard with dagger-like teeth show how our oceans have fundamentally changed since the dinosaur era,https://www.sciencedaily.com/releases/2024/03/240305134258.htm,"Fossils of a strange new species of marine lizard with dagger-like teeth that lived 66 million years ago, show a dramatically more biodiverse ocean ecosystem to what we see today.",[],"Tue, 05 Mar 2024 13:42:58 EST"
519,Running performance helped by mathematical research,https://www.sciencedaily.com/releases/2024/03/240305134255.htm,"A new mathematical model has shown, with great precision, the impact that physiological and psychological parameters have on running performance and provides tips for optimized training.",[],"Tue, 05 Mar 2024 13:42:55 EST"
520,Waist-to-height ratio detects fat obesity in children and adolescents significantly better than BMI,https://www.sciencedaily.com/releases/2024/03/240305134243.htm,An inexpensive measure of obesity in children and adolescents that could replace body mass index (BMI) has been identified in a new study as waist circumference-to-height ratio. This measure detected excess fat mass and distinguished fat mass from muscle mass in children and adolescents more accurately than BMI.,[],"Tue, 05 Mar 2024 13:42:43 EST"
521,Groundbreaking survey reveals secrets of planet birth around dozens of stars,https://www.sciencedaily.com/releases/2024/03/240305134240.htm,"A team of astronomers has shed new light on the fascinating and complex process of planet formation. The research brings together observations of more than 80 young stars that might have planets forming around them, providing astronomers with a wealth of data and unique insights into how planets arise in different regions of our galaxy.",[],"Tue, 05 Mar 2024 13:42:40 EST"
522,One way to improve a fusion reaction: Use weaknesses as strengths,https://www.sciencedaily.com/releases/2024/03/240305134238.htm,"Scientists are embracing imperfection, using less-than-ideal magnetic fields to make the plasma more manageable.",[],"Tue, 05 Mar 2024 13:42:38 EST"
523,Possible 'Trojan Horse' found for treating stubborn bacterial infections,https://www.sciencedaily.com/releases/2024/03/240305134236.htm,"Bacteria can be tricked into sending death signals to stop the growth of their slimy, protective homes that lead to deadly infections, a new study demonstrates. The discovery could someday be harnessed as an alternative to antibiotics for treating difficult infections. The researchers used the messengers, which they named death extracellular vesicles (D-EVs), to reduce growth of the bacterial communities by up to 99.99% in laboratory experiments.",[],"Tue, 05 Mar 2024 13:42:36 EST"
524,Conservation actions for SA's white shark population now a matter of urgency,https://www.sciencedaily.com/releases/2024/03/240305134233.htm,"A group of marine biologists specializing in shark ecology, genetics and fisheries have challenged the findings from a recent study suggesting that South Africa's white shark population has not decreased, but simply redistributed eastwards to flee predation from orcas.",[],"Tue, 05 Mar 2024 13:42:33 EST"
525,"Smoking during pregnancy may increase the risk of behavioral disorders in newborns, predicts AI",https://www.sciencedaily.com/releases/2024/03/240305134228.htm,"Although several studies have linked smoking during pregnancy with neurodevelopmental disorders, the results of behavioral experiments in mice prenatally exposed to nicotine have been inconsistent. In a recent study, scientists from Japan developed a deep learning-based framework to automatically observe and classify mice behavior in such experiments, producing more accurate and unbiased results. They show that prenatal exposure to nicotine could increase the risk of autism spectrum- and attention deficit/hyperactivity disorders in newborns.",[],"Tue, 05 Mar 2024 13:42:28 EST"
526,What makes black holes grow and new stars form? Machine learning helps solve the mystery,https://www.sciencedaily.com/releases/2024/03/240305134225.htm,It takes more than a galaxy merger to make a black hole grow and new stars form: machine learning shows cold gas is needed too to initiate rapid growth -- new research finds.,[],"Tue, 05 Mar 2024 13:42:25 EST"
527,"Sprinting 'like a jet' will produce Premier League strikers of tomorrow, study shows",https://www.sciencedaily.com/releases/2024/03/240305134220.htm,"Sprinting 'like a jet plane taking off' will help produce Premier League star strikers of tomorrow, new research has revealed. A new study of Tottenham Hotspur's academy has shown that just a few words can instantly boost sprinting speed by 3 per cent over 20 meters. It would normally take weeks of targeted training to achieve such a large increase.",[],"Tue, 05 Mar 2024 13:42:20 EST"
528,"Juno spacecraft measures oxygen production on Jupiter's moon, Europa",https://www.sciencedaily.com/releases/2024/03/240305134217.htm,"NASA's Juno spacecraft has directly measured charged oxygen and hydrogen molecules from the atmosphere of one of Jupiter's largest moons, Europa. These observations provide key constraints on the potential oxygenation of its subsurface ocean.",[],"Tue, 05 Mar 2024 13:42:17 EST"
529,"After decades of Arctic sea ice getting faster and more hazardous for transport, models suggest a dramatic reversal is coming",https://www.sciencedaily.com/releases/2024/03/240305134215.htm,"Will ice floating in the Arctic Ocean move faster or slower over the coming decades? The answer to this question will tell us whether marine transportation can be expected to get more or less hazardous. It might also have important implications for the rate of ice cover loss, which is hugely consequential for Northern Indigenous communities, ecosystems, and the global climate system. While observational data suggest the trend has been towards faster sea ice speeds, climate models project that those speeds will slow down during the summer season. This contrast has led to some questions around the plausibility of the model projections.",[],"Tue, 05 Mar 2024 13:42:15 EST"
530,New cardiovascular imaging approach provides a better view of dangerous plaques,https://www.sciencedaily.com/releases/2024/03/240305134213.htm,"Researchers have developed a new catheter-based device that combines two powerful optical techniques to image the dangerous plaques that can build up inside the arteries that supply blood to the heart. By providing new details about plaque, the device could help clinicians and researchers improve treatments for preventing heart attacks and strokes.",[],"Tue, 05 Mar 2024 13:42:13 EST"
531,Robotic-assisted surgery for gallbladder cancer as effective as traditional surgery,https://www.sciencedaily.com/releases/2024/03/240305134210.htm,"Each year, approximately 2,000 people die annually of gallbladder cancer (GBC) in the U.S., with only one in five cases diagnosed at an early stage. With GBC rated as the first biliary tract cancer and the 17th most deadly cancer worldwide, pressing attention for proper management of disease must be addressed. For patients diagnosed, surgery is the most promising curative treatment. While there has been increasing adoption of minimally invasive surgical techniques in gastrointestinal malignancies, including utilization of laparoscopic and robotic surgery, there are reservations in utilizing minimally invasive surgery for gallbladder cancer. A new study has found that robotic-assisted surgery for GBC is as effective as traditional open and laparoscopic methods, with added benefits in precision and quicker post-operative recovery.",[],"Tue, 05 Mar 2024 13:42:10 EST"
532,Astronomers spot oldest 'dead' galaxy yet observed,https://www.sciencedaily.com/releases/2024/03/240306150612.htm,"A galaxy that suddenly stopped forming new stars more than 13 billion years ago has been observed by astronomers. Using the James Webb Space Telescope, astronomers have spotted a 'dead' galaxy when the universe was just 700 million years old, the oldest such galaxy ever observed.",[],"Wed, 06 Mar 2024 15:06:12 EST"
533,Discovery tests theory on cooling of white dwarf stars,https://www.sciencedaily.com/releases/2024/03/240306150510.htm,Open any astronomy textbook to the section on white dwarf stars and you'll likely learn that they are 'dead stars' that continuously cool down over time. Astronomers are challenging this theory after discovering a population of white dwarf stars that stopped cooling for more than eight billion years.,[],"Wed, 06 Mar 2024 15:05:10 EST"
534,Genetic mutation in a quarter of all Labradors hard-wires them for obesity,https://www.sciencedaily.com/releases/2024/03/240306150433.htm,New research finds around a quarter of Labrador retriever dogs face a double-whammy of feeling hungry all the time and burning fewer calories due to a genetic mutation.,[],"Wed, 06 Mar 2024 15:04:33 EST"
535,Do some electric fish sense the world through comrades' auras?,https://www.sciencedaily.com/releases/2024/03/240306144830.htm,"It would be a game-changer if all members of a basketball team could see out of each other's eyes in addition to their own. Biologists have found evidence that this kind of collective sensing occurs in close-knit groups of African weakly electric fish, also known as elephantnose fish. This instantaneous sharing of sensory intelligence could help the fish locate food, friends and foes.",[],"Wed, 06 Mar 2024 14:48:30 EST"
536,Sinking land increases risk for thousands of coastal residents by 2050,https://www.sciencedaily.com/releases/2024/03/240306144815.htm,"A new study provides a new comprehensive look at the potential for flooding in a combined 32 cities along the Atlantic, Pacific, and Gulf coasts. It predicts as many as 500,000 people will be affected by flooding alongside 1 in 35 privately owned properties within the next three decades, and it highlights the racial and socioeconomic demographics of those potentially affected.",[],"Wed, 06 Mar 2024 14:48:15 EST"
537,A smart molecule beats the mutation behind most pancreatic cancer,https://www.sciencedaily.com/releases/2024/03/240305134402.htm,"Researchers have designed a candidate drug that could help make pancreatic cancer, which is almost always fatal, a treatable, perhaps even curable, condition.",[],"Tue, 05 Mar 2024 13:44:02 EST"
538,Using light to precisely control single-molecule devices,https://www.sciencedaily.com/releases/2024/03/240305134359.htm,Researchers flip the switch at the nanoscale by applying light to induce bonding for single-molecule device switching.,[],"Tue, 05 Mar 2024 13:43:59 EST"
539,Fossils of giant sea lizard with dagger-like teeth show how our oceans have fundamentally changed since the dinosaur era,https://www.sciencedaily.com/releases/2024/03/240305134258.htm,"Fossils of a strange new species of marine lizard with dagger-like teeth that lived 66 million years ago, show a dramatically more biodiverse ocean ecosystem to what we see today.",[],"Tue, 05 Mar 2024 13:42:58 EST"
540,Groundbreaking survey reveals secrets of planet birth around dozens of stars,https://www.sciencedaily.com/releases/2024/03/240305134240.htm,"A team of astronomers has shed new light on the fascinating and complex process of planet formation. The research brings together observations of more than 80 young stars that might have planets forming around them, providing astronomers with a wealth of data and unique insights into how planets arise in different regions of our galaxy.",[],"Tue, 05 Mar 2024 13:42:40 EST"
541,What makes black holes grow and new stars form? Machine learning helps solve the mystery,https://www.sciencedaily.com/releases/2024/03/240305134225.htm,It takes more than a galaxy merger to make a black hole grow and new stars form: machine learning shows cold gas is needed too to initiate rapid growth -- new research finds.,[],"Tue, 05 Mar 2024 13:42:25 EST"
542,"Juno spacecraft measures oxygen production on Jupiter's moon, Europa",https://www.sciencedaily.com/releases/2024/03/240305134217.htm,"NASA's Juno spacecraft has directly measured charged oxygen and hydrogen molecules from the atmosphere of one of Jupiter's largest moons, Europa. These observations provide key constraints on the potential oxygenation of its subsurface ocean.",[],"Tue, 05 Mar 2024 13:42:17 EST"
543,Fossil named 'Attenborough's strange bird' was the first in its kind without teeth,https://www.sciencedaily.com/releases/2024/03/240305134206.htm,"A new fossil, named 'Attenborough's strange bird' after naturalist and documentarian Sir David Attenborough, is the first of its kind to evolve a toothless beak. It's from a branch of the bird family tree that went extinct in the mass extinction 66 million years ago, and this strange bird is another puzzle piece that helps explain why some birds -- and their fellow dinosaurs -- went extinct, and others survived to today.",[],"Tue, 05 Mar 2024 13:42:06 EST"
544,Arctic could become 'ice-free' within a decade,https://www.sciencedaily.com/releases/2024/03/240305134203.htm,"While summer sea ice loss in the Arctic is inevitable, it can be reversed if the planet cools down, researchers say.",[],"Tue, 05 Mar 2024 13:42:03 EST"
545,Researchers invent new triple-junction tandem solar cells with world-record efficiency,https://www.sciencedaily.com/releases/2024/03/240304221140.htm,"Scientists have developed a novel triple-junction perovskite/Si tandem solar cell that can achieve a certified world-record power conversion efficiency of 27.1 per cent across a solar energy absorption area of 1 sq cm, representing the best-performing triple-junction perovskite/Si tandem solar cell thus far. To achieve this, the team engineered a new cyanate-integrated perovskite solar cell that is stable and energy efficient.",[],"Mon, 04 Mar 2024 22:11:40 EST"
546,Antarctica's coasts are becoming less icy,https://www.sciencedaily.com/releases/2024/03/240304195458.htm,"Scientists found unexpected evidence the area of polynyas around Antarctica is increasing dramatically, and it follows an intriguing cycle, growing and shrinking roughly every 16 years.",[],"Mon, 04 Mar 2024 19:54:58 EST"
547,Webb unlocks secrets of one of the most distant galaxies ever seen,https://www.sciencedaily.com/releases/2024/03/240304135917.htm,"Looking deeply into space and time, astronomers have studied the exceptionally luminous galaxy GN-z11, which existed when our 13.8 billion-year-old universe was only about 430 million years old.",[],"Mon, 04 Mar 2024 13:59:17 EST"
548,Humans have driven the Earth's freshwater cycle out of its stable state,https://www.sciencedaily.com/releases/2024/03/240304135840.htm,New analysis shows that the global freshwater cycle has shifted far beyond pre-industrial conditions.,[],"Mon, 04 Mar 2024 13:58:40 EST"
549,Tests show high-temperature superconducting magnets are ready for fusion,https://www.sciencedaily.com/releases/2024/03/240304135732.htm,"A comprehensive study of high-temperature superconducting magnets confirms they meet requirements for an economic, compact fusion power plant.",[],"Mon, 04 Mar 2024 13:57:32 EST"
550,An overgrowth of nerve cells appears to cause lingering symptoms after recurrent UTIs,https://www.sciencedaily.com/releases/2024/03/240302171534.htm,"A perplexing problem for people with recurring urinary tract infections (UTIs) is persistent pain, even after antibiotics have successfully cleared the bacteria. Now researchers have identified the likely cause -- an overgrowth of nerve cells in the bladder.",[],"Sat, 02 Mar 2024 17:15:34 EST"
551,Orcas demonstrating they no longer need to hunt in packs to take down the great white shark,https://www.sciencedaily.com/releases/2024/03/240302171519.htm,"An orca (killer whale) has been observed, for the first-ever time, individually consuming a great white shark -- and within just two minutes.",[],"Sat, 02 Mar 2024 17:15:19 EST"
552,AI outperforms humans in standardized tests of creative potential,https://www.sciencedaily.com/releases/2024/03/240301134758.htm,"In a recent study, 151 human participants were pitted against ChatGPT-4 in three tests designed to measure divergent thinking, which is considered to be an indicator of creative thought.",[],"Fri, 01 Mar 2024 13:47:58 EST"
553,Ultraviolet radiation from massive stars shapes planetary systems,https://www.sciencedaily.com/releases/2024/03/240301134610.htm,"Up to a certain point, very luminous stars can have a positive effect on the formation of planets, but from that point on the radiation they emit can cause the material in protoplanetary discs to disperse.",[],"Fri, 01 Mar 2024 13:46:10 EST"
554,Ice shell thickness reveals water temperature on ocean worlds,https://www.sciencedaily.com/releases/2024/02/240229182929.htm,"Astrobiologists have devised a novel way to determine ocean temperatures of distant worlds based on the thickness of their ice shells, effectively conducting oceanography from space.",[],"Thu, 29 Feb 2024 18:29:29 EST"
555,Astronomers measure heaviest black hole pair ever found,https://www.sciencedaily.com/releases/2024/02/240229182830.htm,"Using archival data from the Gemini North telescope, a team of astronomers has measured the heaviest pair of supermassive black holes ever found. The merging of two supermassive black holes is a phenomenon that has long been predicted, though never observed. This massive pair gives clues as to why such an event seems so unlikely in the Universe.",[],"Thu, 29 Feb 2024 18:28:30 EST"
556,Building bionic jellyfish for ocean exploration,https://www.sciencedaily.com/releases/2024/02/240229124647.htm,Researchers show how biohybrid robots based on jellyfish could be used to gather climate science data from deep in the Earth's oceans.,[],"Thu, 29 Feb 2024 12:46:47 EST"
557,Could fiber optic cable help scientists probe the deep layers of the moon?,https://www.sciencedaily.com/releases/2024/02/240229124641.htm,"An increasing number of seismologists are using fiber optic cables to detect seismic waves on Earth -- but how would this technology fare on the Moon, and what would it tell us about the deep layers of our nearest neighbor in space?",[],"Thu, 29 Feb 2024 12:46:41 EST"
558,Astronomers reveal a new link between water and planet formation,https://www.sciencedaily.com/releases/2024/02/240229124620.htm,"Researchers have found water vapor in the disc around a young star exactly where planets may be forming. Water is a key ingredient for life on Earth, and is also thought to play a significant role in planet formation. Yet, until now, we had never been able to map how water is distributed in a stable, cool disc -- the type of disc that offers the most favorable conditions for planets to form around stars.",[],"Thu, 29 Feb 2024 12:46:20 EST"
559,Astronomers discover heavy elements after bright gamma-ray burst from neutron star merger,https://www.sciencedaily.com/releases/2024/02/240229124534.htm,An international team of astronomers obtained observational evidence for the creation of rare heavy elements in the aftermath of a cataclysmic explosion triggered by the merger of two neutron stars.,[],"Thu, 29 Feb 2024 12:45:34 EST"
560,"The Golgi organelle's ribbon structure is not exclusive to vertebrates, contrary to previous consensus",https://www.sciencedaily.com/releases/2024/02/240229124514.htm,"Researchers report that the Golgi ribbon, an organelle structure previously thought to be exclusive to vertebrates, is also present in animal taxa, including mollusks, earthworms, and sea urchins. The function of the Golgi ribbon is still enigmatic, but its presence in diverse animal lineages indicates that its function is not vertebrate specific, as previously thought. The team also showed Golgi ribbons form at a specific timepoint during embryogenesis, which suggests that they might play a role in cell differentiation.",[],"Thu, 29 Feb 2024 12:45:14 EST"
561,'Cosmic lighthouses' that cleared primordial fog identified with JWST,https://www.sciencedaily.com/releases/2024/02/240228132053.htm,"Scientists working with data from NASA's James Webb Space Telescope (JWST) have obtained the first full spectra of some of the earliest starlight in the universe. The images provide the clearest picture yet of very low-mass, newborn galaxies, created less than a billion years after the Big Bang, and suggest the tiny galaxies are central to the cosmic origin story.",[],"Wed, 28 Feb 2024 13:20:53 EST"
562,How molecular 'handedness' emerged in early biology,https://www.sciencedaily.com/releases/2024/02/240228115459.htm,Chemists fill a major gap in origin-of-life theories.,[],"Wed, 28 Feb 2024 11:54:59 EST"
563,Cannabis use linked to increase in heart attack and stroke risk,https://www.sciencedaily.com/releases/2024/02/240228115352.htm,"More frequent use of cannabis was associated with higher odds of adverse cardiovascular outcomes, finds new study.",[],"Wed, 28 Feb 2024 11:53:52 EST"
564,Double trouble at chromosome ends,https://www.sciencedaily.com/releases/2024/02/240228115250.htm,"New findings suggest the end-replication problem, an old standby of biology textbooks, is twice as intricate as once thought.",[],"Wed, 28 Feb 2024 11:52:50 EST"
565,How 40Hz sensory gamma rhythm stimulation clears amyloid in Alzheimer's mice,https://www.sciencedaily.com/releases/2024/02/240228114328.htm,"Stimulating a key brain rhythm with light and sound increases peptide release from interneurons, driving clearance of Alzheimer's protein via the brain's glymphatic system, new study suggests.",[],"Wed, 28 Feb 2024 11:43:28 EST"
566,Change in gene code may explain how human ancestors lost tails,https://www.sciencedaily.com/releases/2024/02/240228114226.htm,A genetic change in our ancient ancestors may partly explain why humans don't have tails like monkeys.,[],"Wed, 28 Feb 2024 11:42:26 EST"
567,New study links placental oxygen levels to fetal brain development,https://www.sciencedaily.com/releases/2024/02/240227172132.htm,"A new study shows oxygenation levels in the placenta, formed during the last three months of fetal development, are an important predictor of cortical growth (development of the outermost layer of the brain or cerebral cortex) and is likely a predictor of childhood cognition and behavior.",[],"Tue, 27 Feb 2024 17:21:32 EST"
568,Scientists use blue-green algae as a surrogate mother for 'meat-like' proteins,https://www.sciencedaily.com/releases/2024/02/240227130709.htm,Researchers have not only succeeded in using blue-green algae as a surrogate mother for a new protein -- they have even coaxed the microalgae to produce 'meat fiber-like' protein strands. The achievement may be the key to sustainable foods that have both the 'right' texture and require minimal processing.,[],"Tue, 27 Feb 2024 13:07:09 EST"
569,Significant glacial retreat in West Antarctica began in 1940s,https://www.sciencedaily.com/releases/2024/02/240226204614.htm,"Among the vast expanse of Antarctica lies the Thwaites Glacier, the world's widest glacier measuring about 80 miles on the western edge of the continent. Despite its size, the massive landform is losing about 50 billion tons of ice more than it is receiving in snowfall, which places it in a precarious position in respect to its stability. Accelerating ice loss has been observed since the 1970s, but it is unclear when this significant melting initiated -- until now. A new study suggests that the significant glacial retreat of two glaciers on the west coast of Antarctica began in the 1940's, likely spurred by climate change.",[],"Mon, 26 Feb 2024 20:46:14 EST"
570,New world record for CIGS solar cells,https://www.sciencedaily.com/releases/2024/02/240226114616.htm,A new record for electrical energy generation from CIGS solar cells has been reached. Scientists have achieved a 23.64 percent efficiency.,[],"Mon, 26 Feb 2024 11:46:16 EST"
571,A novel method for easy and quick fabrication of biomimetic robots with life-like movement,https://www.sciencedaily.com/releases/2024/02/240226114557.htm,"Ultraviolet-laser processing is a promising technique for developing intricate microstructures, enabling complex alignment of muscle cells, required for building life-like biohybrid actuators. Compared to traditional complex methods, this innovative technique enables easy and quick fabrication of microstructures with intricate patterns for achieving different muscle cell arrangements, paving the way for biohybrid actuators capable of complex, flexible movements.",[],"Mon, 26 Feb 2024 11:45:57 EST"
572,Predatory fish use rapid color changes to coordinate attacks,https://www.sciencedaily.com/releases/2024/02/240226114549.htm,"Striped marlin are some of the fastest animals on the planet and one of the ocean's top predators. When hunting in groups, individual marlin will take turns attacking schools of prey fish one at a time. Now a new study helps to explain how they might coordinate this turn-taking style of attack on their prey to avoid injuring each other. The key, according to the new work, is rapid color changes.",[],"Mon, 26 Feb 2024 11:45:49 EST"
573,Blindness from some inherited eye diseases may be caused by gut bacteria,https://www.sciencedaily.com/releases/2024/02/240226114543.htm,"Sight loss in certain inherited eye diseases may be caused by gut bacteria, and is potentially treatable by antimicrobials, finds a new study in mice.",[],"Mon, 26 Feb 2024 11:45:43 EST"
574,Metal scar found on cannibal star,https://www.sciencedaily.com/releases/2024/02/240226114047.htm,"When a star like our Sun reaches the end of its life, it can ingest the surrounding planets and asteroids that were born with it. Now, researchers have found a unique signature of this process for the first time -- a scar imprinted on the surface of a white dwarf star.",[],"Mon, 26 Feb 2024 11:40:47 EST"
575,"Drug limits dangerous reactions to allergy-triggering foods, Stanford Medicine-led study of kids finds",https://www.sciencedaily.com/releases/2024/02/240225212501.htm,"A drug that binds to allergy-causing antibodies can protect children from dangerous reactions to accidentally eating allergy-triggering foods, a new study found.",[],"Sun, 25 Feb 2024 21:25:01 EST"
576,Barriers against Antarctic ice melt disappearing at the double,https://www.sciencedaily.com/releases/2024/02/240223103904.htm,"Undersea anchors of ice that help prevent Antarctica's land ice from slipping into the ocean are shrinking at more than twice the rate compared with 50 years ago, research shows. More than a third of these frozen moorings, known as pinning points, have decreased in size since the turn of the century, experts say. Further deterioration of pinning points, which hold in place the floating ice sheets that fortify Antarctica's land ice, would accelerate the continent's contribution to rising sea levels, scientists warn.",[],"Fri, 23 Feb 2024 10:39:04 EST"
577,Compound vital for all life likely played a role in life's origin,https://www.sciencedaily.com/releases/2024/02/240222214056.htm,"A chemical compound essential to all living things has been synthesized in a lab in conditions that could have occurred on early Earth, suggesting it played a role at the outset of life.",[],"Thu, 22 Feb 2024 21:40:56 EST"
578,Side effects of wide scale forestation could reduce carbon removal benefits by up to a third,https://www.sciencedaily.com/releases/2024/02/240222214045.htm,"The side effects of large-scale forestation initiatives could reduce the CO2 removal benefits by up to a third, a pioneering study has found.",[],"Thu, 22 Feb 2024 21:40:45 EST"
579,Chemists synthesize unique anticancer molecules using novel approach,https://www.sciencedaily.com/releases/2024/02/240222214042.htm,"Nearly 30 years ago, scientists discovered a unique class of anticancer molecules in a family of bryozoans, a phylum of marine invertebrates found in tropical waters. The chemical structures of these molecules, which consist of a dense, highly complex knot of oxidized rings and nitrogen atoms, has attracted the interest of organic chemists worldwide, who aimed to recreate these structures from scratch in the laboratory. However, despite considerable effort, it has remained an elusive task. Until now, that is. A team of chemists has succeeded in synthesizing eight of the compounds for the first time using an approach that combines inventive chemical strategy with the latest technology in small molecule structure determination.",[],"Thu, 22 Feb 2024 21:40:42 EST"
580,Webb finds evidence for neutron star at heart of young supernova remnant,https://www.sciencedaily.com/releases/2024/02/240222214025.htm,"NASA's James Webb Space Telescope has found the best evidence yet for emission from a neutron star at the site of a recently observed supernova. The supernova, known as SN 1987A, was a core-collapse supernova, meaning the compacted remains at its core formed either a neutron star or a black hole. Evidence for such a compact object has long been sought, and while indirect evidence for the presence of a neutron star has previously been found, this is the first time that the effects of high-energy emission from the probable young neutron star have been detected.",[],"Thu, 22 Feb 2024 21:40:25 EST"
581,Combination of group competition and repeated interactions promotes cooperation,https://www.sciencedaily.com/releases/2024/02/240222132201.htm,"How did cooperative behavior prevail in human evolution? Researchers have challenged two prevailing explanations -- repeated interactions on the one hand or group competition on the other. Instead, both mechanisms synergistically contribute to fostering cooperation effectively.",[],"Thu, 22 Feb 2024 13:22:01 EST"
582,A new beginning: The search for more temperate Tatooines,https://www.sciencedaily.com/releases/2024/02/240222122425.htm,"Luke Skywalker's childhood might have been slightly less harsh if he'd grown up on a more temperate Tatooine -- like the ones identified in a new study. According to the study's authors, there are more climate-friendly planets in binary star systems -- in other words, those with two suns -- than previously known. And, they say, it may be a sign that, at least in some ways, the universe leans in the direction of orderly alignment rather than chaotic misalignment.",[],"Thu, 22 Feb 2024 12:24:25 EST"
583,Photon upconversion: Steering light with supercritical coupling,https://www.sciencedaily.com/releases/2024/02/240222122407.htm,"Researchers have unveiled a novel concept termed 'supercritical coupling' that enables several folds increase in photon upconversion efficiency. This discovery not only challenges existing paradigms, but also opens a new direction in the control of light emission.",[],"Thu, 22 Feb 2024 12:24:07 EST"
584,Underlying cause of 'brain fog' linked with long COVID discovered,https://www.sciencedaily.com/releases/2024/02/240222122347.htm,Scientists have announced a major discovery that has profound importance for our understanding of brain fog and cognitive decline seen in some patients with Long COVID. The findings showed that there was disruption to the integrity of the blood vessels in the brains of patients suffering from Long COVID and brain fog. This blood vessel 'leakiness' was able to objectively distinguish those patients with brain fog and cognitive decline compared to patients suffering from Long-COVID but not with brain fog.,[],"Thu, 22 Feb 2024 12:23:47 EST"
585,Treating newly-diagnosed Crohn's patients with advanced therapy leads to dramatic improvements in outcomes,https://www.sciencedaily.com/releases/2024/02/240222122339.htm,"A large-scale clinical trial of treatment strategies for Crohn's disease has shown that offering early advanced therapy to all patients straight after diagnosis can drastically improve outcomes, including by reducing the number of people requiring urgent abdominal surgery for treatment of their disease by ten-fold.",[],"Thu, 22 Feb 2024 12:23:39 EST"
586,Brightest and fastest-growing: Astronomers identify record-breaking quasar,https://www.sciencedaily.com/releases/2024/02/240222122324.htm,"Astronomers have characterized a bright quasar, finding it to be not only the brightest of its kind, but also the most luminous object ever observed. Quasars are the bright cores of distant galaxies and they are powered by supermassive black holes. The black hole in this record-breaking quasar is growing in mass by the equivalent of one Sun per day, making it the fastest-growing black hole to date.",[],"Thu, 22 Feb 2024 12:23:24 EST"
587,Real-time wearable human emotion recognition technology developed,https://www.sciencedaily.com/releases/2024/02/240222122318.htm,A research team has unveiled a groundbreaking technology that can recognize human emotions in real time.,[],"Thu, 22 Feb 2024 12:23:18 EST"
588,"Three years later, search for life on Mars continues",https://www.sciencedaily.com/releases/2024/02/240222122312.htm,"Scientists suspect Mars once had long-lived rivers, lakes and streams. Today, water on Mars is found in ice at the poles and trapped below the Martian surface. Researchers now reveal that Mars also may have had hydrothermal systems based on the hydrated magnesium sulfate the rover identified in the volcanic rocks.",[],"Thu, 22 Feb 2024 12:23:12 EST"
589,Biggest Holocene volcano eruption found by seabed survey,https://www.sciencedaily.com/releases/2024/02/240222004557.htm,"A detailed survey of the volcanic underwater deposits around the Kikai caldera in Japan clarified the deposition mechanisms as well as the event's magnitude. As a result, the research team found that the event 7,300 years ago was the largest volcanic eruption in the Holocene by far.",[],"Thu, 22 Feb 2024 00:45:57 EST"
590,New realistic computer model will help robots collect Moon dust,https://www.sciencedaily.com/releases/2024/02/240222004554.htm,A new computer model mimics Moon dust so well that it could lead to smoother and safer Lunar robot teleoperations.,[],"Thu, 22 Feb 2024 00:45:54 EST"
591,Method identified to double computer processing speeds,https://www.sciencedaily.com/releases/2024/02/240221213907.htm,"Scientists introduce what they call 'simultaneous and heterogeneous multithreading' or SHMT. This system doubles computer processing speeds with existing hardware by simultaneously using graphics processing units (GPUs), hardware accelerators for artificial intelligence (AI) and machine learning (ML), or digital signal processing units to process information.",[],"Wed, 21 Feb 2024 21:39:07 EST"
592,Finding new physics in debris from colliding neutron stars,https://www.sciencedaily.com/releases/2024/03/240306150618.htm,"Neutron star mergers are a treasure trove for new physics signals, with implications for determining the true nature of dark matter, according to physicists.",[],"Wed, 06 Mar 2024 15:06:18 EST"
593,Astronomers spot oldest 'dead' galaxy yet observed,https://www.sciencedaily.com/releases/2024/03/240306150612.htm,"A galaxy that suddenly stopped forming new stars more than 13 billion years ago has been observed by astronomers. Using the James Webb Space Telescope, astronomers have spotted a 'dead' galaxy when the universe was just 700 million years old, the oldest such galaxy ever observed.",[],"Wed, 06 Mar 2024 15:06:12 EST"
594,"Compact chips advance precision timing for communications, navigation and other applications",https://www.sciencedaily.com/releases/2024/03/240306150545.htm,"Precision timing and synchronization are crucial for navigation, communication and radar systems. Scientists have built compact chips capable of converting light into microwaves, which could improve these systems. This technology shrinks a tabletop system into a chip-sized format, reducing power usage and making it more applicable for use in everyday devices.",[],"Wed, 06 Mar 2024 15:05:45 EST"
595,Discovery tests theory on cooling of white dwarf stars,https://www.sciencedaily.com/releases/2024/03/240306150510.htm,Open any astronomy textbook to the section on white dwarf stars and you'll likely learn that they are 'dead stars' that continuously cool down over time. Astronomers are challenging this theory after discovering a population of white dwarf stars that stopped cooling for more than eight billion years.,[],"Wed, 06 Mar 2024 15:05:10 EST"
596,New type of nanoparticle makes vaccines more powerful,https://www.sciencedaily.com/releases/2024/03/240306150424.htm,A type of nanoparticle called a metal organic framework (MOF) could be used to deliver vaccines and act as an adjuvant. Researchers find these particles provoke a strong immune response by activating the innate immune system through cell proteins called toll-like receptors.,[],"Wed, 06 Mar 2024 15:04:24 EST"
597,Advanced noise suppression technology for improved search and rescue drones,https://www.sciencedaily.com/releases/2024/03/240306145036.htm,"Unmanned Aerial Vehicles (UAVs) are beneficial in search and rescue missions during natural disasters like earthquakes. However, current UAVs depend on visual information and cannot detect victims trapped under rubble. While some studies have used sound for detection, the noise from UAV propellers can drown out human sounds. To address this issue, researchers have developed a novel artificial intelligence-based system that effectively suppresses UAV noise and amplifies human sounds.",[],"Wed, 06 Mar 2024 14:50:36 EST"
598,AI can speed design of health software,https://www.sciencedaily.com/releases/2024/03/240306144729.htm,"Artificial intelligence helped clinicians to accelerate the design of diabetes prevention software, a new study finds.",[],"Wed, 06 Mar 2024 14:47:29 EST"
599,Can you tell AI-generated people from real ones?,https://www.sciencedaily.com/releases/2024/03/240306003456.htm,"If you recently had trouble figuring out if an image of a person is real or generated through artificial intelligence (AI), you're not alone. A new study found that people had more difficulty than was expected distinguishing who is a real person and who is artificially generated.",[],"Wed, 06 Mar 2024 00:34:56 EST"
600,Key advance toward removing common herbicide from groundwater,https://www.sciencedaily.com/releases/2024/03/240305204445.htm,Chemists are closing in on a new tool for tackling the global problem of weedkiller-tainted groundwater.,[],"Tue, 05 Mar 2024 20:44:45 EST"
601,Aluminum nanoparticles make tunable green catalysts,https://www.sciencedaily.com/releases/2024/03/240305204424.htm,A nanotechnology pioneer has uncovered a transformative approach to harnessing the catalytic power of aluminum nanoparticles by annealing them in various gas atmospheres at high temperatures.,[],"Tue, 05 Mar 2024 20:44:24 EST"
602,Harmful 'forever chemicals' removed from water with new electrocatalysis method,https://www.sciencedaily.com/releases/2024/03/240305165911.htm,"Scientists have developed new electrochemical approaches to clean up pollution from 'forever chemicals' found in clothing, food packaging, firefighting foams, and a wide array of other products. A new study describes nanocatalysts developed to remediate per- and polyfluoroalkyl substances, known as PFAS.",[],"Tue, 05 Mar 2024 16:59:11 EST"
603,Evolving hydrogen-storage technology: Guidelines developed for the design of anti-evaporation catalysts,https://www.sciencedaily.com/releases/2024/03/240305165709.htm,A research team has identified materials capable of catalyzing the conversion of ortho-hydrogen to para-hydrogen. These catalysts should be essential to the spread of mass-transportation/storage of liquid hydrogen.,[],"Tue, 05 Mar 2024 16:57:09 EST"
604,Using light to precisely control single-molecule devices,https://www.sciencedaily.com/releases/2024/03/240305134359.htm,Researchers flip the switch at the nanoscale by applying light to induce bonding for single-molecule device switching.,[],"Tue, 05 Mar 2024 13:43:59 EST"
605,New method measures the 3D position of individual atoms,https://www.sciencedaily.com/releases/2024/03/240305134352.htm,"Since more than a decade it has been possible for physicists to accurately measure the location of individual atoms to a precision of smaller than one thousandth of a millimeter using a special type of microscope. However, this method has so far only provided the x and y coordinates. Information on the vertical position of the atom -- i.e., the distance between the atom and the microscope objective -- is lacking. A new method has now been developed that can determine all three spatial coordinates of an atom with one single image.",[],"Tue, 05 Mar 2024 13:43:52 EST"
606,"Researchers closing in on genetic treatments for hereditary lung disease, vision loss",https://www.sciencedaily.com/releases/2024/03/240305134330.htm,"Researchers who work with tiny drug carriers known as lipid nanoparticles have developed a new type of material capable of reaching the lungs and the eyes, an important step toward genetic therapy for hereditary conditions like cystic fibrosis and inherited vision loss.",[],"Tue, 05 Mar 2024 13:43:30 EST"
607,Shortcut to Success: Toward fast and robust quantum control through accelerating adiabatic passage,https://www.sciencedaily.com/releases/2024/03/240305134315.htm,"Researchers achieved the acceleration of adiabatic evolution of a single spin qubit in gate-defined quantum dots. After the pulse optimization to suppress quasistatic noises, the spin flip fidelity can be as high as 97.5% in GaAs quantum dots. This work may be useful to achieve fast and high-fidelity quantum computing.",[],"Tue, 05 Mar 2024 13:43:15 EST"
608,Smart optimization paves the way for improved silicon solar cell performance,https://www.sciencedaily.com/releases/2024/03/240305134312.htm,"Bayesian optimization (BO) can help determine the optimal deposition conditions for high-performance passivation films in solar cells. However, simple implementations tend to suggest excessively thick films or infeasible deposition conditions. Recently, researchers from Japan developed a novel constrained BO scheme that combines several prediction models to restrict suggested deposition conditions to realistic values. Their strategy could help minimize trial and error and improve material performance in various fields.",[],"Tue, 05 Mar 2024 13:43:12 EST"
609,'Like a lab in your pocket' -- new test strips raise game in gene-based diagnostics,https://www.sciencedaily.com/releases/2024/03/240305134300.htm,Biosensing technology developed by engineers has made it possible to create gene test strips that rival conventional lab-based tests in quality.,[],"Tue, 05 Mar 2024 13:43:00 EST"
610,Running performance helped by mathematical research,https://www.sciencedaily.com/releases/2024/03/240305134255.htm,"A new mathematical model has shown, with great precision, the impact that physiological and psychological parameters have on running performance and provides tips for optimized training.",[],"Tue, 05 Mar 2024 13:42:55 EST"
611,Groundbreaking survey reveals secrets of planet birth around dozens of stars,https://www.sciencedaily.com/releases/2024/03/240305134240.htm,"A team of astronomers has shed new light on the fascinating and complex process of planet formation. The research brings together observations of more than 80 young stars that might have planets forming around them, providing astronomers with a wealth of data and unique insights into how planets arise in different regions of our galaxy.",[],"Tue, 05 Mar 2024 13:42:40 EST"
612,One way to improve a fusion reaction: Use weaknesses as strengths,https://www.sciencedaily.com/releases/2024/03/240305134238.htm,"Scientists are embracing imperfection, using less-than-ideal magnetic fields to make the plasma more manageable.",[],"Tue, 05 Mar 2024 13:42:38 EST"
613,What makes black holes grow and new stars form? Machine learning helps solve the mystery,https://www.sciencedaily.com/releases/2024/03/240305134225.htm,It takes more than a galaxy merger to make a black hole grow and new stars form: machine learning shows cold gas is needed too to initiate rapid growth -- new research finds.,[],"Tue, 05 Mar 2024 13:42:25 EST"
614,"Juno spacecraft measures oxygen production on Jupiter's moon, Europa",https://www.sciencedaily.com/releases/2024/03/240305134217.htm,"NASA's Juno spacecraft has directly measured charged oxygen and hydrogen molecules from the atmosphere of one of Jupiter's largest moons, Europa. These observations provide key constraints on the potential oxygenation of its subsurface ocean.",[],"Tue, 05 Mar 2024 13:42:17 EST"
615,"After decades of Arctic sea ice getting faster and more hazardous for transport, models suggest a dramatic reversal is coming",https://www.sciencedaily.com/releases/2024/03/240305134215.htm,"Will ice floating in the Arctic Ocean move faster or slower over the coming decades? The answer to this question will tell us whether marine transportation can be expected to get more or less hazardous. It might also have important implications for the rate of ice cover loss, which is hugely consequential for Northern Indigenous communities, ecosystems, and the global climate system. While observational data suggest the trend has been towards faster sea ice speeds, climate models project that those speeds will slow down during the summer season. This contrast has led to some questions around the plausibility of the model projections.",[],"Tue, 05 Mar 2024 13:42:15 EST"
616,New cardiovascular imaging approach provides a better view of dangerous plaques,https://www.sciencedaily.com/releases/2024/03/240305134213.htm,"Researchers have developed a new catheter-based device that combines two powerful optical techniques to image the dangerous plaques that can build up inside the arteries that supply blood to the heart. By providing new details about plaque, the device could help clinicians and researchers improve treatments for preventing heart attacks and strokes.",[],"Tue, 05 Mar 2024 13:42:13 EST"
617,Robotic-assisted surgery for gallbladder cancer as effective as traditional surgery,https://www.sciencedaily.com/releases/2024/03/240305134210.htm,"Each year, approximately 2,000 people die annually of gallbladder cancer (GBC) in the U.S., with only one in five cases diagnosed at an early stage. With GBC rated as the first biliary tract cancer and the 17th most deadly cancer worldwide, pressing attention for proper management of disease must be addressed. For patients diagnosed, surgery is the most promising curative treatment. While there has been increasing adoption of minimally invasive surgical techniques in gastrointestinal malignancies, including utilization of laparoscopic and robotic surgery, there are reservations in utilizing minimally invasive surgery for gallbladder cancer. A new study has found that robotic-assisted surgery for GBC is as effective as traditional open and laparoscopic methods, with added benefits in precision and quicker post-operative recovery.",[],"Tue, 05 Mar 2024 13:42:10 EST"
618,Researchers invent new triple-junction tandem solar cells with world-record efficiency,https://www.sciencedaily.com/releases/2024/03/240304221140.htm,"Scientists have developed a novel triple-junction perovskite/Si tandem solar cell that can achieve a certified world-record power conversion efficiency of 27.1 per cent across a solar energy absorption area of 1 sq cm, representing the best-performing triple-junction perovskite/Si tandem solar cell thus far. To achieve this, the team engineered a new cyanate-integrated perovskite solar cell that is stable and energy efficient.",[],"Mon, 04 Mar 2024 22:11:40 EST"
619,JWST captures the end of planet formation,https://www.sciencedaily.com/releases/2024/03/240304195509.htm,The James Webb Space Telescope is helping scientists uncover how planets form by advancing understanding of their birthplaces and the circumstellar disks surrounding young stars. Scientists have imaged winds from an old planet-forming disk (still very young relative to the Sun) which is actively dispersing its gas content. Knowing when the gas disperses is important as it constrains the time left for nascent planets to consume the gas from their surroundings.,[],"Mon, 04 Mar 2024 19:55:09 EST"
620,Scientists revolutionize wireless communication with three-dimensional processors,https://www.sciencedaily.com/releases/2024/03/240304195504.htm,Scientists have pioneered a method for using semiconductor technology to manufacture processors that significantly enhance the efficiency of transmitting vast amounts of data across the globe.,[],"Mon, 04 Mar 2024 19:55:04 EST"
621,New AI model draws treasure maps to diagnose disease,https://www.sciencedaily.com/releases/2024/03/240304195443.htm,"Researchers have developed an artificial intelligence model that can accurately identify tumors and diseases in medical images. The tool draws a map to explain each diagnosis, helping doctors follow its line of reasoning, check for accuracy, and explain the results to patients.",[],"Mon, 04 Mar 2024 19:54:43 EST"
622,Scientists put forth a smarter way to protect a smarter grid,https://www.sciencedaily.com/releases/2024/03/240304195348.htm,"Scientists have put forth a new approach to protect the electric grid, creating a tool that sorts and prioritizes cyber threats on the fly.",[],"Mon, 04 Mar 2024 19:53:48 EST"
623,3D-printed skin closes wounds and contains hair follicle precursors,https://www.sciencedaily.com/releases/2024/03/240304135920.htm,"Fat tissue holds the key to 3D printing layered living skin and potentially hair follicles, according to researchers who recently harnessed fat cells and supporting structures from clinically procured human tissue to precisely correct injuries in rats. The advancement could have implications for reconstructive facial surgery and even hair growth treatments for humans.",[],"Mon, 04 Mar 2024 13:59:20 EST"
624,Webb unlocks secrets of one of the most distant galaxies ever seen,https://www.sciencedaily.com/releases/2024/03/240304135917.htm,"Looking deeply into space and time, astronomers have studied the exceptionally luminous galaxy GN-z11, which existed when our 13.8 billion-year-old universe was only about 430 million years old.",[],"Mon, 04 Mar 2024 13:59:17 EST"
625,Spontaneous curvature the key to shape-shifting nanomaterials,https://www.sciencedaily.com/releases/2024/03/240304135857.htm,"Inspired by nature, nanotechnology researchers have identified 'spontaneous curvature' as the key factor determining how ultra-thin, artificial materials can transform into useful tubes, twists and helices.",[],"Mon, 04 Mar 2024 13:58:57 EST"
626,Opening a window on the brain,https://www.sciencedaily.com/releases/2024/03/240304135836.htm,"Researchers have introduced a new method for in vivo brain imaging, enabling large-scale and long-term observation of neuronal structures and activities in awake mice. This method is called the 'nanosheet incorporated into light-curable resin' (NIRE) method, and it uses fluoropolymer nanosheets covered with light-curable resin to create larger cranial windows.",[],"Mon, 04 Mar 2024 13:58:36 EST"
627,A key to the future of robots could be hiding in liquid crystals,https://www.sciencedaily.com/releases/2024/03/240304135831.htm,"Robots and cameras of the future could be made of liquid crystals, thanks to a new discovery that significantly expands the potential of the chemicals already common in computer displays and digital watches. The findings are a simple and inexpensive way to manipulate the molecular properties of liquid crystals with light exposure.",[],"Mon, 04 Mar 2024 13:58:31 EST"
628,New dressing robot can 'mimic' the actions of care-workers,https://www.sciencedaily.com/releases/2024/03/240304135829.htm,Scientists have developed a new robot that can 'mimic' the two-handed movements of care-workers as they dress an individual.,[],"Mon, 04 Mar 2024 13:58:29 EST"
629,Network of quantum sensors boosts precision,https://www.sciencedaily.com/releases/2024/03/240304135823.htm,Quantum sensor technology promises even more precise measurements of physical quantities. A team has now compared the signals of up to 91 quantum sensors with each other and thus successfully eliminated the noise caused by interactions with the environment. Correlation spectroscopy can be used to increase the precision of sensor networks.,[],"Mon, 04 Mar 2024 13:58:23 EST"
630,Cost of direct air carbon capture to remain higher than hoped,https://www.sciencedaily.com/releases/2024/03/240304135808.htm,"Researchers estimate the cost of removing 1 ton of CO2 from the air in the year 2050 to be between 230 and 540 US dollars to remove 1 ton. This is twice as high as previous estimates. The researchers compared the potential costs of three technologies that are already in use. From today's perspective, none of these technologies has clear advantages over the others in terms of potential costs. All three technologies should therefore be further developed, say the researchers.",[],"Mon, 04 Mar 2024 13:58:08 EST"
631,Researchers use liquid crystals to control polarization inside laser-written waveguides,https://www.sciencedaily.com/releases/2024/03/240304135803.htm,"Researchers have developed a new way to control and manipulate optical signals by embedding a liquid crystal layer into waveguides created with direct laser writing. The new devices enable electro-optical control of polarization, which could open new possibilities for chip-based devices and complex photonic circuits based on femtosecond-written waveguides.",[],"Mon, 04 Mar 2024 13:58:03 EST"
632,New AI smartphone tool accurately diagnoses ear infections,https://www.sciencedaily.com/releases/2024/03/240304135751.htm,"A new cell phone app developed by physician-scientists, which uses artificial intelligence (AI) to accurately diagnose ear infections, or acute otitis media (AOM), could help decrease unnecessary antibiotic use in young children, according to new research.",[],"Mon, 04 Mar 2024 13:57:51 EST"
633,Study determines the original orientations of rocks drilled on Mars,https://www.sciencedaily.com/releases/2024/03/240304135739.htm,Geologists determined the original orientation of many of the Mars bedrock samples collected by the Perseverance rover. The findings can give scientists clues to the conditions in which the rocks originally formed.,[],"Mon, 04 Mar 2024 13:57:39 EST"
634,Tests show high-temperature superconducting magnets are ready for fusion,https://www.sciencedaily.com/releases/2024/03/240304135732.htm,"A comprehensive study of high-temperature superconducting magnets confirms they meet requirements for an economic, compact fusion power plant.",[],"Mon, 04 Mar 2024 13:57:32 EST"
635,Evolution-capable AI promotes green hydrogen production using more abundant chemical elements,https://www.sciencedaily.com/releases/2024/03/240302171516.htm,"A research team has developed an AI technique capable of expediting the identification of materials with desirable characteristics. Using this technique, the team was able to discover high-performance water electrolyzer electrode materials free of platinum-group elements -- substances previously thought to be indispensable in water electrolysis. These materials may be used to reduce the cost of large-scale production of green hydrogen -- a next-generation energy source.",[],"Sat, 02 Mar 2024 17:15:16 EST"
636,Researchers create coating solution for safer food storage,https://www.sciencedaily.com/releases/2024/03/240301160123.htm,"Galvanized steel containers and surfaces are used for harvested produce because of their durability, strength and lower cost compared to stainless steel. However, bacteria residing in storage containers can cause corrosion. The new coating will reduce corrosion by at least 70 percent, researchers say.",[],"Fri, 01 Mar 2024 16:01:23 EST"
637,AI outperforms humans in standardized tests of creative potential,https://www.sciencedaily.com/releases/2024/03/240301134758.htm,"In a recent study, 151 human participants were pitted against ChatGPT-4 in three tests designed to measure divergent thinking, which is considered to be an indicator of creative thought.",[],"Fri, 01 Mar 2024 13:47:58 EST"
638,Researchers use GPS-tracked icebergs in novel study to improve climate models,https://www.sciencedaily.com/releases/2024/03/240301134753.htm,"Research unearthed new information to help scientists better understand circulation patterns of ocean water around glaciers. In the summers of 2014 and 2019, a group of pioneers in glacial research attached GPS devices to 13 icebergs and tracked hourly changes in their positions as they passed through Greenland's Ilulissat Icefjord toward the ocean. Study results showed circulation in the primary fjord is greatly affected by freshwater flow from connecting tributary fjords, which is critically important to consider in circulation models.",[],"Fri, 01 Mar 2024 13:47:53 EST"
639,Unlocking the potential of lithium-ion batteries with advanced binders,https://www.sciencedaily.com/releases/2024/03/240301134705.htm,"Lithium-ion batteries employ binders that encounter challenges such as poor conductivity and expansion during charging. In a recent study, scientists have developed a high-performing binder using poly(vinylphosphonic acid) for silicon oxide-based anodes in lithium-ion batteries. This binder offers enhanced performance as demonstrated by the superior durability, and discharging capacity of the anodes compared to conventional options. With patents filed internationally, this technology holds promise for broader applications in electric vehicles and beyond.",[],"Fri, 01 Mar 2024 13:47:05 EST"
640,AI-enabled atomic robotic probe to advance quantum material manufacturing,https://www.sciencedaily.com/releases/2024/03/240301134703.htm,"Scientists have pioneered a new methodology of fabricating carbon-based quantum materials at the atomic scale by integrating scanning probe microscopy techniques and deep neural networks. This breakthrough highlights the potential of implementing artificial intelligence at the sub-angstrom scale for enhanced control over atomic manufacturing, benefiting both fundamental research and future applications.",[],"Fri, 01 Mar 2024 13:47:03 EST"
641,New insights on how galaxies are formed,https://www.sciencedaily.com/releases/2024/03/240301134700.htm,Astronomers can use supercomputers to simulate the formation of galaxies from the Big Bang 13.8 billion years ago to the present day. But there are a number of sources of error. An international research team has spent a hundred million computer hours over eight years trying to correct these.,[],"Fri, 01 Mar 2024 13:47:00 EST"
642,Scientists make nanoparticles dance to unravel quantum limits,https://www.sciencedaily.com/releases/2024/03/240301134651.htm,"The question of where the boundary between classical and quantum physics lies is one of the longest-standing pursuits of modern scientific research and in new research, scientists demonstrate a novel platform that could help us find an answer.",[],"Fri, 01 Mar 2024 13:46:51 EST"
643,It's not only opposites that attract -- new study shows like-charged particles can come together,https://www.sciencedaily.com/releases/2024/03/240301134643.htm,"A study shows that similarly charged particles can sometimes attract, rather than repel. The team found that like-charged particles suspended in liquids can attract one another at long-range, depending on the solvent and the sign of the charge. The study has immediate implications for processes that involve interactions in solution across various length-scales, including self-assembly, crystallization, and phase separation.",[],"Fri, 01 Mar 2024 13:46:43 EST"
644,Software speeds up drug development,https://www.sciencedaily.com/releases/2024/03/240301134637.htm,"Sugars cover nearly all proteins present at the surface of the cells in our bodies, forming a shield around the proteins. Thus, these sugars influence how cells interact with their environment including pathogens, playing an important role in medical drug development. GlycoSHIELD, a new computational approach to study the sugar shields of proteins, is resource-reducing, time-efficient and user-friendly.",[],"Fri, 01 Mar 2024 13:46:37 EST"
645,Umbrella for atoms: The first protective layer for 2D quantum materials,https://www.sciencedaily.com/releases/2024/03/240301134631.htm,"As silicon-based computer chips approach their physical limitations in the quest for faster and smaller designs, the search for alternative materials that remain functional at atomic scales is one of science's biggest challenges. In a groundbreaking development, researchers have engineered a protective film that shields quantum semiconductor layers just one atom thick from environmental influences without compromising their revolutionary quantum properties. This puts the application of these delicate atomic layers in ultrathin electronic components within realistic reach.",[],"Fri, 01 Mar 2024 13:46:31 EST"
646,Lithium-ion batteries from drones might find second lives in less 'stressful' devices,https://www.sciencedaily.com/releases/2024/03/240301134616.htm,Taking flight can be stressful -- especially for a lithium-ion battery that powers a drone. Too much strain on these cells causes damage and shortens a device's overall lifespan. Research shows the potential to improve batteries in aerial electric vehicles that take off and land vertically. The team developed a new electrolyte to address these challenges and said the 'stressed out' batteries could also have second lives in less strenuous applications.,[],"Fri, 01 Mar 2024 13:46:16 EST"
647,Ultraviolet radiation from massive stars shapes planetary systems,https://www.sciencedaily.com/releases/2024/03/240301134610.htm,"Up to a certain point, very luminous stars can have a positive effect on the formation of planets, but from that point on the radiation they emit can cause the material in protoplanetary discs to disperse.",[],"Fri, 01 Mar 2024 13:46:10 EST"
648,Hurricanes and power grids: Eliminating large-scale outages with a new approach,https://www.sciencedaily.com/releases/2024/03/240301134253.htm,"Large scale-power outages caused by tropical cyclones can be prevented almost entirely if a small but critical set of power lines is protected against storm damages, a new study finds. Scientists developed a new method that can be used to identify those critical lines and increase the system's resilience.",[],"Fri, 01 Mar 2024 13:42:53 EST"
649,Ice shell thickness reveals water temperature on ocean worlds,https://www.sciencedaily.com/releases/2024/02/240229182929.htm,"Astrobiologists have devised a novel way to determine ocean temperatures of distant worlds based on the thickness of their ice shells, effectively conducting oceanography from space.",[],"Thu, 29 Feb 2024 18:29:29 EST"
650,Improving energy security with policies focused on demand-side solutions,https://www.sciencedaily.com/releases/2024/02/240229182848.htm,"Energy systems essential to supporting our everyday activities face increasing threats from wars, pandemics, climate change, and other unexpected events. An international team of researchers found that demand-oriented solutions have a significantly greater potential to reduce our vulnerability to energy crises compared to supply measures.",[],"Thu, 29 Feb 2024 18:28:48 EST"
651,Astronomers measure heaviest black hole pair ever found,https://www.sciencedaily.com/releases/2024/02/240229182830.htm,"Using archival data from the Gemini North telescope, a team of astronomers has measured the heaviest pair of supermassive black holes ever found. The merging of two supermassive black holes is a phenomenon that has long been predicted, though never observed. This massive pair gives clues as to why such an event seems so unlikely in the Universe.",[],"Thu, 29 Feb 2024 18:28:30 EST"
652,Get Ready for Gemma: The Game-Changing 2B 7B 6Trillion Token Platform,https://www.franksworld.com/2024/03/04/get-ready-for-gemma-the-game-changing-2b-7b-6trillion-token-platform/?utm_source=rss&utm_medium=rss&utm_campaign=get-ready-for-gemma-the-game-changing-2b-7b-6trillion-token-platform,This video is from Sam Witteveen.,[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:48:01 +0000"
653,Nintendo just picked a fight with open-source project Yuzu,https://www.franksworld.com/2024/03/04/nintendo-just-picked-a-fight-with-open-source-project-yuzu/?utm_source=rss&utm_medium=rss&utm_campaign=nintendo-just-picked-a-fight-with-open-source-project-yuzu,This video is from Fireship. Nintendo recently sued an open-source Nintendo Switch emulator called Yuzu. Let&#8217;s take a look at the hacking techniques used to exploit the Switch and find out what this lawsuit means for game developers.,[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:45:36 +0000"
654,The NSA has Picked these Programming Languages,https://www.franksworld.com/2024/03/04/the-nsa-has-picked-these-programming-languages/?utm_source=rss&utm_medium=rss&utm_campaign=the-nsa-has-picked-these-programming-languages,"The NSA has recommended a few programming languages, including Rust, C#, Java, Go and others. https://media.defense.gov/2022/Nov/10/2003112742/-1/-1/0/CSI_SOFTWARE_MEMORY_SAFETY.PDF This video is from Stefan Mischook.",[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:43:59 +0000"
655,Unveiling Microsoft’s Revolutionary 1-bit LLMs in AI Research,https://www.franksworld.com/2024/03/04/unveiling-microsofts-revolutionary-1-bit-llms-in-ai-research/?utm_source=rss&utm_medium=rss&utm_campaign=unveiling-microsofts-revolutionary-1-bit-llms-in-ai-research,"In this video from AI Papers Academy.dive into a recent research paper by Microsoft: &#8220;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&#8221;. This paper introduce an interesting and exciting architecture for large language models, called BitNet b1.58, which significantly reduces LLMs memory consumption, and speeds-up LLMs inference latency. All of [&#8230;]",[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:43:03 +0000"
656,40 APIs Every Developer Should Use (in 12 minutes),https://www.franksworld.com/2024/03/04/40-apis-every-developer-should-use-in-12-minutes/?utm_source=rss&utm_medium=rss&utm_campaign=40-apis-every-developer-should-use-in-12-minutes,"In this video, Coding with Lewis introduces you to 40 APIs that every developer should know and use. From social media to finance to weather, these APIs will help you create amazing applications. Most of these APIs are free without an API key, but might require some sort of sign up or a trial to [&#8230;]",[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:41:03 +0000"
657,Do we create reality with our mind? A physicist’s reply.,https://www.franksworld.com/2024/03/04/do-we-create-reality-with-our-mind-a-physicists-reply/?utm_source=rss&utm_medium=rss&utm_campaign=do-we-create-reality-with-our-mind-a-physicists-reply,"This video is from Sabine Hossenfelder. Do we create reality with our minds? I got this question on twitter the other day and after rolling my eyes about it for some while, I decided it’s actually a good question. You might think the answer is obviously “no”. But it’s not that simple. Let me explain.",[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:40:07 +0000"
658,How to Master Machine Learning: A Beginner’s Guide for 2024,https://www.franksworld.com/2024/03/04/how-to-master-machine-learning-a-beginners-guide-for-2024/?utm_source=rss&utm_medium=rss&utm_campaign=how-to-master-machine-learning-a-beginners-guide-for-2024,"This video is from freeCodeCamp.org. This machine learning course is created for beginners who are learning in 2024. The course begins with a Machine Learning Roadmap for 2024, emphasizing career paths and beginner-friendly theory. Then it the course moves on to hands-on practical applications and a comprehensive end-to-end project using Python. ✏️ Course created by [&#8230;]",[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:37:51 +0000"
659,Maximize Your Editing Efficiency: 7 Must-Have AI Tools in Premiere Pro,https://www.franksworld.com/2024/03/04/maximize-your-editing-efficiency-7-must-have-ai-tools-in-premiere-pro/?utm_source=rss&utm_medium=rss&utm_campaign=maximize-your-editing-efficiency-7-must-have-ai-tools-in-premiere-pro,This video from Lila takes you through the innovations in Adobe Premiere that take full advantage of AI.,[{'name': 'Frank'}],"Mon, 04 Mar 2024 16:36:53 +0000"
660,How to Learn the Gemini API and Create a Multi-Turn Chat Bot (1 Hour Crash Course),https://www.franksworld.com/2024/03/04/how-to-learn-the-gemini-api-and-create-a-multi-turn-chat-bot-1-hour-crash-course/?utm_source=rss&utm_medium=rss&utm_campaign=how-to-learn-the-gemini-api-and-create-a-multi-turn-chat-bot-1-hour-crash-course,This video is from Code with Ania Kubów. Get the video api code here: https://bit.ly/stream-code,[{'name': 'Frank'}],"Mon, 04 Mar 2024 15:26:57 +0000"
661,Combining Rust and Python: The Best of Both Worlds?,https://www.franksworld.com/2024/03/03/combining-rust-and-python-the-best-of-both-worlds/?utm_source=rss&utm_medium=rss&utm_campaign=combining-rust-and-python-the-best-of-both-worlds,"In this video, ArjanCodes shows you how to seamlessly integrate Rust with Python using Pyo3. This library allows you to write Python modules with Rust. This means that we get the speed and safety of Rust along with Python&#8217;s easy-to-use features!",[{'name': 'Frank'}],"Sun, 03 Mar 2024 13:53:09 +0000"
662,Breaking Down the Fastest AI Chip in the World: What You Need to Know,https://www.franksworld.com/2024/03/01/breaking-down-the-fastest-ai-chip-in-the-world-what-you-need-to-know/?utm_source=rss&utm_medium=rss&utm_campaign=breaking-down-the-fastest-ai-chip-in-the-world-what-you-need-to-know,"This video from Anastasi In Tech  discusses how it works, benchmarks, how it compares to other AI accelerators and the future outlook!",[{'name': 'Frank'}],"Fri, 01 Mar 2024 17:57:07 +0000"
663,Elevate Your AI Capabilities with Red Hat OpenShift AI: A Demo,https://www.franksworld.com/2024/03/01/elevate-your-ai-capabilities-with-red-hat-openshift-ai-a-demo/?utm_source=rss&utm_medium=rss&utm_campaign=elevate-your-ai-capabilities-with-red-hat-openshift-ai-a-demo,"In this video, Chris Chase demonstrates a typical workflow that includes creating a project, launching a Jupyter notebook with appropriate cluster resources and training a foundation model from Hugging Face with one’s own data. Once the model is fine-tuned, Chris automates the build using a data science pipeline and serves the model for use in [&#8230;]",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:53:02 +0000"
664,Unveiling the Matrix: A Deep Dive into its Philosophical Themes,https://www.franksworld.com/2024/03/01/unveiling-the-matrix-a-deep-dive-into-its-philosophical-themes/?utm_source=rss&utm_medium=rss&utm_campaign=unveiling-the-matrix-a-deep-dive-into-its-philosophical-themes,"In this video from Einzelgänger take a deeper look into the Matrix. The Matrix, a science fiction film created by the Wachowskis, is probably one of the most influential movies ever made. The story starts when computer programmer Thomas Anderson, operating as a hacker under the alias “Neo,” discovers the truth about the world he’s [&#8230;]",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:48:06 +0000"
665,Navigating the AI Landscape: Lessons from the Butlerian Jihad,https://www.franksworld.com/2024/03/01/navigating-the-ai-landscape-lessons-from-the-butlerian-jihad/?utm_source=rss&utm_medium=rss&utm_campaign=navigating-the-ai-landscape-lessons-from-the-butlerian-jihad,"The following was originally published as part of my LinkedIn newsletter: Frank Digs Data, with a special shout out to all the Dune fans out there. In the realm of popular science fiction, few narratives resonate as powerfully with contemporary technological and ethical debates as the Butlerian Jihad from Frank Herbert&#8217;s &#8220;Dune,&#8221; where robotics and [&#8230;]",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:43:15 +0000"
666,Separating Fact from Fiction: Exploring AI Concerns with Dan Hendrycks on Win-Win,https://www.franksworld.com/2024/03/01/separating-fact-from-fiction-exploring-ai-concerns-with-dan-hendrycks-on-win-win/?utm_source=rss&utm_medium=rss&utm_campaign=separating-fact-from-fiction-exploring-ai-concerns-with-dan-hendrycks-on-win-win,"The rate of AI progress is accelerating, so how can we minimize the risks of this incredible technology, while maximizing the rewards?",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:38:52 +0000"
667,Data Engineering in the Age of AI: Data Intelligence Platforms,https://www.franksworld.com/2024/03/01/data-engineering-in-the-age-of-ai-data-intelligence-platforms/?utm_source=rss&utm_medium=rss&utm_campaign=data-engineering-in-the-age-of-ai-data-intelligence-platforms,"Learn about the Data Intelligence Platform and how Data Engineers benefit in their work from AI infused into every layer of the Databricks Lakehouse. This live demo walks you through the classic data engineering tasks, code generation, fixing, and documentation with the Databricks Assistant, automatic documentation with Unity Catalog, and making use of LLMs such [&#8230;]",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:36:56 +0000"
668,How to Give a Great Data Presentation,https://www.franksworld.com/2024/03/01/how-to-give-a-great-data-presentation/?utm_source=rss&utm_medium=rss&utm_campaign=how-to-give-a-great-data-presentation,This video is from DATAcated. Very excited to host Christopher Chin &#8211; to talk about How to Give a Great Data Presentation!!!,[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:35:40 +0000"
669,Uncovering the Fundamentals of SQL Server and Azure SQL DB Security | Data Exposed,https://www.franksworld.com/2024/03/01/uncovering-the-fundamentals-of-sql-server-and-azure-sql-db-security-data-exposed/?utm_source=rss&utm_medium=rss&utm_campaign=uncovering-the-fundamentals-of-sql-server-and-azure-sql-db-security-data-exposed,"In this video is from Microsoft Developer, learn about SQL Server and Azure SQL Database security fundamentals you won&#8217;t want to miss.",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:32:47 +0000"
670,Uncovering the Truth: The Alarming Rise of Fraud in the Scientific Community,https://www.franksworld.com/2024/03/01/uncovering-the-truth-the-alarming-rise-of-fraud-in-the-scientific-community/?utm_source=rss&utm_medium=rss&utm_campaign=uncovering-the-truth-the-alarming-rise-of-fraud-in-the-scientific-community,"This video is from Sabine Hossenfelder. Science has a big problem and it’s been getting rapidly worse in the past two years or so, to no small part because of recent advances in artificial intelligence. Fraudulent papers are getting published more than ever, and the fraudsters are getting increasingly aggressive. In this episode I want [&#8230;]",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:31:54 +0000"
671,Unleashing Creativity: DeepMind’s New AI Generates Games From Scratch,https://www.franksworld.com/2024/03/01/unleashing-creativity-deepminds-new-ai-generates-games-from-scratch/?utm_source=rss&utm_medium=rss&utm_campaign=unleashing-creativity-deepminds-new-ai-generates-games-from-scratch,This video from Two Minute Papers covers the paper &#8220;Genie: Generative Interactive Environments.&#8221; See? Not all hope for Google is lost.,[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:30:17 +0000"
672,Drizzle ORM in 100 Seconds,https://www.franksworld.com/2024/03/01/drizzle-orm-in-100-seconds/?utm_source=rss&utm_medium=rss&utm_campaign=drizzle-orm-in-100-seconds,"Fireship explains Drizzle in 100 seconds. Drizzle is a serverless TypeScript ORM designed for PostgreSQL, MySQL and SQLite.",[{'name': 'Frank'}],"Fri, 01 Mar 2024 16:28:35 +0000"
673,The Rise of AGI: How to Prepare for the Next Technological Revolution,https://www.franksworld.com/2024/02/29/the-rise-of-agi-how-to-prepare-for-the-next-technological-revolution/?utm_source=rss&utm_medium=rss&utm_campaign=the-rise-of-agi-how-to-prepare-for-the-next-technological-revolution,"This video is from David Ondrej. Soon AGI will be created, yet most people are grossly unprepared. In this AI course I will show you exactly what you need to do to thrive in the post-AGI world.",[{'name': 'Frank'}],"Thu, 29 Feb 2024 18:32:58 +0000"
674,Is it too late to invest in Nvidia?,https://www.franksworld.com/2024/02/29/is-it-too-late-to-invest-in-nvidia/?utm_source=rss&utm_medium=rss&utm_campaign=is-it-too-late-to-invest-in-nvidia,"On this edition of Quartz Smart Investing, is it too late to invest in Nvidia? Eric Beiley, executive managing director of The Beiley Group at Steward Partners, tells Quartz whether he thinks there&#8217;s still room to run for the AI chip stock.",[{'name': 'Frank'}],"Thu, 29 Feb 2024 18:30:15 +0000"
675,OpenAI Sora: A Closer Look!,https://www.franksworld.com/2024/02/29/openai-sora-a-closer-look/?utm_source=rss&utm_medium=rss&utm_campaign=openai-sora-a-closer-look,This video is from Two Minute Papers. 📝 Sora: https://openai.com/research/video-generation-models-as-world-simulators,[{'name': 'Frank'}],"Thu, 29 Feb 2024 18:29:04 +0000"
676,KubeCon EU 2024 Preview: Exploring the Clouds with Aparna Subramanian,https://www.franksworld.com/2024/02/29/kubecon-eu-2024-preview-exploring-the-clouds-with-aparna-subramanian/?utm_source=rss&utm_medium=rss&utm_campaign=kubecon-eu-2024-preview-exploring-the-clouds-with-aparna-subramanian,"With KubeCon EU in Paris around the corner (19th to 22nd March), what better way to get some insight on what is expected than by speaking to people directly involved in the event. Aparna Subramanian is a technologist and cloud-native enthusiast. She started her career as a Software Engineer and has spent most part of [&#8230;]",[{'name': 'Frank'}],"Thu, 29 Feb 2024 18:27:22 +0000"
677,Here comes the SU(N): multivariate quantum gates and gradients,https://quantum-journal.org/papers/q-2024-03-07-1275/,"<p>Quantum 8, 1275 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-03-07-1275\">https://doi.org/10.22331/q-2024-03-07-1275</a><p class=\"abstract\">Variational quantum algorithms use non-convex optimization methods to find the optimal parameters for a parametrized quantum circuit in order to solve a computational problem. The choice of the circuit ansatz, which consists of parameterized gates, is crucial to the success of these algorithms. Here, we propose a gate which fully parameterizes the special unitary group $\\mathrm{SU}(N)$. This gate is generated by a sum of non-commuting operators, and we provide a method for calculating its gradient on quantum hardware. In addition, we provide a theorem for the computational complexity of calculating these gradients by using results from Lie algebra theory. In doing so, we further generalize previous parameter-shift methods. We show that the proposed gate and its optimization satisfy the quantum speed limit, resulting in geodesics on the unitary group. Finally, we give numerical evidence to support the feasibility of our approach and show the advantage of our gate over a standard gate decomposition scheme. In doing so, we show that not only the expressibility of an ansatz matters, but also how it&#039;s explicitly parameterized.</p><p class=\"further-content\">Our code is freely available on Github:
https://github.com/dwierichs/Here-comes-the-SUN

There is a Demo that illustrates some of the key points of the paper:
https://pennylane.ai/qml/demos/tutorial_here_comes_the_sun/</p>","[{'name': 'Roeland Wiersema, Dylan Lewis, David Wierichs, Juan Carrasquilla, and Nathan Killoran'}]","Thu, 07 Mar 2024 16:20:18 +0000"
678,Ergodicity Breaking Under Confinement in Cold-Atom Quantum Simulators,https://quantum-journal.org/papers/q-2024-02-29-1274/,"<p>Quantum 8, 1274 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-29-1274\">https://doi.org/10.22331/q-2024-02-29-1274</a><p class=\"abstract\">The quantum simulation of gauge theories on synthetic quantum matter devices has gained a lot of traction in the last decade, making possible the observation of a range of exotic quantum many-body phenomena. In this work, we consider the spin-$1/2$ quantum link formulation of $1+1$D quantum electrodynamics with a topological $\\theta$-angle, which can be used to tune a confinement-deconfinement transition. Exactly mapping this system onto a PXP model with mass and staggered magnetization terms, we show an intriguing interplay between confinement and the ergodicity-breaking paradigms of quantum many-body scarring and Hilbert-space fragmentation. We map out the rich dynamical phase diagram of this model, finding an ergodic phase at small values of the mass $\\mu$ and confining potential $\\chi$, an emergent integrable phase for large $\\mu$, and a fragmented phase for large values of both parameters. We also show that the latter hosts resonances that lead to a vast array of effective models. We propose experimental probes of our findings, which can be directly accessed in current cold-atom setups.</p>","[{'name': 'Jean-Yves Desaules, Guo-Xian Su, Ian P. McCulloch, Bing Yang, Zlatko Papić, and Jad C. Halimeh'}]","Thu, 29 Feb 2024 16:05:38 +0000"
679,Entanglement dynamics of photon pairs and quantum memories in the gravitational field of the earth,https://quantum-journal.org/papers/q-2024-02-29-1273/,"<p>Quantum 8, 1273 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-29-1273\">https://doi.org/10.22331/q-2024-02-29-1273</a><p class=\"abstract\">We investigate the effect of entanglement dynamics due to gravity – the basis of a mechanism of universal decoherence – for photonic states and quantum memories in Mach-Zehnder and Hong-Ou-Mandel interferometry setups in the gravitational field of the earth. We show that chances are good to witness the effect with near-future technology in Hong-Ou-Mandel interferometry. This would represent an experimental test of theoretical modeling combining a multi-particle effect predicted by the quantum theory of light and an effect predicted by general relativity. Our article represents the first analysis of relativistic gravitational effects on space-based quantum memories which are expected to be an important ingredient for global quantum communication networks.</p>","[{'name': 'Roy Barzel, Mustafa Gündoğan, Markus Krutzik, Dennis Rätzel, and Claus Lämmerzahl'}]","Thu, 29 Feb 2024 15:38:15 +0000"
680,Entanglement-symmetries of covariant channels,https://quantum-journal.org/papers/q-2024-02-29-1272/,"<p>Quantum 8, 1272 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-29-1272\">https://doi.org/10.22331/q-2024-02-29-1272</a><p class=\"abstract\">Let $G$ and $G&#039;$ be monoidally equivalent compact quantum groups, and let $H$ be a Hopf-Galois object realising a monoidal equivalence between these groups&#039; representation categories. This monoidal equivalence induces an equivalence Chan($G$) $\\rightarrow$ Chan($G&#039;$), where Chan($G$) is the category whose objects are finite-dimensional $C*$-algebras with an action of G and whose morphisms are covariant channels. We show that, if the Hopf-Galois object $H$ has a finite-dimensional *-representation, then channels related by this equivalence can simulate each other using a finite-dimensional entangled resource. We use this result to calculate the entanglement-assisted capacities of certain quantum channels.</p>",[{'name': 'Dominic Verdon'}],"Thu, 29 Feb 2024 15:28:17 +0000"
681,Dynamical quantum phase transitions from random matrix theory,https://quantum-journal.org/papers/q-2024-02-29-1271/,"<p>Quantum 8, 1271 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-29-1271\">https://doi.org/10.22331/q-2024-02-29-1271</a><p class=\"abstract\">We uncover a novel dynamical quantum phase transition, using random matrix theory and its associated notion of planar limit. We study it for the isotropic XY Heisenberg spin chain. For this, we probe its real-time dynamics through the Loschmidt echo. This leads to the study of a random matrix ensemble with a complex weight, whose analysis requires novel technical considerations, that we develop. We obtain three main results: 1) There is a third order phase transition at a rescaled critical time, that we determine. 2) The third order phase transition persists away from the thermodynamic limit. 3) For times below the critical value, the difference between the thermodynamic limit and a finite chain decreases exponentially with the system size. All these results depend in a rich manner on the parity of the number of flipped spins of the quantum state conforming the fidelity.</p>","[{'name': 'David Pérez-García, Leonardo Santilli, and Miguel Tierz'}]","Thu, 29 Feb 2024 15:05:00 +0000"
682,Identifying families of multipartite states with non-trivial local entanglement transformations,https://quantum-journal.org/papers/q-2024-02-29-1270/,"<p>Quantum 8, 1270 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-29-1270\">https://doi.org/10.22331/q-2024-02-29-1270</a><p class=\"abstract\">The study of state transformations by spatially separated parties with local operations assisted by classical communication (LOCC) plays a crucial role in entanglement theory and its applications in quantum information processing. Transformations of this type among pure bipartite states were characterized long ago and have a revealing theoretical structure. However, it turns out that generic fully entangled pure multipartite states cannot be obtained from nor transformed to any inequivalent fully entangled state under LOCC. States with this property are referred to as isolated. Nevertheless, multipartite states are classified into families, the so-called SLOCC classes, which possess very different properties. Thus, the above result does not forbid the existence of particular SLOCC classes that are free of isolation, and therefore, display a rich structure regarding LOCC convertibility. In fact, it is known that the celebrated $n$-qubit GHZ and W states give particular examples of such classes and in this work, we investigate this question in general. One of our main results is to show that the SLOCC class of the 3-qutrit totally antisymmetric state is isolation-free as well. Actually, all states in this class can be converted to inequivalent states by LOCC protocols with just one round of classical communication (as in the GHZ and W cases). Thus, we consider next whether there are other classes with this property and we find a large set of negative answers. Indeed, we prove weak isolation (i.e., states that cannot be obtained with finite-round LOCC nor transformed by one-round LOCC) for very general classes, including all SLOCC families with compact stabilizers and many with non-compact stabilizers, such as the classes corresponding to the $n$-qunit totally antisymmetric states for $n\\geq4$. Finally, given the pleasant feature found in the family corresponding to the 3-qutrit totally antisymmetric state, we explore in more detail the structure induced by LOCC and the entanglement properties within this class.</p>","[{'name': 'Nicky Kai Hong Li, Cornelia Spee, Martin Hebenstreit, Julio I. de Vicente, and Barbara Kraus'}]","Thu, 29 Feb 2024 14:27:42 +0000"
683,Discrimination and certification of unknown quantum measurements,https://quantum-journal.org/papers/q-2024-02-29-1269/,"<p>Quantum 8, 1269 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-29-1269\">https://doi.org/10.22331/q-2024-02-29-1269</a><p class=\"abstract\">We study the discrimination of von Neumann measurements in the scenario when we are given a reference measurement and some other measurement. The aim of the discrimination is to determine whether the other measurement is the same as the first one. We consider the cases when the reference measurement is given without the classical description and when its classical description is known. Both cases are studied in the symmetric and asymmetric discrimination setups. Moreover, we provide optimal certification schemes enabling us to certify a known quantum measurement against the unknown one.</p>","[{'name': 'Aleksandra Krawiec, Łukasz Pawela, and Zbigniew Puchała'}]","Thu, 29 Feb 2024 13:43:58 +0000"
684,Bicolor loop models and their long range entanglement,https://quantum-journal.org/papers/q-2024-02-29-1268/,"<p>Quantum 8, 1268 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-29-1268\">https://doi.org/10.22331/q-2024-02-29-1268</a><p class=\"abstract\">Quantum loop models are well studied objects in the context of lattice gauge theories and topological quantum computing. They usually carry long range entanglement that is captured by the topological entanglement entropy. I consider generalization of the toric code model to bicolor loop models and show that the long range entanglement can be reflected in three different ways: a topologically invariant constant, a sub-leading logarithmic correction to the area law, or a modified bond dimension for the area-law term. The Hamiltonians are not exactly solvable for the whole spectra, but admit a tower of area-law exact excited states corresponding to the frustration free superposition of loop configurations with arbitrary pairs of localized vertex defects. The continuity of color along loops imposes kinetic constraints on the model and results in Hilbert space fragmentation, unless plaquette operators involving two neighboring faces are introduced to the Hamiltonian.</p>",[{'name': 'Zhao Zhang'}],"Thu, 29 Feb 2024 13:35:09 +0000"
685,Towards a measurement theory in QFT: “Impossible” quantum measurements are possible but not ideal,https://quantum-journal.org/papers/q-2024-02-27-1267/,"<p>Quantum 8, 1267 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-27-1267\">https://doi.org/10.22331/q-2024-02-27-1267</a><p class=\"abstract\">Naive attempts to put together relativity and quantum measurements lead to signaling between space-like separated regions. In QFT, these are known as $\\textit{impossible measurements}$. We show that the same problem arises in non-relativistic quantum physics, where joint nonlocal measurements (i.e., between systems kept spatially separated) in general lead to signaling, while one would expect no-signaling (based for instance on the $\\textit{principle of no-nonphysical communication}$). This raises the question: Which nonlocal quantum measurements are physically possible? We review and develop further a non-relativistic quantum information approach developed independently of the impossible measurements in QFT, and show that these two have been addressing virtually the same problem. The non-relativistic solution shows that all nonlocal measurements are $localizable$ (i.e., they can be carried out at a distance without violating no-signaling) but they (i) may require arbitrarily large entangled resources and (ii) cannot in general be $ideal$, i.e., are not immediately reproducible. These considerations could help guide the development of a complete theory of measurement in QFT.</p>",[{'name': 'Nicolas Gisin and Flavio Del Santo'}],"Tue, 27 Feb 2024 10:27:18 +0000"
686,Improved Accuracy for Trotter Simulations Using Chebyshev Interpolation,https://quantum-journal.org/papers/q-2024-02-26-1266/,"<p>Quantum 8, 1266 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-26-1266\">https://doi.org/10.22331/q-2024-02-26-1266</a><p class=\"abstract\">Quantum metrology allows for measuring properties of a quantum system at the optimal Heisenberg limit. However, when the relevant quantum states are prepared using digital Hamiltonian simulation, the accrued algorithmic errors will cause deviations from this fundamental limit. In this work, we show how algorithmic errors due to Trotterized time evolution can be mitigated through the use of standard polynomial interpolation techniques. Our approach is to extrapolate to zero Trotter step size, akin to zero-noise extrapolation techniques for mitigating hardware errors. We perform a rigorous error analysis of the interpolation approach for estimating eigenvalues and time-evolved expectation values, and show that the Heisenberg limit is achieved up to polylogarithmic factors in the error. Our work suggests that accuracies approaching those of state-of-the-art simulation algorithms may be achieved using Trotter and classical resources alone for a number of relevant algorithmic tasks.</p><p class=\"further-content\"></p>","[{'name': 'Gumaro Rendon, Jacob Watkins, and Nathan Wiebe'}]","Mon, 26 Feb 2024 14:34:15 +0000"
687,Quantum Vision Transformers,https://quantum-journal.org/papers/q-2024-02-22-1265/,"<p>Quantum 8, 1265 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-22-1265\">https://doi.org/10.22331/q-2024-02-22-1265</a><p class=\"abstract\">In this work, quantum transformers are designed and analysed in detail by extending the state-of-the-art classical transformer neural network architectures known to be very performant in natural language processing and image analysis. Building upon the previous work, which uses parametrised quantum circuits for data loading and orthogonal neural layers, we introduce three types of quantum transformers for training and inference, including a quantum transformer based on compound matrices, which guarantees a theoretical advantage of the quantum attention mechanism compared to their classical counterpart both in terms of asymptotic run time and the number of model parameters. These quantum architectures can be built using shallow quantum circuits and produce qualitatively different classification models. The three proposed quantum attention layers vary on the spectrum between closely following the classical transformers and exhibiting more quantum characteristics. As building blocks of the quantum transformer, we propose a novel method for loading a matrix as quantum states as well as two new trainable quantum orthogonal layers adaptable to different levels of connectivity and quality of quantum computers. We performed extensive simulations of the quantum transformers on standard medical image datasets that showed competitively, and at times better performance compared to the classical benchmarks, including the best-in-class classical vision transformers. The quantum transformers we trained on these small-scale datasets require fewer parameters compared to standard classical benchmarks. Finally, we implemented our quantum transformers on superconducting quantum computers and obtained encouraging results for up to six qubit experiments.</p>","[{'name': 'El Amine Cherrat, Iordanis Kerenidis, Natansh Mathur, Jonas Landman, Martin Strahm, and Yun Yvonna Li'}]","Thu, 22 Feb 2024 13:34:50 +0000"
688,Accelerating Quantum Algorithms with Precomputation,https://quantum-journal.org/papers/q-2024-02-22-1264/,"<p>Quantum 8, 1264 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-22-1264\">https://doi.org/10.22331/q-2024-02-22-1264</a><p class=\"abstract\">Real-world applications of computing can be extremely time-sensitive. It would be valuable if we could accelerate such tasks by performing some of the work ahead of time. Motivated by this, we propose a cost model for quantum algorithms that allows quantum precomputation; i.e., for a polynomial amount of ``free&#039;&#039; computation before the input to an algorithm is fully specified, and methods for taking advantage of it. We analyze two families of unitaries that are asymptotically more efficient to implement in this cost model than in the standard one. The first example of quantum precomputation, based on density matrix exponentiation, could offer an exponential advantage under certain conditions. The second example uses a variant of gate teleportation to achieve a quadratic advantage when compared with implementing the unitaries directly. These examples hint that quantum precomputation may offer a new arena in which to seek quantum advantage.</p>",[{'name': 'William J. Huggins and Jarrod R. McClean'}],"Thu, 22 Feb 2024 13:10:25 +0000"
689,Analogue Quantum Simulation with Fixed-Frequency Transmon Qubits,https://quantum-journal.org/papers/q-2024-02-22-1263/,"<p>Quantum 8, 1263 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-22-1263\">https://doi.org/10.22331/q-2024-02-22-1263</a><p class=\"abstract\">We experimentally assess the suitability of transmon qubits with fixed frequencies and fixed interactions for the realization of analogue quantum simulations of spin systems. We test a set of necessary criteria for this goal on a commercial quantum processor using full quantum process tomography and more efficient Hamiltonian tomography. Significant single qubit errors at low amplitudes are identified as a limiting factor preventing the realization of analogue simulations on currently available devices. We additionally find spurious dynamics in the absence of drive pulses, which we identify with coherent coupling between the qubit and a low dimensional environment. With moderate improvements, analogue simulation of a rich family of time-dependent many-body spin Hamiltonians may be possible.</p>","[{'name': 'Sean Greenaway, Adam Smith, Florian Mintert, and Daniel Malz'}]","Thu, 22 Feb 2024 13:02:06 +0000"
690,Taming the Rotating Wave Approximation,https://quantum-journal.org/papers/q-2024-02-21-1262/,"<p>Quantum 8, 1262 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-21-1262\">https://doi.org/10.22331/q-2024-02-21-1262</a><p class=\"abstract\">The interaction between light and matter is one of the oldest research areas of quantum mechanics, and a field that just keeps on delivering new insights and applications. With the arrival of cavity and circuit quantum electrodynamics we can now achieve strong light-matter couplings which form the basis of most implementations of quantum technology. But quantum information processing also has high demands requiring total error rates of fractions of percentage in order to be scalable (fault-tolerant) to useful applications. Since errors can also arise from modelling, this has brought into center stage one of the key approximations of quantum theory, the Rotating Wave Approximation (RWA) of the quantum Rabi model, leading to the Jaynes-Cummings Hamiltonian. While the RWA is often very good and incredibly useful to understand light-matter interactions, there is also growing experimental evidence of regimes where it is a bad approximation. Here, we ask and answer a harder question: for which experimental parameters is the RWA, although perhaps qualitatively adequate, already not good enough to match the demands of scalable quantum technology? For example, when is the error at least, and when at most, 1%? To answer this, we develop rigorous non-perturbative bounds taming the RWA.
We find that these bounds not only depend, as expected, on the ratio of the coupling strength and the oscillator frequency, but also on the average number of photons in the initial state. This confirms recent experiments on photon-dressed Bloch-Siegert shifts. We argue that with experiments reporting controllable cavity states with hundreds of photons and with quantum error correcting codes exploring more and more of Fock space, this state-dependency of the RWA is increasingly relevant for the field of quantum computation, and our results pave the way towards a better understanding of those experiments.</p><p class=\"further-content\"></p>","[{'name': 'Daniel Burgarth, Paolo Facchi, Robin Hillier, and Marilena Ligabò'}]","Wed, 21 Feb 2024 13:07:52 +0000"
691,Stabilizer Formalism for Operator Algebra Quantum Error Correction,https://quantum-journal.org/papers/q-2024-02-21-1261/,"<p>Quantum 8, 1261 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-21-1261\">https://doi.org/10.22331/q-2024-02-21-1261</a><p class=\"abstract\">We introduce a stabilizer formalism for the general quantum error correction framework called operator algebra quantum error correction (OAQEC), which generalizes Gottesman&#039;s formulation for traditional quantum error correcting codes (QEC) and Poulin&#039;s for operator quantum error correction and subsystem codes (OQEC). The construction generates hybrid classical-quantum stabilizer codes and we formulate a theorem that fully characterizes the Pauli errors that are correctable for a given code, generalizing the fundamental theorems for the QEC and OQEC stabilizer formalisms. We discover hybrid versions of the Bacon-Shor subsystem codes motivated by the formalism, and we apply the theorem to derive a result that gives the distance of such codes. We show how some recent hybrid subspace code constructions are captured by the formalism, and we also indicate how it extends to qudits.</p>","[{'name': 'Guillaume Dauphinais, David W. Kribs, and Michael Vasmer'}]","Wed, 21 Feb 2024 13:00:46 +0000"
692,Hierarchical generalization of dual unitarity,https://quantum-journal.org/papers/q-2024-02-20-1260/,"<p>Quantum 8, 1260 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-20-1260\">https://doi.org/10.22331/q-2024-02-20-1260</a><p class=\"abstract\">Quantum dynamics with local interactions in lattice models display rich physics, but is notoriously hard to study. Dual-unitary circuits allow for exact answers to interesting physical questions in clean or disordered one- and higher-dimensional quantum systems. However, this family of models shows some non-universal features, like vanishing correlations inside the light-cone and instantaneous thermalization of local observables. In this work we propose a generalization of dual-unitary circuits where the exactly calculable spatial-temporal correlation functions display richer behavior, and have non-trivial thermalization of local observables. This is achieved by generalizing the single-gate condition to a hierarchy of multi-gate conditions, where the first level recovers dual-unitary models, and the second level exhibits these new interesting features. We also extend the discussion and provide exact solutions to correlators with few-site observables and discuss higher-orders, including the ones after a quantum quench. In addition, we provide exhaustive parametrizations for qubit cases, and propose a new family of models for local dimensions larger than two, which also provides a new family of dual-unitary models.</p>","[{'name': 'Xie-Hang Yu, Zhiyuan Wang, and Pavel Kos'}]","Tue, 20 Feb 2024 14:43:10 +0000"
693,A hybrid quantum algorithm to detect conical intersections,https://quantum-journal.org/papers/q-2024-02-20-1259/,"<p>Quantum 8, 1259 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-20-1259\">https://doi.org/10.22331/q-2024-02-20-1259</a><p class=\"abstract\">Conical intersections are topologically protected crossings between the potential energy surfaces of a molecular Hamiltonian, known to play an important role in chemical processes such as photoisomerization and non-radiative relaxation. They are characterized by a non-zero Berry phase, which is a topological invariant defined on a closed path in atomic coordinate space, taking the value $\\pi$ when the path encircles the intersection manifold. In this work, we show that for real molecular Hamiltonians, the Berry phase can be obtained by tracing a local optimum of a variational ansatz along the chosen path and estimating the overlap between the initial and final state with a control-free Hadamard test. Moreover, by discretizing the path into $N$ points, we can use $N$ single Newton-Raphson steps to update our state non-variationally. Finally, since the Berry phase can only take two discrete values (0 or $\\pi$), our procedure succeeds even for a cumulative error bounded by a constant; this allows us to bound the total sampling cost and to readily verify the success of the procedure. We demonstrate numerically the application of our algorithm on small toy models of the formaldimine molecule (${H_2C=NH}$).</p>","[{'name': 'Emiel Koridon, Joana Fraxanet, Alexandre Dauphin, Lucas Visscher, Thomas E. O&#039;Brien, and Stefano Polla'}]","Tue, 20 Feb 2024 14:32:41 +0000"
694,Reqomp: Space-constrained Uncomputation for Quantum Circuits,https://quantum-journal.org/papers/q-2024-02-19-1258/,"<p>Quantum 8, 1258 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-19-1258\">https://doi.org/10.22331/q-2024-02-19-1258</a><p class=\"abstract\">Quantum circuits must run on quantum computers with tight limits on qubit and gate counts. To generate circuits respecting both limits, a promising opportunity is exploiting $uncomputation$ to trade qubits for gates. We present Reqomp, a method to automatically synthesize correct and efficient uncomputation of ancillae while respecting hardware constraints. For a given circuit, Reqomp can offer a wide range of trade-offs between tightly constraining qubit count or gate count. Our evaluation demonstrates that Reqomp can significantly reduce the number of required ancilla qubits by up to 96%. On 80% of our benchmarks, the ancilla qubits required can be reduced by at least 25% while never incurring a gate count increase beyond 28%.</p>","[{'name': 'Anouk Paradis, Benjamin Bichsel, and Martin Vechev'}]","Mon, 19 Feb 2024 10:26:13 +0000"
695,Spacetime-Efficient Low-Depth Quantum State Preparation with Applications,https://quantum-journal.org/papers/q-2024-02-15-1257/,"<p>Quantum 8, 1257 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-15-1257\">https://doi.org/10.22331/q-2024-02-15-1257</a><p class=\"abstract\">We propose a novel deterministic method for preparing arbitrary quantum states. When our protocol is compiled into CNOT and arbitrary single-qubit gates, it prepares an $N$-dimensional state in depth $O(\\log(N))$ and $\\textit{spacetime allocation}$ (a metric that accounts for the fact that oftentimes some ancilla qubits need not be active for the entire circuit) $O(N)$, which are both optimal. When compiled into the $\\{\\mathrm{H,S,T,CNOT}\\}$ gate set, we show that it requires asymptotically fewer quantum resources than previous methods. Specifically, it prepares an arbitrary state up to error $\\epsilon$ with optimal depth of $O(\\log(N) + \\log (1/\\epsilon))$ and spacetime allocation $O(N\\log(\\log(N)/\\epsilon))$, improving over $O(\\log(N)\\log(\\log (N)/\\epsilon))$ and $O(N\\log(N/\\epsilon))$, respectively. We illustrate how the reduced spacetime allocation of our protocol enables rapid preparation of many disjoint states with only constant-factor ancilla overhead – $O(N)$ ancilla qubits are reused efficiently to prepare a product state of $w$ $N$-dimensional states in depth $O(w + \\log(N))$ rather than $O(w\\log(N))$, achieving effectively constant depth per state. We highlight several applications where this ability would be useful, including quantum machine learning, Hamiltonian simulation, and solving linear systems of equations. We provide quantum circuit descriptions of our protocol, detailed pseudocode, and gate-level implementation examples using Braket.</p>","[{'name': 'Kaiwen Gui, Alexander M. Dalzell, Alessandro Achille, Martin Suchara, and Frederic T. Chong'}]","Thu, 15 Feb 2024 15:13:43 +0000"
696,Photonic entanglement during a zero-g flight,https://quantum-journal.org/papers/q-2024-02-15-1256/,"<p>Quantum 8, 1256 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-15-1256\">https://doi.org/10.22331/q-2024-02-15-1256</a><p class=\"abstract\">Quantum technologies have matured to the point that we can test fundamental quantum phenomena under extreme conditions. Specifically, entanglement, a cornerstone of modern quantum information theory, can be robustly produced and verified in various adverse environments. We take these tests further and implement a high-quality Bell experiment during a parabolic flight, transitioning from microgravity to hypergravity of 1.8 g while continuously observing Bell violation, with Bell-CHSH parameters between $S=-2.6202$ and $-2.7323$, an average of $\\overline{S} = -2.680$, and average standard deviation of $\\overline{\\Delta S} = 0.014$. This violation is unaffected both by uniform and non-uniform acceleration. This experiment demonstrates the stability of current quantum communication platforms for space-based applications and adds an important reference point for testing the interplay of non-inertial motion and quantum information.</p>","[{'name': 'Julius Arthur Bittermann, Lukas Bulla, Sebastian Ecker, Sebastian Philipp Neumann, Matthias Fink, Martin Bohmann, Nicolai Friis, Marcus Huber, and Rupert Ursin'}]","Thu, 15 Feb 2024 10:46:23 +0000"
697,Quantitative relations between different measurement contexts,https://quantum-journal.org/papers/q-2024-02-14-1255/,"<p>Quantum 8, 1255 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-14-1255\">https://doi.org/10.22331/q-2024-02-14-1255</a><p class=\"abstract\">In quantum theory, a measurement context is defined by an orthogonal basis in a Hilbert space, where each basis vector represents a specific measurement outcome. The precise quantitative relation between two different measurement contexts can thus be characterized by the inner products of nonorthogonal states in that Hilbert space. Here, we use measurement outcomes that are shared by different contexts to derive specific quantitative relations between the inner products of the Hilbert space vectors that represent the different contexts. It is shown that the probabilities that describe the paradoxes of quantum contextuality can be derived from a very small number of inner products, revealing details of the fundamental relations between measurement contexts that go beyond a basic violation of noncontextual limits. The application of our analysis to a product space of two systems reveals that the nonlocality of quantum entanglement can be traced back to a local inner product representing the relation between measurement contexts in only one system. Our results thus indicate that the essential nonclassical features of quantum mechanics can be traced back to the fundamental difference between quantum superpositions and classical alternatives.</p>",[{'name': 'Ming Ji and Holger F. Hofmann'}],"Wed, 14 Feb 2024 11:28:15 +0000"
698,Continuous-time quantum walks for MAX-CUT are hot,https://quantum-journal.org/papers/q-2024-02-13-1254/,"<p>Quantum 8, 1254 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-13-1254\">https://doi.org/10.22331/q-2024-02-13-1254</a><p class=\"abstract\">By exploiting the link between time-independent Hamiltonians and thermalisation, heuristic predictions on the performance of continuous-time quantum walks for MAX-CUT are made. The resulting predictions depend on the number of triangles in the underlying MAX-CUT graph. We extend these results to the time-dependent setting with multi-stage quantum walks and Floquet systems. The approach followed here provides a novel way of understanding the role of unitary dynamics in tackling combinatorial optimisation problems with continuous-time quantum algorithms.</p>","[{'name': 'Robert J. Banks, Ehsan Haque, Farah Nazef, Fatima Fethallah, Fatima Ruqaya, Hamza Ahsan, Het Vora, Hibah Tahir, Ibrahim Ahmad, Isaac Hewins, Ishaq Shah, Krish Baranwal, Mannan Arora, Mateen Asad, Mubasshirah Khan, Nabian Hasan, Nuh Azad, Salgai Fedaiee, Shakeel Majeed, Shayam Bhuyan, Tasfia Tarannum, Yahya Ali, Dan E. Browne, and P. A. Warburton'}]","Tue, 13 Feb 2024 13:56:51 +0000"
699,Rapid quantum approaches for combinatorial optimisation inspired by optimal state-transfer,https://quantum-journal.org/papers/q-2024-02-13-1253/,"<p>Quantum 8, 1253 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-13-1253\">https://doi.org/10.22331/q-2024-02-13-1253</a><p class=\"abstract\">We propose a new design heuristic to tackle combinatorial optimisation problems, inspired by Hamiltonians for optimal state-transfer. The result is a rapid approximate optimisation algorithm. We provide numerical evidence of the success of this new design heuristic. We find this approach results in a better approximation ratio than the Quantum Approximate Optimisation Algorithm at lowest depth for the majority of problem instances considered, while utilising comparable resources. This opens the door to investigating new approaches for tackling combinatorial optimisation problems, distinct from adiabatic-influenced approaches.</p>","[{'name': 'Robert J. Banks, Dan E. Browne, and P.A. Warburton'}]","Tue, 13 Feb 2024 12:59:17 +0000"
700,Adaptive variational simulation for open quantum systems,https://quantum-journal.org/papers/q-2024-02-13-1252/,"<p>Quantum 8, 1252 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-13-1252\">https://doi.org/10.22331/q-2024-02-13-1252</a><p class=\"abstract\">Emerging quantum hardware provides new possibilities for quantum simulation. While much of the research has focused on simulating closed quantum systems, the real-world quantum systems are mostly open. Therefore, it is essential to develop quantum algorithms that can effectively simulate open quantum systems. Here we present an adaptive variational quantum algorithm for simulating open quantum system dynamics described by the Lindblad equation. The algorithm is designed to build resource-efficient ansatze through the dynamical addition of operators by maintaining the simulation accuracy. We validate the effectiveness of our algorithm on both noiseless simulators and IBM quantum processors and observe good quantitative and qualitative agreement with the exact solution. We also investigate the scaling of the required resources with system size and accuracy and find polynomial behavior. Our results demonstrate that near-future quantum processors are capable of simulating open quantum systems.</p>","[{'name': 'Huo Chen, Niladri Gomes, Siyuan Niu, and Wibe Albert de Jong'}]","Tue, 13 Feb 2024 10:17:44 +0000"
701,Fast simulation of planar Clifford circuits,https://quantum-journal.org/papers/q-2024-02-12-1251/,"<p>Quantum 8, 1251 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-12-1251\">https://doi.org/10.22331/q-2024-02-12-1251</a><p class=\"abstract\">A general quantum circuit can be simulated classically in exponential time. If it has a planar layout, then a tensor-network contraction algorithm due to Markov and Shi has a runtime exponential in the square root of its size, or more generally exponential in the treewidth of the underlying graph. Separately, Gottesman and Knill showed that if all gates are restricted to be Clifford, then there is a polynomial time simulation. We combine these two ideas and show that treewidth and planarity can be exploited to improve Clifford circuit simulation. Our main result is a classical algorithm with runtime scaling asymptotically as $ n^{\\omega/2}$ $\\lt$ $n^{1.19}$ which samples from the output distribution obtained by measuring all $n$ qubits of a planar graph state in given Pauli bases. Here $\\omega$ is the matrix multiplication exponent. We also provide a classical algorithm with the same asymptotic runtime which samples from the output distribution of any constant-depth Clifford circuit in a planar geometry. Our work improves known classical algorithms with cubic runtime.

A key ingredient is a mapping which, given a tree decomposition of some graph $G$, produces a Clifford circuit with a structure that mirrors the tree decomposition and which emulates measurement of the corresponding graph state. We provide a classical simulation of this circuit with the runtime stated above for planar graphs and otherwise $nt^{\\omega-1}$ where $t$ is the width of the tree decomposition. Our algorithm incorporates two subroutines which may be of independent interest. The first is a matrix-multiplication-time version of the Gottesman-Knill simulation of multi-qubit measurement on stabilizer states. The second is a new classical algorithm for solving symmetric linear systems over $\\mathbb{F}_2$ in a planar geometry, extending previous works which only applied to non-singular linear systems in the analogous setting.</p><p class=\"further-content\"></p>","[{'name': 'David Gosset, Daniel Grier, Alex Kerzner, and Luke Schaeffer'}]","Mon, 12 Feb 2024 15:28:25 +0000"
702,Efficient learning of $t$-doped stabilizer states with single-copy measurements,https://quantum-journal.org/papers/q-2024-02-12-1250/,"<p>Quantum 8, 1250 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-12-1250\">https://doi.org/10.22331/q-2024-02-12-1250</a><p class=\"abstract\">One of the primary objectives in the field of quantum state learning is to develop algorithms that are time-efficient for learning states generated from quantum circuits. Earlier investigations have demonstrated time-efficient algorithms for states generated from Clifford circuits with at most $\\log(n)$ non-Clifford gates. However, these algorithms necessitate multi-copy measurements, posing implementation challenges in the near term due to the requisite quantum memory. On the contrary, using solely single-qubit measurements in the computational basis is insufficient in learning even the output distribution of a Clifford circuit with one additional $T$ gate under reasonable post-quantum cryptographic assumptions. In this work, we introduce an efficient quantum algorithm that employs only nonadaptive single-copy measurement to learn states produced by Clifford circuits with a maximum of $O(\\log n)$ non-Clifford gates, filling a gap between the previous positive and negative results.</p>","[{'name': 'Nai-Hui Chia, Ching-Yi Lai, and Han-Hsuan Lin'}]","Mon, 12 Feb 2024 15:20:13 +0000"
703,Stabilizer Codes with Exotic Local-dimensions,https://quantum-journal.org/papers/q-2024-02-12-1249/,"<p>Quantum 8, 1249 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-12-1249\">https://doi.org/10.22331/q-2024-02-12-1249</a><p class=\"abstract\">Traditional stabilizer codes operate over prime power local-dimensions. In this work we extend the stabilizer formalism using the local-dimension-invariant setting to import stabilizer codes from these standard local-dimensions to other cases. In particular, we show that any traditional stabilizer code can be used for analog continuous-variable codes, and consider restrictions in phase space and discretized phase space. This puts this framework on an equivalent footing as traditional stabilizer codes. Following this, using extensions of prior ideas, we show that a stabilizer code originally designed with a finite field local-dimension can be transformed into a code with the same $n$, $k$, and $d$ parameters for any integral domain. This is of theoretical interest and can be of use for systems whose local-dimension is better described by mathematical rings, which permits the use of traditional stabilizer codes for protecting their information as well.</p>",[{'name': 'Lane G. Gunderman'}],"Mon, 12 Feb 2024 14:18:38 +0000"
704,Gravitational quantum switch on a superposition of spherical shells,https://quantum-journal.org/papers/q-2024-02-12-1248/,"<p>Quantum 8, 1248 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-12-1248\">https://doi.org/10.22331/q-2024-02-12-1248</a><p class=\"abstract\">In the absence of a complete theory of quantum gravity, phenomenological models built upon minimal assumptions have been explored for the analysis of possible quantum effects in gravitational systems. Implications of a superposition of geometries have been considered in such models, including the occurrence of processes with indefinite order. In a gravitational quantum switch, in particular, the order of operations applied by two agents on a target system is entangled with the state of the geometry. We consider a model describing the superposition of geometries produced by distinct arrangements of spherical mass shells, and show that a protocol for the implementation of a gravitational quantum switch can be formulated in such a system. The geometries in superposition are identical in an exterior region outside a given radius, and differ within such a radius. The exterior region provides a classical frame from which the superposition of geometries in the interior region can be probed. One of the agents crosses the interior region and becomes entangled with the geometry, which is explored as a resource for the implementation of the quantum switch. Novel features of the protocol include the superposition of nonisometric geometries, the existence of a region with a definite geometry, and the fact that the agent that experiences the superposition of geometries is in free fall, preventing information on the global geometry to be obtained from within its laboratory.</p><p class=\"further-content\">Presentation at Perimeter Institute \"<a href=\"https://pirsa.org/24010078?__hstc=261081490.d970f744ba129097d273c94d9522ba42.1691152397211.1707222076809.1707264080021.47&#38;__hssc=261081490.3.1707264080021&#38;__hsfp=2499221673\">Indefinite temporal order on a superposition of spherical shells</a>\" by  Natália Salomé Móller <span class=\"institution field field--name-field-sp-institution-name field--type-string field--label-hidden field__items\">Slovak Academy of Sciences - Institute of Physics</span></p>","[{'name': 'Natália S. Móller, Bruna Sahdo, and Nelson Yokomizo'}]","Mon, 12 Feb 2024 14:10:00 +0000"
705,Qibolab: an open-source hybrid quantum operating system,https://quantum-journal.org/papers/q-2024-02-12-1247/,"<p>Quantum 8, 1247 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-12-1247\">https://doi.org/10.22331/q-2024-02-12-1247</a><p class=\"abstract\">We present $\\texttt{Qibolab}$, an open-source software library for quantum hardware control integrated with the $\\texttt{Qibo}$ quantum computing middleware framework. $\\texttt{Qibolab}$ provides the software layer required to automatically execute circuit-based algorithms on custom self-hosted quantum hardware platforms. We introduce a set of objects designed to provide programmatic access to quantum control through pulses-oriented drivers for instruments, transpilers and optimization algorithms. $\\texttt{Qibolab}$ enables experimentalists and developers to delegate all complex aspects of hardware implementation to the library so they can standardize the deployment of quantum computing algorithms in a extensible hardware-agnostic way, using superconducting qubits as the first officially supported quantum technology. We first describe the status of all components of the library, then we show examples of control setup for superconducting qubits platforms. Finally, we present successful application results related to circuit-based algorithms.</p>","[{'name': 'Stavros Efthymiou, Alvaro Orgaz-Fuertes, Rodolfo Carobene, Juan Cereijo, Andrea Pasquale, Sergi Ramos-Calderer, Simone Bordoni, David Fuentes-Ruiz, Alessandro Candido, Edoardo Pedicillo, Matteo Robbiati, Yuanzheng Paul Tan, Jadwiga Wilkens, Ingo Roth, José Ignacio Latorre, and Stefano Carrazza'}]","Mon, 12 Feb 2024 13:56:02 +0000"
706,Incompatibility of quantum instruments,https://quantum-journal.org/papers/q-2024-02-12-1246/,"<p>Quantum 8, 1246 (2024).</p><a href=\"https://doi.org/10.22331/q-2024-02-12-1246\">https://doi.org/10.22331/q-2024-02-12-1246</a><p class=\"abstract\">Quantum instruments describe outcome probability as well as state change induced by measurement of a quantum system. Incompatibility of two instruments, i. e. the impossibility to realize them simultaneously on a given quantum system, generalizes incompatibility of channels and incompatibility of positive operator-valued measures (POVMs). We derive implications of instrument compatibility for the induced POVMs and channels. We also study relation of instrument compatibility to the concept of non-disturbance. Finally, we prove equivalence between instrument compatibility and postprocessing of certain instruments, which we term complementary instruments. We illustrate our findings on examples of various classes of instruments.</p>",[{'name': 'Leevi Leppäjärvi and Michal Sedlák'}],"Mon, 12 Feb 2024 13:43:17 +0000"
707,Reflecting on National Cybersecurity Awareness Month!,https://wp.quantumcomputinginc.com/blog/reflecting-on-national-cybersecurity-awareness-month/,"<img alt=\"\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"150\" src=\"https://wp.quantumcomputinginc.com/wp-content/uploads/2023/11/IMG_1003-1-150x150.jpg\" width=\"150\" />As a first-to-market nanophotonic-based quantum technology company, Quantum Computing Inc. publishes a monthly newsletter to share the latest announcements and news involving quantum technologies.&#160;looks back on National Cybersecurity Awareness Month and delves into QCi&#8217;s cybersecurity product offerings, as well as the efforts the company is taking to define its presence in the cybersecurity sector. It&#8217;s ... <div class=\"read-more-link\"><a class=\"more-link\" href=\"https://wp.quantumcomputinginc.com/blog/reflecting-on-national-cybersecurity-awareness-month/\">Read Article</a></div>",[{'name': 'QCi'}],"Thu, 02 Nov 2023 02:18:13 +0000"
708,September’s Cyber Defense: Introducing QCi’s Cyber Solutions,https://wp.quantumcomputinginc.com/blog/septembers-cyber-defense-introducing-qcis-cyber-solutions/,"<img alt=\"\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"150\" src=\"https://wp.quantumcomputinginc.com/wp-content/uploads/2023/10/1695248209708-150x150.jpeg\" width=\"150\" />As a first-to-market nanophotonic-based quantum technology company, Quantum Computing Inc. publishes a monthly newsletter to share the latest announcements and news involving quantum technologies.&#160;This month&#8217;s issue focuses on QCi&#8217;s cybersecurity solutions, partnerships, and participation in cyber conferences in the United States and overseas. World Says Cybersecurity, We Say Solutions Our QCi Team, including&#160;Robert Liscouski, CEO;&#160;Dr. ... <div class=\"read-more-link\"><a class=\"more-link\" href=\"https://wp.quantumcomputinginc.com/blog/septembers-cyber-defense-introducing-qcis-cyber-solutions/\">Read Article</a></div>",[{'name': 'QCi'}],"Thu, 05 Oct 2023 11:39:03 +0000"
709,Quantum Leaps Ahead – QCi’s R&D Surge!,https://wp.quantumcomputinginc.com/blog/quantum-leaps-ahead-qcis-rd-surge/,"<img alt=\"QCi scientists in the lab\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"150\" src=\"https://wp.quantumcomputinginc.com/wp-content/uploads/2023/08/Untitled-design-29-150x150.png\" width=\"150\" />As a first-to-market nanophotonic-based quantum technology company, Quantum Computing Inc. publishes a monthly newsletter to share the latest announcements and news involving quantum technologies. This month's issue highlights QCi's research and development projects, covering what our team has been working on, our newest products, and upcoming events.",[{'name': 'Quantum Computing Inc.'}],"Wed, 30 Aug 2023 13:33:22 +0000"
710,Flying Cars Meet Quantum Computing,https://wp.quantumcomputinginc.com/blog/flying-cars-meet-quantum-computing/,<img alt=\"\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"150\" src=\"https://wp.quantumcomputinginc.com/wp-content/uploads/2023/08/Untitled-design-42-150x150.png\" width=\"150\" />Experience the not-so-distant future at the crossroads of flying cars and quantum computing. Aerial vehicles are poised to take to the sky and Quantum Computing Inc.’s technology is paving the way. Read our CTO’s insights on the trajectory of quantum technology and its revolutionary impact on the future of mobility.,[{'name': 'QCi'}],"Fri, 25 Aug 2023 16:10:44 +0000"
711,Navigating the Landscape of Licenses for Cybersecurity and US Patents,https://wp.quantumcomputinginc.com/blog/navigating-the-landscape-of-licenses-for-cybersecurity-and-us-patents/,"<img alt=\"\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"150\" src=\"https://wp.quantumcomputinginc.com/wp-content/uploads/2023/08/1FB3AB5C-8AEC-436E-B629-045A591FDA08-150x150.png\" width=\"150\" />In today&#8217;s fast-changing digital era, where cyber threats are an increasing concern, defending new cybersecurity solutions has become necessary. As technology continues to influence our society, cybersecurity licenses and US patents have grown in relevance. But what are the complexities? How do you obtain such a license? In this article, we&#8217;ll look into the intricacies ... <div class=\"read-more-link\"><a class=\"more-link\" href=\"https://wp.quantumcomputinginc.com/blog/navigating-the-landscape-of-licenses-for-cybersecurity-and-us-patents/\">Read Article</a></div>",[{'name': 'QCi'}],"Tue, 15 Aug 2023 14:41:02 +0000"
712,QCi & Quantum in the News,https://wp.quantumcomputinginc.com/blog/qci-quantum-in-the-news/,"<img alt=\"\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"150\" src=\"https://wp.quantumcomputinginc.com/wp-content/uploads/2023/07/53002877954_439bd21781_o-150x150.jpg\" width=\"150\" />As a first-to-market nanophotonic-based quantum technology company, Quantum Computing Inc. publishes a monthly newsletter to share the latest announcements and news involving quantum technologies. This month features QCi&#8217;s new products and subcontract awards with NASA. Quantum Computing Inc. Full Stack Quantum Solutions &#124; Value for Customers Today The Future of Quantum, Today Robert Liscouski, QCi ... <div class=\"read-more-link\"><a class=\"more-link\" href=\"https://wp.quantumcomputinginc.com/blog/qci-quantum-in-the-news/\">Read Article</a></div>",[{'name': 'Quantum Computing Inc.'}],"Tue, 25 Jul 2023 14:56:20 +0000"
713,Intro to Quantum Random Number Generators,https://wp.quantumcomputinginc.com/blog/intro-to-quantum-random-number-generators/,"<img alt=\"Intro to Quantum Random Number Generators\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"150\" src=\"https://wp.quantumcomputinginc.com/wp-content/uploads/2023/05/Intro-to-Quantum-Random-Number-Generators-150x150.jpg\" width=\"150\" />This is part three of a three-part series on quantum security – how it works, the implications for society and business, and what it will mean for leaders of organizations that process sensitive data and rely on keeping that data secure. Part one looked at the basics of quantum computing and cryptography. Part two talked ... <div class=\"read-more-link\"><a class=\"more-link\" href=\"https://wp.quantumcomputinginc.com/blog/intro-to-quantum-random-number-generators/\">Read Article</a></div>",[{'name': 'David Bean'}],"Tue, 09 May 2023 13:51:39 +0000"
714,Why QCi uniform Quantum Random Number Generator?,https://wp.quantumcomputinginc.com/blog/why-qci-uniform-quantum-random-number-generator/,"<img alt=\"\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"150\" src=\"https://wp.quantumcomputinginc.com/wp-content/uploads/2023/04/qrng-blog-post-2-150x150.jpg\" width=\"150\" />From the dawn of time, humans have survived because of their ability to find and identify patterns. For example, patterns helped us distinguish edible berries from poisonous ones, related agricultural activities to seasons, and navigate the world using the stars. These days data mining, machine learning, neural networks, time series analysis, and regression analysis are ... <div class=\"read-more-link\"><a class=\"more-link\" href=\"https://wp.quantumcomputinginc.com/blog/why-qci-uniform-quantum-random-number-generator/\">Read Article</a></div>","[{'name': 'Malvika Garikapati, Quantum Physicist'}]","Wed, 12 Apr 2023 20:39:37 +0000"
715,What does “steal now decrypt later” mean for cybersecurity?,https://wp.quantumcomputinginc.com/blog/what-does-steal-now-decrypt-later-mean-for-cybersecurity/,"<img alt=\"\" class=\"attachment-thumbnail size-thumbnail wp-post-image\" height=\"150\" src=\"https://wp.quantumcomputinginc.com/wp-content/uploads/2023/04/What-does-steal-now-decrypt-later-mean-for-cybersecurity-scaled-1-150x150.jpg\" width=\"150\" />This is part two of a three-part series on quantum security – how it works, the implications for society and business, and what it will mean for leaders of organizations that process sensitive data and rely on keeping that data secure. Part one looked at the basics of quantum computing and cryptography. Part two focuses ... <div class=\"read-more-link\"><a class=\"more-link\" href=\"https://wp.quantumcomputinginc.com/blog/what-does-steal-now-decrypt-later-mean-for-cybersecurity/\">Read Article</a></div>",[{'name': 'QCi'}],"Tue, 11 Apr 2023 20:27:40 +0000"
716,Direct-to-consumer casket startup says it’s time to bury our unhealthy switch to daylight savings,https://www.geekwire.com/2024/direct-to-consumer-casket-startup-says-its-time-to-bury-our-unhealthy-switch-to-daylight-savings/,"<img alt=\"actor David Dastmalchian\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"709\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/Untitled-1260x709.png\" width=\"1260\" /><br />Before you spring forward this weekend into daylight saving time, a casket company hopes you&#8217;ll consider whether it&#8217;s time to bury what some consider to be an unhealthy ritual. In a new ad campaign and promotional push on its website, Titan Casket makes a somewhat satirical pitch to stop with all this clock winding and rewinding and let standard time stand. &#8220;At Titan Casket, we obviously have an interest in you dying,&#8221; actor David Dastmalchian (&#8220;Dune,&#8221; &#8220;The Suicide Squad&#8221;) says in the ad, before adding, &#8220;But, we can wait.&#8221; The company&#8217;s new pitchman directs viewers to Titan&#8217;s website to learn&#8230; <a href=\"https://www.geekwire.com/2024/direct-to-consumer-casket-startup-says-its-time-to-bury-our-unhealthy-switch-to-daylight-savings/\">Read More</a>",[{'name': 'Kurt Schlosser'}],"Thu, 07 Mar 2024 21:35:46 +0000"
717,Washington state lawmakers pass bill to create task force to guide policy on AI,https://www.geekwire.com/2024/washington-state-lawmakers-pass-bill-to-create-task-force-to-guide-policy-on-ai/,"<img alt=\"Bob Ferguson\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"840\" src=\"https://cdn.geekwire.com/wp-content/uploads/2017/10/1657-GWSummit-Day-1-2017-1260x840.jpg\" width=\"1260\" /><br />The Washington State Legislature passed a bipartisan proposal from Attorney General Bob Ferguson to create an Artificial Intelligence Task Force that will help lawmakers better understand the benefits and risks of the rapidly developing technology. Senate Bill 5838 was approved with a 30-19 vote this week and will now be sent to Gov. Jay Inslee for his signature. Sponsored by Sen. Joe Nguyen, D-White Center, the bill previously passed the House and Senate before the latest vote to reconcile amendments from both chambers. The companion bill in the House was HB 1934. The task force will be made up of&#8230; <a href=\"https://www.geekwire.com/2024/washington-state-lawmakers-pass-bill-to-create-task-force-to-guide-policy-on-ai/\">Read More</a>",[{'name': 'Kurt Schlosser'}],"Thu, 07 Mar 2024 20:29:32 +0000"
718,Redfin rolls out new AI assistant tool that answers homebuyer questions,https://www.geekwire.com/2024/redfin-rolls-out-new-ai-assistant-tool-that-answers-homebuyer-questions/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"1066\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/Screenshot-2024-03-06-at-9.21.02%E2%80%AFPM.png\" width=\"548\" /><br />Redfin wants to help answer homebuyer questions in a more efficient way with the assistance of AI. The real estate giant is rolling out a new generative AI-powered virtual assistant called &#8220;Ask Redfin&#8221; that can quickly answer questions about a particular home listing. The tool combines the latest in large language model technology with Redfin&#8217;s vast data on homes and markets across the country. It also connects with Redfin&#8217;s customer support team when users can&#8217;t get their questions answered by the tool. The conversational interface can make it easier for some customers to get answers more quickly, versus scrolling up&#8230; <a href=\"https://www.geekwire.com/2024/redfin-rolls-out-new-ai-assistant-tool-that-answers-homebuyer-questions/\">Read More</a>",[{'name': 'Taylor Soper'}],"Thu, 07 Mar 2024 17:30:00 +0000"
719,Zulily’s rebirth: Reality TV star and new owner Marcus Lemonis on how online retailer will return to its roots,https://www.geekwire.com/2024/zulilys-rebirth-new-owner-says-online-retailer-will-return-to-its-flash-sales-roots/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"945\" src=\"https://cdn.geekwire.com/wp-content/uploads/2023/09/IMG_9276-1260x945.jpg\" width=\"1260\" /><br />CNBC no longer produces The Profit. But Marcus Lemonis is still trying to turn around businesses. Lemonis, who starred on the reality show for eight years, recently became executive chairman of Beyond — the company that just paid $4.5 million to acquire the assets of embattled online retailer Zulily. In an interview with GeekWire on Thursday, Lemonis said he hopes to spark some of the secret sauce that helped Zulily blossom in its early days as a flash sales leader that focused on women and moms. &#8220;The flash sale, get-it-before-it-runs-out model —&#160;that&#8217;s how it will be rebirthed,&#8221; he said. Lemonis&#8230; <a href=\"https://www.geekwire.com/2024/zulilys-rebirth-new-owner-says-online-retailer-will-return-to-its-flash-sales-roots/\">Read More</a>",[{'name': 'Taylor Soper'}],"Thu, 07 Mar 2024 16:07:43 +0000"
720,AI2 Incubator secures $200M in AI compute resources for portfolio companies,https://www.geekwire.com/2024/ai2-incubator-secures-200m-in-ai-compute-resources-for-portfolio-companies/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"536\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/Logo-Example.png\" width=\"956\" /><br />Companies building artificial intelligence models into their software products need a lot of computation power, also known as &#8220;compute.&#8221; But this can be a costly expense. That&#8217;s why the AI2 Incubator in Seattle decided to find a way to provide its portfolio companies with free AI compute. The organization announced Thursday that it has secured $200 million in computing power from unnamed cloud providers and data centers. &#8220;This is, to our knowledge, the single biggest compute allocation available to startups right now,&#8221; said Jacob Colker, managing director at the AI2 Incubator. In 2022 the AI2 Incubator spun off from its&#8230; <a href=\"https://www.geekwire.com/2024/ai2-incubator-secures-200m-in-ai-compute-resources-for-portfolio-companies/\">Read More</a>",[{'name': 'Taylor Soper'}],"Thu, 07 Mar 2024 16:00:00 +0000"
721,Amazon-backed AI startup Anthropic is growing its Seattle workforce,https://www.geekwire.com/2024/amazon-backed-ai-startup-anthropic-is-growing-its-seattle-workforce/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"841\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/amazon-anthropic-1260x841.jpg\" width=\"1260\" /><br />Anthropic, one of the most valuable privately held AI companies going head-to-head in the generative AI race with OpenAI, is expanding its presence in the Seattle area. The company currently has more than 20 open positions with Seattle listed as a potential location for new hires. Those listings also include New York City, London, and San Francisco, where the company is based. There are several Seattle-based employees working for Anthropic, including many who just joined. It&#8217;s not clear if Anthropic has a physical office in the Seattle region. We&#8217;ve contacted the company for more details. San Francisco-based Anthropic is among&#8230; <a href=\"https://www.geekwire.com/2024/amazon-backed-ai-startup-anthropic-is-growing-its-seattle-workforce/\">Read More</a>",[{'name': 'Taylor Soper'}],"Thu, 07 Mar 2024 15:00:00 +0000"
722,"Beyond pays $4.5M to acquire Zulily brand, plans to relaunch online retailer later this year",https://www.geekwire.com/2024/beyond-pays-4-5m-to-acquire-zulily-brand-plans-to-relaunch-flash-sale-site-later-this-year/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"945\" src=\"https://cdn.geekwire.com/wp-content/uploads/2023/10/IMG_9524-1260x945.jpeg\" width=\"1260\" /><br />Beyond, the publicly traded company behind Bed Bath &#38; Beyond and Overstock, confirmed Thursday that it has acquired brand assets from Zulily, the online retailer that shut down last year. Beyond — led by Marcus Lemonis, star of CNBC&#8217;s reality TV show &#8220;The Profit&#8221; —&#160;paid $4.5 million in cash to buy Zulily&#8217;s website, domain names, trademarks, customer database, social media accounts, and software, the company said in a press release. GeekWire reported late Wednesday that Beyond appeared to be the new owner following posts on social media teasing a relaunch. With the acquisition, Beyond will get its hands on a&#8230; <a href=\"https://www.geekwire.com/2024/beyond-pays-4-5m-to-acquire-zulily-brand-plans-to-relaunch-flash-sale-site-later-this-year/\">Read More</a>",[{'name': 'Taylor Soper'}],"Thu, 07 Mar 2024 13:55:05 +0000"
723,"Zulily set to relaunch under new owner Beyond, now led by Marcus Lemonis of ‘The Profit’ fame",https://www.geekwire.com/2024/zulily-set-to-relaunch-under-new-owner-beyond-now-led-by-marcus-lemonis-of-the-profit-fame/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"570\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/lemonis-zulily.png\" width=\"1110\" /><br />FOLLOW-UP COVERAGE: Beyond pays $4.5M to acquire Zulily brand, plans to relaunch retailer later this year &#8230; Zulily’s rebirth: Reality TV star Marcus Lemonis on what’s next for e-commerce company Zulily&#8217;s back. The embattled online retailer, which laid off hundreds of employees last year as part of a surprising shutdown, appears to be relaunching in a remarkable new chapter. The company&#8217;s social media accounts sent a message Wednesday evening: &#8220;The only purple that saves you money. Coming soon.&#8221; Marcus Lemonis, the TV star of CNBC reality show The Profit, posted the same message on Wednesday from his own account. His&#8230; <a href=\"https://www.geekwire.com/2024/zulily-set-to-relaunch-under-new-owner-beyond-now-led-by-marcus-lemonis-of-the-profit-fame/\">Read More</a>",[{'name': 'Taylor Soper'}],"Thu, 07 Mar 2024 03:35:16 +0000"
724,LinkedIn suffers outage a day after Facebook and Instagram — and X is left as our social media fix,https://www.geekwire.com/2024/linkedin-suffers-outage-a-day-after-facebook-and-instagram-and-x-is-left-as-our-social-media-fix/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"751\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/Screenshot-2024-03-06-at-1.28.56%E2%80%AFPM.png\" width=\"1182\" /><br />So much for my after-lunch job search and profile lurking. A day after a Meta outage knocked out Facebook, Messenger and Instagram, LinkedIn suffered a similar fate on Wednesday, as the social media platform for business professionals went down. &#8220;An error has occurred,&#8221; read a message across the site shortly after 1 p.m. PST. On the company&#8217;s status page, a message said that LinkedIn was &#8220;continuing to investigate the issue.&#8221; Update: The site came back online for me before I could even post this story to Tumblr. The Microsoft-owned platform boasts about 1 billion members worldwide. Half of them tried&#8230; <a href=\"https://www.geekwire.com/2024/linkedin-suffers-outage-a-day-after-facebook-and-instagram-and-x-is-left-as-our-social-media-fix/\">Read More</a>",[{'name': 'Kurt Schlosser'}],"Wed, 06 Mar 2024 21:50:10 +0000"
725,Seattle startup lands $6.4M to build AI medical model using smartphone and wearable data,https://www.geekwire.com/2024/seattle-health-startup-lands-6-4m-to-build-ai-medical-model-using-smartphone-and-wearable-data/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"370\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/Screenshot-2024-03-06-at-4.59.31%E2%80%AFPM.png\" width=\"746\" /><br />PreemptiveAI came out of stealth mode this week, announcing $6.4 million in seed funding that will help the Seattle startup build what it describes as the &#8220;most predictive large medical model.&#8221; The company, incubated at the AI2 Incubator in Seattle, takes biomedical signals from smartphones and wearable devices to create AI software aimed at predicting health outcomes. The startup aims to answer questions about the likelihood of a heart attack or stroke, and response to changes in medication dosage, for example. Potential customers include providers, drug developers, and insurance companies. PreemptiveAI has a strategic partnership with Duke University to monitor&#8230; <a href=\"https://www.geekwire.com/2024/seattle-health-startup-lands-6-4m-to-build-ai-medical-model-using-smartphone-and-wearable-data/\">Read More</a>",[{'name': 'Taylor Soper'}],"Wed, 06 Mar 2024 20:59:49 +0000"
726,The quotable Kara Swisher: Insights and zingers from the ‘Burn Book’ event at Town Hall Seattle,https://www.geekwire.com/2024/the-quotable-kara-swisher-insights-and-zingers-from-the-burn-book-event-at-seattles-town-hall/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"992\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/kara-swisher-1260x992.jpg\" width=\"1260\" /><br />The title and subtitle of Kara Swisher&#8216;s new book, &#8220;Burn Book: A Tech Love Story,&#8221; perfectly illustrate the dichotomy of her career as a journalist and entrepreneur, in which she has been simultaneously captivated by the rise of digital technologies and disappointed by many of the people bringing these innovations into the world. As a longtime follower of Swisher&#8217;s career, listener of her podcasts, and admirer of her reporting, I finished the memoir over the weekend. I found it enlightening and inspiring, filling in the gaps in her personal story and her approach as a journalist, with plenty of behind-the-scenes&#8230; <a href=\"https://www.geekwire.com/2024/the-quotable-kara-swisher-insights-and-zingers-from-the-burn-book-event-at-seattles-town-hall/\">Read More</a>",[{'name': 'Todd Bishop'}],"Wed, 06 Mar 2024 18:45:26 +0000"
727,Steve Ballmer’s USAFacts offers insights on the State of the Union ahead of Biden’s address,https://www.geekwire.com/2024/steve-ballmers-usafacts-offers-insights-on-the-state-of-the-union-ahead-of-bidens-address/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"506\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/joebiden.jpg\" width=\"900\" /><br />In time for President Biden&#8217;s State of the Union address on Thursday, USAFacts has released its annual &#8220;nonpartisan, data-driven snapshot&#8221; of how things are going in the country. The &#8220;State of the Union In Numbers&#8221; is intended to be an unbiased report to inform policy discussions, debates, and decisions — which could prove especially useful in an election year when Biden&#8217;s job is on the line. Former Microsoft CEO Steve Ballmer launched USAFacts in 2017 to provide insight into how the nation is being run. The site draws on data from more than 70 government sources and regularly crunches numbers in an&#8230; <a href=\"https://www.geekwire.com/2024/steve-ballmers-usafacts-offers-insights-on-the-state-of-the-union-ahead-of-bidens-address/\">Read More</a>",[{'name': 'Kurt Schlosser'}],"Wed, 06 Mar 2024 18:21:43 +0000"
728,Univ. of Washington launches research center to tackle computer science education challenges,https://www.geekwire.com/2024/univ-of-washington-launches-research-center-to-tackle-computer-science-education-challenges/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"945\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/PXL_20240306_010944685.PORTRAIT-1260x945.jpg\" width=\"1260\" /><br />Despite technology seeping into practically every corner of our lives, computer-related education in Washington is woefully limited. Last year, 8.4% of the Washington&#8217;s high school students attending either public or state-tribal schools took a computer science class, according to state data. That&#8217;s fewer than 31,000 teens. Roughly half of the public or state-tribal schools even offer a computer science class. &#8220;It&#8217;s like a watercooler for people who are passionate about these things.&#8221; &#8211; Ben Shapiro, associate professor in the Allen School But fixing the situation won&#8217;t be easy. There aren&#8217;t enough computer science teachers. There aren&#8217;t enough programs for training&#8230; <a href=\"https://www.geekwire.com/2024/univ-of-washington-launches-research-center-to-tackle-computer-science-education-challenges/\">Read More</a>",[{'name': 'Lisa Stiffler'}],"Wed, 06 Mar 2024 16:26:14 +0000"
729,Alliance of Angels raising new fund to back more startups in Seattle and beyond,https://www.geekwire.com/2024/alliance-of-angels-raising-new-fund-to-back-more-startups-in-seattle-and-beyond/,"<img alt=\"Eric Larsen\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"300\" src=\"https://cdn.geekwire.com/wp-content/uploads/2016/10/larsen.jpg\" width=\"300\" /><br />Seattle-based investment group Alliance of Angels is gearing up to raise a new fund that will help support a new batch of startups across the Pacific Northwest. The fund will be the third annual &#8220;Innovation Fund,&#8221; which AoA launched in 2022. It raised $1.3 million for the initial fund and $1.9 million for the second fund. AoA expects to raise a similar amount or slightly more for the third fund and invest in around 14 companies, said Eric Larsen, AoA vice chairperson. Founded in 1998, AoA is known for its network of nearly 200 angel investors that make individual investment&#8230; <a href=\"https://www.geekwire.com/2024/alliance-of-angels-raising-new-fund-to-back-more-startups-in-seattle-and-beyond/\">Read More</a>",[{'name': 'Taylor Soper'}],"Wed, 06 Mar 2024 16:00:00 +0000"
730,Mental health startup Cognito raises $2M to provide more services in Canada,https://www.geekwire.com/2024/mental-health-startup-cognito-raises-2m-to-provide-more-services-in-canada/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"992\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/65e63b937dc44aaf91c49bbe_Cognito-App-1260x992.png\" width=\"1260\" /><br />Cognito, a Victoria, B.C.-based startup that provides virtual mental health services, raised $2 million in seed funding.",[{'name': 'Taylor Soper'}],"Wed, 06 Mar 2024 15:51:41 +0000"
731,AI’s trust problem: Richard Edelman on the risk from the tech industry’s rapid rollouts,https://www.geekwire.com/2024/ais-trust-problem-richard-edelman-on-the-risk-from-the-tech-industrys-rapid-rollouts/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"837\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/edelman-richard-1260x837.jpg\" width=\"1260\" /><br />A new report points to a crisis of trust in innovation, and the risk that rapid technological change — especially in the field of artificial intelligence — will fuel increased populism and polarization across societies. Richard Edelman, CEO of Edelman, discussed these and other findings from the 2024 Edelman Trust Barometer during visits last week with the global communications firm&#8217;s clients in tech-heavy Seattle and San Francisco. His message, as he explained in a blog post this week, was that &#8220;acceptance of innovation cannot be taken for granted, that we must spend much more of our time on adaptation and&#8230; <a href=\"https://www.geekwire.com/2024/ais-trust-problem-richard-edelman-on-the-risk-from-the-tech-industrys-rapid-rollouts/\">Read More</a>",[{'name': 'Todd Bishop'}],"Wed, 06 Mar 2024 15:17:38 +0000"
732,Phase Genomics gets $1.5M grant from Gates Foundation to fight cow burp methane,https://www.geekwire.com/2024/phase-genomics-gets-1-5m-grant-from-gates-foundation-to-fight-cow-burp-methane/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"533\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/51037438917_d9691b4d1d_c1-1.jpg\" width=\"799\" /><br />Seattle biotech startup Phase Genomics is receiving $1.5 million from the Bill &#38; Melinda Gates Foundation to fund research into two important areas: killing drug-resistant bacteria and battling methane-producing microorganisms that live in the guts of cows and other ruminants. The new grant will support a year-long project to create an antimicrobial discovery platform to tackle these challenges. The focus is on Campylobacter infections, which are among the most common human infections and can cause severe diarrhea. Methane released in cow burps are the No. 1 source of greenhouse gases for the agricultural sector. The startup&#8217;s work is centered on&#8230; <a href=\"https://www.geekwire.com/2024/phase-genomics-gets-1-5m-grant-from-gates-foundation-to-fight-cow-burp-methane/\">Read More</a>",[{'name': 'Lisa Stiffler'}],"Tue, 05 Mar 2024 18:33:50 +0000"
733,"Glowforge releases Spark, another low-priced 3D laser engraver to attract more home crafters",https://www.geekwire.com/2024/glowforge-releases-spark-another-low-priced-3d-laser-engraver-to-attract-more-home-crafters/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"780\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/Spark-Head-On-Shadow-1-1260x780.png\" width=\"1260\" /><br />Seattle-based 3D laser engraver maker Glowforge is making another pitch to home crafters with another lower-priced machine, the company announced Tuesday. Less than a year after the release of the $1,200 Aura — designed to attract a wider audience of consumers —&#160;Glowforge is releasing the new Spark, which will sell for $699. Glowforge&#8217;s two higher-end machines include the Glowforge Plus ($4,995) and Glowforge Pro ($6,995). The Spark is a little smaller than the Aura, but is built around the same laser technology, capable of cutting materials ranging from paper to leather to wood. The bed size of the Spark is&#8230; <a href=\"https://www.geekwire.com/2024/glowforge-releases-spark-another-low-priced-3d-laser-engraver-to-attract-more-home-crafters/\">Read More</a>",[{'name': 'Kurt Schlosser'}],"Tue, 05 Mar 2024 17:00:00 +0000"
734,Jeff Bezos bumps Elon Musk from billionaires top spot with net worth of $200.3B,https://www.geekwire.com/2024/jeff-bezos-bumps-elon-musk-from-billionaires-top-spot-with-net-worth-of-200-3b/,"<img alt=\"Musk and Bezos\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"542\" src=\"https://cdn.geekwire.com/wp-content/uploads/2019/04/190410-musk-bezos.jpg\" width=\"933\" /><br />Jeff Bezos is back on top of the Bloomberg Billionaires Index for the first time since 2021, overtaking Elon Musk as the richest person in the world with a net worth of $200.3 billion. Bloomberg reported that Musk lost his top ranking on Monday after Tesla shares tumbled 7.2%, leaving the automaker&#8217;s CEO with a net worth of $197.7 billion. The stock fell after February vehicle shipments fell in China, the company&#8217;s largest market. Bezos&#8217; wealth has been on an upward trajectory thanks to the same trend for Amazon shares, which have more than doubled since late 2022. Tesla, meanwhile,&#8230; <a href=\"https://www.geekwire.com/2024/jeff-bezos-bumps-elon-musk-from-billionaires-top-spot-with-net-worth-of-200-3b/\">Read More</a>",[{'name': 'Kurt Schlosser'}],"Tue, 05 Mar 2024 16:25:49 +0000"
735,"Enzzo, a PSL spinout led by Seattle startup vet, raises $3M to use AI for hardware development",https://www.geekwire.com/2024/enzzo-a-psl-spinout-led-by-seattle-startup-vet-raises-3m-to-use-ai-for-hardware-development/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"840\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/enzzo-team-1260x840.jpg\" width=\"1260\" /><br />Building hardware is hard. It can be especially challenging in the early part of product creation, when designers need to set parameters and requirements, also known as the &#8220;definition phase.&#8221; Can AI help speed up that process? That&#8217;s what a group of longtime investors are betting on with a $3 million seed investment in Enzzo, a new Seattle startup that spun out of Pioneer Square Labs and aims to accelerate hardware development. The 4-person startup is led by veteran entrepreneur and tech leader Ford Davidson, who most recently spent time at Meta and Amazon in product leadership roles. He previously&#8230; <a href=\"https://www.geekwire.com/2024/enzzo-a-psl-spinout-led-by-seattle-startup-vet-raises-3m-to-use-ai-for-hardware-development/\">Read More</a>",[{'name': 'Taylor Soper'}],"Tue, 05 Mar 2024 16:24:29 +0000"
736,Will AI change how we shop? What it’s like to use Amazon’s new Rufus assistant,https://www.geekwire.com/2024/will-ai-change-how-we-shop-what-its-like-to-use-amazons-new-rufus-assistant/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"841\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/rufus2-1260x841.jpg\" width=\"1260\" /><br />Searching on Amazon for a trainer to convert my road bike into a stationary cycle, I knew the make and model of my bike, but had no clue about its wheel size, or whether it would be compatible with the products I was considering. So I pulled up from the bottom of my screen in the Amazon app, and called on Rufus for help: &#8220;Can you recommend a trainer for my bike, which is a Raleigh cadent 2?&#8221; Rufus responded, &#8220;The Raleigh Cadent 2 is a versatile hybrid bike that can handle both road and light off-road riding. When choosing&#8230; <a href=\"https://www.geekwire.com/2024/will-ai-change-how-we-shop-what-its-like-to-use-amazons-new-rufus-assistant/\">Read More</a>",[{'name': 'Todd Bishop'}],"Tue, 05 Mar 2024 10:00:00 +0000"
737,"Agility Robotics names new CEO: Peggy Johnson, former Magic Leap CEO and Microsoft exec",https://www.geekwire.com/2024/agility-robotic/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"495\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/peggy-final.jpg\" width=\"373\" /><br />Peggy Johnson, the former Microsoft executive who stepped down as Magic Leap CEO last fall, is the new CEO of Agility Robotics, the Corvalis, Ore.-based company known for the Digit bipedal humanoid warehouse robot. Announcing the news this morning, Agility said Johnson will lead its &#8220;next phase of growth, focusing on sales, strategic industry partnerships, future fundraising, and delivering robots at scale to meet overwhelming demand.&#8221; Damion Shelton, the company&#8217;s CEO since its inception in 2015, will now serve as Agility&#8217;s president as part of Johnson&#8217;s leadership team. Johnson will be based in California in the new role. Agility has&#8230; <a href=\"https://www.geekwire.com/2024/agility-robotic/\">Read More</a>",[{'name': 'Todd Bishop'}],"Mon, 04 Mar 2024 21:18:39 +0000"
738,"Peer-to-peer laundry startup Loopie acquires Bidbud, a gig economy job-bidding platform",https://www.geekwire.com/2024/peer-to-peer-laundry-startup-loopie-acquires-bidbud-a-gig-economy-job-bidding-platform/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"584\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/loopiebidbud-1260x584.jpeg\" width=\"1260\" /><br />Loopie, the Seattle peer-to-peer laundry service, announced the acquisition Monday of Bidbud, a gig economy platform that allows clients to post job opportunities that can be bid on by freelancers and service providers. In a news release, Loopie called the move a strategic way to revolutionize its mobile app marketplace. Launched in July 2018, Loopie is led by co-founder and CEO John Vincent Lee. The mobile app laundromat, connects people who don’t have the time or desire to deal with their own dirty laundry with people who can wash and dry on their own and earn money doing it. Lee&#8230; <a href=\"https://www.geekwire.com/2024/peer-to-peer-laundry-startup-loopie-acquires-bidbud-a-gig-economy-job-bidding-platform/\">Read More</a>",[{'name': 'Kurt Schlosser'}],"Mon, 04 Mar 2024 19:31:11 +0000"
739,"Blue Origin targets 2025 for cargo lander’s inaugural moon trip, with humans to follow",https://www.geekwire.com/2024/blue-origin-2025-moon-lander/,"<img alt=\"Blue Moon MK1 lander\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"744\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/240304-bluemoon2-1260x744.jpg\" width=\"1260\" /><br />Jeff Bezos&#8217; Blue Origin space venture is aiming to send an uncrewed lander to the surface of the moon in the next 12 to 16 months, according to the executive in charge of the development program. John Couluris, senior vice president for lunar permanence at Blue Origin, provided an update on the company&#8217;s moon lander program on CBS&#8217; &#8220;60 Minutes&#8221; news program on Sunday. &#8220;We&#8217;re expecting to land on the moon between 12 and 16 months from today,&#8221; Couluris said. &#8220;I understand I&#8217;m saying that publicly, but that&#8217;s what our team is aiming towards.&#8221; Couluris was referring to a pathfinder&#8230; <a href=\"https://www.geekwire.com/2024/blue-origin-2025-moon-lander/\">Read More</a>",[{'name': 'Alan Boyle'}],"Mon, 04 Mar 2024 18:59:41 +0000"
740,"Data from Seattle food delivery drivers shows higher hourly earnings, but less customer demand",https://www.geekwire.com/2024/data-from-seattle-food-delivery-drivers-shows-higher-hourly-earnings-but-less-customer-demand/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"840\" src=\"https://cdn.geekwire.com/wp-content/uploads/2019/06/UberEats_Photo_Home_Delivery_Handoff-1-1260x840.jpg\" width=\"1260\" /><br />Seattle-area gig workers are making higher hourly wages while they&#8217;re delivering food for platforms such as Uber Eats and DoorDash following the implementation of a new minimum wage law. But they&#8217;re also getting fewer tips, and consumer demand has likely fallen. Those are some takeaways from data provided to GeekWire from Solo, a Seattle-based software startup that helps more than 250,000 independent contractors manage their earnings. The controversial law went into effect in January and set a minimum per-minute and per-mile amount for app-based workers delivering for tech platforms. The companies responded by adding substantial new fees for consumers — $5 for each order — and&#8230; <a href=\"https://www.geekwire.com/2024/data-from-seattle-food-delivery-drivers-shows-higher-hourly-earnings-but-less-customer-demand/\">Read More</a>",[{'name': 'Taylor Soper'}],"Mon, 04 Mar 2024 17:09:17 +0000"
741,Why India’s thriving software talent base is important for the Seattle innovation ecosystem,https://www.geekwire.com/2024/why-indias-thriving-software-talent-base-is-important-for-the-seattle-innovation-ecosystem/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"840\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/02/7k5a3657-1260x840.jpg\" width=\"1260\" /><br />Editor&#8217;s note: This guest post was written by Kellan Carter, founding partner at Bellevue, Wash.-based venture capital firm FUSE. On a recent trip to India, something caught my attention inside a tech company office. Bunk beds. The Chennai-based team working for Zuper turned their conference room into a sleeping area for employees who were staying late at the office working to improve the company’s software platform. This was just one example highlighting the unmatched determination of founders and teams in India — which has become a critical part of the innovation ecosystem in Seattle despite being halfway across the world.&#8230; <a href=\"https://www.geekwire.com/2024/why-indias-thriving-software-talent-base-is-important-for-the-seattle-innovation-ecosystem/\">Read More</a>",[{'name': 'Kellan Carter'}],"Mon, 04 Mar 2024 16:00:00 +0000"
742,Protein design pioneer David Baker on why sharing code is key to building biotech startups,https://www.geekwire.com/2024/protein-design-pioneer-david-baker-on-why-sharing-code-is-important-for-biotech-startups/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"932\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/Jenny-Cronin-and-David-Baker-5-1260x932.jpg\" width=\"1260\" /><br />The Institute for Protein Design at the University of Washington is a startup factory, spinning out companies developing new protein-based therapeutics, industrial enzymes, materials, biosensors and more. A key to the institute&#8217;s entrepreneurial success is a mindset of open inquiry and collaboration &#8212; and an emphasis on making the code for its protein-design tools readily available. “I really believed from the beginning that we should share everything that we did,” said IPD director and UW professor David Baker, speaking at an event on campus in Seattle last week. More than two decades ago, Baker and his colleagues developed an early&#8230; <a href=\"https://www.geekwire.com/2024/protein-design-pioneer-david-baker-on-why-sharing-code-is-important-for-biotech-startups/\">Read More</a>",[{'name': 'Charlotte Schubert'}],"Mon, 04 Mar 2024 15:53:45 +0000"
743,"Week in Review: Most popular stories on GeekWire for the week of Feb. 25, 2024",https://www.geekwire.com/2024/geekwire-weekly-roundup-2024-02-25/,"<img alt=\"GeekWire Week in Review\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"630\" src=\"https://cdn.geekwire.com/wp-content/uploads/2015/11/geekwire-week-in-review1.png\" width=\"1200\" /><br />See the technology stories that people were reading on GeekWire for the week of Feb. 25, 2024.&#8230; <a href=\"https://www.geekwire.com/2024/geekwire-weekly-roundup-2024-02-25/\">Read More</a>",[{'name': 'GeekWire'}],"Sun, 03 Mar 2024 16:00:06 +0000"
744,Pfizer to close Seagen manufacturing plant in Seattle area following $43B acquisition,https://www.geekwire.com/2024/pfizer-to-close-seagen-manufacturing-plant-in-seattle-area-following-43b-acquisition/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"840\" src=\"https://cdn.geekwire.com/wp-content/uploads/2023/03/Seagen-Building-10-Lobby-1-1260x840.jpeg\" width=\"1260\" /><br />Pharma giant Pfizer, fresh off its $43 billion acquisition of Seagen, said it will shut down construction on a new 270,000 square-foot manufacturing facility Seagen was building north of Seattle near its headquarters. Previously: How Pfizer’s $43B acquisition of Seagen may impact Seattle and broader biotech market",[{'name': 'Taylor Soper'}],"Sat, 02 Mar 2024 16:19:40 +0000"
745,"Seattle vanlife gurus see tech and AI’s potential as a copilot, to boost human connections on road trips",https://www.geekwire.com/2024/seattle-vanlife-gurus-see-tech-and-ais-potential-as-a-copilot-to-boost-human-connections-on-road-trips/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"921\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/02/peacevans1-1260x921.jpeg\" width=\"1260\" /><br />With all the makings of a buddy road trip adventure, two longtime Seattle friends and tech veterans have teamed up to breathe new life into a popular travel app under the umbrella of a thriving business built around a love for &#8220;vanlife.&#8221; Jordan Schwartz, the software entrepreneur who previously founded and led Seattle startup Pathable, is the new chief product officer at Sēkr, an app to help travelers plan trips, find campsites and more. Founded in 2018 and originally based in San Diego, Sēkr was acquired last year by Peace Vans and owner Harley Sitner, whose nationally known business south&#8230; <a href=\"https://www.geekwire.com/2024/seattle-vanlife-gurus-see-tech-and-ais-potential-as-a-copilot-to-boost-human-connections-on-road-trips/\">Read More</a>",[{'name': 'Kurt Schlosser'}],"Sat, 02 Mar 2024 16:00:00 +0000"
746,"How Microsoft, T-Mobile, and Amazon use big bets to build durable businesses in turbulent times",https://www.geekwire.com/2024/how-microsoft-t-mobile-and-amazon-use-big-bets-to-build-durable-businesses-in-turbulent-times/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"813\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/106-crop-1260x813.jpg\" width=\"1260\" /><br />At the outset of their new book, &#8220;Big Bet Leadership,&#8221; authors John Rossman and Kevin McCaffrey identify three &#8220;mega forces&#8221; that they believe could define the years ahead for businesses and the economy: They write in the book, &#8220;These mega forces will feed into each other like a vortex, building an overriding theme for business and society—that of a chaotic environment of dramatic change with successful business operators realizing productivity and cost model advantages that separate them from their competition.&#8221; Their thesis: companies that thrive in this chaos will be the ones that master the art of &#8220;big bets,&#8221; transformative&#8230; <a href=\"https://www.geekwire.com/2024/how-microsoft-t-mobile-and-amazon-use-big-bets-to-build-durable-businesses-in-turbulent-times/\">Read More</a>",[{'name': 'Todd Bishop'}],"Sat, 02 Mar 2024 15:48:11 +0000"
747,Transplant organ freezing and rewarming technique wins U. of Washington health innovation challenge,https://www.geekwire.com/2024/transplant-organ-freezing-and-rewarming-technique-wins-u-of-washington-health-innovation-challenge/,"<img alt=\"BioLegacy\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"600\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/2024-HIC_Grand-Prize_BioLegacy_800x600.jpeg\" width=\"800\" /><br />A team working on prolonging the lifespan of transplant organs took home the top prize in the 9th annual Hollomon Health Innovation Challenge at the University of Washington. BioLegacy, made up of Seattle University students (mechanical engineering, chemistry, and finance) and University of Washington students (mechanical engineering) was awarded the $15,000 WRF Capital Grand Prize for its organ cryopreservation and rewarming innovation. The team was one of 22 that competed in this year&#8217;s final round of competition at the UW Foster School’s Buerk Center for Entrepreneurship. The challenge attracted a record-breaking 53 applicants from student teams across the Pacific Northwest and British Columbia.&#8230; <a href=\"https://www.geekwire.com/2024/transplant-organ-freezing-and-rewarming-technique-wins-u-of-washington-health-innovation-challenge/\">Read More</a>",[{'name': 'Kurt Schlosser'}],"Fri, 01 Mar 2024 20:46:34 +0000"
748,"5 days in the office? Some tech startups are bringing workers together all week, every week",https://www.geekwire.com/2024/5-days-in-the-office-these-tech-startups-are-bringing-workers-together-all-week-every-week/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"762\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/02/20240212_141204-EDIT-1260x762.jpg\" width=\"1260\" /><br />BELLEVUE, Wash. — At precisely 2 p.m. Pacific time on a recent Monday afternoon, employees of the technology startup Statsig were alerted to their regular standup meeting not by a notification on their screens, or a buzzing of their devices, but by the sound of engineering lead Marcos Arribas tapping the taut surface of one of several musical instruments specially designated for the purpose. This is the drumbeat of daily work in a place where workers are in the office every day. The fact that it&#8217;s a novelty shows just how much our world has changed. At a time when&#8230; <a href=\"https://www.geekwire.com/2024/5-days-in-the-office-these-tech-startups-are-bringing-workers-together-all-week-every-week/\">Read More</a>",[{'name': 'Todd Bishop'}],"Fri, 01 Mar 2024 16:38:04 +0000"
749,Tech Moves: AWS vet lands at health startup; Lumen hires longtime Microsoft GM; ex-Qualtrics exec joins Typeform board,https://www.geekwire.com/2024/tech-moves-aws-vet-lands-at-health-startup-ex-qualtrics-exec-joins-typeform-board/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"800\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/03/1632520923858.jpeg\" width=\"800\" /><br />— Harshit Shah, a former head of engineering at Amazon Web Services, is now chief technology officer at Boston, Mass-based healthcare software company Kyruus Health. Shah, based in the Seattle region, most recently held the same position at Spring Health, a mental health company. He also previously spent a decade at Microsoft in various managerial roles. Founded in 2010, Kyruus describes itself as a &#8220;care access platform&#8221; that develops various software products for health systems, hospitals, medical groups, and health plans. The company raised a $42 million round in 2020. It has offices in the Portland, Ore., area and has&#8230; <a href=\"https://www.geekwire.com/2024/tech-moves-aws-vet-lands-at-health-startup-ex-qualtrics-exec-joins-typeform-board/\">Read More</a>",[{'name': 'Taylor Soper'}],"Fri, 01 Mar 2024 16:34:26 +0000"
750,"Univ. of Washington’s famed cherry blossoms have some bright, tech-infused competition on campus",https://www.geekwire.com/2024/univ-of-washingtons-famed-cherry-blossoms-have-some-bright-tech-infused-competition-on-campus/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"945\" src=\"https://cdn.geekwire.com/wp-content/uploads/2024/02/UWtree1-1260x945.jpeg\" width=\"1260\" /><br />The cherry blossoms at the University of Washington offer a spectacular welcome to spring every year on the school&#8217;s Seattle campus. Thursday night, a brightly shining, tech-infused tree in front of the Husky Union Building got a jump on nature. BECU, in partnership with the UW, unveiled the 12-foot-tall work of art in a promotion aimed at symbolizing the credit union&#8217;s &#8220;commitment to supporting the UW community&#8217;s financial well-being.&#8221; The colorful specimen features 550 3D-printed blossoms with LED lights and about 1,200 laser-cut mirrored mylar leaves that flutter and reflect the illumination source. It was created by Intention Space, a&#8230; <a href=\"https://www.geekwire.com/2024/univ-of-washingtons-famed-cherry-blossoms-have-some-bright-tech-infused-competition-on-campus/\">Read More</a>",[{'name': 'Kurt Schlosser'}],"Fri, 01 Mar 2024 15:45:34 +0000"
751,"LLMs Land on Laptops: NVIDIA, HP CEOs Celebrate AI PCs",https://blogs.nvidia.com/blog/llms-hp-laptops/,"2024 will be the year generative AI gets personal, the CEOs of NVIDIA and HP said today in a fireside chat, unveiling new laptops that can build, test and run large language models. “This is a renaissance of the personal computer,” said NVIDIA founder and CEO Jensen Huang at HP Amplify, a gathering in Las		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/llms-hp-laptops/\">
			Read Article			<span></span>
		</a>",[{'name': 'Allen Bourgoyne'}],"Thu, 07 Mar 2024 20:25:40 +0000"
752,First Class: NVIDIA Introduces Generative AI Professional Certification,https://blogs.nvidia.com/blog/generative-ai-professional-certification/,"NVIDIA is offering a new professional certification in generative AI to enable developers to establish technical credibility in this important domain. Generative AI is revolutionizing industries worldwide, yet there’s a critical skills gap and need to uplevel employees to more fully harness the technology. Available for the first time from NVIDIA, this new professional certification		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/generative-ai-professional-certification/\">
			Read Article			<span></span>
		</a>",[{'name': 'Craig Clawson'}],"Thu, 07 Mar 2024 17:02:33 +0000"
753,Don’t Pass This Up: Day Passes Now Available on GeForce NOW,https://blogs.nvidia.com/blog/geforce-now-thursday-day-pass-cygames/,"Gamers can now seize the day with Day Passes, available to purchase for 24-hour continuous access to powerful cloud gaming with all the benefits of a GeForce NOW Ultimate or Priority membership — no commitment required. Publisher Cygames brings its next triple-A title to the cloud. Granblue Fantasy: Relink leads eight new games joining the		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-day-pass-cygames/\">
			Read Article			<span></span>
		</a>",[{'name': 'GeForce NOW Community'}],"Thu, 07 Mar 2024 14:00:51 +0000"
754,"Bria Builds Responsible Generative AI for Enterprises Using NVIDIA NeMo, Picasso",https://blogs.nvidia.com/blog/bria-builds-responsible-generative-ai-using-nemo-picasso/,"As visual generative AI matures from research to the enterprise domain, businesses are seeking responsible ways to integrate the technology into their products. Bria, a startup based in Tel Aviv, is responding with an open platform for visual generative AI that emphasizes model transparency alongside fair attribution and copyright protections. Currently offering models that convert		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/bria-builds-responsible-generative-ai-using-nemo-picasso/\">
			Read Article			<span></span>
		</a>",[{'name': 'Arham Mehta'}],"Wed, 06 Mar 2024 16:00:52 +0000"
755,"AI Decoded: Demystifying AI and the Hardware, Software and Tools That Power It",https://blogs.nvidia.com/blog/ai-decoded-rtx-pc/,"With the 2018 launch of RTX technologies and the first consumer GPU built for AI — GeForce RTX — NVIDIA accelerated the shift to AI computing. Since then, AI on RTX PCs and workstations has grown into a thriving ecosystem with more than 100 million users and 500 AI applications.",[{'name': 'Jesse Clayton'}],"Wed, 06 Mar 2024 14:00:30 +0000"
756,The Magic Behind the Screen: Celebrating the 96th Academy Awards Nominees for Best Visual Effects,https://blogs.nvidia.com/blog/academy-awards-vfx-openusd/,"The 96th Academy Awards nominees for Best Visual Effects are a testament to the incredible technological advancements pushing the boundaries of what’s possible in film. Whether showcasing colossal destruction scenes, heart-pumping action sequences or interstellar adventures, each nominee demonstrates unique contributions in visual effects, or VFX — and they all used cutting-edge NVIDIA technologies in		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/academy-awards-vfx-openusd/\">
			Read Article			<span></span>
		</a>",[{'name': 'Rick Champagne'}],"Tue, 05 Mar 2024 17:00:50 +0000"
757,Robo Rendezvous: Robotics Innovators and AI Leaders to Converge at NVIDIA GTC,https://blogs.nvidia.com/blog/robotics-innovators-ai-gtc-2024/,"Bringing together pioneers in robotics and AI, NVIDIA GTC will be a state-of-the-art showcase of applied AI for autonomous machines. The conference, running March 18-21 at the San Jose Convention Center and online, boasts a star-studded lineup. This includes a fireside chat with Marc Raibert, executive director of The AI Institute, and Dieter Fox, senior		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/robotics-innovators-ai-gtc-2024/\">
			Read Article			<span></span>
		</a>",[{'name': 'Jason Black'}],"Mon, 04 Mar 2024 18:55:55 +0000"
758,Automakers Electrify Geneva International Motor Show,https://blogs.nvidia.com/blog/automakers-geneva-motor-show/,"The Geneva International Motor Show, one of the most important and long-standing global auto exhibitions, opened this week, with the spotlight on several China and U.S. EV makers building on NVIDIA DRIVE that are expanding their presence in Europe. BYD One of the key reveals is BYD’s Yangweng U8 plug-in hybrid large SUV, built on		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/automakers-geneva-motor-show/\">
			Read Article			<span></span>
		</a>",[{'name': 'Danny Shapiro'}],"Fri, 01 Mar 2024 19:13:33 +0000"
759,No Noobs Here: Top Pro Gamers Bolster Software Quality Assurance Testing,https://blogs.nvidia.com/blog/nvidia-life-pro-gamers/,"For some NVIDIANs, it’s always game day. Our Santa Clara-based software quality assurance team boasts some of the world’s top gamers, whose search for bugs and errors is as strategic as their battle plans for toppling top-tier opponents in video games. Two team members of the QA team — friendly colleagues in the office but		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/nvidia-life-pro-gamers/\">
			Read Article			<span></span>
		</a>",[{'name': 'Samantha Zee'}],"Fri, 01 Mar 2024 17:14:58 +0000"
760,What Is Trustworthy AI?,https://blogs.nvidia.com/blog/what-is-trustworthy-ai/,"Artificial intelligence, like any transformative technology, is a work in progress — continually growing in its capabilities and its societal impact. Trustworthy AI initiatives recognize the real-world effects that AI can have on people and society, and aim to channel that power responsibly for positive change. What Is Trustworthy AI? Trustworthy AI is an approach		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/what-is-trustworthy-ai/\">
			Read Article			<span></span>
		</a>",[{'name': 'Nikki Pope'}],"Fri, 01 Mar 2024 17:00:24 +0000"
761,Live at GTC: Hear From Industry Leaders Using AI to Drive Innovation and Agility,https://blogs.nvidia.com/blog/industry-leaders-ai-innovation-gtc-2024/,Enterprise execs across broad sectors to share their AI strategies and success stories.,[{'name': 'Ben Oliveri'}],"Fri, 01 Mar 2024 16:00:28 +0000"
762,Battle.net Leaps Into the Cloud With GeForce NOW,https://blogs.nvidia.com/blog/geforce-now-thursday-battlenet-march-games-list/,"GFN Thursday celebrates this leap day with the addition of a popular game store to the cloud. Stream the first titles from Blizzard Entertainment’s Battle.net, including Diablo IV, Overwatch 2, Call of Duty HQ and Hearthstone, now playable across more devices than ever. They’re all part of the 30 new games coming to GeForce NOW		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/geforce-now-thursday-battlenet-march-games-list/\">
			Read Article			<span></span>
		</a>",[{'name': 'GeForce NOW Community'}],"Thu, 29 Feb 2024 14:00:55 +0000"
763,What Is Sovereign AI?,https://blogs.nvidia.com/blog/what-is-sovereign-ai/,"Nations have long invested in domestic infrastructure to advance their economies, control their own data and take advantage of technology opportunities in areas such as transportation, communications, commerce, entertainment and healthcare. AI, the most important technology of our time, is turbocharging innovation across every facet of society. It’s expected to generate trillions of dollars in		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/what-is-sovereign-ai/\">
			Read Article			<span></span>
		</a>",[{'name': 'Keith Strier'}],"Wed, 28 Feb 2024 20:31:53 +0000"
764,And … Action! Cuebric CEO Provides Insights Into Filmmaking Using AI,https://blogs.nvidia.com/blog/pinar-demirdag-cuebric/,"These days, just about everyone is a content creator. But can generative AI help make people create high-quality films and other content affordably? Find out from Pinar Seyhan Demirdag, cofounder and CEO of Cuebric, during his conversation with NVIDIA AI Podcast host Noah Kravitz. Cuebric is on a mission to offer new solutions in filmmaking		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/pinar-demirdag-cuebric/\">
			Read Article			<span></span>
		</a>",[{'name': 'Brian Caulfield'}],"Wed, 28 Feb 2024 14:00:55 +0000"
765,Time to Skill Up: Game Reviewer Ralph Panebianco Wields NVIDIA RTX for the Win,https://blogs.nvidia.com/blog/skillup-adobe-premiere-pro-photoshop/,"YouTube content creator Ralph Panebianco really, really loves video games.",[{'name': 'Gerardo Delgado'}],"Wed, 28 Feb 2024 14:00:13 +0000"
766,Rack ‘n’ Roll: NVIDIA Grace Hopper Systems Gather at GTC,https://blogs.nvidia.com/blog/mgx-accelerated-systems-gtc/,"The spirit of software legend Grace Hopper will live on at NVIDIA GTC. Accelerated systems using powerful processors — named in honor of the pioneer of programming — will be on display at the global AI conference running March 18-21, ready to take computing to the next level. System makers will show more than 500		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/mgx-accelerated-systems-gtc/\">
			Read Article			<span></span>
		</a>",[{'name': 'Ivan Goldwasser'}],"Tue, 27 Feb 2024 16:00:36 +0000"
767,Meet the Omnivore: Mode Maison Harnesses OpenUSD to Drive Innovations in Retail With High-Fidelity Digital Twins,https://blogs.nvidia.com/blog/mode-maison-openusd-omniverse/,"Editor’s note: This post is a part of our Meet the Omnivore series, which features individual creators and developers who use OpenUSD to build tools, applications and services for 3D workflows and physically accurate virtual worlds. A failed furniture-shopping trip turned into a business idea for Steven Gay, cofounder and CEO of company Mode Maison.		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/mode-maison-openusd-omniverse/\">
			Read Article			<span></span>
		</a>",[{'name': 'Nicole Castro'}],"Tue, 27 Feb 2024 16:00:34 +0000"
768,NVIDIA RTX 500 and 1000 Professional Ada Generation Laptop GPUs Drive AI-Enhanced Workflows From Anywhere,https://blogs.nvidia.com/blog/rtx-ada-ai-workflows/,"With generative AI and hybrid work environments becoming the new standard, nearly every professional, whether a content creator, researcher or engineer, needs a powerful, AI-accelerated laptop to help users tackle their industry’s toughest challenges — even on the go. The new NVIDIA RTX 500 and 1000 Ada Generation Laptop GPUs will be available in new,		<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/rtx-ada-ai-workflows/\">
			Read Article			<span></span>
		</a>",[{'name': 'John Della Bona'}],"Mon, 26 Feb 2024 08:00:23 +0000"
769,Streamline Your Machine Learning Workflow with Scikit-learn Pipelines,https://www.kdnuggets.com/streamline-your-machine-learning-workflow-with-scikit-learn-pipelines?utm_source=rss&utm_medium=rss&utm_campaign=streamline-your-machine-learning-workflow-with-scikit-learn-pipelines,Learn how to enhance the quality of your machine learning code using Scikit-learn Pipeline and ColumnTransformer.,[{'name': 'Abid Ali Awan'}],"Thu, 07 Mar 2024 15:00:45 +0000"
770,"Master Data Science in a Year: The Ultimate Guide to Affordable, Self-Paced Learning",https://www.kdnuggets.com/master-data-science-in-a-year-the-ultimate-guide-to-affordable-self-paced-learning?utm_source=rss&utm_medium=rss&utm_campaign=master-data-science-in-a-year-the-ultimate-guide-to-affordable-self-paced-learning,Ready to start a career in data science? Put your commitment hat on because I found 4 courses you need to become a master in a year!,[{'name': 'Nisha Arya'}],"Thu, 07 Mar 2024 13:00:13 +0000"
771,5 Free Courses to Master SQL for Data Science,https://www.kdnuggets.com/5-free-courses-to-master-sql-for-data-science?utm_source=rss&utm_medium=rss&utm_campaign=5-free-courses-to-master-sql-for-data-science,Want to become an SQL expert? Check out these free courses to learn and hone your SQL skills for data science.,[{'name': 'Bala Priya C'}],"Wed, 06 Mar 2024 15:00:10 +0000"
772,Exploring the Potential of Transfer Learning in Small Data Scenarios,https://www.kdnuggets.com/exploring-the-potential-of-transfer-learning-in-small-data-scenarios?utm_source=rss&utm_medium=rss&utm_campaign=exploring-the-potential-of-transfer-learning-in-small-data-scenarios,"This article has shown how transfer learning can be used to help you solve small data problems, while also highlighting the benefits of using it in other fields.",[{'name': 'Nahla Davies'}],"Wed, 06 Mar 2024 13:00:53 +0000"
773,Best Free Resources to Learn Data Analysis and Data Science,https://www.kdnuggets.com/2024/03/365datascience-best-free-resources-learn-data-analysis-data-science?utm_source=rss&utm_medium=rss&utm_campaign=best-free-resources-to-learn-data-analysis-and-data-science,"This article introduces six top-notch, free data science resources ideal for aspiring data analysts, data scientists, or anyone aiming to enhance their analytical skills.",[{'name': 'KDnuggets'}],"Tue, 05 Mar 2024 18:00:53 +0000"
774,Extractive Summarization with LLM using BERT,https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert?utm_source=rss&utm_medium=rss&utm_campaign=extractive-summarization-with-llm-using-bert,"An in-depth overview of extractive text summarization, how state-of-the-art NLP models like BERT can enhance it, and a coding tutorial for using BERT to generate extractive summaries.",[{'name': 'Kevin Vu'}],"Tue, 05 Mar 2024 17:00:01 +0000"
775,5 Data Science Communities to Advance Your Career,https://www.kdnuggets.com/5-data-science-communities-to-advance-your-career?utm_source=rss&utm_medium=rss&utm_campaign=5-data-science-communities-to-advance-your-career,The best way to improve our knowledge is by learning together with communities.,[{'name': 'Cornellius Yudha Wijaya'}],"Tue, 05 Mar 2024 15:00:02 +0000"
776,5 Free University Courses to Learn Databases and SQL,https://www.kdnuggets.com/5-free-university-courses-to-learn-databases-and-sql?utm_source=rss&utm_medium=rss&utm_campaign=5-free-university-courses-to-learn-databases-and-sql,"Looking to learn SQL and databases to level up your data science skills? Learn SQL, database internals, and much more with these free university courses.",[{'name': 'Bala Priya C'}],"Tue, 05 Mar 2024 13:00:03 +0000"
777,Data Science and the Go Programming Language,https://www.kdnuggets.com/2024/03/nwu-data-science-go-programming-language?utm_source=rss&utm_medium=rss&utm_campaign=data-science-and-the-go-programming-language,Northwestern’s School of Professional Studies uses Go in Its Master of Science in Data Science Program.,[{'name': 'KDnuggets'}],"Mon, 04 Mar 2024 18:00:02 +0000"
778,Unlock the Secrets of LLMs in 60-Minute with Andrej Karpathy,https://www.kdnuggets.com/unlock-the-secrets-of-llms-in-a-60-minute-with-andrej-karpathy?utm_source=rss&utm_medium=rss&utm_campaign=unlock-the-secrets-of-llms-in-a-60-minute-with-andrej-karpathy,"Karpathy's talk provides a comprehensive yet accessible introduction to large language models, explaining their capabilities, future potential, and associated security risks in an engaging manner.",[{'name': 'Kanwal Mehreen'}],"Mon, 04 Mar 2024 17:00:50 +0000"
779,5 Courses to Master LLMs,https://www.kdnuggets.com/5-courses-to-master-llms?utm_source=rss&utm_medium=rss&utm_campaign=5-courses-to-master-llms,"The future world is full of LLM, and you don’t want to miss this most sought skill.",[{'name': 'Cornellius Yudha Wijaya'}],"Mon, 04 Mar 2024 15:00:41 +0000"
780,2024 Reading List: 5 Essential Reads on Artificial Intelligence,https://www.kdnuggets.com/2024-reading-list-5-essential-reads-on-artificial-intelligence?utm_source=rss&utm_medium=rss&utm_campaign=2024-reading-list-5-essential-reads-on-artificial-intelligence,Transform your understanding of current and future tech with these top 5 AI reads to explore the minds shaping our future.,[{'name': 'Nisha Arya'}],"Mon, 04 Mar 2024 13:00:55 +0000"
781,Top 5 Linux Distro for Data Science,https://www.kdnuggets.com/top-5-linux-distro-for-data-science?utm_source=rss&utm_medium=rss&utm_campaign=top-5-linux-distro-for-data-science,"If you are considering transitioning from Microsoft Windows to another operating system that suits your needs, check out these five Linux distributions for data science and machine learning.",[{'name': 'Abid Ali Awan'}],"Fri, 01 Mar 2024 15:00:03 +0000"
782,Top 6 YouTube Series for Data Science Beginners,https://www.kdnuggets.com/top-6-youtube-series-for-data-science-beginners?utm_source=rss&utm_medium=rss&utm_campaign=top-6-youtube-series-for-data-science-beginners,"Want to start your data science journey from home, for free, and work at your own pace? Have a dive into this data science roadmap using the YouTube series.",[{'name': 'Nisha Arya'}],"Fri, 01 Mar 2024 13:00:46 +0000"
783,Free Site Reliability Engineering Course From Google + Uplimit,https://www.kdnuggets.com/2024/02/uplimit-free-site-reliability-engineering-course-from-google?utm_source=rss&utm_medium=rss&utm_campaign=free-site-reliability-engineering-course-from-google-uplimit,Claim your spot for the free Google Site Reliability Engineering in partnership with Uplimit right now! Starts March 11.,[{'name': 'KDnuggets'}],"Thu, 29 Feb 2024 18:00:25 +0000"
784,5 Podcasts Every Machine Learning Enthusiast Should Follow,https://www.kdnuggets.com/5-podcasts-every-machine-learning-enthusiast-should-follow?utm_source=rss&utm_medium=rss&utm_campaign=5-podcasts-every-machine-learning-enthusiast-should-follow,The podcasts that would improve your ML knowledge.,[{'name': 'Cornellius Yudha Wijaya'}],"Thu, 29 Feb 2024 15:00:23 +0000"
785,5 Free Courses to Master Statistics for Data Science,https://www.kdnuggets.com/5-free-courses-to-master-statistics-for-data-science?utm_source=rss&utm_medium=rss&utm_campaign=5-free-courses-to-master-statistics-for-data-science,Want to learn statistics for data science? Check out these free courses to learn essential statistics concepts.,[{'name': 'Bala Priya C'}],"Thu, 29 Feb 2024 13:00:26 +0000"
786,AI Con USA: Navigate the Future of AI,https://www.kdnuggets.com/2024/02/techwell-ai-con-usa-navigate-the-future-of-ai?utm_source=rss&utm_medium=rss&utm_campaign=ai-con-usa-navigate-the-future-of-ai,"AI Con USA is scheduled for June 2-7 in Las Vegas, and it's bringing together some of the brightest minds in the realm of artificial intelligence and machine learning.",[{'name': 'KDnuggets'}],"Wed, 28 Feb 2024 18:00:50 +0000"
787,"Collection of Free Courses to Learn Data Science, Data Engineering, Machine Learning, MLOps, and LLMOps",https://www.kdnuggets.com/collection-of-free-courses-to-learn-data-science-data-engineering-machine-learning-mlops-and-llmops?utm_source=rss&utm_medium=rss&utm_campaign=collection-of-free-courses-to-learn-data-science-data-engineering-machine-learning-mlops-and-llmops,Begin your data professional journey from the basics of statistics to building a production-grade AI application.,[{'name': 'Abid Ali Awan'}],"Wed, 28 Feb 2024 15:00:15 +0000"
788,"Vector Database for LLMs, Generative AI, and Deep Learning",https://www.kdnuggets.com/vector-database-for-llms-generative-ai-and-deep-learning?utm_source=rss&utm_medium=rss&utm_campaign=vector-database-for-llms-generative-ai-and-deep-learning,Exploring the limitless possibilities of AI and making it context-aware.,[{'name': 'Kevin Vu'}],"Wed, 28 Feb 2024 13:00:30 +0000"
789,How to Learn Python Basics With ChatGPT,https://www.kdnuggets.com/how-to-learn-python-basics-with-chatgpt?utm_source=rss&utm_medium=rss&utm_campaign=how-to-learn-python-basics-with-chatgpt,Your Ultimate Learning Companion.,[{'name': 'Nate Rosidi'}],"Tue, 27 Feb 2024 15:00:06 +0000"
790,Free Data Analyst Bootcamp for Beginners,https://www.kdnuggets.com/free-data-analyst-bootcamp-for-beginners?utm_source=rss&utm_medium=rss&utm_campaign=free-data-analyst-bootcamp-for-beginners,Want to become a data analyst? This free beginner-friendly data analyst bootcamp is all you need.,[{'name': 'Bala Priya C'}],"Tue, 27 Feb 2024 13:00:41 +0000"
791,8 Built-in Python Decorators to Write Elegant Code,https://www.kdnuggets.com/8-built-in-python-decorators-to-write-elegant-code?utm_source=rss&utm_medium=rss&utm_campaign=8-built-in-python-decorators-to-write-elegant-code,"Developers can modify a function's behavior using decorators, without changing its source code. This provides a concise and flexible way to enhance and extend the functionality of functions.",[{'name': 'Kanwal Mehreen'}],"Mon, 26 Feb 2024 15:00:31 +0000"
792,7 Free Harvard University Courses to Advance Your Skills,https://www.kdnuggets.com/7-free-harvard-university-courses-to-advance-your-skills?utm_source=rss&utm_medium=rss&utm_campaign=7-free-harvard-university-courses-to-advance-your-skills,Transform your tech career with one of the best universities in the world!,[{'name': 'Nisha Arya'}],"Mon, 26 Feb 2024 13:00:19 +0000"
793,Everything You Need to Know About MLOps: A KDnuggets Tech Brief,https://www.kdnuggets.com/tech-brief-everything-you-need-to-know-about-mlops?utm_source=rss&utm_medium=rss&utm_campaign=everything-you-need-to-know-about-mlops-a-kdnuggets-tech-brief,"KDnuggets' first Tech Brief is now available, and it outlines everything you need to know about MLOps.",[{'name': 'KDnuggets'}],"Fri, 23 Feb 2024 15:25:10 +0000"
794,Free Mastery Course: Become a Large Language Model Expert,https://www.kdnuggets.com/ree-mastery-course-become-a-large-language-model-expert?utm_source=rss&utm_medium=rss&utm_campaign=free-mastery-course-become-a-large-language-model-expert,It is a self-paced course that covers fundamental and advanced concepts of LLMs and teaches how to deploy them in production.,[{'name': 'Abid Ali Awan'}],"Fri, 23 Feb 2024 15:00:19 +0000"
795,3 Inspirational Stories of Leaders in AI,https://www.kdnuggets.com/3-inspirational-stories-of-leaders-in-ai?utm_source=rss&utm_medium=rss&utm_campaign=3-inspirational-stories-of-leaders-in-ai,"Every leader has their origin story, and here are some that might inspire you.",[{'name': 'Cornellius Yudha Wijaya'}],"Fri, 23 Feb 2024 13:00:57 +0000"
796,5 Airflow Alternatives for Data Orchestration,https://www.kdnuggets.com/5-airflow-alternatives-for-data-orchestration?utm_source=rss&utm_medium=rss&utm_campaign=5-airflow-alternatives-for-data-orchestration,Top list of open-source tools for building and managing workflows.,[{'name': 'Abid Ali Awan'}],"Thu, 22 Feb 2024 15:00:05 +0000"
797,7 Free Kaggle Micro-Courses for Data Science Beginners,https://www.kdnuggets.com/7-free-kaggle-micro-courses-for-data-science-beginners?utm_source=rss&utm_medium=rss&utm_campaign=7-free-kaggle-micro-courses-for-data-science-beginners,Interested in learning data science? Check out these free micro-courses from Kaggle to learn essential data science skills.,[{'name': 'Bala Priya C'}],"Thu, 22 Feb 2024 13:00:29 +0000"
798,The Right Way to Access Dictionaries in Python,https://www.kdnuggets.com/the-right-way-to-access-dictionaries-in-python?utm_source=rss&utm_medium=rss&utm_campaign=the-right-way-to-access-dictionaries-in-python,Effectively accessing dictionaries data with Python’s get() and setdefault().,[{'name': 'Josep Ferrer'}],"Wed, 21 Feb 2024 15:00:17 +0000"
799,Free Amazon Courses to Learn Generative AI: For All Levels,https://www.kdnuggets.com/free-amazon-courses-to-learn-generative-ai-for-all-levels?utm_source=rss&utm_medium=rss&utm_campaign=free-amazon-courses-to-learn-generative-ai-for-all-levels,"Upskill with these free courses to master generative AI, regardless of your job title.",[{'name': 'Nisha Arya'}],"Wed, 21 Feb 2024 13:00:56 +0000"
800,Python in Finance: Real Time Data Streaming within Jupyter Notebook,https://www.kdnuggets.com/python-in-finance-real-time-data-streaming-within-jupyter-notebook?utm_source=rss&utm_medium=rss&utm_campaign=python-in-finance-real-time-data-streaming-within-jupyter-notebook,"Learn a modern approach to stream real-time data in Jupyter Notebook. This guide covers dynamic visualizations, a Python for quant finance use case, and Bollinger Bands analysis with live data.",[{'name': 'Mudit Srivastava'}],"Tue, 20 Feb 2024 17:00:53 +0000"
801,Navigating the Data Revolution: Exploring the Booming Trends in Data Science and Machine Learning,https://www.kdnuggets.com/navigating-the-data-revolution-exploring-the-booming-trends-in-data-science-and-machine-learning?utm_source=rss&utm_medium=rss&utm_campaign=navigating-the-data-revolution-exploring-the-booming-trends-in-data-science-and-machine-learning,"Dive into transformative trends in data science, encompassing AI-powered automation, NLP, ethical considerations, decentralized computing, and interdisciplinary collaboration.",[{'name': 'Aryan Garg'}],"Tue, 20 Feb 2024 15:00:08 +0000"
802,6 YouTube Channels to Learn about AI,https://www.kdnuggets.com/6-youtube-channels-to-learn-about-ai?utm_source=rss&utm_medium=rss&utm_campaign=6-youtube-channels-to-learn-about-ai,Are you looking into learning about AI? YouTube is your first stop.,[{'name': 'Nisha Arya'}],"Tue, 20 Feb 2024 13:00:07 +0000"
803,Prompt Engineering: An Integrated Dream,https://www.kdnuggets.com/prompt-engineering-an-integrated-dream?utm_source=rss&utm_medium=rss&utm_campaign=prompt-engineering-an-integrated-dream,"Clickbait headlines like \"AI's Hottest Job\" have promised a career that anyone who knows how to chat with AI could pay a six-figure salary with no computer background. But is this reality, or just another internet pipe dream? Let's ditch the sensationalism and delve into the actual job market data to find out.",[{'name': 'Mahdi Ahmadi'}],"Mon, 19 Feb 2024 17:00:41 +0000"
804,Introduction to Memory Profiling in Python,https://www.kdnuggets.com/introduction-to-memory-profiling-in-python?utm_source=rss&utm_medium=rss&utm_campaign=introduction-to-memory-profiling-in-python,"So where did all the memory go? To figure out, learn how to profile your Python code for memory usage using the memory-profiler package.",[{'name': 'Bala Priya C'}],"Mon, 19 Feb 2024 15:00:12 +0000"
805,A Roadmap For Your Data Career,https://www.kdnuggets.com/a-roadmap-for-your-data-career?utm_source=rss&utm_medium=rss&utm_campaign=a-roadmap-for-your-data-career,"As you design your career in data, you’ve got to avoid getting stuck in your comfort zone or allowing your manager or current situation to determine your path.",[{'name': 'Stan Pugsley'}],"Mon, 19 Feb 2024 13:00:56 +0000"
806,Master The Art Of Command Line With This GitHub Repository,https://www.kdnuggets.com/master-the-art-of-command-line-with-this-github-repository?utm_source=rss&utm_medium=rss&utm_campaign=master-the-art-of-command-line-with-this-github-repository,"Whether you are a beginner or an experienced user, this guide is perfect for familiarizing yourself with basic and advanced command line tools.",[{'name': 'Abid Ali Awan'}],"Fri, 16 Feb 2024 15:00:51 +0000"
807,7 Steps to Mastering Exploratory Data Analysis,https://www.kdnuggets.com/7-steps-to-mastering-exploratory-data-analysis?utm_source=rss&utm_medium=rss&utm_campaign=7-steps-to-mastering-exploratory-data-analysis,"A Step-by-Step Approach to Unearthing Trends, Outliers, and Insights in your Data.",[{'name': 'Josep Ferrer'}],"Fri, 16 Feb 2024 13:00:50 +0000"
808,Jupyter Notebook Magic Methods Cheat Sheet,https://www.kdnuggets.com/jupyter-notebook-magic-methods-cheat-sheet?utm_source=rss&utm_medium=rss&utm_campaign=jupyter-notebook-magic-methods-cheat-sheet,KDnuggets' latest original cheat sheet covers Jupyter Notebook magic methods. Check it out now and become a notebook magician.,[{'name': 'KDnuggets'}],"Thu, 15 Feb 2024 17:00:54 +0000"
809,Large Language Models Explained in 3 Levels of Difficulty,https://www.kdnuggets.com/large-language-models-explained-in-3-levels-of-difficulty?utm_source=rss&utm_medium=rss&utm_campaign=large-language-models-explained-in-3-levels-of-difficulty,"Simple explanations, no matter what your level is right now.",[{'name': 'Cornellius Yudha Wijaya'}],"Thu, 15 Feb 2024 15:00:44 +0000"
810,Semantic Layers are the Missing Piece for AI-Enabled Analytics,https://www.kdnuggets.com/2024/02/cube-semantic-layers-missing-piece-ai-enabled-analytics?utm_source=rss&utm_medium=rss&utm_campaign=semantic-layers-are-the-missing-piece-for-ai-enabled-analytics,"Integrating a semantic layer with Language Learning Models (LLMs) presents a clean solution to this, particularly in the realm of AI chatbots. This combination empowers businesses to generate fast responses and reports based on their data. Leveraging AI and semantic layers is advancing business intelligence, making it easier than ever for people to interact with data.",[{'name': 'KDnuggets'}],"Wed, 14 Feb 2024 18:00:10 +0000"
811,Top 5 DataCamp Courses for Mastering Generative AI,https://www.kdnuggets.com/top-5-datacamp-courses-for-mastering-generative-ai?utm_source=rss&utm_medium=rss&utm_campaign=top-5-datacamp-courses-for-mastering-generative-ai,"Learn the skills you require to kickstart your Generative AI journey with DataCamp - beginner, intermediate, and expert!",[{'name': 'Nisha Arya'}],"Wed, 14 Feb 2024 15:00:36 +0000"
812,"What Is Data Lineage, And Why Does It Matter?",https://www.kdnuggets.com/what-is-data-lineage-and-why-does-it-matter?utm_source=rss&utm_medium=rss&utm_campaign=what-is-data-lineage-and-why-does-it-matter,"If you’ve ever had conversations with data professionals, you’ve probably heard “data lineage” pop up quite a few times. So what is data lineage all about, and why is it important?",[{'name': 'Bala Priya C'}],"Wed, 14 Feb 2024 13:00:23 +0000"
813,"Generative AI Playground: Text-to-Image Stable Diffusion with Stability AI, Stable Diffusion XL, and CompVis on the Latest Intel® GPU",https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-text-to-image-stable-diffusion?utm_source=rss&utm_medium=rss&utm_campaign=generative-ai-playground-text-to-image-stable-diffusion-with-stability-ai-stable-diffusion-xl-and-compvis-on-the-latest-intel-gpu,"Stable Diffusion models are revolutionizing digital artistry, transforming mere text into stunning, lifelike images. Explore further here.",[{'name': 'KDnuggets'}],"Tue, 13 Feb 2024 18:00:30 +0000"
814,3 Research-Driven Advanced Prompting Techniques for LLM Efficiency and Speed Optimization,https://www.kdnuggets.com/3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization?utm_source=rss&utm_medium=rss&utm_campaign=3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization,This article has explored three promising prompting techniques that have been developed to reduce the occurrence of hallucinations in large language models.,[{'name': 'Abid Ali Awan'}],"Tue, 13 Feb 2024 17:00:06 +0000"
815,2024 Tech Trends: AI Breakthroughs & Development Insights from O’Reilly’s Free Report,https://www.kdnuggets.com/2024-tech-trends-ai-breakthroughs-development-insights-oreilly-free-report?utm_source=rss&utm_medium=rss&utm_campaign=2024-tech-trends-ai-breakthroughs-development-insights-from-oreillys-free-report,Want to prepare your tech career for 2024 and onwards? Have a look at O’Reilly’s FREE technology trends report.,[{'name': 'Nisha Arya'}],"Tue, 13 Feb 2024 15:00:51 +0000"
816,How To Comment Your Python Code as a Data Scientist,https://www.kdnuggets.com/how-to-comment-your-python-code-as-a-data-scientist?utm_source=rss&utm_medium=rss&utm_campaign=how-to-comment-your-python-code-as-a-data-scientist,Don’t overlook these essential aspects of programming activity.,[{'name': 'Cornellius Yudha Wijaya'}],"Tue, 13 Feb 2024 13:00:18 +0000"
817,Synthetic Data for Machine Learning,https://www.kdnuggets.com/synthetic-data-for-machine-learning?utm_source=rss&utm_medium=rss&utm_campaign=synthetic-data-for-machine-learning,You don't always have high-quality labeled datasets for supervised machine learning. Learn about why you should augment your real data with synthetic data as well as the ways to generate it.,[{'name': 'Michael Galarnyk'}],"Mon, 12 Feb 2024 17:00:56 +0000"
818,Free Data Engineering Course for Beginners,https://www.kdnuggets.com/free-data-engineering-course-for-beginners?utm_source=rss&utm_medium=rss&utm_campaign=free-data-engineering-course-for-beginners,Interested in data engineering but don't know where to start? Get up to speed in data engineering fundamentals with this free course.,[{'name': 'Bala Priya C'}],"Mon, 12 Feb 2024 15:00:58 +0000"
819,Learn Data Science on a Budget,https://www.kdnuggets.com/learn-data-science-on-a-budget?utm_source=rss&utm_medium=rss&utm_campaign=learn-data-science-on-a-budget,This blog will go through platforms and courses you can take that will get you from 0-100 on your data science knowledge.,[{'name': 'Nisha Arya'}],"Mon, 12 Feb 2024 13:00:56 +0000"
820,University of Cincinnati MS Business Analytics Summer 2024 Information Session,https://www.kdnuggets.com/2024/02/uc-business-analytics-summer-2024-information-session?utm_source=rss&utm_medium=rss&utm_campaign=university-of-cincinnati-ms-business-analytics-summer-2024-information-session,Don't miss this chance to chart your course toward a successful career in business analytics. Reserve your spot now and embark on a journey of knowledge and growth!,[{'name': 'KDnuggets'}],"Fri, 09 Feb 2024 18:10:21 +0000"
821,The Only Free Course You Need To Become a MLOps Engineer,https://www.kdnuggets.com/the-only-free-course-you-need-to-become-a-mlops-engineer?utm_source=rss&utm_medium=rss&utm_campaign=the-only-free-course-you-need-to-become-a-mlops-engineer,"Unlock the secrets to building, deploying, and monitoring models like a pro.",[{'name': 'Abid Ali Awan'}],"Fri, 09 Feb 2024 15:00:58 +0000"
822,5 Cheap Books to Master Machine Learning,https://www.kdnuggets.com/5-cheap-books-to-master-machine-learning?utm_source=rss&utm_medium=rss&utm_campaign=5-cheap-books-to-master-machine-learning,"Machine Learning is a skill that everyone should have, and these cheap books would facilitate that learning process.",[{'name': 'Cornellius Yudha Wijaya'}],"Fri, 09 Feb 2024 13:00:00 +0000"
823,Navigating Today’s Data and AI Market Uncertainty,https://www.kdnuggets.com/2024/02/altair-navigating-todays-data-ai-market-uncertainty?utm_source=rss&utm_medium=rss&utm_campaign=navigating-todays-data-and-ai-market-uncertainty,It’s more important than ever to think long-term about the analytics partnerships you forge. Are you choosing technologies that will stand the test of time? Are you choosing companies with proven track records?,[{'name': 'KDnuggets'}],"Thu, 08 Feb 2024 18:00:47 +0000"
824,Top 5 AI Coding Assistants You Must Try,https://www.kdnuggets.com/top-5-ai-coding-assistants-you-must-try?utm_source=rss&utm_medium=rss&utm_campaign=top-5-ai-coding-assistants-you-must-try,"Discover the top AI coding assistants that can 10X your productivity overnight - #5 has the best autocomplete feature, and #1 is the most advanced code assistant tool ever seen!",[{'name': 'Abid Ali Awan'}],"Thu, 08 Feb 2024 15:00:31 +0000"
825,Free Data Science Interview Book to Land Your Dream Job,https://www.kdnuggets.com/free-data-science-interview-book-to-land-your-dream-job?utm_source=rss&utm_medium=rss&utm_campaign=free-data-science-interview-book-to-land-your-dream-job,Are you preparing for your dream data science job but feeling overwhelmed by the vast amount of online resources? Look no further than this free and easily accessible web-based book to help you brush up on your skills and feel confident for your upcoming interview.,[{'name': 'Kanwal Mehreen'}],"Thu, 08 Feb 2024 13:00:45 +0000"
826,Generative AI Playground: LLMs with Camel-5b and Open LLaMA 3B on the Latest Intel® GPU,https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-llms-with-camel-5b-and-open-llama-3b?utm_source=rss&utm_medium=rss&utm_campaign=generative-ai-playground-llms-with-camel-5b-and-open-llama-3b-on-the-latest-intel-gpu,"Intel offers a thrilling glimpse into the next generation of AI, showcasing the power of Camel-5b and Open LLaMA 3B LLMs.",[{'name': 'KDnuggets'}],"Wed, 07 Feb 2024 18:00:37 +0000"
827,Sentiment Analysis in Python: Going Beyond Bag of Words,https://www.kdnuggets.com/sentiment-analysis-in-python-going-beyond-bag-of-words?utm_source=rss&utm_medium=rss&utm_campaign=sentiment-analysis-in-python-going-beyond-bag-of-words,"This code based tutorial provides a brief introduction to Sentiment Analysis, a method used to predict emotions, similar to a digital psychologist.",[{'name': 'Nate Rosidi'}],"Wed, 07 Feb 2024 15:00:57 +0000"
828,5 Free Courses to Master Python for Data Science,https://www.kdnuggets.com/5-free-courses-to-master-python-for-data-science?utm_source=rss&utm_medium=rss&utm_campaign=5-free-courses-to-master-python-for-data-science,Want to learn Python to kickstart your career in data? Here are five free courses to help you master Python for data science.,[{'name': 'Bala Priya C'}],"Wed, 07 Feb 2024 13:00:52 +0000"
829,"Research Papers in February 2024: A LoRA Successor, Small Finetuned LLMs Vs Generalist LLMs, and Transparent LLM Research",https://magazine.sebastianraschka.com/p/research-papers-in-february-2024,"Once again, this has been an exciting month in AI research. This month, I'm covering two new openly available LLMs, insights into small finetuned LLMs, and a new parameter-efficient LLM finetuning technique. The two LLMs mentioned above stand out for several reasons. One LLM (OLMo) is completely open source, meaning that everything from the training code to the dataset to the log files is openly shared.","[{'name': 'Sebastian Raschka, PhD'}]","Sun, 03 Mar 2024 11:53:57 GMT"
830,Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch,https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch,"Low-rank adaptation (LoRA) is a machine learning technique that modifies a pretrained model (for example, an LLM or vision transformer) to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters.","[{'name': 'Sebastian Raschka, PhD'}]","Sun, 18 Feb 2024 18:35:24 GMT"
831,"Research Papers in Jan 2024: Model Merging, Mixtures of Experts, and Towards Smaller LLMs",https://magazine.sebastianraschka.com/p/research-papers-in-january-2024,"Model Merging, Mixtures of Experts, and Towards Smaller LLMs","[{'name': 'Sebastian Raschka, PhD'}]","Sat, 03 Feb 2024 11:55:32 GMT"
832,"Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs",https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention,"This article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama. Self-attention and related mechanisms are core components of LLMs, making them a useful topic to understand when working with these models.",[],"Sun, 14 Jan 2024 11:55:06 GMT"
833,Ten Noteworthy AI Research Papers of 2023,https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023,"This year has felt distinctly different. I've been working in, on, and with machine learning and AI for over a decade, yet I can't recall a time when these fields were as popular and rapidly evolving as they have been this year. To conclude an eventful 2023 in machine learning and AI research, I'm excited to share 10 noteworthy papers I've read this year. My personal focus has been more on large language models, so you'll find a heavier emphasis on large language model (LLM) papers than computer vision papers this year.","[{'name': 'Sebastian Raschka, PhD'}]","Sat, 30 Dec 2023 12:20:19 GMT"
834,"Research Papers in Nov 2023: Tackling Hallucinations, Boosting Reasoning Abilities, and New Insights into the Transformer Architecture",https://magazine.sebastianraschka.com/p/research-papers-in-november-2023,"This month, I want to focus on three papers that address three distinct problem categories of Large Language Models (LLMs): Reducing hallucinations. Enhancing the reasoning capabilities of small, openly available models. Deepening our understanding of, and potentially simplifying, the transformer architecture.","[{'name': 'Sebastian Raschka, PhD'}]","Sat, 09 Dec 2023 11:56:15 GMT"
835,Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation),https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms,Things I Learned From Hundreds of Experiments,"[{'name': 'Sebastian Raschka, PhD'}]","Sun, 19 Nov 2023 12:11:26 GMT"
836,Research Papers in Oct 2023: A Potential Successor to RLHF for Efficient LLM Alignment and the Resurgence of CNNs,https://magazine.sebastianraschka.com/p/research-papers-october-2023,"From Vision Transformers to innovative large language model finetuning techniques, the AI community has been very active with lots of interesting research this past month. Here's a snapshot of the highlights I am covering in this article: In the paper","[{'name': 'Sebastian Raschka, PhD'}]","Sat, 04 Nov 2023 10:51:40 GMT"
837,AI and Open Source in 2023,https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023,The Highs and Lows: A Year in Review,"[{'name': 'Sebastian Raschka, PhD'}]","Mon, 23 Oct 2023 10:53:13 GMT"
838,"LLM Business and Busyness: Recent Company Investments and AI Adoption, New Small Openly Available LLMs, and LoRA Research",https://magazine.sebastianraschka.com/p/ahead-of-ai-12-llm-businesses,"Discussing Recent Company Investments and AI Adoption, New Small Openly Available LLMs, and LoRA Research","[{'name': 'Sebastian Raschka, PhD'}]","Sun, 08 Oct 2023 10:55:14 GMT"
839,Research Papers Aug-Sep 2023: From Self-Alignment to LongLoRA,https://magazine.sebastianraschka.com/p/research-highlights-in-three-sentences-3d5,"Another month, another round of interesting research papers ranging from large language modeling to computer vision. One recent focus is on refining Large Language Models (LLMs). For instance, introducing models like Platypus and the Reinforced Self-Training (ReST) method are the latest attempts to improve alignment with human preferences.","[{'name': 'Sebastian Raschka, PhD'}]","Sat, 23 Sep 2023 11:12:03 GMT"
840,LLM Training: RLHF and Its Alternatives,https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives,"I frequently reference a process called Reinforcement Learning with Human Feedback (RLHF) when discussing LLMs, whether in the research news or tutorials. RLHF is an integral part of the modern LLM training pipeline due to its ability to incorporate human preferences into the optimization landscape, which can improve the model's helpfulness and safety.","[{'name': 'Sebastian Raschka, PhD'}]","Sun, 10 Sep 2023 11:33:03 GMT"
841,The Missing Bits: Llama 2 Weights Have Changed,https://magazine.sebastianraschka.com/p/the-missing-bits-llama-2-weights,"Due to the extensive length of the regular Ahead of AI #11: New Foundation Models article, I removed some interesting tidbits around the Llama 2 weights from the main newsletter. However, it might be nice to include those as a small bonus for the supporters of Ahead of AI. Thanks again for the kind support!","[{'name': 'Sebastian Raschka, PhD'}]","Sun, 27 Aug 2023 14:38:46 GMT"
842,New Foundation Models: CodeLlama and other highlights in Open-Source AI,https://magazine.sebastianraschka.com/p/ahead-of-ai-11-new-foundation-models,"In this edition of the newsletter, we direct our attention to one of the most prominent highlights of the summer: the release of the Llama 2 base and chat models, as well as CodeLlama, the latest highlights in the open-source AI large language model (LLM) landscape.","[{'name': 'Sebastian Raschka, PhD'}]","Sat, 26 Aug 2023 11:34:03 GMT"
843,"Research Highlights Jul-Aug 2023: Llama 2, Flash-Attention 2, and More",https://magazine.sebastianraschka.com/p/research-highlights-in-three-sentences,"Every month is a busy month for LLM research. However, this month has been particularly interesting due to the release of new state-of-the-art base models, such as Meta's Llama 2 model suite. Double kudos: this new iteration of Llama models comes without any major restrictions and a very detailed 77-page report on arXiv!","[{'name': 'Sebastian Raschka, PhD'}]","Sat, 12 Aug 2023 16:41:26 GMT"
844,Large Language Models and Nearest Neighbors,https://magazine.sebastianraschka.com/p/large-language-models-and-nearest,"Instead of jumping on the latest trend of the week, I wanted to dive into a recent, fascinating application of nearest-neighbor methods in the context of large language models (LLMs) that made big waves in July. You may know that I like simple yet elegant and baselines, but I also found this method quite refreshing given that most of the current research is about scaling already massive LLMs. While they may not scale to all kinds of problems where LLMs currently excel, seemingly simple methods such as nearest neighbor algorithms have a certain beauty to them. It also shows that there are still many opportunities to innovate and make significant contributions based on foundational or \"classic\" techniques.","[{'name': 'Sebastian Raschka, PhD'}]","Sun, 30 Jul 2023 12:03:29 GMT"
845,"AI Research Highlights June-July 2023: Long Contexts and Scaling Transformers to 1,000,000,000 Tokens",https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences-738,"In this article, I compiled and annotated 24 AI research highlights form June to July 2023. A lot of exciting developments are currently happening, once again, in the fields of natural language processing and computer vision! In addition, if you are curious about last month's highlights, you can find them here:","[{'name': 'Sebastian Raschka, PhD'}]","Sat, 15 Jul 2023 12:50:53 GMT"
846,State of Computer Vision 2023: From Vision Transformers to Neural Radiance Fields,https://magazine.sebastianraschka.com/p/ahead-of-ai-10-state-of-computer,"Large language model development (LLM) development is still happening at a rapid pace. At the same time, leaving AI regulation debates aside, LLM news seem to be arriving at a just slightly slower rate than usual. This is a good opportunity to give the spotlight to computer vision once in a while, discussing the current state of research and development in this field. And this theme also goes nicely with a recap of CVPR 2023 in Vancouver, which was a wonderful conference at probably the nicest conference venue I have attended so far.","[{'name': 'Sebastian Raschka, PhD'}]","Thu, 06 Jul 2023 12:48:11 GMT"
847,Accelerating PyTorch Model Training,https://magazine.sebastianraschka.com/p/accelerating-pytorch-model-training,Using Mixed-Precision and Fully Sharded Data Parallelism,"[{'name': 'Sebastian Raschka, PhD'}]","Mon, 26 Jun 2023 11:47:03 GMT"
848,Understanding Encoder And Decoder LLMs,https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder,Several people asked me to dive a bit deeper into large language model (LLM) jargon and explain some of the more technical terms we nowadays take for granted. This includes references to \"encoder-style\" and \"decoder-style\" LLMs. What do these terms mean?,"[{'name': 'Sebastian Raschka, PhD'}]","Sat, 17 Jun 2023 12:45:03 GMT"
849,MOSK improves support for AI and Windows workloads,https://ai-techpark.com/mosk-improves-support-for-ai-and-windows-workloads/,"<p>Virtualizes NVIDIA GPUs, improves Windows support, and can replace VMware at lower cost Mirantis today announced the latest release of Mirantis OpenStack for Kubernetes (MOSK). The release incorporates hundreds of improvements from the open source upstream, plus dozens of innovations including graphics processing unit (GPU) virtualization – especially useful for cost-effectively running artificial intelligence...</p>
<p>The post <a href=\"https://ai-techpark.com/mosk-improves-support-for-ai-and-windows-workloads/\">MOSK improves support for AI and Windows workloads</a> first appeared on <a href=\"https://ai-techpark.com\">AI-TechPark</a>.</p>",[{'name': 'Business Wire'}],"Thu, 07 Mar 2024 13:30:00 +0000"
850,DataStax achieves the AWS Generative AI Competency,https://ai-techpark.com/datastax-achieves-the-aws-generative-ai-competency/,"<p>New Competency Recognizes DataStax as a Trusted AWS Partner for Helping Developers and Enterprises Build Generative AI Applications on AWS DataStax, the generative AI data company, announced today that it has achieved the Amazon Web Services (AWS) Generative AI Competency. This specialization recognizes DataStax as an AWS Partner that helps...</p>
<p>The post <a href=\"https://ai-techpark.com/datastax-achieves-the-aws-generative-ai-competency/\">DataStax achieves the AWS Generative AI Competency</a> first appeared on <a href=\"https://ai-techpark.com\">AI-TechPark</a>.</p>",[{'name': 'Business Wire'}],"Thu, 07 Mar 2024 09:15:00 +0000"
851,Altimetrik taps Adam Caplan as President of Digital Business and AI,https://ai-techpark.com/altimetrik-taps-adam-caplan-as-president-of-digital-business-and-ai/,"<p>Altimetrik, a pure-play digital business services company, has appointed Adam Caplan as President of Digital Business and AI. Caplan will report CEO Raj Sundaresan. Caplan’s appointment aligns with Altimetrik’s business growth plan as the company sees increased demand for its services across pharmaceutical, manufacturing, retail, financial services, and other industries....</p>
<p>The post <a href=\"https://ai-techpark.com/altimetrik-taps-adam-caplan-as-president-of-digital-business-and-ai/\">Altimetrik taps Adam Caplan as President of Digital Business and AI</a> first appeared on <a href=\"https://ai-techpark.com\">AI-TechPark</a>.</p>",[{'name': 'Business Wire'}],"Thu, 07 Mar 2024 08:30:00 +0000"
852,SAP introduces innovations to help customers succeed in the era of AI,https://ai-techpark.com/sap-introduces-innovations-to-help-customers-succeed-in-the-era-of-ai/,"<p>SAP SE (NYSE: SAP) today announced transformative data innovations that will help customers harness the full power of their data to drive deeper insights, faster growth and more efficiency in the era of AI. New capabilities in the SAP Datasphere solution, including new generative-AI features, transform enterprise planning through simplified data landscapes...</p>
<p>The post <a href=\"https://ai-techpark.com/sap-introduces-innovations-to-help-customers-succeed-in-the-era-of-ai/\">SAP introduces innovations to help customers succeed in the era of AI</a> first appeared on <a href=\"https://ai-techpark.com\">AI-TechPark</a>.</p>",[{'name': 'PR Newswire'}],"Thu, 07 Mar 2024 07:45:00 +0000"
853,"Ibex, PathPresenter to advance AI-powered Digital Pathology adoption",https://ai-techpark.com/ibex-pathpresenter-to-advance-ai-powered-digital-pathology-adoption/,"<p>PathPresenter and Ibex&#160;Partner to Advance Digital Pathology and AI Solutions Ibex Medical Analytics (Ibex), the leader in AI-powered cancer diagnostics, and PathPresenter, the global image sharing platform for pathology, today announced a partnership to advance the adoption of AI-powered digital pathology. The two companies will work together to support joint customers via...</p>
<p>The post <a href=\"https://ai-techpark.com/ibex-pathpresenter-to-advance-ai-powered-digital-pathology-adoption/\">Ibex, PathPresenter to advance AI-powered Digital Pathology adoption</a> first appeared on <a href=\"https://ai-techpark.com\">AI-TechPark</a>.</p>",[{'name': 'PR Newswire'}],"Thu, 07 Mar 2024 07:30:00 +0000"
854,Fierce Medtech names Genesis Therapeutics a Fierce 15 Company of 2023,https://ai-techpark.com/fierce-medtech-names-genesis-therapeutics-a-fierce-15-company-of-2023/,"<p>Genesis Therapeutics, a company pioneering generative and predictive artificial intelligence (AI) technologies to develop breakthrough therapeutics for underserved patient populations, today announced that Fierce Medtech has named it as one of 2023’s “Fierce 15” medtech companies. The annual special report features the most innovative private companies in the industry. Genesis has...</p>
<p>The post <a href=\"https://ai-techpark.com/fierce-medtech-names-genesis-therapeutics-a-fierce-15-company-of-2023/\">Fierce Medtech names Genesis Therapeutics a Fierce 15 Company of 2023</a> first appeared on <a href=\"https://ai-techpark.com\">AI-TechPark</a>.</p>",[{'name': 'Business Wire'}],"Wed, 06 Mar 2024 14:00:00 +0000"
855,DTEX Systems raises $50M led by Alphabet’s CapitalG,https://ai-techpark.com/dtex-systems-raises-50m-led-by-alphabets-capitalg/,"<p>After growing its SaaS business by 118% in FY2023 at near break-even cash flow, DTEX is set to innovate the Insider Risk Management space through the application of behavioral science research to accelerate investigations and decision-making. DTEX Systems, the global leader for insider risk management, today announced that it closed...</p>
<p>The post <a href=\"https://ai-techpark.com/dtex-systems-raises-50m-led-by-alphabets-capitalg/\">DTEX Systems raises $50M led by Alphabet’s CapitalG</a> first appeared on <a href=\"https://ai-techpark.com\">AI-TechPark</a>.</p>",[{'name': 'Business Wire'}],"Wed, 06 Mar 2024 12:30:00 +0000"
856,ABBYY and Novelis Innovation announced expanded partnership,https://ai-techpark.com/abbyy-and-novelis-innovation-announced-expanded-partnership/,"<p>Novelis Innovation’s momentum for deploying ABBYY purpose-built artificial intelligence (AI) solutions in Europe is expanding into the United States. Novelis successfully used ABBYY’s portfolio of intelligent automation solutions to drive greater document efficiency and strategic process visibility for several European clients in government, banking, and insurance, and Novelis will replicate...</p>
<p>The post <a href=\"https://ai-techpark.com/abbyy-and-novelis-innovation-announced-expanded-partnership/\">ABBYY and Novelis Innovation announced expanded partnership</a> first appeared on <a href=\"https://ai-techpark.com\">AI-TechPark</a>.</p>",[{'name': 'Business Wire'}],"Wed, 06 Mar 2024 12:15:00 +0000"
857,BlueCat appoints Scott Fulton as Chief Product and Technology Officer,https://ai-techpark.com/bluecat-appoints-scott-fulton-as-chief-product-and-technology-officer/,"<p>An accomplished executive, he will lead the next phase of product development and growth BlueCat Networks, a leading provider of secure and automated DNS, DHCP, and IP address management (collectively known as DDI) services and solutions for mission-critical networks, announced that Scott Fulton has joined the company as its new...</p>
<p>The post <a href=\"https://ai-techpark.com/bluecat-appoints-scott-fulton-as-chief-product-and-technology-officer/\">BlueCat appoints Scott Fulton as Chief Product and Technology Officer</a> first appeared on <a href=\"https://ai-techpark.com\">AI-TechPark</a>.</p>",[{'name': 'Globe Newswire'}],"Wed, 06 Mar 2024 10:45:00 +0000"
858,GreyOrange demonstrates Results-Driven Robotics during MODEX,https://ai-techpark.com/greyorange-demonstrates-results-driven-robotics-during-modex/,"<p>GreyOrange will showcase how the vendor-agnostic GreyMatter fulfillment orchestration platform ensures seamless warehouse automation during live demos with partners in Booth #C5692 during MODEX GreyOrange Inc., a leader in AI-driven fulfillment automation, is demonstrating what tailored results-driven robotics within fulfillment environments look like during MODEX 2024 in Atlanta at the Georgia World...</p>
<p>The post <a href=\"https://ai-techpark.com/greyorange-demonstrates-results-driven-robotics-during-modex/\">GreyOrange demonstrates Results-Driven Robotics during MODEX</a> first appeared on <a href=\"https://ai-techpark.com\">AI-TechPark</a>.</p>",[{'name': 'Globe Newswire'}],"Wed, 06 Mar 2024 10:00:00 +0000"
859,Unleashing Generative AI's Next Wave at the Edge,https://aibusiness.com/responsible-ai/unleashing-generative-ai-s-next-wave-at-the-edge,"Generative AI has captured widespread interest by offering business and consumers unprecedented opportunities to directly utilize AI in ways that were previously the stuff of science fiction. Yet, the very expansion of computing power and AI capabilities that has enabled these advancements is now becoming a challenge, as AI training and inference emerge as the dominant computing tasks of the 2020s.",[{}],"Mon, 22 Apr 2024 18:00:00 GMT"
860,Transforming Enterprise Capabilities with GenAI: Best Practices for Integrating GenAI at Scale,https://aibusiness.com/responsible-ai/transforming-enterprise-capabilities-with-genai-best-practices-for-integrating-genai-at-scale,"<p>A compelling case study from technical and product leadership at Clarivate, AI21 Labs, and NorthBay Solutions, who will illustrate how clients can work with AI model providers and solutions partners to optimize strategies for integrating GenAI into your enterprise.</p>
<p>You’ll learn how Clarivate, a global leader in trusted intelligence research services, solved a key challenge around the information retrieval experience for researchers by introducing an innovative, faster way to access academic literature. Search engines fell short when it came to user experience, and most AI solutions offered questionable integrity in the results they offered - making their answers problematic and unreliable for academic use.</p>
<p>Join us to discover how Clarivate succeeded with AI21’s Task-Specific AI models.</p>",[{'name': 'Susie Harrison'}],"Tue, 19 Mar 2024 17:00:00 GMT"
861,UK's Post Office Scandal: An Ethics Lesson for Tech Adoption,https://aibusiness.com/verticals/uk-s-post-office-scandal-ethics-lesson-for-tech-adoption,An IT expert involved in the case imparts advice on auditing your company's software,[{'name': 'Ben Wodecki'}],"Thu, 07 Mar 2024 21:32:10 GMT"
862,Gen AI Advice from a Google Cloud Technical Director,https://aibusiness.com/verticals/gen-ai-advice-from-a-google-cloud-technical-director,"John Abel imparts advice on tackling generative AI projects, at Tech Show London",[{'name': 'Ben Wodecki'}],"Thu, 07 Mar 2024 19:20:01 GMT"
863,OpenAI’s GPT-4 is the Most Infringing AI Model - Study,https://aibusiness.com/responsible-ai/openai-s-gpt-4-is-the-most-infringing-ai-model-new-research-shows,"GPT-4 produced ‘exact reproductions’ of book content, on average, in 44% of carefully crafted prompts",[{'name': 'Ben Wodecki'}],"Thu, 07 Mar 2024 18:41:02 GMT"
864,The Leader’s Strategic Mindset for AI Success,https://aibusiness.com/verticals/case-study-the-leader-s-strategic-mindset-for-ai-success,"Leaders tend to rely on simplified strategic mindsets shaped by past experiences when facing new  market disruptions, like by AI. Here's how to change it.","[{'name': 'Pernille Ryden, Torsten Ringberg, Omar A. Sawy'}]","Thu, 07 Mar 2024 00:10:11 GMT"
865,BofE Data Chief: Invest in Upskilling Non-Tech Staff in AI,https://aibusiness.com/responsible-ai/bofe-data-chief-invest-in-upskilling-non-tech-staff-in-ai,"The Bank of England’s data board chair urges upskilling for all employees, especially those over 40",[{'name': 'Ben Wodecki'}],"Wed, 06 Mar 2024 20:38:29 GMT"
866,‘We Definitely Messed Up’: Google Co-founder on Gemini Image Fails,https://aibusiness.com/nlp/-we-definitely-messed-up-google-co-founder-on-gemini-image-fails,Sergey Brin said he does not know why it &quot;leans left in many cases.&quot;,[{'name': 'Ben Wodecki'}],"Wed, 06 Mar 2024 19:14:28 GMT"
867,OpenAI to Musk: You're Upset We Succeeded Without You,https://aibusiness.com/responsible-ai/openai-claims-musk-supported-for-profit-pivot-in-lawsuit-rebuttal,"OpenAI responds to Elon Musk's lawsuit, which claims breach of the startup's founding pledge to be a nonprofit.","[{'name': 'Ben Wodecki, Deborah Yao'}]","Wed, 06 Mar 2024 17:39:16 GMT"
868,Hugging Face Launches New Code Generation Models,https://aibusiness.com/ml/hugging-face-launches-new-starcoder-code-generation-models,Nvidia was brought in to help train these compact yet powerful new coding models,[{'name': 'Ben Wodecki'}],"Tue, 05 Mar 2024 17:45:50 GMT"
869,"Elon Musk on the Future of AI, Self-Driving Cars at Bosch Connected World",https://aibusiness.com/verticals/elon-musk-on-the-future-of-ai-self-driving-cars-at-bosch-connected-world,"In a video Q&amp;A, Musk spoke about the need to ramp up sustainable power generation to meet the rising demands of AI and autonomous vehicles",[{'name': 'Scarlett Evans'}],"Tue, 05 Mar 2024 16:46:12 GMT"
870,This Virus Steals Your Data from Generative AI Tools,https://aibusiness.com/nlp/this-virus-steals-your-data-from-generative-ai-tools,Morris II covertly extracts data from AI tools - malware for generative AI,[{'name': 'Ben Wodecki'}],"Tue, 05 Mar 2024 16:32:46 GMT"
871,AI Startup Roundup: The Startup that Blew Away OpenAI,https://aibusiness.com/verticals/ai-startup-roundup-the-startup-backed-by-openai-jeff-bezos-microsoft,Also - An enterprise AI assistant startup and Israeli company using AI for underground mapping,[{'name': 'Deborah Yao'}],"Mon, 04 Mar 2024 21:52:19 GMT"
872,Anthropic Unveils Business Friendly Claude 3 AI Models,https://aibusiness.com/nlp/anthropic-s-claude-3-models-focus-on-enterprise-needs,"The OpenAI rival's Claude 3 multimodal models are more capable, accurate and offer competitive pricing",[{'name': 'Deborah Yao'}],"Mon, 04 Mar 2024 19:49:51 GMT"
873,Connected Tech Woos Crowds at Mobile World Congress,https://aibusiness.com/verticals/connected-tech-woos-crowds-at-mobile-world-congress,"What the mobile ecosystem predicts consumers will wear, carry and ride in 2024 and beyond",[{'name': 'Berenice Baker'}],"Mon, 04 Mar 2024 17:11:18 GMT"
874,Human-in-the-Loop: Mission Critical for AI Usage and Evaluation,https://aibusiness.com/ml/human-in-the-loop-mission-critical-for-ai-usage-and-evaluation,An opinion piece by the vice president of advanced analytics at SAS,[{'name': 'Udo Sglavo'}],"Fri, 01 Mar 2024 22:45:21 GMT"
875,AI News Roundup: SEC Probing OpenAI Over CEO Firing,https://aibusiness.com/nlp/ai-news-roundup-sec-probing-openai-over-ceo-firing,Also - Stack Overflow is getting an integration with Google Gemini,[{'name': 'Ben Wodecki'}],"Fri, 01 Mar 2024 21:26:58 GMT"
876,Elon Musk Sues OpenAI to Save Humanity,https://aibusiness.com/responsible-ai/elon-musk-sues-openai-to-save-humanity,"It is a peculiar lawsuit in which the beneficiary is all of society, not Musk himself","[{'name': 'Ben Wodecki, Deborah Yao'}]","Fri, 01 Mar 2024 20:19:26 GMT"
877,Unlocking the Power of Large Graphical Models (LGMs) for Forecasting Excellence,https://aibusiness.com/responsible-ai/solving-the-enterprise-generative-ai-data-problem,"<p>According to IBM, at least 2.5 quintillion bytes of data are generated every single day, representing an explosion of real-time data generation. In this environment, organizations that can put Generative AI to work at scale will emerge as leaders. While there are a host of text- and voice-based AI applications focused on business processes like customer support and service, LLMs are not sufficient for making sense of transactional time-series data for forecasting and resource optimization.</p>
<p>Join Ikigai Labs for an exclusive webinar where we'll delve into the transformative potential of Large Graphical Models (LGMs) for consolidating enterprise time-series data, providing predictive insights, and planning at any scale or frequency. Kamal Ahluwalia, President of Ikigai Labs, and co-author of &quot;What's Next for You&quot; and &quot;Deep Talent,&quot; will share his extensive experience in leading hyper-growth organizations and scaling businesses with AI, and AI-first strategies for success.</p>",[{}],"Fri, 01 Mar 2024 19:00:00 GMT"
878,"This Week's Most Read: Gemini's Image Flaws, Protecting Your Job from AI",https://aibusiness.com/nlp/this-week-s-most-read-gemini-s-image-flaws-protecting-your-job-from-ai,"Also, updates from Mobile World Congress 2024 including Lenovo’s transparent display laptop",[{'name': 'Ben Wodecki'}],"Thu, 29 Feb 2024 23:40:04 GMT"
879,Google DeepMind's Genie Makes Super Mario-like Games from Images,https://aibusiness.com/nlp/google-deepmind-s-genie-making-video-games-from-images,Genie transforms images into interactive Super Mario-like games. The science behind it could be a stepping stone to AGI.,[{}],"Thu, 29 Feb 2024 23:00:00 GMT"
880,Inside Sora: OpenAI’s Amazing Video Generation Model,https://aibusiness.com/nlp/inside-sora-openai-s-jaw-dropping-video-generation-model,How is Sora able to produce cinematic-quality videos? Here is how it works,[{'name': 'Ben Wodecki'}],"Thu, 29 Feb 2024 22:36:56 GMT"
881,Google CEO: Gemini’s Racial Gaff is ‘Completely Unacceptable’,https://aibusiness.com/responsible-ai/google-ceo-gemini-s-racial-gaff-is-completely-unacceptable-,Google also explains exactly what went wrong with Gemini,[{'name': 'Ben Wodecki'}],"Thu, 29 Feb 2024 19:04:59 GMT"
882,Disrupting Ethically With AI: EY Thought Leadership - MWC 2024,https://aibusiness.com/responsible-ai/disrupting-ethically-with-ai-ey-thought-leadership-mwc-2024,"Unlike prior technologies, which are process-driven, AI is objective-driven, making ethical considerations paramount",[{'name': 'Berenice Baker'}],"Thu, 29 Feb 2024 18:01:57 GMT"
883,"Bosch, Microsoft to Use Generative AI to Make Roads Safer, Bosch Connected World 2024",https://aibusiness.com/verticals/bosch-microsoft-to-use-generative-ai-to-make-roads-safer-bosch-connected-world-2024,The collaboration aims to improve automated driving functions using generative AI,[{'name': 'Liz Hughes'}],"Thu, 29 Feb 2024 17:28:41 GMT"
884,CDW UK’s Chief Technologist on Emerging Themes in Gen AI,https://aibusiness.com/nlp/cdw-uk-s-chief-technologist-on-emerging-themes-in-gen-ai,"Tim Russell joins the AI Business Podcast to discuss generative AI security, FOMO and confusion around the tech",[{'name': 'Tom Taulli'}],"Wed, 28 Feb 2024 22:45:00 GMT"
885,Apple Puts Brakes on EV Plans to Focus on Gen AI,https://aibusiness.com/nlp/apple-puts-brakes-on-ev-plans-to-focus-on-gen-ai,"Apple is pivoting to generative AI from electric vehicles, closing down one of the company's most ambitious projects",[{'name': 'Deborah Yao'}],"Wed, 28 Feb 2024 22:09:45 GMT"
886,Microsoft Vows Fair AI Practices Amid Antitrust Scrutiny – MWC 2024,https://aibusiness.com/responsible-ai/microsoft-vows-fair-ai-practices-amid-antitrust-scrutiny-mwc-2024,Microsoft unveils its 'AI Access Principles' to show it is playing fair in the AI market. A key pledge: making it easy to switch cloud providers,[{'name': 'Ben Wodecki'}],"Wed, 28 Feb 2024 20:53:41 GMT"
887,Generative AI Journeys with CDW UK's Chief Technologist,https://aibusiness.com/nlp/generative-ai-journeys-with-cdw-uk-s-chief-technologist,,[{}],"Wed, 28 Feb 2024 19:52:41 GMT"
888,This AI Tool Aids Enterprise Workflows by Analyzing Images,https://aibusiness.com/nlp/this-ai-tool-streamlines-workflows-by-analyzing-images-for-enterprises,"Palmyra-Vision from AI startup Writer can check an ad to see if it is compliant with regulations, among other use cases",[{'name': 'Ben Wodecki'}],"Wed, 28 Feb 2024 17:34:26 GMT"
889,OpenAI: NY Times 'Hacked' ChatGPT So It Could Sue Us,https://aibusiness.com/responsible-ai/openai-ny-times-hacked-chatgpt-for-its-copyright-lawsuit,OpenAI accuses The New York Times of manipulating ChatGPT to fabricate lawsuit evidence. The paper is suing for alleged copyright infringement.,[{'name': 'Ben Wodecki'}],"Wed, 28 Feb 2024 16:51:18 GMT"
890,Lenovo's Transparent Display Laptop and AI PCs – MWC 2024,https://aibusiness.com/verticals/lenovo-debuts-transparent-display-laptop-mwc-2024,The PC giant showcased its AI-infused products at Mobile World Congress 2024,[{'name': 'Deborah Yao'}],"Tue, 27 Feb 2024 22:53:45 GMT"
891,"HPE, Juniper to Deliver Modern AI Architecture – MWC 2024",https://aibusiness.com/ml/hpe-juniper-m-a-to-deliver-modern-ai-architecture-mwc-2024,The AI-driven architecture would encompass the cloud all the way to the edge,[{'name': 'Ben Wodecki'}],"Tue, 27 Feb 2024 21:11:47 GMT"
892,Telco Giants Embrace AI to Power New Revenue Streams – MWC 2024,https://aibusiness.com/ml/telco-giants-embrace-ai-to-power-new-revenue-streams-mwc-2024,Telco leaders explore AI's impact: from monetizing networks to improving service reliability,[{'name': 'Ben Wodecki'}],"Tue, 27 Feb 2024 20:03:04 GMT"
893,Huawei Builds an AI Model for Telcos – MWC 2024,https://aibusiness.com/verticals/huawei-builds-an-ai-model-for-telcos-mwc-2024,The Telecom Foundation Model will power sector-specific applications,[{'name': 'Ben Wodecki'}],"Tue, 27 Feb 2024 19:33:02 GMT"
894,Experts Spotlight Industrial Metaverse Use Cases - MWC 2024,https://aibusiness.com/verticals/experts-spotlight-industrial-metaverse-use-cases-mwc-2024,Impacting real-world outcomes through virtual representation requires a full-stack approach,[{'name': 'Berenice Baker'}],"Tue, 27 Feb 2024 18:57:25 GMT"
895,"Mistral Goes Large: Microsoft Deal, New Flagship Model",https://aibusiness.com/nlp/mistral-unveils-its-most-advanced-llm-and-chatgpt-rival,"Mistral also unveiled an AI chatbot called 'Le Chat,' meaning cat in French and a play on the English word 'chat'",[{'name': 'Ben Wodecki'}],"Tue, 27 Feb 2024 18:30:15 GMT"
896,AI Startup Roundup: Chinese LLM Startup Raises Over $1 Billion,https://aibusiness.com/nlp/ai-startup-roundup-chinese-llm-startup-raises-1-billion,Also - Capital raise for the startup behind Langchain,"[{'name': 'Ben Wodecki, Deborah Yao'}]","Mon, 26 Feb 2024 22:03:34 GMT"
897,"Google DeepMind CEO on AGI, OpenAI and Beyond – MWC 2024",https://aibusiness.com/nlp/google-deepmind-ceo-on-agi-openai-and-beyond-mwc-2024,Google DeepMind CEO Demis Hassabis also gave a glimpse into how Google lost ground to OpenAI in the AI race,"[{'name': 'Ben Wodecki, Deborah Yao'}]","Mon, 26 Feb 2024 20:56:07 GMT"
898,AI-powered Technique Could Help Quantum Computers Scale,https://aibusiness.com/verticals/ai-powered-technique-could-help-quantum-computers-scale,A collaboration between DeepMind and Quantinuum leads to a quantum computing optimization breakthrough,[{'name': 'Ben Wodecki'}],"Mon, 26 Feb 2024 18:56:00 GMT"
899,How Telcos are Using Generative AI - MWC 2024,https://aibusiness.com/nlp/how-telcos-are-using-generative-ai,Verizon and Telia executives discuss how they incorporate Gen AI into their businesses,[{'name': 'Ben Wodecki'}],"Mon, 26 Feb 2024 17:54:44 GMT"
900,Google DeepMind CEO Defends Gemini's Image Flaws - MWC 2024,https://aibusiness.com/responsible-ai/google-deepmind-ceo-defends-gemini-s-well-intended-image-flaws,DeepMind CEO Demis Hassabis said the problem should be fixed in the next couple of weeks,[{'name': 'Ben Wodecki'}],"Mon, 26 Feb 2024 17:23:42 GMT"
901,Deloitte: How to Prevent AI from Taking Your Job,https://aibusiness.com/responsible-ai/deloitte-how-to-prevent-ai-from-taking-your-job,"An interview with David Mallon, managing director of Deloitte Consulting",[{'name': 'Deborah Yao'}],"Mon, 26 Feb 2024 13:00:00 GMT"
902,Google to Train AI Models on Reddit Posts. What Could Go Wrong?,https://aibusiness.com/nlp/your-reddit-posts-could-be-used-to-train-google-s-ai-models,Posts are infamously candid and can be offensive. Redditors think Google will have to train its models more carefully,[{'name': 'Ben Wodecki'}],"Fri, 23 Feb 2024 20:47:40 GMT"
903,AI News Roundup: Mistral’s Open Source AI Models Heading to AWS,https://aibusiness.com/nlp/ai-news-roundup-mistral-s-open-source-ai-models-heading-to-aws,Also – Adobe unveils AI assistant and Stability AI sells Clipdrop to Jasper,"[{'name': 'Ben Wodecki, Deborah Yao'}]","Fri, 23 Feb 2024 20:05:21 GMT"
904,AI in Crime-Fighting: DOJ Launches Justice AI,https://aibusiness.com/responsible-ai/ai-in-crime-fighting-doj-launches-justice-ai,"The U.S. Department of Justice's new initiative, Justice AI, aims to modernize crime-fighting while adhering to ethical standards.",[{'name': 'Ben Wodecki'}],"Fri, 23 Feb 2024 18:55:26 GMT"
905,Microsoft's Free AI Security Tester for Gen AI Models,https://aibusiness.com/nlp/microsoft-offers-free-use-of-ai-security-tool-for-language-models,"Microsoft's PyRIT generates malicious prompts to stress-test models, reducing testing time from weeks to hours",[{'name': 'Ben Wodecki'}],"Fri, 23 Feb 2024 18:01:39 GMT"
906,"Stable Diffusion 3: More Realistic, Better Speller",https://aibusiness.com/nlp/stable-diffusion-3-more-realistic-better-speller,"The latest version of the popular text-to-image model uses a new architecture, leading to improved performance",[{'name': 'Ben Wodecki'}],"Fri, 23 Feb 2024 17:07:48 GMT"
907,This Week's Most Read: Can Quantum Help AI Understand?,https://aibusiness.com/nlp/this-week-s-most-read-can-quantum-help-ai-understand-,"Also, Air Canada pays up for its chatbot’s mistake and Hugging Face’s new AI assistant builder",[{'name': 'Ben Wodecki'}],"Thu, 22 Feb 2024 23:46:06 GMT"
908,The Rising Importance of Orchestration in Gen AI Development,https://aibusiness.com/nlp/the-rising-importance-of-orchestration-in-generative-ai-development,"As generative AI applications get more complex, there is a need for tools to help with orchestration",[{'name': 'Tom Taulli'}],"Thu, 22 Feb 2024 23:00:00 GMT"
909,Google engineer stole AI tech for Chinese firms,https://www.artificialintelligence-news.com/2024/03/07/google-engineer-stole-ai-tech-for-chinese-firms/,"<p>A former Google engineer has been charged with stealing trade secrets related to the company&#8217;s AI technology and secretly working with two Chinese firms. Linwei Ding, a 38-year-old Chinese national, was arrested on Wednesday in Newark, California, and faces four counts of federal trade secret theft, each punishable by up to 10 years in prison.<a class=\"excerpt-read-more\" href=\"https://www.artificialintelligence-news.com/2024/03/07/google-engineer-stole-ai-tech-for-chinese-firms/\" title=\"ReadGoogle engineer stole AI tech for Chinese firms\">... Read more &#187;</a></p>
<p>The post <a href=\"https://www.artificialintelligence-news.com/2024/03/07/google-engineer-stole-ai-tech-for-chinese-firms/\">Google engineer stole AI tech for Chinese firms</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",[{'name': 'Ryan Daws'}],"Thu, 07 Mar 2024 17:04:05 +0000"
910,Pace of innovation in AI is fierce – but is ethics able to keep up?,https://www.artificialintelligence-news.com/2024/03/07/pace-of-innovation-in-ai-is-fierce-but-is-ethics-able-to-keep-up/,"<p>If a week is traditionally a long time in politics, it is a yawning chasm when it comes to AI. The pace of innovation from the leading providers is one thing; the ferocity of innovation as competition hots up is quite another. But are the ethical implications of AI technology being left behind by this<a class=\"excerpt-read-more\" href=\"https://www.artificialintelligence-news.com/2024/03/07/pace-of-innovation-in-ai-is-fierce-but-is-ethics-able-to-keep-up/\" title=\"ReadPace of innovation in AI is fierce – but is ethics able to keep up?\">... Read more &#187;</a></p>
<p>The post <a href=\"https://www.artificialintelligence-news.com/2024/03/07/pace-of-innovation-in-ai-is-fierce-but-is-ethics-able-to-keep-up/\">Pace of innovation in AI is fierce – but is ethics able to keep up?</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",[{'name': 'James Bourne'}],"Thu, 07 Mar 2024 09:41:57 +0000"
911,OpenAI: Musk wanted us to merge with Tesla or take ‘full control’,https://www.artificialintelligence-news.com/2024/03/06/openai-musk-wanted-merge-tesla-or-take-full-control/,"<p>Elon Musk, the billionaire CEO of Tesla and SpaceX, allegedly wanted the AI research company OpenAI to either merge with Tesla or give him full control of the organisation. A blog post from OpenAI, in response to a lawsuit filed by Musk against the company, revealed email communications from 2015 to 2018 when Musk was<a class=\"excerpt-read-more\" href=\"https://www.artificialintelligence-news.com/2024/03/06/openai-musk-wanted-merge-tesla-or-take-full-control/\" title=\"ReadOpenAI: Musk wanted us to merge with Tesla or take &#8216;full control&#8217;\">... Read more &#187;</a></p>
<p>The post <a href=\"https://www.artificialintelligence-news.com/2024/03/06/openai-musk-wanted-merge-tesla-or-take-full-control/\">OpenAI: Musk wanted us to merge with Tesla or take &#8216;full control&#8217;</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",[{'name': 'Ryan Daws'}],"Wed, 06 Mar 2024 12:52:15 +0000"
912,Anthropic’s latest AI model beats rivals and achieves industry first,https://www.artificialintelligence-news.com/2024/03/05/anthropic-latest-ai-model-beats-rivals-achieves-industry-first/,"<p>Anthropic’s latest cutting-edge language model, Claude 3, has surged ahead of competitors like ChatGPT and Google&#8217;s Gemini to set new industry standards in performance and capability. According to Anthropic, Claude 3 has not only surpassed its predecessors but has also achieved &#8220;near-human&#8221; proficiency in various tasks. The company attributes this success to rigorous testing and<a class=\"excerpt-read-more\" href=\"https://www.artificialintelligence-news.com/2024/03/05/anthropic-latest-ai-model-beats-rivals-achieves-industry-first/\" title=\"ReadAnthropic&#8217;s latest AI model beats rivals and achieves industry first\">... Read more &#187;</a></p>
<p>The post <a href=\"https://www.artificialintelligence-news.com/2024/03/05/anthropic-latest-ai-model-beats-rivals-achieves-industry-first/\">Anthropic&#8217;s latest AI model beats rivals and achieves industry first</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",[{'name': 'Ryan Daws'}],"Tue, 05 Mar 2024 11:52:32 +0000"
913,AIs in India will need government permission before launching,https://www.artificialintelligence-news.com/2024/03/04/ai-india-need-government-permission-before-launching/,"<p>In an advisory issued by India’s Ministry of Electronics and Information Technology (MeitY) last Friday, it was declared that any AI technology still in development must acquire explicit government permission before being released to the public. Developers will also only be able to deploy these technologies after labelling the potential fallibility or unreliability of the<a class=\"excerpt-read-more\" href=\"https://www.artificialintelligence-news.com/2024/03/04/ai-india-need-government-permission-before-launching/\" title=\"ReadAIs in India will need government permission before launching\">... Read more &#187;</a></p>
<p>The post <a href=\"https://www.artificialintelligence-news.com/2024/03/04/ai-india-need-government-permission-before-launching/\">AIs in India will need government permission before launching</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",[{'name': 'Ryan Daws'}],"Mon, 04 Mar 2024 17:03:13 +0000"
914,Elon Musk sues OpenAI over alleged breach of nonprofit agreement,https://www.artificialintelligence-news.com/2024/03/01/elon-musk-sues-openai-alleged-breach-nonprofit-agreement/,"<p>Elon Musk has filed a lawsuit against OpenAI and its CEO, Sam Altman, citing a violation of their nonprofit agreement. The legal battle, unfolding in the Superior Court of California for the County of San Francisco, revolves around OpenAI&#8217;s departure from its foundational mission of advancing open-source artificial general intelligence (AGI) for the betterment of<a class=\"excerpt-read-more\" href=\"https://www.artificialintelligence-news.com/2024/03/01/elon-musk-sues-openai-alleged-breach-nonprofit-agreement/\" title=\"ReadElon Musk sues OpenAI over alleged breach of nonprofit agreement\">... Read more &#187;</a></p>
<p>The post <a href=\"https://www.artificialintelligence-news.com/2024/03/01/elon-musk-sues-openai-alleged-breach-nonprofit-agreement/\">Elon Musk sues OpenAI over alleged breach of nonprofit agreement</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",[{'name': 'Ryan Daws'}],"Fri, 01 Mar 2024 13:09:25 +0000"
915,UK and France to collaborate on AI following Horizon membership,https://www.artificialintelligence-news.com/2024/02/29/uk-and-france-collaborate-ai-following-horizon-membership/,"<p>The UK and France have announced new funding initiatives and partnerships aimed at advancing global AI safety. The developments come in the wake of the UK&#8217;s association with Horizon Europe, a move that was broadly seen as putting the divisions of Brexit in the past and the repairing of relations for the good of the<a class=\"excerpt-read-more\" href=\"https://www.artificialintelligence-news.com/2024/02/29/uk-and-france-collaborate-ai-following-horizon-membership/\" title=\"ReadUK and France to collaborate on AI following Horizon membership\">... Read more &#187;</a></p>
<p>The post <a href=\"https://www.artificialintelligence-news.com/2024/02/29/uk-and-france-collaborate-ai-following-horizon-membership/\">UK and France to collaborate on AI following Horizon membership</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",[{'name': 'Ryan Daws'}],"Thu, 29 Feb 2024 10:07:19 +0000"
916,Mistral AI unveils LLM rivalling major players,https://www.artificialintelligence-news.com/2024/02/27/mistral-ai-unveils-llm-rivalling-major-players/,"<p>Mistral AI, a France-based startup, has introduced a new large language model (LLM) called Mistral Large that it claims can compete with several top AI systems on the market.   Mistral AI stated that Mistral Large outscored most major LLMs except for OpenAI&#8217;s recently launched GPT-4 in tests of language understanding. It also performed strongly in<a class=\"excerpt-read-more\" href=\"https://www.artificialintelligence-news.com/2024/02/27/mistral-ai-unveils-llm-rivalling-major-players/\" title=\"ReadMistral AI unveils LLM rivalling major players\">... Read more &#187;</a></p>
<p>The post <a href=\"https://www.artificialintelligence-news.com/2024/02/27/mistral-ai-unveils-llm-rivalling-major-players/\">Mistral AI unveils LLM rivalling major players</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",[{'name': 'Ryan Daws'}],"Tue, 27 Feb 2024 12:59:49 +0000"
917,UK Home Secretary sounds alarm over deepfakes ahead of elections,https://www.artificialintelligence-news.com/2024/02/26/uk-home-secretary-alarm-deepfakes-ahead-elections/,"<p>Criminals and hostile state actors could hijack Britain&#8217;s democratic process by deploying AI-generated &#8220;deepfakes&#8221; to mislead voters, UK Home Secretary James Cleverly cautioned in remarks ahead of meetings with major tech companies.  Speaking to The Times, Cleverly emphasised the rapid advancement of AI technology and its potential to undermine elections not just in the UK<a class=\"excerpt-read-more\" href=\"https://www.artificialintelligence-news.com/2024/02/26/uk-home-secretary-alarm-deepfakes-ahead-elections/\" title=\"ReadUK Home Secretary sounds alarm over deepfakes ahead of elections\">... Read more &#187;</a></p>
<p>The post <a href=\"https://www.artificialintelligence-news.com/2024/02/26/uk-home-secretary-alarm-deepfakes-ahead-elections/\">UK Home Secretary sounds alarm over deepfakes ahead of elections</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",[{'name': 'Ryan Daws'}],"Mon, 26 Feb 2024 16:46:48 +0000"
918,Stability AI previews Stable Diffusion 3 text-to-image model,https://www.artificialintelligence-news.com/2024/02/23/stability-ai-previews-stable-diffusion-3-text-to-image-model/,"<p>London-based AI lab Stability AI has announced an early preview of its new text-to-image model, Stable Diffusion 3. The advanced generative AI model aims to create high-quality images from text prompts with improved performance across several key areas. The announcement comes just days after Stability AI’s largest rival, OpenAI, unveiled Sora—a brand new AI model<a class=\"excerpt-read-more\" href=\"https://www.artificialintelligence-news.com/2024/02/23/stability-ai-previews-stable-diffusion-3-text-to-image-model/\" title=\"ReadStability AI previews Stable Diffusion 3 text-to-image model\">... Read more &#187;</a></p>
<p>The post <a href=\"https://www.artificialintelligence-news.com/2024/02/23/stability-ai-previews-stable-diffusion-3-text-to-image-model/\">Stability AI previews Stable Diffusion 3 text-to-image model</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>",[{'name': 'Ryan Daws'}],"Fri, 23 Feb 2024 16:49:01 +0000"
919,The Fear That Inspired Elon Musk and Sam Altman to Create OpenAI,https://www.wired.com/story/the-fear-that-inspired-elon-musk-sam-altman-openai/,"Emails released by OpenAI to refute a lawsuit from Elon Musk show how he, Sam Altman, and other cofounders were motivated by fears of Google's dominance.",[{'name': 'Will Knight'}],"Thu, 07 Mar 2024 17:00:00 +0000"
920,Google’s GenAI Bots Are Struggling. But so Are Its Humans,https://www.wired.com/story/gadget-lab-podcast-635/,"This week, we talk about the rocky rollout of Google’s Gemini image generator, and some internal tensions recently reported by company employees.","[{'name': 'Michael Calore, Lauren Goode'}]","Thu, 07 Mar 2024 13:00:00 +0000"
921,"Google Used a Black, Deaf Worker to Tout Its Diversity. Now She’s Suing for Discrimination",https://www.wired.com/story/google-black-deaf-worker-diversity-suing-discrimination/,Jalon Hall was featured on Google’s corporate social media accounts “for making #LifeAtGoogle more inclusive!” She says the company discriminated against her on the basis of her disability and race.,[{'name': 'Paresh Dave'}],"Thu, 07 Mar 2024 11:00:00 +0000"
922,"5 Years After San Francisco Banned Face Recognition, Voters Ask for More Surveillance",https://www.wired.com/story/san-francisco-banned-face-recognition-voters-ask-for-more-surveillance/,"On Tuesday the country’s techiest city backed a ballot proposition that tapped into concerns about crime, giving the police more freedom to use drones and other surveillance technology.","[{'name': 'Lauren Goode, Tom Simonite'}]","Wed, 06 Mar 2024 23:05:19 +0000"
923,AI Tools Are Still Generating Misleading Election Images,https://www.wired.com/story/genai-images-election-fraud/,"Though AI companies said they put some guardrails in place, researchers were able to easily create images related to claims of election fraud.",[{'name': 'Vittoria Elliott'}],"Wed, 06 Mar 2024 14:33:50 +0000"
924,The Dark Side of Open Source AI Image Generators,https://www.wired.com/story/dark-side-open-source-ai-image-generators/,Open source tools allow anyone to make AI art. They have also been used to produce nonconsensual deepfake porn.,[{'name': 'Lydia Morrish'}],"Wed, 06 Mar 2024 12:00:00 +0000"
925,Google Is Finally Trying to Kill AI Clickbait,https://www.wired.com/story/google-search-artificial-intelligence-clickbait-spam-crackdown/,"On Tuesday, Google announced changes to combat AI spam in search. An SEO expert says these new rules could “change everything.”",[{'name': 'Kate Knibbs'}],"Tue, 05 Mar 2024 21:16:52 +0000"
926,What Is OpenAI’s ChatGPT Plus? Here’s What You Should Know,https://www.wired.com/story/what-is-chatgpt-plus-gpt4-openai/,OpenAI’s subscription-only service costs $20 a month and includes access to the GPT-4 model. We signed up and tried it out.,[{'name': 'Reece Rogers'}],"Sun, 03 Mar 2024 12:00:00 +0000"
927,The Wild Claim at the Heart of Elon Musk’s OpenAI Lawsuit,https://www.wired.com/story/wild-claim-at-the-heart-of-elon-musks-openai-lawsuit/,Elon Musk’s lawsuit against OpenAI hinges on a dubious claim that the company has already developed ‘artificial general intelligence’—and handed it over to Microsoft.,[{'name': 'Will Knight'}],"Fri, 01 Mar 2024 22:50:40 +0000"
928,The Mind-Blowing Experience of a Chatbot That Answers Instantly,https://www.wired.com/story/plaintext-groq-mindblowing-chatbot-answers-instantly/,AI chips from startup Groq allow chatbots to answer queries almost instantly. That could open up whole new use cases for generative AI helpers.,[{'name': 'Steven Levy'}],"Fri, 01 Mar 2024 14:00:00 +0000"
929,AI can speed design of health software,https://www.sciencedaily.com/releases/2024/03/240306144729.htm,"Artificial intelligence helped clinicians to accelerate the design of diabetes prevention software, a new study finds.",[],"Wed, 06 Mar 2024 14:47:29 EST"
930,New dressing robot can 'mimic' the actions of care-workers,https://www.sciencedaily.com/releases/2024/03/240304135829.htm,Scientists have developed a new robot that can 'mimic' the two-handed movements of care-workers as they dress an individual.,[],"Mon, 04 Mar 2024 13:58:29 EST"
931,New AI model could streamline operations in a robotic warehouse,https://www.sciencedaily.com/releases/2024/02/240228115434.htm,Researchers applied deep-learning approaches from vehicle routing to streamline planning trajectories for robots in an e-commerce warehouse. Their method breaks the problem down into smaller chunks and then predicts the best chunks to solve with traditional algorithms.,[],"Wed, 28 Feb 2024 11:54:34 EST"
932,A novel method for easy and quick fabrication of biomimetic robots with life-like movement,https://www.sciencedaily.com/releases/2024/02/240226114557.htm,"Ultraviolet-laser processing is a promising technique for developing intricate microstructures, enabling complex alignment of muscle cells, required for building life-like biohybrid actuators. Compared to traditional complex methods, this innovative technique enables easy and quick fabrication of microstructures with intricate patterns for achieving different muscle cell arrangements, paving the way for biohybrid actuators capable of complex, flexible movements.",[],"Mon, 26 Feb 2024 11:45:57 EST"
933,Method identified to double computer processing speeds,https://www.sciencedaily.com/releases/2024/02/240221213907.htm,"Scientists introduce what they call 'simultaneous and heterogeneous multithreading' or SHMT. This system doubles computer processing speeds with existing hardware by simultaneously using graphics processing units (GPUs), hardware accelerators for artificial intelligence (AI) and machine learning (ML), or digital signal processing units to process information.",[],"Wed, 21 Feb 2024 21:39:07 EST"
934,How ancient sea creatures can inform soft robotics,https://www.sciencedaily.com/releases/2024/02/240212153511.htm,"Fossils of a marine animal that lived 500 million years ago, combined with computer simulations, informed the design of a new soft robot.",[],"Mon, 12 Feb 2024 15:35:11 EST"
935,Why insects navigate more efficiently than robots,https://www.sciencedaily.com/releases/2024/02/240212133153.htm,"Engineers have studied how insects navigate, for the purpose of developing energy-efficient robots.",[],"Mon, 12 Feb 2024 13:31:53 EST"
936,Innovations in depth from focus/defocus pave the way to more capable computer vision systems,https://www.sciencedaily.com/releases/2024/02/240209134440.htm,"In an image, estimating the distance between objects and the camera by using the blur in the images as clue, also known as depth from focus/defocus, is essential in computer vision. However, model-based methods fail when texture-less surfaces are present, and learning-based methods require the same camera settings during training and testing. Now, researchers have come up with an innovative strategy for depth estimation that combines the best of both the worlds to solve these limitations, extending the applicability of depth from focus/defocus.",[],"Fri, 09 Feb 2024 13:44:40 EST"
937,A sleeker facial recognition technology tested on Michelangelo's David,https://www.sciencedaily.com/releases/2024/02/240202132613.htm,"Many people are familiar with facial recognition systems that unlock smartphones and game systems or allow access to our bank accounts online. But the current technology can require boxy projectors and lenses. Now, researchers report on a sleeker 3D surface imaging system with flatter, simplified optics. In proof-of-concept demonstrations, the new system recognized the face of Michelangelo's David just as well as an existing smartphone system.",[],"Fri, 02 Feb 2024 13:26:13 EST"
938,Robot trained to read braille at twice the speed of humans,https://www.sciencedaily.com/releases/2024/01/240129122413.htm,Researchers have developed a robotic sensor that incorporates artificial intelligence techniques to read braille at speeds roughly double that of most human readers.,[],"Mon, 29 Jan 2024 12:24:13 EST"
939,Utilizing active microparticles for artificial intelligence,https://www.sciencedaily.com/releases/2024/01/240129122349.htm,Artificial intelligence using neural networks performs calculations digitally with the help of microelectronic chips. Physicists have now created a type of neural network that works not with electricity but with so-called active colloidal particles.The researchers describe how these microparticles can be used as a physical system for artificial intelligence and the prediction of time series.,[],"Mon, 29 Jan 2024 12:23:49 EST"
940,Scientists design a two-legged robot powered by muscle tissue,https://www.sciencedaily.com/releases/2024/01/240126140516.htm,"Compared to robots, human bodies are flexible, capable of fine movements, and can convert energy efficiently into movement. Drawing inspiration from human gait, researchers from Japan crafted a two-legged biohybrid robot by combining muscle tissues and artificial materials. This method allows the robot to walk and pivot.",[],"Fri, 26 Jan 2024 14:05:16 EST"
941,"Chats with AI shift attitudes on climate change, Black Lives Matter",https://www.sciencedaily.com/releases/2024/01/240126001915.htm,People who were more skeptical of human-caused climate change or the Black Lives Matter movement who took part in conversation with a popular AI chatbot were disappointed with the experience but left the conversation more supportive of the scientific consensus on climate change or BLM. This is according to researchers studying how these chatbots handle interactions from people with different cultural backgrounds.,[],"Fri, 26 Jan 2024 00:19:15 EST"
942,Autonomous synthesis robot uses AI to speed up chemical discovery,https://www.sciencedaily.com/releases/2024/01/240125145938.htm,"Chemists have developed an autonomous chemical synthesis robot with an integrated AI-driven machine learning unit. Dubbed 'RoboChem', the benchtop device can outperform a human chemist in terms of speed and accuracy while also displaying a high level of ingenuity. As the first of its kind, it could significantly accelerate chemical discovery of molecules for pharmaceutical and many other applications.",[],"Thu, 25 Jan 2024 14:59:38 EST"
943,"Mini-robots modeled on insects may be smallest, lightest, fastest ever developed",https://www.sciencedaily.com/releases/2024/01/240118122235.htm,"Two insect-like robots, a mini-bug and a water strider may be the smallest, lightest and fastest fully functional micro-robots ever known to be created. Such miniature robots could someday be used for work in areas such as artificial pollination, search and rescue, environmental monitoring, micro-fabrication or robotic-assisted surgery. Reporting on their work in the proceedings of the IEEE Robotics and Automation Society's International Conference on Intelligent Robots and Systems, the mini-bug weighs in at eight milligrams while the water strider weighs 55 milligrams. Both can move at about six millimeters a second.",[],"Thu, 18 Jan 2024 12:22:35 EST"
944,Study identifies new findings on implant positioning and stability during robotic-assisted knee revision surgery,https://www.sciencedaily.com/releases/2024/01/240117183753.htm,"An innovative study explores the use of robotic-assisted joint replacement in revision knee scenarios, comparing the pre- and post-revision implant positions in a series of revision total knee arthroplasties (TKA) using a state-of-the-art robotic arm system.",[],"Wed, 17 Jan 2024 18:37:53 EST"
945,'Smart glove' can boost hand mobility of stroke patients,https://www.sciencedaily.com/releases/2024/01/240116131818.htm,"This month, a group of stroke survivors in British Columbia will test a new technology designed to aid their recovery, and ultimately restore use of their limbs and hands. Participants will wear a new groundbreaking 'smart glove' capable of tracking their hand and finger movements during rehabilitation exercises.",[],"Tue, 16 Jan 2024 13:18:18 EST"
946,Artificial muscle device produces force 34 times its weight,https://www.sciencedaily.com/releases/2024/01/240111113200.htm,"Scientists developed a soft fluidic switch using an ionic polymer artificial muscle that runs with ultra-low power to lift objects 34 times greater than its weight. Its light weight and small size make it applicable to various industrial fields such as soft electronics, smart textiles, and biomedical devices by controlling fluid flow with high precision, even in narrow spaces.",[],"Thu, 11 Jan 2024 11:32:00 EST"
947,Researchers developing AI to make the internet more accessible,https://www.sciencedaily.com/releases/2024/01/240109170521.htm,"In an effort to make the internet more accessible for people with disabilities, researchers have begun developing an artificial intelligence agent that could complete complex tasks on any website using simple language commands.",[],"Tue, 09 Jan 2024 17:05:21 EST"
948,"New soft robots roll like tires, spin like tops and orbit like moons",https://www.sciencedaily.com/releases/2024/01/240108153135.htm,"Researchers have developed a new soft robot design that engages in three simultaneous behaviors: rolling forward, spinning like a record, and following a path that orbits around a central point. The device, which operates without human or computer control, holds promise for developing soft robotic devices that can be used to navigate and map unknown environments.",[],"Mon, 08 Jan 2024 15:31:35 EST"
949,AI alters middle managers' work,https://www.sciencedaily.com/releases/2023/12/231221012822.htm,"The introduction of artificial intelligence is a significant part of the digital transformation bringing challenges and changes to the job descriptions among management. A study shows that integrating artificial intelligence systems into service teams increases demands imposed on middle management in the financial services field. In that sector, the advent of artificial intelligence has been fast and AI applications can implement a large proportion of routine work that was previously done by people. Many professionals in the service sector work in teams which include both humans and artificial intelligence systems, which sets new expectations on interactions, human relations, and leadership.",[],"Thu, 21 Dec 2023 01:28:22 EST"
950,"Meet 'Coscientist,' your AI lab partner",https://www.sciencedaily.com/releases/2023/12/231221012729.htm,"An artificial intelligence-driven system has autonomously learned about certain Nobel Prize-winning chemical reactions and designed a successful laboratory procedure to make them. The AI did so in just a few minutes and correctly on its first attempt. According to the authors, this is the first time that a non-organic intelligence planned, designed and executed this complex reaction that was invented by humans.",[],"Thu, 21 Dec 2023 01:27:29 EST"
951,Artificial intelligence can predict events in people's lives,https://www.sciencedaily.com/releases/2023/12/231218125850.htm,"Artificial intelligence can analyze registry data on people's residence, education, income, health and working conditions and, with high accuracy, predict life events.",[],"Mon, 18 Dec 2023 12:58:50 EST"
952,AI chatbot shows potential as diagnostic partner,https://www.sciencedaily.com/releases/2023/12/231211114509.htm,Physician-investigators compared a chatbot's probabilistic reasoning to that of human clinicians. The findings suggest that artificial intelligence could serve as useful clinical decision support tools for physicians.,[],"Mon, 11 Dec 2023 11:45:09 EST"
953,Exposure to soft robots decreases human fears about working with them,https://www.sciencedaily.com/releases/2023/12/231205114751.htm,"Seeing robots made with soft, flexible parts in action appears to lower people's anxiety about working with them or even being replaced by them. A study found that watching videos of a soft robot working with a person at picking and placing tasks lowered the viewers' safety concerns and feelings of job insecurity. This was true even when the soft robot was shown working in close proximity to the person. This finding shows soft robots hold a potential psychological advantage over rigid robots made of metal or other hard materials.",[],"Tue, 05 Dec 2023 11:47:51 EST"
954,AI networks are more vulnerable to malicious attacks than previously thought,https://www.sciencedaily.com/releases/2023/12/231204135128.htm,"Artificial intelligence tools hold promise for applications ranging from autonomous vehicles to the interpretation of medical images. However, a new study finds these AI tools are more vulnerable than previously thought to targeted attacks that effectively force AI systems to make bad decisions.",[],"Mon, 04 Dec 2023 13:51:28 EST"
955,A color-based sensor to emulate skin's sensitivity,https://www.sciencedaily.com/releases/2023/12/231201173202.htm,"In a step toward more autonomous soft robots and wearable technologies, researchers have created a device that uses color to simultaneously sense multiple mechanical and temperature stimuli.",[],"Fri, 01 Dec 2023 17:32:02 EST"
956,Brainstorming with a bot,https://www.sciencedaily.com/releases/2023/12/231201123612.htm,"Electronic nanomaterials experts have imagined how recent advances in artificial intelligence (AI) and machine learning (ML) could aid scientific brainstorming and ideation. To accomplish this, they have developed a chatbot with knowledge in the kinds of science he's been engaged in.",[],"Fri, 01 Dec 2023 12:36:12 EST"
957,Scientists use A.I.-generated images to map visual functions in the brain,https://www.sciencedaily.com/releases/2023/11/231130145431.htm,Researchers have demonstrated the use of AI-selected natural images and AI-generated synthetic images as neuroscientific tools for probing the visual processing areas of the brain. The goal is to apply a data-driven approach to understand how vision is organized while potentially removing biases that may arise when looking at responses to a more limited set of researcher-selected images.,[],"Thu, 30 Nov 2023 14:54:31 EST"
958,2D material reshapes 3D electronics for AI hardware,https://www.sciencedaily.com/releases/2023/11/231130145423.htm,"Researchers demonstrated monolithic 3D integration of layered 2D material into novel processing hardware for artificial intelligence computing. The new approach provides a material-level solution for fully integrating many functions into a single, small electronic chip -- and paves the way for advanced AI computing.",[],"Thu, 30 Nov 2023 14:54:23 EST"
959,How do you make a robot smarter? Program it to know what it doesn't know,https://www.sciencedaily.com/releases/2023/11/231128172842.htm,Engineers have come up with a new way to teach robots to know when they don't know. The technique involves quantifying the fuzziness of human language and using that measurement to tell robots when to ask for further directions. Telling a robot to pick up a bowl from a table with only one bowl is fairly clear. But telling a robot to pick up a bowl when there are five bowls on the table generates a much higher degree of uncertainty -- and triggers the robot to ask for clarification.,[],"Tue, 28 Nov 2023 17:28:42 EST"
960,Researchers engineer a material that can perform different tasks depending on temperature,https://www.sciencedaily.com/releases/2023/11/231128132308.htm,Researchers report that they have developed a new composite material designed to change behaviors depending on temperature in order to perform specific tasks. These materials are poised to be part of the next generation of autonomous robotics that will interact with the environment.,[],"Tue, 28 Nov 2023 13:23:08 EST"
961,Defending your voice against deepfakes,https://www.sciencedaily.com/releases/2023/11/231127180707.htm,"Computer scientists have developed AntiFake, a tool to protect voice recordings from unauthorized speech synthesis.",[],"Mon, 27 Nov 2023 18:07:07 EST"
962,New method uses crowdsourced feedback to help train robots,https://www.sciencedaily.com/releases/2023/11/231127132237.htm,A new technique enables an AI agent to be guided by data crowdsourced asynchronously from nonexpert human users as it learns to complete a task through reinforcement learning. The method trains the robot faster and better than other approaches.,[],"Mon, 27 Nov 2023 13:22:37 EST"
963,How we play together,https://www.sciencedaily.com/releases/2023/11/231121175256.htm,Psychologists are using EEG to research what games reveal about our ability to cooperate.,[],"Tue, 21 Nov 2023 17:52:56 EST"
964,"AI can 'lie and BS' like its maker, but still not intelligent like humans",https://www.sciencedaily.com/releases/2023/11/231120170942.htm,"A researcher contends that the understanding of AI is muddled by linguistics: That while indeed intelligent, AI cannot be intelligent in the way that humans are, even though 'it can lie and BS like its maker.'",[],"Mon, 20 Nov 2023 17:09:42 EST"
965,Creativity in the age of generative AI: A new era of creative partnerships,https://www.sciencedaily.com/releases/2023/11/231120170939.htm,"Generative AI (e.g., ChatGPT) has propelled AI into the mainstream, raising concerns about job displacement and creative work. Experts now emphasize a need to focus on 'co-creativity,' the human-AI interaction instead. Extensive research is needed for comprehending co-creativity which is crucial for the future development of AI.",[],"Mon, 20 Nov 2023 17:09:39 EST"
966,AI system self-organizes to develop features of brains of complex organisms,https://www.sciencedaily.com/releases/2023/11/231120124246.htm,Scientists have shown that placing physical constraints on an artificially-intelligent system -- in much the same way that the human brain has to develop and operate within physical and biological constraints -- allows it to develop features of the brains of complex organisms in order to solve tasks.,[],"Mon, 20 Nov 2023 12:42:46 EST"
967,AI: Researchers develop automatic text recognition for ancient cuneiform tablets,https://www.sciencedaily.com/releases/2023/11/231120124148.htm,"A new artificial intelligence (AI) software is now able to decipher difficult-to-read texts on cuneiform tablets. Instead of photos, the AI system uses 3D models of the tablets, delivering significantly more reliable results than previous methods. This makes it possible to search through the contents of multiple tablets to compare them with each other. It also paves the way for entirely new research questions.",[],"Mon, 20 Nov 2023 12:41:48 EST"
968,The mind's eye of a neural network system,https://www.sciencedaily.com/releases/2023/11/231116141058.htm,"A new tool, based on topology, makes finding the areas where neural networks are confused as simple as spotting mountaintops from an airplane. The ability to spot and address those areas of confusion should enable more confident application of neural networks in high-stakes decision scenarios or image prediction tasks like healthcare and research.",[],"Thu, 16 Nov 2023 14:10:58 EST"
969,Realistic talking faces created from only an audio clip and a person's photo,https://www.sciencedaily.com/releases/2023/11/231116140827.htm,"A team of researchers has developed a computer program that creates realistic videos that reflect the facial expressions and head movements of the person speaking, only requiring an audio clip and a face photo.   DIverse yet Realistic Facial Animations, or DIRFA, is an artificial intelligence-based program that takes audio and a photo and produces a 3D video showing the person demonstrating realistic and consistent facial animations synchronised with the spoken audio (see videos).",[],"Thu, 16 Nov 2023 14:08:27 EST"
970,Use it or lose it: New robotic system assesses mobility after stroke,https://www.sciencedaily.com/releases/2023/11/231116140354.htm,"Stroke is a leading cause of long-term disability worldwide. Each year more than 15 million people worldwide have strokes, and three-quarters of stroke survivors will experience impairment, weakness and paralysis in their arms and hands. Many stroke survivors rely on their stronger arm to complete daily tasks, from carrying groceries to combing their hair, even when the weaker arm has the potential to improve.",[],"Thu, 16 Nov 2023 14:03:54 EST"
971,This 3D printer can watch itself fabricate objects,https://www.sciencedaily.com/releases/2023/11/231115113339.htm,"Engineers have developed a high-throughput, multimaterial 3D inkjet printer that uses computer vision to rapidly and automatically control the amount of material being deposited during the printing process in real time. This enables the use of a wide range of materials for fabrication.",[],"Wed, 15 Nov 2023 11:33:39 EST"
972,Individual back training machine developed,https://www.sciencedaily.com/releases/2023/11/231114143740.htm,"18 percent of reported sick leave relates to musculoskeletal ailments, in particular back-related disorders. The GyroTrainer is an intelligent training device that resembles a balance board. It uses artificial intelligence to adjust the difficulty level to the individual patient's current ability.",[],"Tue, 14 Nov 2023 14:37:40 EST"
973,Engineers are on a failure-finding mission,https://www.sciencedaily.com/releases/2023/11/231109121552.htm,Engineers developed a technique to quickly identify a range of potential failures in a system before they are deployed in the real world.,[],"Thu, 09 Nov 2023 12:15:52 EST"
974,How human faces can teach androids to smile,https://www.sciencedaily.com/releases/2023/11/231109121541.htm,"A research team used 125 physical markers to understand the detailed mechanics of 44 different human facial motions. The aim was to better understand how to convey emotions with artificial faces. Beyond helping with the design of robots and androids, this research can also benefit computer graphics, facial recognition, and medical diagnoses.",[],"Thu, 09 Nov 2023 12:15:41 EST"
975,Machine learning gives users 'superhuman' ability to open and control tools in virtual reality,https://www.sciencedaily.com/releases/2023/11/231107131923.htm,Researchers have developed a virtual reality application where a range of 3D modelling tools can be opened and controlled using just the movement of a user's hand.,[],"Tue, 07 Nov 2023 13:19:23 EST"
976,Neuromorphic computing will be great... if hardware can handle the workload,https://www.sciencedaily.com/releases/2023/11/231106202950.htm,Scientists believe they may have discovered a way to rework the hardware of AI. By mimicking the synapses of the human brain.,[],"Mon, 06 Nov 2023 20:29:50 EST"
977,450-million-year-old organism finds new life in Softbotics,https://www.sciencedaily.com/releases/2023/11/231106202936.htm,"Researchers have used fossil evidence to engineer a soft robotic replica of pleurocystitids, a marine organism that existed nearly 450 million years ago and is believed to be one of the first echinoderms capable of movement using a muscular stem.",[],"Mon, 06 Nov 2023 20:29:36 EST"
978,AI should be better understood and managed -- new research warns,https://www.sciencedaily.com/releases/2023/11/231102135100.htm,"Artificial Intelligence (AI) and algorithms can and are being used to radicalize, polarize, and spread racism and political instability, says an academic. An expert argues that AI and algorithms are not just tools deployed by national security agencies to prevent malicious activity online, but can be contributors to polarization, radicalism and political violence -- posing a threat to national security.",[],"Thu, 02 Nov 2023 13:51:00 EDT"
979,Nanowire 'brain' network learns and remembers 'on the fly',https://www.sciencedaily.com/releases/2023/11/231101134804.htm,"Like a collection of 'Pick Up Sticks', this neural network has passed a critical step for developing machine intelligence. For the first time, a physical neural network has successfully been shown to learn and remember 'on the fly', in a way inspired by and similar to how the brain's neurons work. The result opens a pathway for developing efficient and low-energy machine intelligence for more complex, real-world learning and memory tasks.",[],"Wed, 01 Nov 2023 13:48:04 EDT"
980,The brain may learn about the world the same way some computational models do,https://www.sciencedaily.com/releases/2023/10/231030194459.htm,"New studies support the idea that the brain uses a process similar to a machine-learning approach known as 'self-supervised learning.' This type of machine learning allows computational models to learn about visual scenes based solely on the similarities and differences between them, with no labels or other information.",[],"Mon, 30 Oct 2023 19:44:59 EDT"
981,Robot stand-in mimics movements in VR,https://www.sciencedaily.com/releases/2023/10/231027110752.htm,Researchers have developed a souped-up telepresence robot that responds automatically and in real-time to a remote user's movements and gestures made in virtual reality.,[],"Fri, 27 Oct 2023 11:07:52 EDT"
982,Engineers develop breakthrough 'robot skin',https://www.sciencedaily.com/releases/2023/10/231026131623.htm,"Smart, stretchable and highly sensitive, a new soft sensor opens the door to a wide range of applications in robotics and prosthetics. When applied to the surface of a prosthetic arm or a robotic limb, the sensor skin provides touch sensitivity and dexterity, enabling tasks that can be difficult for machines such as picking up a piece of soft fruit. The sensor is also soft to the touch, like human skin, which helps make human interactions safer and more lifelike.",[],"Thu, 26 Oct 2023 13:16:23 EDT"
983,Vision via sound for the blind,https://www.sciencedaily.com/releases/2023/10/231025223433.htm,"Smart glasses that use a technique similar to a bat's echolocation could help blind and low-vision people navigate their surroundings, according to researchers.",[],"Wed, 25 Oct 2023 22:34:33 EDT"
984,Can AI grasp related concepts after learning only one?,https://www.sciencedaily.com/releases/2023/10/231025163006.htm,"Researchers have now developed a technique that advances the ability of these tools, such as ChatGPT, to make compositional generalizations. This technique, Meta-learning for Compositionality, outperforms existing approaches and is on par with, and in some cases better than, human performance.",[],"Wed, 25 Oct 2023 16:30:06 EDT"
985,Plant-based materials give 'life' to tiny soft robots,https://www.sciencedaily.com/releases/2023/10/231023124412.htm,"A team of researchers has created smart, advanced materials that will be the building blocks for a future generation of soft medical microrobots. These tiny robots have the potential to conduct medical procedures, such as biopsy, and cell and tissue transport, in a minimally invasive fashion.",[],"Mon, 23 Oct 2023 12:44:12 EDT"
986,Adaptive optical neural network connects thousands of artificial neurons,https://www.sciencedaily.com/releases/2023/10/231023124404.htm,"Physicists working with computer specialists have developed a so-called event-based architecture, using photonic processors. In a similar way to the brain, this makes possible the continuous adaptation of the connections within the neural network.",[],"Mon, 23 Oct 2023 12:44:04 EDT"
987,New cyber algorithm shuts down malicious robotic attack,https://www.sciencedaily.com/releases/2023/10/231011202416.htm,"Researchers have designed an algorithm that can intercept a man-in-the-middle (MitM) cyberattack on an unmanned military robot and shut it down in seconds. The algorithm, tested in real time, achieved a 99% success rate.",[],"Wed, 11 Oct 2023 20:24:16 EDT"
988,AI language models could help diagnose schizophrenia,https://www.sciencedaily.com/releases/2023/10/231009191615.htm,"Scientists have developed new tools, based on AI language models, that can characterize subtle signatures in the speech of patients diagnosed with schizophrenia.",[],"Mon, 09 Oct 2023 19:16:15 EDT"
989,Andreessen Horowitz Closes In On Up To $7B In New Funds — Report,https://news.crunchbase.com/venture/andreessen-horowitz-vc-fundraise-ai-gaming/,"Andreessen Horowitz, known for early bets on then-startups such as Facebook, Instagram and Airbnb, is reportedly “weeks away” from closing on as much as $7 billion in new funds.",[{'name': 'Chris Metinko'}],"Thu, 07 Mar 2024 18:00:04 +0000"
990,Eye On AI: Oh The Humanity — Everybody’s In A Rush To Reassure AI Development Will Help Us,https://news.crunchbase.com/ai/musk-openai-lawsuit-altman-msft/,The big AI news this week is Elon Musk’s lawsuit against OpenAI and its executives — including co-founder and CEO Sam Altman — claiming breach of OpenAI’s founding agreement.,[{'name': 'Chris Metinko'}],"Thu, 07 Mar 2024 12:00:52 +0000"
991,Global VC Funding Settles Around $20B In February But AI Share Increases,https://news.crunchbase.com/venture/monthly-global-funding-recap-february-2024/,"Global venture capital funding reached $21.5 billion in February 2024 — flat month over month and slightly up from February 2023, Crunchbase data shows.",[{'name': 'Gené Teare'}],"Thu, 07 Mar 2024 12:00:11 +0000"
992,"Battery Ventures Sees AI’s Massive Impact On Tech, And It’s Here Already",https://news.crunchbase.com/ai/artificial-intelligence-tech-dharmesh-thakker-battery-ventures/,"We sat down recently with Battery Ventures general partner Dharmesh Thakker to talk about the AI technologies he’s most excited about, and where he sees them already creating billions of dollars in value.",[{'name': 'Gené Teare'}],"Wed, 06 Mar 2024 12:00:33 +0000"
993,5 Interesting Startup Deals You May Have Missed In February,https://news.crunchbase.com/startups/interesting-deals-healthcare-agtech-february-2024/,"Some may have missed a few of the more unique startups that raised fresh cash in the year’s second month. With that, let’s take a look back and recap some of the most intriguing raises of February.",[{'name': 'Chris Metinko'}],"Wed, 06 Mar 2024 12:00:04 +0000"
994,Perplexity Poised To Become Latest AI Startup To Hit Unicorn Status — Report,https://news.crunchbase.com/ai/startup-perplexity-poised-unicorn-status-nvda/,Perplexity AI is reportedly nearing completion of a funding round that will give the AI search startup a unicorn valuation of around $1 billion.,[{'name': 'Chris Metinko'}],"Tue, 05 Mar 2024 17:50:37 +0000"
995,Which Overlooked Startup Sectors Will Be Powerful Investments In 2024?,https://news.crunchbase.com/venture/overlooked-startup-sectors-2024-dukach-one-way/,"While investors debated whether to bet on image-generation AI or medtech AI last year, several extremely consequential startup sectors ended up being overshadowed. Which sectors will grab investors' attention and dollars in 2024? Semyon Dukach, founding partner of One Way Ventures, shares what he thinks.",[{'name': 'Guest Author'}],"Tue, 05 Mar 2024 12:00:31 +0000"
996,The 10 Biggest Rounds Of February: Epic Games Leads The List Thanks To House Of Mouse Investment,https://news.crunchbase.com/venture/biggest-rounds-february-2024-ai-biotech-unicorn-epic/,February was a big month for big rounds. Companies needed to raise $170 million or more just to make the tailend of our list of largest U.S. venture funding deals.,[{'name': 'Chris Metinko'}],"Tue, 05 Mar 2024 12:00:22 +0000"
997,"No One Gushes About Govtech, But It Can Produce Some Nice Returns",https://news.crunchbase.com/policy-regulation/government-tech-startups-funding-opengov-acquisition-valuation/,"Startups have been quietly making some major contributions to help us deal with government entities. From modernizing parking payments or streamlining permit applications, venture-backed software and app developers are behind an assortment of tools getting funded.",[{'name': 'Joanna Glasner'}],"Mon, 04 Mar 2024 12:00:42 +0000"
998,The AI Gold Rush: How Startups Can Stake Their Claim In A Competitive Frontier,https://news.crunchbase.com/ai/startups-venture-funding-competition-gozes-entree/,"To succeed in this new age of AI maturation, entrepreneurs need to focus on building moats, Entrée Capital's Adi Gozes writes, and offers tips on how to get started.",[{'name': 'Guest Author'}],"Mon, 04 Mar 2024 12:00:05 +0000"
999,Veritas: Delivering Real-World Data through Datavant on Databricks,https://www.databricks.com/blog/veritas-delivering-real-world-data-through-datavant-databricks,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">This post was written in collaboration with Jason Labonte, Chief Executive Officer, Veritas Data Research In the realm of healthcare and life sciences...</div>",[],"Thu, 07 Mar 2024 09:13:05 GMT"
1000,Databricks Expands Brickbuilder Program to Include Unity Catalog Accelerators,https://www.databricks.com/blog/databricks-expands-brickbuilder-program-include-unity-catalog-accelerators,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">Today, we're excited to announce the launch of Brickbuilder Unity Catalog Accelerators. This is an expansion to the Brickbuilder Accelerator program, which pairs...</div>",[],"Thu, 07 Mar 2024 06:00:00 GMT"
1001,Simplify PySpark testing with DataFrame equality functions,https://www.databricks.com/blog/simplify-pyspark-testing-dataframe-equality-functions,<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">The DataFrame equality test functions were introduced in Apache Spark™ 3.5 and Databricks Runtime 14.2 to simplify PySpark unit testing. The full set o...</div>,[],"Wed, 06 Mar 2024 10:28:07 GMT"
1002,KX and Databricks Integration: Advancing Time-series Data Analytics in Capital Markets and Beyond,https://www.databricks.com/blog/kx-and-databricks-integration-advancing-time-series-data-analytics-capital-markets-and-beyond,<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">KX and Databricks have partnered to develop time series analytics solutions for the capital markets sector to support many use cases including quant...</div>,[],"Wed, 06 Mar 2024 08:20:09 GMT"
1003,Common Sense Product Recommendations using Large Language Models,https://www.databricks.com/blog/common-sense-product-recommendations-using-large-language-models,<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">Check out our LLM Solution Accelerators for Retail for more details and to download the notebooks. Product recommendations are a core feature of...</div>,[],"Tue, 05 Mar 2024 16:21:29 GMT"
1004,StreamNative and Databricks Unite to Power Real-Time Data Processing with Pulsar-Spark Connector,https://www.databricks.com/blog/streamnative-and-databricks-unite-power-real-time-data-processing-pulsar-spark-connector,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">StreamNative, a leading Apache Pulsar-based real-time data platform solutions provider, and Databricks, the Data Intelligence Platform, are thrilled to announce the enhanced Pulsar-Spark...</div>",[],"Tue, 05 Mar 2024 06:00:00 GMT"
1005,Adding Intelligence to Databricks Search,https://www.databricks.com/blog/adding-intelligence-to-databricks-search,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">We are thrilled to announce major improvements to the search capabilities in your Databricks workspace. These enhancements build on DatabricksIQ, the Data Intelligence...</div>",[],"Mon, 04 Mar 2024 11:53:45 GMT"
1006,Coastal Community Bank Builds a Thriving Financial Ecosystem on Databricks Data Intelligence Platform,https://www.databricks.com/blog/coastal-community-bank-builds-thriving-financial-ecosystem-databricks-data-intelligence,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">Special thanks to Barb MacLean, SVP, Head of Technology Operations and Implementation at Coastal Community Bank (Coastal) and Rob Cavallo, President at Cavallo...</div>",[],"Mon, 04 Mar 2024 10:18:35 GMT"
1007,Smarter Manufacturing: The Role of Governance in Streamlining Gen AI,https://www.databricks.com/blog/smarter-manufacturing-role-governance-streamlining-gen-ai,<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">Artificial Intelligence (AI) is going to be embedded in every product and service a business produces and customers interact with. With Generative AI...</div>,[],"Mon, 04 Mar 2024 08:53:31 GMT"
1008,Announcing Public Preview of Delta Sharing with Cloudflare R2 Integration,https://www.databricks.com/blog/announcing-public-preview-delta-sharing-cloudflare-r2-integration,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">Special thanks to Phillip Jones, Senior Product Manager, and Harshal Brahmbhatt, Systems Engineer from Cloudflare for their contributions to this blog. Organizations across...</div>",[],"Thu, 29 Feb 2024 08:02:04 GMT"
1009,A Deep Dive into the Latest Performance Improvements of Stateful Pipelines in Apache Spark Structured Streaming,https://www.databricks.com/blog/deep-dive-latest-performance-improvements-stateful-pipelines-apache-spark-structured-streaming,<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">This post is the second part of our two-part series on the latest performance improvements of stateful pipelines. The first part of this...</div>,[],"Wed, 28 Feb 2024 13:17:23 GMT"
1010,Performance Improvements for Stateful Pipelines in Apache Spark Structured Streaming,https://www.databricks.com/blog/performance-improvements-stateful-pipelines-apache-spark-structured-streaming,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">Introduction Apache Spark™ Structured Streaming is a popular open-source stream processing platform that provides scalability and fault tolerance, built on top of the S...</div>",[],"Wed, 28 Feb 2024 07:46:12 GMT"
1011,Fine-Grained Human Feedback,https://www.databricks.com/blog/fine-grained-human-feedback,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">(This post written in collaboration with Zeqiu (Ellen) Wu and Yushi Hu, both PhD students affiliated with the University of Washington, and co-first...</div>",[],"Tue, 27 Feb 2024 23:22:44 GMT"
1012,The Unconscious Patient Problem: A Look at the Importance Of Entity Resolution in Healthcare and Life Sciences,https://www.databricks.com/blog/unconscious-patient-problem-look-importance-entity-resolution-healthcare-and-life-sciences,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">This blog was written in collaboration with Tim Sedlak, Senior Solutions Architect at Stardog In healthcare and life sciences, accuracy is everything. That's...</div>",[],"Tue, 27 Feb 2024 08:56:32 GMT"
1013,Revolutionizing Tech Marketing,https://www.databricks.com/blog/revolutionizing-tech-marketing,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">Introduction On January 4th, a new era in digital marketing began as Google initiated the gradual removal of third-party cookies, marking a seismic...</div>",[],"Mon, 26 Feb 2024 09:07:14 GMT"
1014,Strengthening Cyber Resilience through Efficient Data Management: A Response to M-21-31,https://www.databricks.com/blog/strengthening-cyber-resilience-through-efficient-data-management-response-m-21-31,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">In today's environment, proactive cybersecurity is crucial to any public sector agency. For many organizations, log data that security professionals need for effective...</div>",[],"Thu, 22 Feb 2024 10:19:38 GMT"
1015,Announcing the General Availability of Unity Catalog Volumes,https://www.databricks.com/blog/announcing-general-availability-unity-catalog-volumes,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">Today, we are excited to announce that Unity Catalog Volumes is now generally available on AWS, Azure, and GCP. Unity Catalog provides a...</div>",[],"Thu, 22 Feb 2024 07:00:00 GMT"
1016,Announcing the General Availability of Azure Private Link and Azure Storage firewall support for Databricks SQL Serverless,https://www.databricks.com/blog/announcing-general-availability-azure-private-link-and-azure-storage-firewall-support,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">We are excited to announce the upcoming general availability of Azure Private Link support for Databricks SQL (DBSQL) Serverless, planned in April 2024...</div>",[],"Wed, 21 Feb 2024 11:16:41 GMT"
1017,Databricks adds new migration Brickbuilder Solutions to help customers succeed with AI,https://www.databricks.com/blog/databricks-adds-new-migration-brickbuilder-solutions-help-customers-succeed-ai,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">For the past two years, Databricks has collaborated with leading consulting partners to build innovative solutions for industry, migration, and data and AI...</div>",[],"Thu, 15 Feb 2024 07:49:27 GMT"
1018,Accelerating Success with Databricks: A Deep Dive into antuit.ai's Decision and Customer Impact,https://www.databricks.com/blog/accelerating-success-databricks-deep-dive-antuitais-decision-and-customer-impact,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">In the dynamic realm of AI-driven forecasting, businesses navigate a landscape where strategic choices shape their trajectory. One such pivotal decision was made...</div>",[],"Thu, 15 Feb 2024 07:02:01 GMT"
1019,LIMIT: Less is More for Instruction Tuning,https://www.databricks.com/blog/limit-less-more-instruction-tuning,<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">How should you finetune a large language model for general-purpose question answering? One intriguing approach is that of supervised finetuning on a small...</div>,[],"Sat, 10 Feb 2024 00:54:32 GMT"
1020,US Air Force Hackathon: How Large Language Models Will Revolutionize USAF Flight Test,https://www.databricks.com/blog/us-air-force-hackathon-how-large-language-models-will-revolutionize-usaf-flight-test,<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">[DISTRIBUTION STATEMENT A. Approved for public release; Distribution is unlimited 412TW-PA-24004] The views expressed are those of the author and do not reflect...</div>,[],"Fri, 09 Feb 2024 09:46:38 GMT"
1021,Furthering Our Commitment to Responsible AI Development Through Industry and Government Organizations,https://www.databricks.com/blog/furthering-our-commitment-responsible-ai-development-through-industry-and-government,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">At Databricks, we've upheld principles of responsible development throughout our long-standing history of building innovative data and AI products. We are committed to...</div>",[],"Thu, 08 Feb 2024 09:57:32 GMT"
1022,"Linking the unlinkables; simple, automated, scalable data linking with Databricks ARC",https://www.databricks.com/blog/linking-unlinkables-simple-automated-scalable-data-linking-databricks-arc,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">In April 2023 we announced the release of Databricks ARC to enable simple, automated linking of data within a single table. Today we...</div>",[],"Tue, 06 Feb 2024 13:30:01 GMT"
1023,What’s New in Data Engineering and Streaming - January 2024,https://www.databricks.com/blog/whats-new-data-engineering-and-streaming-january-2024,"<div class=\"clearfix text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item\">Databricks recently announced the Data Intelligence Platform, a natural evolution of the lakehouse architecture we pioneered. The idea of a Data Intelligence Platform...</div>",[],"Tue, 06 Feb 2024 12:45:21 GMT"
1024,Gemma: Introducing new state-of-the-art open models,https://deepmind.google/discover/blog/gemma-introducing-new-state-of-the-art-open-models/,Gemma is built for responsible AI development from the same research and technology used to create Gemini models.,[],"Wed, 21 Feb 2024 13:06:16 +0000"
1025,Our next-generation model: Gemini 1.5,https://deepmind.google/discover/blog/our-next-generation-model-gemini-15/,"The model delivers dramatically enhanced performance, with a breakthrough in long-context understanding across modalities.",[],"Thu, 15 Feb 2024 15:00:50 +0000"
1026,The next chapter of our Gemini era,https://deepmind.google/discover/blog/google-gemini-update-sundar-pichai-2024/,We're bringing Gemini to more Google products,[],"Thu, 08 Feb 2024 13:00:00 +0000"
1027,AlphaGeometry: An Olympiad-level AI system for geometry,https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/,Advancing AI reasoning in mathematics,[],"Wed, 17 Jan 2024 16:00:00 +0000"
1028,Shaping the future of advanced robotics,https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/,"Introducing AutoRT, SARA-RT, and RT-Trajectory",[],"Thu, 04 Jan 2024 11:39:00 +0000"
1029,Images altered to trick machine vision can influence humans too,https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-influence-humans-too/,"In a series of experiments published in Nature Communications, we found evidence that human judgments are indeed systematically influenced by adversarial perturbations.",[],"Tue, 02 Jan 2024 16:00:00 +0000"
1030,2023: A Year of Groundbreaking Advances in AI and Computing,https://deepmind.google/discover/blog/2023-a-year-of-groundbreaking-advances-in-ai-and-computing/,This has been a year of incredible progress in the field of Artificial Intelligence (AI) research and its practical applications.,[],"Fri, 22 Dec 2023 13:30:00 +0000"
1031,FunSearch: Making new discoveries in mathematical sciences using Large Language Models,https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/,"In a paper published in Nature, we introduce FunSearch, a method for searching for “functions” written in computer code, and find new solutions in mathematics and computer science. FunSearch works by pairing a pre-trained LLM, whose goal is to provide creative solutions in the form of computer code, with an automated “evaluator”, which guards against hallucinations and incorrect ideas.",[],"Thu, 14 Dec 2023 16:00:00 +0000"
1032,Google DeepMind at NeurIPS 2023,https://deepmind.google/discover/blog/google-deepmind-at-neurips-2023/,"The Neural Information Processing Systems (NeurIPS) is the largest artificial intelligence (AI) conference in the world. NeurIPS 2023 will be taking place December 10-16 in New Orleans, USA.Teams from across Google DeepMind are presenting more than 150 papers at the main conference and workshops.",[],"Fri, 08 Dec 2023 15:01:00 +0000"
1033,Introducing Gemini: our largest and most capable AI model,https://deepmind.google/discover/blog/introducing-gemini-our-largest-and-most-capable-ai-model/,Making AI more helpful for everyone,[],"Wed, 06 Dec 2023 15:13:00 +0000"
1034,Millions of new materials discovered with deep learning,https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/,"We share the discovery of 2.2 million new crystals  –  equivalent to nearly 800 years’ worth of knowledge. We introduce Graph Networks for Materials Exploration (GNoME), our new deep learning tool that dramatically increases the speed and efficiency of discovery by predicting the stability of new materials.",[],"Wed, 29 Nov 2023 16:04:00 +0000"
1035,Transforming the future of music creation,https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/,"Announcing our most advanced music generation model and two new AI experiments, designed to open a new playground for creativity",[],"Thu, 16 Nov 2023 07:20:00 +0000"
1036,Empowering the next generation for an AI-enabled world,https://deepmind.google/discover/blog/empowering-the-next-generation-for-an-ai-enabled-world/,Experience AI's course and resources are expanding on a global scale,[],"Wed, 15 Nov 2023 10:00:00 +0000"
1037,GraphCast: AI model for faster and more accurate global weather forecasting,https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/,"We introduce GraphCast, a state-of-the-art AI model able to make medium-range weather forecasts with unprecedented accuracy",[],"Tue, 14 Nov 2023 15:00:00 +0000"
1038,A glimpse of the next generation of AlphaFold,https://deepmind.google/discover/blog/a-glimpse-of-the-next-generation-of-alphafold/,"Progress update: Our latest AlphaFold model shows significantly improved accuracy and expands coverage beyond proteins to other biological molecules, including ligands.",[],"Tue, 31 Oct 2023 13:00:00 +0000"
1039,Evaluating social and ethical risks from generative AI,https://deepmind.google/discover/blog/evaluating-social-and-ethical-risks-from-generative-ai/,Introducing a context-based framework for comprehensively evaluating the social and ethical risks of AI systems,[],"Thu, 19 Oct 2023 15:00:00 +0000"
1040,Scaling up learning across many different robot types,https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/,"Robots are great specialists, but poor generalists. Typically, you have to train a model for each task, robot, and environment. Changing a single variable often requires starting from scratch. But what if we could combine the knowledge across robotics and create a way to train a general-purpose robot?",[],"Tue, 03 Oct 2023 15:00:00 +0000"
1041,A catalogue of genetic mutations to help pinpoint the cause of diseases,https://deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/,New AI tool classifies the effects of 71 million ‘missense’ mutations.,[],"Tue, 19 Sep 2023 13:37:00 +0000"
1042,Identifying AI-generated images with SynthID,https://deepmind.google/discover/blog/identifying-ai-generated-images-with-synthid/,New tool helps watermark and identify synthetic images created by Imagen,[],"Tue, 29 Aug 2023 00:00:00 +0000"
1043,RT-2: New model translates vision and language into action,https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/,"Robotic Transformer 2 (RT-2) is a novel vision-language-action (VLA) model that learns from both web and robotics data, and translates this knowledge into generalised instructions for robotic control.",[],"Fri, 28 Jul 2023 00:00:00 +0000"
1044,Using AI to fight climate change,https://deepmind.google/discover/blog/using-ai-to-fight-climate-change/,"AI is a powerful technology that will transform our future, so how can we best apply it to help combat climate change and find sustainable solutions?",[],"Fri, 21 Jul 2023 00:00:00 +0000"
1045,Google DeepMind’s latest research at ICML 2023,https://deepmind.google/discover/blog/google-deepmind-research-at-icml-2023/,"Exploring AI safety, adaptability, and efficiency for the real world",[],"Thu, 20 Jul 2023 00:00:00 +0000"
1046,Developing reliable AI tools for healthcare,https://deepmind.google/discover/blog/codoc-developing-reliable-ai-tools-for-healthcare/,"We’ve published our joint paper with Google Research in Nature Medicine, which proposes CoDoC (Complementarity-driven Deferral-to-Clinical Workflow), an AI system that learns when to rely on predictive AI tools or defer to a clinician for the most accurate interpretation of medical images.",[],"Mon, 17 Jul 2023 00:00:00 +0000"
1047,Exploring institutions for global AI governance,https://deepmind.google/discover/blog/exploring-institutions-for-global-ai-governance/,New white paper investigates models and functions of international institutions that could help manage opportunities and mitigate risks of advanced AI.,[],"Tue, 11 Jul 2023 00:00:00 +0000"
1048,RoboCat: A self-improving robotic agent,https://deepmind.google/discover/blog/robocat-a-self-improving-robotic-agent/,"Robots are quickly becoming part of our everyday lives, but they’re often only programmed to perform specific tasks well. While harnessing recent advances in AI could lead to robots that could help in many more ways, progress in building general-purpose robots is slower in part because of the time needed to collect real-world training data. Our latest paper introduces a self-improving AI agent for robotics, RoboCat, that learns to perform a variety of tasks across different arms, and then self-generates new training data to improve its technique.",[],"Tue, 20 Jun 2023 00:00:00 +0000"
1049,AlphaDev discovers faster sorting algorithms,https://deepmind.google/discover/blog/alphadev-discovers-faster-sorting-algorithms/,New algorithms will transform the foundations of computing,[],"Wed, 07 Jun 2023 00:00:00 +0000"
1050,An early warning system for novel AI risks,https://deepmind.google/discover/blog/an-early-warning-system-for-novel-ai-risks/,New research proposes a framework for evaluating general-purpose models against novel threats,[],"Thu, 25 May 2023 00:00:00 +0000"
1051,DeepMind’s latest research at ICLR 2023,https://deepmind.google/discover/blog/deepminds-latest-research-at-iclr-2023/,"Next week marks the start of the 11th International Conference on Learning Representations (ICLR), taking place 1-5 May in Kigali, Rwanda. This will be the first major artificial intelligence (AI) conference to be hosted in Africa and the first in-person event since the start of the pandemic. Researchers from around the world will gather to share their cutting-edge work in deep learning spanning the fields of AI, statistics and data science, and applications including machine vision, gaming and robotics. We’re proud to support the conference as a Diamond sponsor and DEI champion.",[],"Thu, 27 Apr 2023 00:00:00 +0000"
1052,How can we build human values into AI?,https://deepmind.google/discover/blog/how-can-we-build-human-values-into-ai/,Drawing from philosophy to identify fair principles for ethical AI...,[],"Mon, 24 Apr 2023 00:00:00 +0000"
1053,Announcing Google DeepMind,https://deepmind.google/discover/blog/announcing-google-deepmind/,DeepMind and the Brain team from Google Research will join forces to accelerate progress towards a world in which AI helps solve the biggest challenges facing humanity.,[],"Thu, 20 Apr 2023 00:00:00 +0000"
1054,Competitive programming with AlphaCode,https://deepmind.google/discover/blog/competitive-programming-with-alphacode/,Solving novel problems and setting a new milestone in competitive programming.,[],"Thu, 08 Dec 2022 00:00:00 +0000"
1055,AI for the board game Diplomacy,https://deepmind.google/discover/blog/ai-for-the-board-game-diplomacy/,"Successful communication and cooperation have been crucial for helping societies advance throughout history. The closed environments of board games can serve as a sandbox for modelling and investigating interaction and communication – and we can learn a lot from playing them. In our recent paper, published today in Nature Communications, we show how artificial agents can use communication to better cooperate in the board game Diplomacy, a vibrant domain in artificial intelligence (AI) research, known for its focus on alliance building.",[],"Tue, 06 Dec 2022 00:00:00 +0000"
1056,"Mastering Stratego, the classic game of imperfect information",https://deepmind.google/discover/blog/mastering-stratego-the-classic-game-of-imperfect-information/,Game-playing artificial intelligence (AI) systems have advanced to a new frontier.,[],"Thu, 01 Dec 2022 00:00:00 +0000"
1057,DeepMind’s latest research at NeurIPS 2022,https://deepmind.google/discover/blog/deepminds-latest-research-at-neurips-2022/,"NeurIPS is the world’s largest conference in artificial intelligence (AI) and machine learning (ML), and we’re proud to support the event as Diamond sponsors, helping foster the exchange of research advances in the AI and ML community. Teams from across DeepMind are presenting 47 papers, including 35 external collaborations in virtual panels and poster sessions.",[],"Fri, 25 Nov 2022 00:00:00 +0000"
1058,Building interactive agents in video game worlds,https://deepmind.google/discover/blog/building-interactive-agents-in-video-game-worlds/,"Most artificial intelligence (AI) researchers now believe that writing computer code which can capture the nuances of situated interactions is impossible. Alternatively, modern machine learning (ML) researchers have focused on learning about these types of interactions from data. To explore these learning-based approaches and quickly build agents that can make sense of human instructions and safely perform actions in open-ended conditions, we created a research framework within a video game environment.Today, we’re publishing a paper [INSERT LINK] and collection of videos, showing our early steps in building video game AIs that can understand fuzzy human concepts – and therefore, can begin to interact with people on their own terms.",[],"Wed, 23 Nov 2022 00:00:00 +0000"
1059,Benchmarking the next generation of never-ending learners,https://deepmind.google/discover/blog/benchmarking-the-next-generation-of-never-ending-learners/,Learning how to build upon knowledge by tapping 30 years of computer vision research,[],"Tue, 22 Nov 2022 00:00:00 +0000"
1060,Best practices for data enrichment,https://deepmind.google/discover/blog/best-practices-for-data-enrichment/,Building a responsible approach to data collection with the Partnership on AI...,[],"Wed, 16 Nov 2022 00:00:00 +0000"
1061,"The pursuit of AI education—past, present, and future",https://deepmind.google/discover/blog/the-pursuit-of-ai-educationpast-present-and-future/,"Meet Sylvia Christie, our education partnerships manager who’s played a leading role in expanding our scholarship programme, which is marking its five-year anniversary.",[],"Tue, 08 Nov 2022 00:00:00 +0000"
1062,Stopping malaria in its tracks,https://deepmind.google/discover/blog/stopping-malaria-in-its-tracks/,Developing a vaccine that could save hundreds of thousands of lives,[],"Thu, 13 Oct 2022 15:00:00 +0000"
1063,Measuring perception in AI models,https://deepmind.google/discover/blog/measuring-perception-in-ai-models/,"Perception – the process of experiencing the world through senses – is a significant part of intelligence. And building agents with human-level perceptual understanding of the world is a central but challenging task, which is becoming increasingly important in robotics, self-driving cars, personal assistants, medical imaging, and more. So today, we’re introducing the Perception Test, a multimodal benchmark using real-world videos to help evaluate the perception capabilities of a model.",[],"Wed, 12 Oct 2022 00:00:00 +0000"
1064,How undesired goals can arise with correct rewards,https://deepmind.google/discover/blog/how-undesired-goals-can-arise-with-correct-rewards/,"As we build increasingly advanced artificial intelligence (AI) systems, we want to make sure they don’t pursue undesired goals. Such behaviour in an AI agent is often the result of specification gaming – exploiting a poor choice of what they are rewarded for. In our latest paper, we explore a more subtle mechanism by which AI systems may unintentionally learn to pursue undesired goals: goal misgeneralisation (GMG). GMG occurs when a system's capabilities generalise successfully but its goal does not generalise as desired, so the system competently pursues the wrong goal. Crucially, in contrast to specification gaming, GMG can occur even when the AI system is trained with a correct specification.",[],"Fri, 07 Oct 2022 00:00:00 +0000"
1065,Discovering novel algorithms with AlphaTensor,https://deepmind.google/discover/blog/discovering-novel-algorithms-with-alphatensor/,"In our paper, published today in Nature, we introduce AlphaTensor, the first artificial intelligence (AI) system for discovering novel, efficient, and provably correct algorithms for fundamental tasks such as matrix multiplication. This sheds light on a 50-year-old open question in mathematics about finding the fastest way to multiply two matrices. This paper is a stepping stone in DeepMind’s mission to advance science and unlock the most fundamental problems using AI. Our system, AlphaTensor, builds upon AlphaZero, an agent that has shown superhuman performance on board games, like chess, Go and shogi, and this work shows the journey of AlphaZero from playing games to tackling unsolved mathematical problems for the first time.",[],"Wed, 05 Oct 2022 00:00:00 +0000"
1066,Fighting osteoporosis before it starts,https://deepmind.google/discover/blog/fighting-osteoporosis-before-it-starts/,Detecting signs of disease before bones start to break,[],"Tue, 27 Sep 2022 14:16:00 +0000"
1067,Understanding the faulty proteins linked to cancer and autism,https://deepmind.google/discover/blog/understanding-the-faulty-proteins-linked-to-cancer-and-autism/,Helping uncover how protein mutations cause diseases and disorders,[],"Mon, 26 Sep 2022 15:19:00 +0000"
1068,Supporting the next generation of AI leaders,https://deepmind.google/discover/blog/supporting-the-next-generation-of-ai-leaders/,We’re partnering with six education charities and social enterprises in the United Kingdom (UK) to co-create a bespoke education programme to help tackle the gaps in STEM education and boost existing programmes.,[],"Mon, 26 Sep 2022 00:00:00 +0000"
1069,Solving the mystery of how an ancient bird went extinct,https://deepmind.google/discover/blog/solving-the-mystery-of-how-an-ancient-bird-went-extinct/,"Creating a tool to study extinct species from 50,000 years ago",[],"Thu, 22 Sep 2022 15:27:00 +0000"
1070,Building safer dialogue agents,https://deepmind.google/discover/blog/building-safer-dialogue-agents/,"In our latest paper, we introduce Sparrow – a dialogue agent that’s useful and reduces the risk of unsafe and inappropriate answers. Our agent is designed to talk with a user, answer questions, and search the internet using Google when it’s helpful to look up evidence to inform its responses.",[],"Thu, 22 Sep 2022 00:00:00 +0000"
1071,Targeting early-onset Parkinson’s with AI,https://deepmind.google/discover/blog/targeting-early-onset-parkinsons-with-ai/,Predictions that  pave the way to new treatments,[],"Wed, 21 Sep 2022 15:37:00 +0000"
1072,How our principles helped define AlphaFold’s release,https://deepmind.google/discover/blog/how-our-principles-helped-define-alphafolds-release/,"Our Operating Principles have come to define both our commitment to prioritising widespread benefit, as well as the areas of research and applications we refuse to pursue. These principles have been at the heart of our decision making since DeepMind was founded, and continue to be refined as the AI landscape changes and grows. They are designed for our role as a research-driven science company and consistent with Google’s AI principles.",[],"Wed, 14 Sep 2022 00:00:00 +0000"
1073,Maximising the impact of our breakthroughs,https://deepmind.google/discover/blog/maximising-the-impact-of-our-breakthroughs/,"Colin, CBO at DeepMind, discusses collaborations with Alphabet and how we integrate ethics, accountability, and safety into everything we do.",[],"Fri, 09 Sep 2022 00:00:00 +0000"
1074,My journey from DeepMind intern to mentor,https://deepmind.google/discover/blog/my-journey-from-deepmind-intern-to-mentor/,"Former intern turned intern manager, Richard Everett, describes his journey to DeepMind, sharing tips and advice for aspiring DeepMinders. The 2023 internship applications will open on the 16th September, please visit https://dpmd.ai/internshipsatdeepmind for more information.",[],"Thu, 08 Sep 2022 00:00:00 +0000"
1075,In conversation with AI: building better language models,https://deepmind.google/discover/blog/in-conversation-with-ai-building-better-language-models/,"Our new paper, In conversation with AI: aligning language models with human values, explores a different approach, asking what successful communication between humans and an artificial conversational agent might look like and what values should guide conversation in these contexts.",[],"Tue, 06 Sep 2022 00:00:00 +0000"
1076,From motor control to embodied intelligence,https://deepmind.google/discover/blog/from-motor-control-to-embodied-intelligence/,"Using human and animal motions to teach robots to dribble a ball, and simulated humanoid characters to carry boxes and play football",[],"Wed, 31 Aug 2022 00:00:00 +0000"
1077,Advancing conservation with AI-based facial recognition of turtles,https://deepmind.google/discover/blog/advancing-conservation-with-ai-based-facial-recognition-of-turtles/,"We came across Zindi – a dedicated partner with complementary goals – who are the largest community of African data scientists and host competitions that focus on solving Africa’s most pressing problems. Our Science team’s Diversity, Equity, and Inclusion (DE&amp;I) team worked with Zindi to identify a scientific challenge that could help advance conservation efforts and grow involvement in AI. Inspired by Zindi’s bounding box turtle challenge, we landed on a project with the potential for real impact: turtle facial recognition.",[],"Thu, 25 Aug 2022 00:00:00 +0000"
1078,Discovering when an agent is present in a system,https://deepmind.google/discover/blog/discovering-when-an-agent-is-present-in-a-system/,"We want to build safe, aligned artificial general intelligence (AGI) systems that pursue the intended goals of its designers. Causal influence diagrams (CIDs) are a way to model decision-making situations that allow us to reason about agent incentives. By relating training setups to the incentives that shape agent behaviour, CIDs help illuminate potential risks before training an agent and can inspire better agent designs. But how do we know when a CID is an accurate model of a training setup?",[],"Thu, 18 Aug 2022 00:00:00 +0000"
1079,Realising scientists are the real superheroes,https://deepmind.google/discover/blog/realising-scientists-are-the-real-superheroes/,"Meet Edgar Duéñez-Guzmán, a research engineer on our Multi-Agent Research team who’s drawing on knowledge of game theory, computer science, and social evolution to get AI agents working better together.",[],"Thu, 11 Aug 2022 00:00:00 +0000"
1080,The race to cure a billion people from a deadly parasitic disease,https://deepmind.google/discover/blog/the-race-to-cure-a-billion-people-from-a-deadly-parasitic-disease/,Accelerating the search for life saving leishmaniasis treatments,[],"Thu, 28 Jul 2022 16:49:00 +0000"
1081,Tracing the evolution of proteins back to the origin of life,https://deepmind.google/discover/blog/tracing-the-evolution-of-proteins-back-to-the-origin-of-life/,Looking into a protein’s past to unlock the mysteries of life itself,[],"Thu, 28 Jul 2022 16:30:00 +0000"
1082,How the honeybee could help protect species around the world,https://deepmind.google/discover/blog/how-the-honeybee-could-help-protect-species-around-the-world/,New insights into immunity to help protect the world’s flora,[],"Thu, 28 Jul 2022 16:25:00 +0000"
1083,AlphaFold transforms biology for millions around the world,https://deepmind.google/discover/blog/alphafold-transforms-biology-for-millions-around-the-world/,Big data that leads to discoveries that benefit everyone,[],"Thu, 28 Jul 2022 16:19:00 +0000"
1084,Advancing discovery of better drugs and medicine,https://deepmind.google/discover/blog/advancing-discovery-of-better-drugs-and-medicine/,Researchers are designing more effective drugs than ever before,[],"Thu, 28 Jul 2022 16:14:00 +0000"
1085,Creating plastic-eating enzymes that could save us from pollution,https://deepmind.google/discover/blog/creating-plastic-eating-enzymes-that-could-save-us-from-pollution/,Helping plastics become 100% recyclable,[],"Thu, 28 Jul 2022 16:10:00 +0000"
1086,AlphaFold unlocks one of the greatest puzzles in biology,https://deepmind.google/discover/blog/alphafold-unlocks-one-of-the-greatest-puzzles-in-biology/,Piecing together one of the largest molecular structures in human cells,[],"Thu, 28 Jul 2022 15:59:00 +0000"
1087,Accelerating the race against antibiotic resistance,https://deepmind.google/discover/blog/accelerating-the-race-against-antibiotic-resistance/,Unlocking a decade of data in minutes to help beat antibiotic resistance,[],"Thu, 28 Jul 2022 15:48:00 +0000"
1088,AlphaFold reveals the structure of the protein universe,https://deepmind.google/discover/blog/alphafold-reveals-the-structure-of-the-protein-universe/,"Today, in partnership with EMBL’s European Bioinformatics Institute (EMBL-EBI), we’re now releasing predicted structures for nearly all catalogued proteins known to science, which will expand the AlphaFold DB by over 200x - from nearly 1 million structures to over 200 million structures - with the potential to dramatically increase our understanding of biology.",[],"Thu, 28 Jul 2022 00:00:00 +0000"
1089,Putting the power of AlphaFold into the world’s hands,https://deepmind.google/discover/blog/putting-the-power-of-alphafold-into-the-worlds-hands/,"When we announced AlphaFold 2 last December, it was hailed as a solution to the 50-year old protein folding problem. Last week, we published the scientific paper and source code explaining how we created this highly innovative system, and today we’re sharing high-quality predictions for the shape of every single protein in the human body, as well as for the proteins of 20 additional organisms that scientists rely on for their research.",[],"Fri, 22 Jul 2022 00:00:00 +0000"
1090,The virtuous cycle of AI research,https://deepmind.google/discover/blog/the-virtuous-cycle-of-ai-research/,"We recently caught up with Petar Veličković, a research scientist at DeepMind. Along with his co-authors, Petar is presenting his paper The CLRS Algorithmic Reasoning Benchmark at ICML 2022 in Baltimore, Maryland, USA.",[],"Tue, 19 Jul 2022 00:00:00 +0000"
1091,"Perceiver AR: general-purpose, long-context autoregressive generation",https://deepmind.google/discover/blog/perceiver-ar-general-purpose-long-context-autoregressive-generation/,"We develop Perceiver AR, an autoregressive, modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking. Perceiver AR can directly attend to over a hundred thousand tokens, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms.",[],"Sat, 16 Jul 2022 00:00:00 +0000"
1092,DeepMind’s latest research at ICML 2022,https://deepmind.google/discover/blog/deepminds-latest-research-at-icml-2022/,"Starting this weekend, the thirty-ninth International Conference on Machine Learning (ICML 2022) is meeting from 17-23 July, 2022 at the Baltimore Convention Center in Maryland, USA, and will be running as a hybrid event. Researchers working across artificial intelligence, data science, machine vision, computational biology, speech recognition, and more are presenting and publishing their cutting-edge work in machine learning.",[],"Fri, 15 Jul 2022 00:00:00 +0000"
1093,Intuitive physics learning in a deep-learning model inspired by developmental psychology,https://deepmind.google/discover/blog/intuitive-physics-learning-in-a-deep-learning-model-inspired-by-developmental-psychology/,"Despite significant effort, current AI systems pale in their understanding of intuitive physics, in comparison to even very young children. In the present work, we address this AI problem, specifically by drawing on the field of developmental psychology.",[],"Mon, 11 Jul 2022 00:00:00 +0000"
1094,Human-centred mechanism design with Democratic AI,https://deepmind.google/discover/blog/human-centred-mechanism-design-with-democratic-ai/,"In our recent paper, published in Nature Human Behaviour, we provide a proof-of-concept demonstration that deep reinforcement learning (RL) can be used to find economic policies that people will vote for by majority in a simple game. The paper thus addresses a key challenge in AI research - how to train AI systems that align with human values.",[],"Mon, 04 Jul 2022 00:00:00 +0000"
1095,Leading a movement to strengthen machine learning in Africa,https://deepmind.google/discover/blog/leading-a-movement-to-strengthen-machine-learning-in-africa/,"Avishkar Bhoopchand, a research engineer on the Game Theory and Multi-agent team, shares his journey to DeepMind and how he’s working to raise the profile of deep learning across Africa.",[],"Thu, 23 Jun 2022 00:00:00 +0000"
1096,BYOL-Explore: Exploration with Bootstrapped Prediction,https://deepmind.google/discover/blog/byol-explore-exploration-with-bootstrapped-prediction/,"We present BYOL-Explore, a conceptually simple yet general approach for curiosity-driven exploration in visually-complex environments. BYOL-Explore learns a world representation, the world dynamics, and an exploration policy all-together by optimizing a single prediction loss in the latent space with no additional auxiliary objective. We show that BYOL-Explore is effective in DM-HARD-8, a challenging partially-observable continuous-action hard-exploration benchmark with visually-rich 3-D environments.",[],"Mon, 20 Jun 2022 00:00:00 +0000"
1097,Unlocking High-Accuracy Differentially Private Image Classification through Scale,https://deepmind.google/discover/blog/unlocking-high-accuracy-differentially-private-image-classification-through-scale/,"According to empirical evidence from prior works, utility degradation in DP-SGD becomes more severe on larger neural network models – including the ones regularly used to achieve the best performance on challenging image classification benchmarks. Our work investigates this phenomenon and proposes a series of simple modifications to both the training procedure and model architecture, yielding a significant improvement on the accuracy of DP training on standard image classification benchmarks.",[],"Fri, 17 Jun 2022 00:00:00 +0000"
1098,Bridging DeepMind research with Alphabet products,https://deepmind.google/discover/blog/bridging-deepmind-research-with-alphabet-products/,"Today we caught up with Gemma Jennings, a product manager on the Applied team, who led a session on vision language models at the AI Summit, one of the world’s largest AI events for business.",[],"Wed, 15 Jun 2022 00:00:00 +0000"
1099,Advocating for the LGBTQ+ community in AI research,https://deepmind.google/discover/blog/advocating-for-the-lgbtq-community-in-ai-research/,"Research scientist, Kevin McKee, tells how his early love of science fiction and social psychology inspired his career, and how he’s helping advance research in ‘queer fairness’, support human-AI collaboration, and study the effects of AI on the LGBTQ+ community.",[],"Wed, 01 Jun 2022 00:00:00 +0000"
1100,Evaluating Multimodal Interactive Agents,https://deepmind.google/discover/blog/evaluating-multimodal-interactive-agents/,"In this paper, we assess the merits of these existing evaluation metrics and present a novel approach to evaluation called the Standardised Test Suite (STS). The STS uses behavioural scenarios mined from real human interaction data.",[],"Fri, 27 May 2022 00:00:00 +0000"
1101,Kyrgyzstan to King’s Cross: the star baker cooking up code,https://deepmind.google/discover/blog/kyrgyzstan-to-kings-cross-the-star-baker-cooking-up-code/,"My day can vary, it really depends on which phase of the project I'm on. Let’s say we want to add a feature to our product – my tasks could range from designing solutions and working with the team to find the best one, to deploying new features into production and doing maintenance. Along the way, I’ll communicate changes to our stakeholders, write docs, code and test solutions, build analytics dashboards, clean-up old code, and fix bugs.",[],"Thu, 26 May 2022 00:00:00 +0000"
1102,Dynamic language understanding: adaptation to new knowledge in parametric and semi-parametric models,https://deepmind.google/discover/blog/dynamic-language-understanding-adaptation-to-new-knowledge-in-parametric-and-semi-parametric-models/,"To study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge, we construct a new large-scale dataset, StreamingQA, with human written and generated questions asked on a given date, to be answered from 14 years of time-stamped news articles. We evaluate our models quarterly as they read new articles not seen in pre-training. We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting.",[],"Thu, 26 May 2022 00:00:00 +0000"
1103,Building a culture of pioneering responsibly,https://deepmind.google/discover/blog/building-a-culture-of-pioneering-responsibly/,"When I joined DeepMind as COO, I did so in large part because I could tell that the founders and team had the same focus on positive social impact. In fact, at DeepMind, we now champion a term that perfectly captures my own values and hopes for integrating technology into people’s daily lives: pioneering responsibly. I believe pioneering responsibly should be a priority for anyone working in tech. But I also recognise that it’s especially important when it comes to powerful, widespread technologies like artificial intelligence. AI is arguably the most impactful technology being developed today. It has the potential to benefit humanity in innumerable ways – from combating climate change to preventing and treating disease. But it’s essential that we account for both its positive and negative downstream impacts.",[],"Tue, 24 May 2022 00:00:00 +0000"
1104,Open-sourcing MuJoCo,https://deepmind.google/discover/blog/open-sourcing-mujoco/,"In October 2021, we announced that we acquired the MuJoCo physics simulator, and made it freely available for everyone to support research everywhere. We also committed to developing and maintaining MuJoCo as a free, open-source, community-driven project with best-in-class capabilities. Today, we’re thrilled to report that open sourcing is complete and the entire codebase is on GitHub! Here, we explain why MuJoCo is a great platform for open-source collaboration and share a preview of our roadmap going forward.",[],"Mon, 23 May 2022 00:00:00 +0000"
1105,From LEGO competitions to DeepMind's robotics lab,https://deepmind.google/discover/blog/from-lego-competitions-to-deepminds-robotics-lab/,"If you want to be at DeepMind, go for it. Apply, interview, and just try. You might not get it the first time but that doesn’t mean you can’t try again. I never thought DeepMind would accept me, and when they did, I thought it was a mistake. Everyone doubts themselves – I’ve never felt like the smartest person in the room. I’ve often felt the opposite. But I’ve learned that, despite those feelings, I do belong and I do deserve to work at a place like this. And that journey, for me, started with just trying.",[],"Thu, 19 May 2022 00:00:00 +0000"
1106,Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning,https://deepmind.google/discover/blog/emergent-bartering-behaviour-in-multi-agent-reinforcement-learning/,"In our recent paper, we explore how populations of deep reinforcement learning (deep RL) agents can learn microeconomic behaviours, such as production, consumption, and trading of goods. We find that artificial agents learn to make economically rational decisions about production, consumption, and prices, and react appropriately to supply and demand changes.",[],"Mon, 16 May 2022 00:00:00 +0000"
1107,A Generalist Agent,https://deepmind.google/discover/blog/a-generalist-agent/,"Inspired by progress in large-scale language modelling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.",[],"Thu, 12 May 2022 00:00:00 +0000"
1108,Active offline policy selection,https://deepmind.google/discover/blog/active-offline-policy-selection/,"To make RL more applicable to real-world applications like robotics, we propose using an intelligent evaluation procedure to select the policy for deployment, called active offline policy selection (A-OPS). In A-OPS, we make use of the prerecorded dataset and allow limited interactions with the real environment to boost the selection quality.",[],"Fri, 06 May 2022 00:00:00 +0000"
1109,Tackling multiple tasks with a single visual language model,https://deepmind.google/discover/blog/tackling-multiple-tasks-with-a-single-visual-language-model/,"We introduce Flamingo, a single visual language model (VLM) that sets a new state of the art in few-shot learning on a wide range of open-ended multimodal tasks.",[],"Thu, 28 Apr 2022 00:00:00 +0000"
1110,When a passion for bass and brass help build better tools,https://deepmind.google/discover/blog/when-a-passion-for-bass-and-brass-help-build-better-tools/,"We caught up with Kevin Millikin, a software engineer on the DevTools team. He’s in Salt Lake City this week to present at PyCon US, the largest annual gathering for those using and developing the open-source Python programming language.",[],"Thu, 28 Apr 2022 00:00:00 +0000"
1111,DeepMind’s latest research at ICLR 2022,https://deepmind.google/discover/blog/deepminds-latest-research-at-iclr-2022/,"Beyond supporting the event as sponsors and regular workshop organisers, our research teams are presenting 29 papers, including 10 collaborations this year. Here’s a brief glimpse into our upcoming oral, spotlight, and poster presentations.",[],"Mon, 25 Apr 2022 00:00:00 +0000"
1112,An empirical analysis of compute-optimal large language model training,https://deepmind.google/discover/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training/,"We ask the question: “What is the optimal model size and number of training tokens for a given compute budget?” To answer this question, we train models of various sizes and with various numbers of tokens, and estimate this trade-off empirically. Our main finding is that the current large language models are far too large for their compute budget and are not being trained on enough data.",[],"Tue, 12 Apr 2022 00:00:00 +0000"
1113,GopherCite: Teaching language models to support answers with verified quotes,https://deepmind.google/discover/blog/gophercite-teaching-language-models-to-support-answers-with-verified-quotes/,"Language models like Gopher can “hallucinate” facts that appear plausible but are actually fake. Those who are familiar with this problem know to do their own fact-checking, rather than trusting what language models say. Those who are not, may end up believing something that isn’t true. This paper describes GopherCite, a model which aims to address the problem of language model hallucination. GopherCite attempts to back up all of its factual claims with evidence from the web.",[],"Wed, 16 Mar 2022 00:00:00 +0000"
1114,Predicting the past with Ithaca,https://deepmind.google/discover/blog/predicting-the-past-with-ithaca/,"The birth of human writing marked the dawn of History and is crucial to our understanding of past civilisations and the world we live in today. For example, more than 2,500 years ago, the Greeks began writing on stone, pottery, and metal to document everything from leases and laws to calendars and oracles, giving a detailed insight into the Mediterranean region. Unfortunately, it’s an incomplete record. Many of the surviving inscriptions have been damaged over the centuries or moved from their original location. In addition, modern dating techniques, such as radiocarbon dating, cannot be used on these materials, making inscriptions difficult and time-consuming to interpret.",[],"Wed, 09 Mar 2022 00:00:00 +0000"
1115,Learning Robust Real-Time Cultural Transmission without Human Data,https://deepmind.google/discover/blog/learning-robust-real-time-cultural-transmission-without-human-data/,"In this work, we use deep reinforcement learning to generate artificial agents capable of test-time cultural transmission. Once trained, our agents can infer and recall navigational knowledge demonstrated by experts. This knowledge transfer happens in real time and generalises across a vast space of previously unseen tasks.",[],"Thu, 03 Mar 2022 00:00:00 +0000"
1116,Probing Image-Language Transformers for Verb Understanding,https://deepmind.google/discover/blog/probing-image-language-transformers-for-verb-understanding/,"Multimodal Image-Language transformers have achieved impressive results on a variety of tasks that rely on fine-tuning (e.g., visual question answering and image retrieval). We are interested in shedding light on the quality of their pretrained representations--in particular, if these models can distinguish verbs or they only use the nouns in a given sentence. To do so, we collect a dataset of image-sentence pairs consisting of 447 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset). We use this dataset to evaluate the pretrained models in a zero-shot way.",[],"Wed, 23 Feb 2022 00:00:00 +0000"
1117,Accelerating fusion science through learned plasma control,https://deepmind.google/discover/blog/accelerating-fusion-science-through-learned-plasma-control/,Successfully controlling the nuclear fusion plasma in a tokamak with deep reinforcement learning,[],"Wed, 16 Feb 2022 00:00:00 +0000"
1118,MuZero’s first step from research into the real world,https://deepmind.google/discover/blog/muzeros-first-step-from-research-into-the-real-world/,Collaborating with YouTube to optimise video compression in the open source VP9 codec.,[],"Fri, 11 Feb 2022 00:00:00 +0000"
1119,Red Teaming Language Models with Language Models,https://deepmind.google/discover/blog/red-teaming-language-models-with-language-models/,"In our recent paper, we show that it is possible to automatically find inputs that elicit harmful text from language models by generating inputs using language models themselves. Our approach provides one tool for finding harmful model behaviours before users are impacted, though we emphasize that it should be viewed as one component alongside many other techniques that will be needed to find harms and mitigate them once found.",[],"Mon, 07 Feb 2022 00:00:00 +0000"
1120,DeepMind: The Podcast returns for Season 2,https://deepmind.google/discover/blog/deepmind-the-podcast-returns-for-season-2/,We believe artificial intelligence (AI) is one of the most significant technologies of our age and we want to help people understand its potential and how it’s being created.,[],"Tue, 25 Jan 2022 00:00:00 +0000"
1121,Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents,https://deepmind.google/discover/blog/spurious-normativity-enhances-learning-of-compliance-and-enforcement-behavior-in-artificial-agents/,"In our recent paper we explore how multi-agent deep reinforcement learning can serve as a model of complex social interactions, like the formation of social norms. This new class of models could provide a path to create richer, more detailed simulations of the world.",[],"Tue, 18 Jan 2022 00:00:00 +0000"
1122,AlphaFold: Using AI for scientific discovery,https://deepmind.google/discover/blog/alphafold-using-ai-for-scientific-discovery/,We’re excited to share DeepMind’s first significant milestone in demonstrating how artificial intelligence research can drive and accelerate new scientific discoveries.,[],"Sat, 15 Jan 2022 00:00:00 +0000"
1123,Simulating matter on the quantum scale with AI,https://deepmind.google/discover/blog/simulating-matter-on-the-quantum-scale-with-ai/,"Solving some of the major challenges of the 21st Century, such as producing clean electricity or developing high temperature superconductors, will require us to design new materials with specific properties. To do this on a computer requires the simulation of electrons, the subatomic particles that govern how atoms bond to form molecules and are also responsible for the flow of electricity in solids.",[],"Thu, 09 Dec 2021 00:00:00 +0000"
1124,Teaching Language Models How to Coordinate,https://ieeetv.ieee.org/channels/computer-society/teaching-language-models-how-to-coordinate,"<span>Teaching Language Models How to Coordinate</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/teaching.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Thu, 02/29/2024 - 06:23</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 17 times\">17 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Thu, 29 Feb 2024 14:23:38 +0000"
1125,DBOS A Database oriented Operating System,https://ieeetv.ieee.org/video/dbos-a-database-oriented-operating-system,"<span>DBOS A Database oriented Operating System</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/DBOS.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Mon, 02/05/2024 - 12:22</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 49 times\">49 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Mon, 05 Feb 2024 20:22:13 +0000"
1126,Nonfungible Tokens (NFTs): Transformative Potential and Risk Management,https://ieeetv.ieee.org/video/nonfungible-tokens-transformative-potential-and-risk-management,"<span>Nonfungible Tokens (NFTs): Transformative Potential and Risk Management</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Park%20cover%20slide%201_0.jpg\" width=\"185\" /></div>
      
<span><span>p.wesling@ieee.org</span></span>

<span>Fri, 12/08/2023 - 11:24</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 6 times\">6 views</a>",[{'email': 'p.wesling@ieee.org'}],"Fri, 08 Dec 2023 19:24:23 +0000"
1127,ICADS '23 Panel Discussion: Should AI development really take a pause?,https://ieeetv.ieee.org/video/icads-23-panel-discussion-should-ai-development-really-take-a-pause,"<span>ICADS '23 Panel Discussion: Should AI development really take a pause?</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/paneldiscussion.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Tue, 08/01/2023 - 10:48</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 56 times\">56 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Tue, 01 Aug 2023 17:48:49 +0000"
1128,ICADS '23: Using Machine Learning Tools in the Cloud: Experience Gained from the Ask4Summary research project,https://ieeetv.ieee.org/video/icads-23-using-machine-learning-tools-in-the-cloud-experience-gained-from-the-ask4summary-research-project,"<span>ICADS '23: Using Machine Learning Tools in the Cloud: Experience Gained from the Ask4Summary research project</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/MACHINELEARNING.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Tue, 08/01/2023 - 10:48</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 22 times\">22 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Tue, 01 Aug 2023 17:48:32 +0000"
1129,ICADS '23 Keynote: AI-based Decision Frameworks for Smart Environments – Some Case Studies,https://ieeetv.ieee.org/video/icads-23-keynote-ai-based-decision-frameworks-for-smart-environments-some-case-studies,"<span>ICADS '23 Keynote: AI-based Decision Frameworks for Smart Environments – Some Case Studies</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/AIBASEDDECISION.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Tue, 08/01/2023 - 10:48</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 13 times\">13 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Tue, 01 Aug 2023 17:48:16 +0000"
1130,ICADS '23 talk: Performance-based content generation for language learning,https://ieeetv.ieee.org/video/icads-23-talk-performance-based-content-generation-for-language-learning,"<span>ICADS '23 talk: Performance-based content generation for language learning</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/performancebased.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Tue, 08/01/2023 - 10:47</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 9 times\">9 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Tue, 01 Aug 2023 17:47:43 +0000"
1131,World-Wide Camera Networks,https://ieeetv.ieee.org/video/world-wide-camera-networks,"<span>World-Wide Camera Networks</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/worldwidecameranetworks.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Mon, 04/24/2023 - 08:29</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 21 times\">21 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Mon, 24 Apr 2023 15:29:21 +0000"
1132,Security in Quantum Computing Era,https://ieeetv.ieee.org/video/security-in-quantum-computing-era,"<span>Security in Quantum Computing Era</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/SECURITYINQUANTUMCOMUTINGERA.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Tue, 03/07/2023 - 07:21</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 26 times\">26 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Tue, 07 Mar 2023 15:21:04 +0000"
1133,IEEE Quarter Tech Talk Table 8.0 (Part 1) | IEEE QT3 Series,https://ieeetv.ieee.org/channels/computer-society/ieee-quarter-tech-talk-table-80-part-1-ieee-qt3-series,"<span>IEEE Quarter Tech Talk Table 8.0 (Part 1) | IEEE QT3 Series</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/part1thumb.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Tue, 01/24/2023 - 11:46</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 403 times\">403 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Tue, 24 Jan 2023 19:46:52 +0000"
1134,"SSCI2022 - Plenary Talk 1 - Federated Learning, Knowledge Transfer, and Knowledge Distillation: At the Junction of Green Machine Learning and Granular Computing",https://ieeetv.ieee.org/video/ssci2022-plenary-talk-1-federated-learning-knowledge-transfer-and-knowledge-distillation-at-the-junction-of-green-machine-learning-and-granular-computing,"<span>SSCI2022 - Plenary Talk 1 - Federated Learning, Knowledge Transfer, and Knowledge Distillation: At the Junction of Green Machine Learning and Granular Computing</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/TALK1.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Thu, 02/16/2023 - 14:06</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 41 times\">41 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Thu, 16 Feb 2023 22:06:31 +0000"
1135,"SSCI2022 - Plenary Talk 2 - Evolutionary Machine Learning: Research, Applications and Challenges",https://ieeetv.ieee.org/video/ssci2022-plenary-talk-2-evolutionary-machine-learning-research-applications-and-challenges,"<span>SSCI2022 - Plenary Talk 2 - Evolutionary Machine Learning: Research, Applications and Challenges</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/TALK2.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Thu, 02/16/2023 - 14:06</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 24 times\">24 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Thu, 16 Feb 2023 22:06:04 +0000"
1136,SSCI2022 - Plenary Talk 3 - Composing Music by Evolutionary Computation,https://ieeetv.ieee.org/video/ssci2022-plenary-talk-3-composing-music-by-evolutionary-computation,"<span>SSCI2022 - Plenary Talk 3 - Composing Music by Evolutionary Computation</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/TALK3.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Thu, 02/16/2023 - 14:05</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 8 times\">8 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Thu, 16 Feb 2023 22:05:41 +0000"
1137,"SSCI2022 - Plenary Talk 4 - Learning with DNNs: Optimizer, Architecture and Formulation",https://ieeetv.ieee.org/video/ssci2022-plenary-talk-4-learning-with-dnns-optimizer-architecture-and-formulation,"<span>SSCI2022 - Plenary Talk 4 - Learning with DNNs: Optimizer, Architecture and Formulation</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/TALK4.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Thu, 02/16/2023 - 13:50</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 11 times\">11 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Thu, 16 Feb 2023 21:50:18 +0000"
1138,SSCI2022 - Plenary Talk 5 - Lifelong Learning on the Edge with Adaptive Resonance Theory and Incremental Cluster Validity Indices,https://ieeetv.ieee.org/video/ssci2022-plenary-talk-5-lifelong-learning-on-the-edge-with-adaptive-resonance-theory-and-incremental-cluster-validity-indices,"<span>SSCI2022 - Plenary Talk 5 - Lifelong Learning on the Edge with Adaptive Resonance Theory and Incremental Cluster Validity Indices</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/TALK5.png\" width=\"185\" /></div>
      
<span><span>n.lehotzky@ieee.org</span></span>

<span>Thu, 02/16/2023 - 13:30</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 68 times\">68 views</a>",[{'email': 'n.lehotzky@ieee.org'}],"Thu, 16 Feb 2023 21:30:59 +0000"
1139,Democratizing NLP: Considerations From Resources To Algorithms,https://ieeetv.ieee.org/video/democratizing-nlp,"<span>Democratizing NLP: Considerations From Resources To Algorithms</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Democratizing%20NLP.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Tue, 12/20/2022 - 13:41</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 45 times\">45 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Tue, 20 Dec 2022 21:41:40 +0000"
1140,IEEE ICDL 2022 Keynote 7 - Modeling the Learning Circuits of the Insect Brain,https://ieeetv.ieee.org/video/ieee-icdl-2022-keynote-7,"<span>IEEE ICDL 2022 Keynote 7 - Modeling the Learning Circuits of the Insect Brain</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/IEEE%20ICDL%202022%20Keynote%207.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Thu, 12/15/2022 - 10:25</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 15 times\">15 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Thu, 15 Dec 2022 18:25:56 +0000"
1141,IEEE ICDL 2022 Keynote 6 - The Mind of a Bee,https://ieeetv.ieee.org/video/ieee-icdl-2022-keynote-6,"<span>IEEE ICDL 2022 Keynote 6 - The Mind of a Bee</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/IEEE%20ICDL%202022%20Keynote%206.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Thu, 12/15/2022 - 09:59</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 6 times\">6 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Thu, 15 Dec 2022 17:59:27 +0000"
1142,IEEE ICDL 2022 Keynote 5 - Towards a Conscious Machine,https://ieeetv.ieee.org/video/ieee-icdl-2022-keynote-5,"<span>IEEE ICDL 2022 Keynote 5 - Towards a Conscious Machine</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/IEEE%20ICDL%202022%20Keynote%205.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Thu, 12/15/2022 - 09:56</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 7 times\">7 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Thu, 15 Dec 2022 17:56:33 +0000"
1143,IEEE ICDL 2022 Keynote 4 - The Transformative Power of Modern AI Methods,https://ieeetv.ieee.org/video/ieee-icdl-2022-keynote-4,"<span>IEEE ICDL 2022 Keynote 4 - The Transformative Power of Modern AI Methods</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/IEEE%20ICDL%202022%20Keynote%204.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Thu, 12/15/2022 - 09:51</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 13 times\">13 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Thu, 15 Dec 2022 17:51:59 +0000"
1144,SPCC SIG Virtual Seminar: Recent Advances in Non-Orthogonal Multiple Access in 6G Wireless Networks,https://ieeetv.ieee.org/video/spcc-sig-virtual-seminar-recent-advances-in-non-orthogonal-multiple-access-in-6g-wireless-networks,"<span>SPCC SIG Virtual Seminar: Recent Advances in Non-Orthogonal Multiple Access in 6G Wireless Networks  </span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/SPCC%20SIG%20Virtual%20Seminar-%20Recent%20Advances.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Tue, 01/18/2022 - 09:01</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 23 times\">23 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Tue, 18 Jan 2022 17:01:12 +0000"
1145,Darryl Burton & John Greaves: Fireside Chat on Blockchain Elevated Health Care Regulatory Impact,https://ieeetv.ieee.org/future_directions/darryl-burton-john-greaves-fireside-chat-on-blockchain-elevated-health-care-regulatory-impact,"<span>Darryl Burton & John Greaves: Fireside Chat on Blockchain Elevated Health Care Regulatory Impact</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Darryl%20Burton%20%26%20John%20Greaves-%20Fireside%20Chat%20on%20Blockchain.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Tue, 09/14/2021 - 06:24</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 14 times\">14 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Tue, 14 Sep 2021 13:24:50 +0000"
1146,Sandra Ro & John Greaves: Fireside Chat on Blockchain Elevated Health Care Standards,https://ieeetv.ieee.org/future_directions/sandra-ro-john-greaves-fireside-chat-on-blockchain-elevated-health-care-standards,"<span>Sandra Ro & John Greaves: Fireside Chat on Blockchain Elevated Health Care Standards</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Sandra%20Ro%20%26%20John%20Greaves-%20Fireside%20Chat%20on%20Blockchain.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Tue, 09/14/2021 - 06:10</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 17 times\">17 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Tue, 14 Sep 2021 13:10:25 +0000"
1147,John Palfreyman & John Greaves: Fireside Chat on Enterprises Blockchain in Health care,https://ieeetv.ieee.org/future_directions/john-palfreyman-john-greaves-fireside-chat-on-enterprises-blockchain-in-health-care,"<span>John Palfreyman & John Greaves: Fireside Chat on Enterprises Blockchain in Health care</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/John%20Palfreyman%20%26%20John%20Greaves-%20Fireside%20Chat%20on%20Enterprises%20Blockchain%20in%20Health%20care.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Tue, 09/14/2021 - 05:21</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 12 times\">12 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Tue, 14 Sep 2021 12:21:28 +0000"
1148,Aman Khera & John Greaves: Fireside Chat on Blockchain Elevated Health Care,https://ieeetv.ieee.org/future_directions/aman-khera-john-greaves-fireside-chat-on-blockchain-elevated-health-care,"<span>Aman Khera & John Greaves: Fireside Chat on Blockchain Elevated Health Care</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Aman%20Khera%20%26%20John%20Greaves-%20Fireside%20Chat.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Tue, 09/14/2021 - 04:59</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 29 times\">29 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Tue, 14 Sep 2021 11:59:34 +0000"
1149,Gora Datta - Fireside Chat - IEEE Healthcare: Blockchain & AI Virtual Series,https://ieeetv.ieee.org/future_directions/gora-datta-ieee-healthcare-blockchain-ai-virtual-series-session-9,"<span>Gora Datta - Fireside Chat - IEEE Healthcare: Blockchain & AI Virtual Series</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/John%20Greaves-%20Fireside%20Chat%20on%20Present%20%26%20Future%20Trends%20for%20Blockchain%20Elevated%20Healthcare_0.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Tue, 09/14/2021 - 04:35</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 50 times\">50 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Tue, 14 Sep 2021 11:35:10 +0000"
1150,IEEE Healthcare Blockchain AI Virtual Forum Session 6 Seg4 Panel3,https://ieeetv.ieee.org/video/ieee-healthcare-blockchain-ai-virtual-forum-session-6-seg4-panel3,"<span>IEEE Healthcare Blockchain AI Virtual Forum Session 6 Seg4 Panel3</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/IEEE%20Healthcare%20Blockchain%20AI%20Virtual%20Forum%20Session%206%20Seg4%20Panel3.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Fri, 04/23/2021 - 15:37</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 16 times\">16 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Fri, 23 Apr 2021 22:37:37 +0000"
1151,IEEE Healthcare Blockchain AI Virtual Forum Session 6 Seg3 Panel2,https://ieeetv.ieee.org/video/ieee-healthcare-blockchain-ai-virtual-forum-session-6-seg3-panel2,"<span>IEEE Healthcare Blockchain AI Virtual Forum Session 6 Seg3 Panel2</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/IEEE%20Healthcare%20Blockchain%20and%20AI-%20Future%20of%20Work%20and%20Certification%20Panel%202.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Fri, 04/23/2021 - 15:31</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 19 times\">19 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Fri, 23 Apr 2021 22:31:54 +0000"
1152,IEEE Healthcare Blockchain AI Session6 Seg2 Panel1,https://ieeetv.ieee.org/video/ieee-healthcare-blockchain-ai-session6-seg2-panel1,"<span>IEEE Healthcare Blockchain AI Session6 Seg2 Panel1</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/IEEE%20Healthcare%20Blockchain%20AI%20Session6%20Seg2%20Panel1.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Fri, 04/23/2021 - 15:24</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 21 times\">21 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Fri, 23 Apr 2021 22:24:43 +0000"
1153,Preparing Teachers for a Quantum Tomorrow,https://ieeetv.ieee.org/video/preparing-teachers-for-a-quantum-tomorrow,"<span>Preparing Teachers for a Quantum Tomorrow</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Preparing%20Teachers%20for%20a%20Quantum%20Tomorrow.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Fri, 04/23/2021 - 12:33</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 47 times\">47 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Fri, 23 Apr 2021 19:33:50 +0000"
1154,Process to Check Your IEEE Membership Number,https://ieeetv.ieee.org/video/process-to-check-your-ieee-membership-number,"<span>Process to Check Your IEEE Membership Number</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Process%20to%20Check%20Your%20IEEE%20Membership%20Number.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Wed, 09/29/2021 - 10:21</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 1373 times\">1373 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Wed, 29 Sep 2021 17:21:18 +0000"
1155,Registration Process to Become a Professional Member,https://ieeetv.ieee.org/video/registration-process-to-become-a-professional-member,"<span>Registration Process to Become a Professional Member</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Registration%20Process%20to%20Become%20a%20Professional%20Member.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Wed, 09/29/2021 - 10:04</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 625 times\">625 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Wed, 29 Sep 2021 17:04:13 +0000"
1156,Registration Process to Become a Student Member,https://ieeetv.ieee.org/video/registration-process-to-become-a-student-member,"<span>Registration Process to Become a Student Member</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Registration%20Process%20to%20Become%20a%20Student%20Member.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Wed, 09/29/2021 - 09:37</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 1428 times\">1428 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Wed, 29 Sep 2021 16:37:20 +0000"
1157,IEEE Brain Neurotech Entrepreneurs Workshop: Panel Discussion,https://ieeetv.ieee.org/ieee-brain-neurotech-entrepreneurs-workshop-panel-discussion,"<span>IEEE Brain Neurotech Entrepreneurs Workshop: Panel Discussion</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/133478.jpg\" width=\"185\" /></div>
      
<span><span>lbboiko@handma…</span></span>

<span>Mon, 09/07/2020 - 05:31</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 32 times\">32 views</a>",[{'email': 'lbboiko@handmadeinteractive.com'}],"Mon, 07 Sep 2020 12:31:13 +0000"
1158,IEEE Brain: Funding Your Neurotech Start-Up with NIH SBIR/STTR Awards,https://ieeetv.ieee.org/ieee-brain-funding-your-neurotech-start-up-with-nih-sbir-sttr-awards,"<span>IEEE Brain: Funding Your Neurotech Start-Up with NIH SBIR/STTR Awards </span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/133333.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Sun, 06/28/2020 - 23:39</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 57 times\">57 views</a>",[{'name': 'admin'}],"Mon, 29 Jun 2020 06:39:44 +0000"
1159,IEEE Brain: Raising Capital From Angel Investors,https://ieeetv.ieee.org/ieee-brain-raising-capital-from-angel-investors,"<span>IEEE Brain: Raising Capital From Angel Investors</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/133326.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Sun, 06/28/2020 - 23:38</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 37 times\">37 views</a>",[{'name': 'admin'}],"Mon, 29 Jun 2020 06:38:45 +0000"
1160,IEEE Brain: Abbott Neuromodulation - The Importance and Practice of Late-stage Innovation,https://ieeetv.ieee.org/ieee-brain-abbott-neuromodulation-the-importance-and-practice-of-late-stage-innovation,"<span>IEEE Brain: Abbott Neuromodulation - The Importance and Practice of Late-stage Innovation</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/133322.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Sun, 06/28/2020 - 23:38</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 109 times\">109 views</a>",[{'name': 'admin'}],"Mon, 29 Jun 2020 06:38:45 +0000"
1161,IEEE Brain: Platypus Neuro : Applied Neuroscience and Optimized Human Performance at Scale,https://ieeetv.ieee.org/ieee-brain-platypus-neuro-applied-neuroscience-and-optimized-human-performance-at-scale,"<span>IEEE Brain: Platypus Neuro : Applied Neuroscience and Optimized Human Performance at Scale</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/133321.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Sun, 06/28/2020 - 23:38</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 74 times\">74 views</a>",[{'name': 'admin'}],"Mon, 29 Jun 2020 06:38:45 +0000"
1162,"IEEE Brain: Backyard Brains: How We Bootstrapped a Low-Fi, High-Tech Neuroscience Company",https://ieeetv.ieee.org/ieee-brain-backyard-brains-how-we-bootstrapped-a-low-fi-high-tech-neuroscience-company,"<span>IEEE Brain: Backyard Brains: How We Bootstrapped a Low-Fi, High-Tech Neuroscience Company</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/133320.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Sun, 06/28/2020 - 23:38</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 31 times\">31 views</a>",[{'name': 'admin'}],"Mon, 29 Jun 2020 06:38:45 +0000"
1163,IEEE Brain: Nia Therapeutics: Building an Early-stage Medical Device Business,https://ieeetv.ieee.org/ieee-brain-nia-therapeutics-building-an-early-stage-medical-device-business,"<span>IEEE Brain: Nia Therapeutics: Building an Early-stage Medical Device Business</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/133319.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Sun, 06/28/2020 - 23:38</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 192 times\">192 views</a>",[{'name': 'admin'}],"Mon, 29 Jun 2020 06:38:45 +0000"
1164,Opportunities in the Internet of Things,https://ieeetv.ieee.org/careers/opportunities-in-the-internet-of-things,"<span>Opportunities in the Internet of Things</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/132561.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Mon, 12/16/2019 - 23:49</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 512 times\">512 views</a>",[{'name': 'admin'}],"Tue, 17 Dec 2019 07:49:03 +0000"
1165,Fengrui Shi: Game Theoretic and Auction-based Algorithms Towards Opportunistic Edge-Processing in LPWA LoRa Networks: WF-IoT 2016,https://ieeetv.ieee.org/fengrui-shi-game-theoretic-and-auction-based-algorithms-towards-opportunistic-edge-processing-in-lpwa-lora-networks-wf-iot-2016,"<span>Fengrui Shi: Game Theoretic and Auction-based Algorithms Towards Opportunistic Edge-Processing in LPWA LoRa Networks: WF-IoT 2016</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/131671.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Tue, 02/20/2018 - 03:43</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 639 times\">639 views</a>",[{'name': 'admin'}],"Tue, 20 Feb 2018 11:43:19 +0000"
1166,Levente Klein: Drone-based Reconstruction for 3D Geospatial Data Processing: WF-IoT 2016,https://ieeetv.ieee.org/levente-klein-drone-based-reconstruction-for-3d-geospatial-data-processing-wf-iot-2016,"<span>Levente Klein: Drone-based Reconstruction for 3D Geospatial Data Processing: WF-IoT 2016</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/131670.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Tue, 02/20/2018 - 03:43</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 644 times\">644 views</a>",[{'name': 'admin'}],"Tue, 20 Feb 2018 11:43:19 +0000"
1167,Fengrui Shi: OppNet: Enabling Citizen-Centric Urban IoT Data Collection Through Opportunistic Connectivity Service: WF-IoT 2016,https://ieeetv.ieee.org/fengrui-shi-oppnet-enabling-citizen-centric-urban-iot-data-collection-through-opportunistic-connectivity-service-wf-iot-2016,"<span>Fengrui Shi: OppNet: Enabling Citizen-Centric Urban IoT Data Collection Through Opportunistic Connectivity Service: WF-IoT 2016</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/131669.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Tue, 02/20/2018 - 03:43</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 572 times\">572 views</a>",[{'name': 'admin'}],"Tue, 20 Feb 2018 11:43:19 +0000"
1168,Compressive Sensing Tutorial: A Game Changing Technology for Energy Efficient IoT Sensor Networks: WF-IoT 2016,https://ieeetv.ieee.org/compressive-sensing-tutorial-a-game-changing-technology-for-energy-efficient-iot-sensor-networks-wf-iot-2016,"<span>Compressive Sensing Tutorial: A Game Changing Technology for Energy Efficient IoT Sensor Networks: WF-IoT 2016</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/131668.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Tue, 02/20/2018 - 03:43</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 867 times\">867 views</a>",[{'name': 'admin'}],"Tue, 20 Feb 2018 11:43:19 +0000"
1169,Fengrui Shi: Knowledge Co-creation in the OrganiCity: Data Annotation with JAMAiCA,https://ieeetv.ieee.org/fengrui-shi-knowledge-co-creation-in-the-organicity-data-annotation-with-jamaica,"<span>Fengrui Shi: Knowledge Co-creation in the OrganiCity: Data Annotation with JAMAiCA</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/131667.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Tue, 02/20/2018 - 03:43</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 553 times\">553 views</a>",[{'name': 'admin'}],"Tue, 20 Feb 2018 11:43:19 +0000"
1170,Q&A: Pindar Wong Keynote: WF-IoT 2016,https://ieeetv.ieee.org/q-and-a-pindar-wong-keynote-wf-iot-2016,"<span>Q&amp;A: Pindar Wong Keynote: WF-IoT 2016</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/131660.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Tue, 02/20/2018 - 03:43</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 453 times\">453 views</a>",[{'name': 'admin'}],"Tue, 20 Feb 2018 11:43:19 +0000"
1171,Keynote Pindar Wong: Calculating Consensus Reality For an Internet of Trust: WF-IoT 2016,https://ieeetv.ieee.org/keynote-pindar-wong-calculating-consensus-reality-for-an-internet-of-trust-wf-iot-2016,"<span>Keynote Pindar Wong: Calculating Consensus Reality For an Internet of Trust: WF-IoT 2016</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/131659.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Tue, 02/20/2018 - 03:43</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 518 times\">518 views</a>",[{'name': 'admin'}],"Tue, 20 Feb 2018 11:43:19 +0000"
1172,Q&A: Paul Mockapetris Keynote: WF-IoT 2016,https://ieeetv.ieee.org/q-and-a-paul-mockapetris-keynote-wf-iot-2016,"<span>Q&amp;A: Paul Mockapetris Keynote: WF-IoT 2016</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/131658.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Tue, 02/20/2018 - 03:43</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 433 times\">433 views</a>",[{'name': 'admin'}],"Tue, 20 Feb 2018 11:43:19 +0000"
1173,Keynote Paul Mockapetris: The Internet: How the Most Useful Tool in the World Became Dangerous: WF-IoT 2016,https://ieeetv.ieee.org/keynote-paul-mockapetris-the-internet-how-the-most-useful-tool-in-the-world-became-dangerous-wf-iot-2016,"<span>Keynote Paul Mockapetris: The Internet: How the Most Useful Tool in the World Became Dangerous: WF-IoT 2016</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/131657.jpg\" width=\"185\" /></div>
      
<span><span>admin</span></span>

<span>Tue, 02/20/2018 - 03:43</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 437 times\">437 views</a>",[{'name': 'admin'}],"Tue, 20 Feb 2018 11:43:19 +0000"
1174,Assistive Robots for People with Impairments,https://ieeetv.ieee.org/video/assistive-robots-for-people-with-impairments,"<span>Assistive Robots for People with Impairments</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Screen%20Shot%202022-12-20%20at%209.11.02%20AM.png\" width=\"185\" /></div>
      
<span><span>r.umali@ieee.org</span></span>

<span>Tue, 12/20/2022 - 05:59</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 233 times\">233 views</a>",[{'email': 'r.umali@ieee.org'}],"Tue, 20 Dec 2022 13:59:10 +0000"
1175,ROBUST DEPTH ESTIMATION FOR ROBOTS WITH STEREO CAMERAS USING BESPOKE DNNS,https://ieeetv.ieee.org/video/robust-depth-estimation-for-robots-with-stereo-cameras-using-bespoke-dnns,"<span>ROBUST DEPTH ESTIMATION FOR ROBOTS WITH STEREO CAMERAS USING BESPOKE DNNS</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Screen%20Shot%202022-12-20%20at%209.18.54%20AM.png\" width=\"185\" /></div>
      
<span><span>r.umali@ieee.org</span></span>

<span>Tue, 12/20/2022 - 05:57</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 207 times\">207 views</a>",[{'email': 'r.umali@ieee.org'}],"Tue, 20 Dec 2022 13:57:40 +0000"
1176,How Humanoid General Purpose Robots Can Solve the World’s Labor Crisis,https://ieeetv.ieee.org/video/how-humanoid-general-purpose-robots-can-solve-the-worlds-labor-crisis,"<span>How Humanoid General Purpose Robots Can Solve the World’s Labor Crisis</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/How%20Humanoid%20General%20Purpose%20Robots%20Can%20Solve.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Fri, 07/01/2022 - 13:31</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 268 times\">268 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Fri, 01 Jul 2022 20:31:22 +0000"
1177,"Recent Advances in ROS 2, SCV Robotics & Automation Society Chapter Meeting",https://ieeetv.ieee.org/video/recent-advances-in-ros-2-scv-robotics-automation-society-chapter-meeting,"<span>Recent Advances in ROS 2, SCV Robotics & Automation Society Chapter Meeting</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Recent-Advances-in-ROS-2.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Thu, 05/12/2022 - 06:56</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 92 times\">92 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Thu, 12 May 2022 13:56:14 +0000"
1178,Open-Source Dynamic Server & Modular Controller Package,https://ieeetv.ieee.org/video/open-source-dynamic-server-modular-controller-package,"<span>Open-Source Dynamic Server & Modular Controller Package</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Open-Source%20Dynamic%20Server%20%26%20Modular%20Controller%20Package.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Mon, 04/04/2022 - 11:25</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 51 times\">51 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Mon, 04 Apr 2022 18:25:34 +0000"
1179,"Telepresence & How It Is Changing Our Society,  January 2022 SCV RAS Chapter Meeting",https://ieeetv.ieee.org/video/telepresence-and-how-it-is-changing-our-society-january-2022-scv-ras-chapter-meeting,"<span>Telepresence & How It Is Changing Our Society,  January 2022 SCV RAS Chapter Meeting</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/Telepresence%20How%20It%20Is%20Changing%20Our%20Society%20January%202022.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Mon, 02/14/2022 - 11:33</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 57 times\">57 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Mon, 14 Feb 2022 19:33:18 +0000"
1180,ICBNA 2022,https://ieeetv.ieee.org/video/icbna-2022,"<span>ICBNA 2022</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/ICBNA%202022_0.png\" width=\"185\" /></div>
      
<span><span>ingrid@ingridm…</span></span>

<span>Fri, 02/04/2022 - 09:43</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 71 times\">71 views</a>",[{'email': 'ingrid@ingridmarie.com'}],"Fri, 04 Feb 2022 17:43:54 +0000"
1181,Robotics History: Narratives and Networks Oral Histories: Steve Cousins,https://ieeetv.ieee.org/channels/ras/robotics-history-narratives-and-networks-oral-histories-steve-cousins,"<span>Robotics History: Narratives and Networks Oral Histories: Steve Cousins</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/initial/medium/0.jpg\" width=\"185\" /></div>
      
<span><span>alexis.simoes@…</span></span>

<span>Tue, 11/23/2021 - 08:07</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 40 times\">40 views</a>",[{'email': 'alexis.simoes@ieee.org'}],"Tue, 23 Nov 2021 16:07:42 +0000"
1182,Robotics History: Narratives and Networks Oral Histories: Lydia Kavraki,https://ieeetv.ieee.org/channels/ras/robotics-history-narratives-and-networks-oral-histories-lydia-kavraki,"<span>Robotics History: Narratives and Networks Oral Histories: Lydia Kavraki</span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img alt=\"\" class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/initial/medium/0.jpg\" width=\"185\" /></div>
      
<span><span>alexis.simoes@…</span></span>

<span>Tue, 11/23/2021 - 06:35</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 0 times\">0 views</a>",[{'email': 'alexis.simoes@ieee.org'}],"Tue, 23 Nov 2021 14:35:57 +0000"
1183,Future of Robotics,https://ieeetv.ieee.org/channels/societies/future-of-robotics-,"<span>Future of Robotics </span>

            <div class=\"field field--name-field-video-medium-image field--type-image field--label-hidden field--item\">  <img class=\"img-responsive\" height=\"104\" src=\"https://ieeetv.ieee.org/assets/video-images/medium/future%20of%20robotics%20thumbnail.png\" width=\"185\" /></div>
      
<span><span>alexis.simoes@…</span></span>

<span>Tue, 11/09/2021 - 12:04</span>
<a class=\"view-counter\" href=\"https://ieeetv.ieee.org/\" title=\"Video has been viewed 6124 times\">6124 views</a>",[{'email': 'alexis.simoes@ieee.org'}],"Tue, 09 Nov 2021 20:04:20 +0000"
1184,Shrimper — A Small Search Engine Crafted in Rust,https://medium.com/artificialis/shrimper-a-small-search-engine-crafted-in-rust-577734fb592f?source=rss----a81c8d170222---4,"<h3>Shrimper — A Small Search Engine Crafted in Rust</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*H9IUzZ3fGZmvu4ug1G8XUA.png\" /><figcaption>image by some robot</figcaption></figure><p>INTRODUCTION</p><p>Creating a search engine in Rust is an excellent way to start exploring the language’s strengths in performance and safety.</p><p>This project transitions indexing and searching concepts into Rust’s ecosystem, challenging but rewarding due to Rust’s unique syntax and paradigms.</p><p>We’d start by setting up the Rust environment, including essential tools and dependencies. Then, defining data models using structs and Rust crates tantivy for indexing/searching and serde for serialization. Through implementing a basic search engine, you'll learn to manage indexing and execute search queries!</p><h3>OUTLINE &amp; REQUIREMENTS:</h3><h3>1. Setup Environment</h3><ul><li>Rust and Cargo (Rust’s package manager and build system) installed. If not, you can install them from the official Rust website.</li></ul><h3>2. Project Setup</h3><p>Create a new Rust project:</p><pre>cargo new shrimp_engine<br />cd shrimp_engine</pre><h3>3. Dependencies</h3><p>You might need a few crates (Rust libraries) to help with parsing and data handling. For example:</p><ul><li><strong>tantivy</strong> for indexing and searching text (similar to Lucene in the Java world).</li><li><strong>serde</strong> and <strong>serde_json</strong> for JSON parsing if your data is in JSON format.</li></ul><p>Add these to your <strong>Cargo.toml</strong> file:</p><pre>[package]<br />name = &quot;shrimp-engine&quot;<br />version = &quot;0.1.0&quot;<br />edition = &quot;2021&quot;<br /><br />[dependencies]<br />tantivy = &quot;0.17&quot;<br />serde = &quot;1.0&quot;<br />serde_json = &quot;1.0&quot;</pre><h3>4. Define Data Structure</h3><p>Decide on the structure of the documents you’ll be indexing. For a basic example, consider a simple struct representing documents with a title and body.</p><pre>use serde::{Serialize, Deserialize};</pre><pre>#[derive(Serialize, Deserialize, Debug)]<br />struct Document {<br />    title: String,<br />    body: String,<br />}</pre><h3>5 . The Index</h3><p>Using <strong>tantivy</strong>, create an index schema based on your data structure, and then add documents to the index.</p><pre>rustCopy code<br />use tantivy::{schema::*, Index, doc};</pre><pre>fn create_index() -&gt; tantivy::Result&lt;()&gt; {<br />    // Define the schema<br />    let mut schema_builder = Schema::builder();<br />    schema_builder.add_text_field(&quot;title&quot;, TEXT | STORED);<br />    schema_builder.add_text_field(&quot;body&quot;, TEXT);<br />    let schema = schema_builder.build();</pre><pre>    // Create the index in a directory<br />    let index = Index::create_in_ram(schema.clone());</pre><pre>    // Get the index writer<br />    let mut index_writer = index.writer(50_000_000)?;</pre><pre>    // Add documents<br />    let title = schema.get_field(&quot;title&quot;).unwrap();<br />    let body = schema.get_field(&quot;body&quot;).unwrap();</pre><pre>    // Example document<br />    let doc = doc!(title =&gt; &quot;Example Title&quot;, body =&gt; &quot;This is the body of the document.&quot;);<br />    index_writer.add_document(doc)?;</pre><pre>    // Commit the documents to the index<br />    index_writer.commit()?;</pre><pre>    Ok(())<br />}</pre><h3>6. Searching</h3><p>Implement a function to search the index.</p><p>You’ll need to create <strong>a searcher</strong> and <strong>query parser.</strong></p><pre>use tantivy::query::QueryParser;<br />use tantivy::collector::TopDocs;</pre><pre>fn search_index(index: &amp;Index, query_str: &amp;str) -&gt; tantivy::Result&lt;()&gt; {<br />    let reader = index.reader()?;<br />    let searcher = reader.searcher();</pre><pre>    let schema = index.schema();<br />    let title = schema.get_field(&quot;title&quot;).unwrap();<br />    let body = schema.get_field(&quot;body&quot;).unwrap();<br />    let query_parser = QueryParser::for_index(&amp;index, vec![title, body]);</pre><pre>    let query = query_parser.parse_query(query_str)?;<br />    let top_docs = searcher.search(&amp;query, &amp;TopDocs::with_limit(10))?;</pre><pre>    for (_, doc_address) in top_docs {<br />        let retrieved_doc = searcher.doc(doc_address)?;<br />        println!(&quot;{:?}&quot;, retrieved_doc);<br />    }</pre><pre>    Ok(())<br />}</pre><h3>Putting it all together 🍤</h3><p>Now, let’s combine the indexing and searching into a main function, where we can modify the documents, the index, and queries:</p><pre>use serde::{Serialize, Deserialize};<br />use tantivy::{schema::*, Index, doc, query::QueryParser, collector::TopDocs, TantivyError};<br /><br /><br /><br /><br />#[derive(Serialize, Deserialize, Debug)]<br />struct Document {<br />    title: String,<br />    body: String,<br />}<br /><br />fn create_index() -&gt; Result&lt;Index, TantivyError&gt; {<br />    let mut schema_builder = Schema::builder();<br />    schema_builder.add_text_field(&quot;title&quot;, TEXT | STORED);<br />    schema_builder.add_text_field(&quot;body&quot;, TEXT);<br />    let schema = schema_builder.build();<br /><br />    let index = Index::create_in_ram(schema.clone());<br /><br />    let mut index_writer = index.writer(50_000_000)?;<br />    let title = schema.get_field(&quot;title&quot;).unwrap();<br />    let body = schema.get_field(&quot;body&quot;).unwrap();<br />    let doc = doc!(title =&gt; &quot;Example Title&quot;, body =&gt; &quot;the body of the document.&quot;);<br />    index_writer.add_document(doc)?;<br />    index_writer.commit()?;<br /><br />    Ok(index)<br />}<br /><br />fn search_index(index: &amp;Index, query_str: &amp;str) -&gt; Result&lt;(), TantivyError&gt; {<br />    let reader = index.reader()?;<br />    let searcher = reader.searcher();<br /><br />    let schema = index.schema();<br />    let title = schema.get_field(&quot;title&quot;).unwrap();<br />    let body = schema.get_field(&quot;body&quot;).unwrap();<br />    let query_parser = QueryParser::for_index(&amp;index, vec![title, body]);<br /><br />    let query = query_parser.parse_query(query_str)?;<br />    let top_docs = searcher.search(&amp;query, &amp;TopDocs::with_limit(10))?;<br /><br />    for (_, doc_address) in top_docs {<br />        let retrieved_doc = searcher.doc(doc_address)?;<br />        println!(&quot;{:?}&quot;, retrieved_doc);<br />    }<br /><br />    Ok(())<br />}<br /><br />fn main() -&gt; Result&lt;(), TantivyError&gt; {<br />    println!(&quot;Hello, Shrimp!&quot;);<br /><br />    // Create the index and store it<br />    let index = create_index()?;<br /><br />    // Search within the created index<br />    search_index(&amp;index, &quot;Example&quot;)?;<br /><br />    Ok(())<br />}</pre><p>Let’s break down the crucial components and their roles in the system:</p><h4>Serde</h4><ul><li>serde::{Serialize, Deserialize}: These traits allow for the easy conversion of Rust structs to and from a format suitable for saving (like JSON), which is essential for working with data that needs to be indexed or retrieved.</li></ul><h4>Tantivy</h4><ul><li>tantivy::{schema::*, Index, doc, query::QueryParser, collector::TopDocs, TantivyError}:</li></ul><p>The components from the tantivy crate are used for building the search engine's core functionality, from creating an index to querying it.</p><h4>Document Struct</h4><ul><li>Document Struct: Represents the data structure for documents to be indexed. Each document has a title and a body, mimicking a simple webpage or document in a real-world search engine.</li></ul><h4>the Schema</h4><p><strong>The schema</strong> defines the structure of the index, specifying which fields (here, title and body) should be indexed and how (e.g., stored, text-analyzed). An in-memory index is created, and documents are added to this index. Each document added is defined by the Document struct, which is then serialized for indexing. Changes are committed to the index, making it searchable.</p><h3>The Shrimps’ Core Mechanism :</h3><h4>1- Index Reader and Searcher:</h4><p>To search the index, an index reader is instantiated, creating a searcher capable of executing queries against the index.</p><h4><strong>2- Query Parsing and Execution</strong></h4><p>A query parser interprets a query string, transforming it into a query object based on the defined schema. The searcher then uses this query to find and rank relevant documents.</p><h4>3- Retrieving and Displaying Results</h4><p>The top matching documents (up to a limit) are retrieved and displayed. The ability to extract and review indexed content based on search queries.</p><h3>Main Function</h3><p>The main function ties everything together, first creating an index with at least one document and then performing a search within this index.</p><p>The simplicity of this setup demonstrates a fully functional search engine capable of indexing and searching text 🍤</p><h3>Key Takeaways</h3><ul><li>The use of tantivy for indexing and searching provides a Rust-centric approach to text search, which offers high performance and safety.</li><li>serde's role ensures that complex data structures can be easily managed, serialized, and deserialized within the Rust ecosystem.</li><li>This example serves as a foundational framework, illustrating how Rust can be used to build search solutions 🍤</li></ul><h3>Conclusion</h3><p>This example is intended to give you a starting point in search engine construction. Rust’s ownership and concurrency model, along with its type system, provide a robust foundation for building more complex and high-performance search engines.</p><p>We can expand this project by adding features like real-time indexing, advanced text processing, and custom scoring algorithms. Expect those features in the series of articles dedicated to search engines and information retrieval — all in Rust 🍤</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=577734fb592f\" width=\"1\" /><hr /><p><a href=\"https://medium.com/artificialis/shrimper-a-small-search-engine-crafted-in-rust-577734fb592f\">Shrimper — A Small Search Engine Crafted in Rust</a> was originally published in <a href=\"https://medium.com/artificialis\">Artificialis</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",[{'name': 'Iva @ AkashaDB'}],"Mon, 12 Feb 2024 16:20:09 GMT"
1185,How to Evaluate an LLM's Ability to Follow Instructions,https://medium.com/artificialis/how-to-evaluate-an-llms-ability-to-follow-instructions-9c6ac57a8e22?source=rss----a81c8d170222---4,"<h4>Assessing the Impact of Decoding Strategies on the Instruction Following Evaluation for Large Language Models Benchmark</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*V4QdGHrrt9ncmNok\" /><figcaption>Photo by <a href=\"https://unsplash.com/@paus_d_?utm_source=medium&amp;utm_medium=referral\">Sean D</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Recently I’ve been intellectually obsessed with two things:</p><ol><li>How do models generate text? (Trying to grok how various LLM decoding strategies impact the resulting generations)</li><li>And how do we gauge how good they are at it? (The minefield known as LLM evaluation)</li></ol><p>It’s not just idle curiosity. It’s my job.</p><p>I’ve been handed this cool yet daunting task: give the new kid on the block, <a href=\"https://deci.ai/blog/introducing-decilm-7b-the-fastest-and-most-accurate-7b-large-language-model-to-date/\">DeciLM-7B</a>, a thorough vibe check. So, here’s the deal. It’s supposed to stand toe-to-toe with the big gun in town — Mistral-7B-v0.1. That’s our competition.</p><blockquote>Author’s note: I originally wrote all this code prior to the release of Mistral-7B-Instruct-v0.2.</blockquote><p><strong>And me?</strong></p><p>I’m digging deep, comparing these two. Imagine late nights, endless papers, tons of coffee — nine yards of a research binge. Then bam! Call it fate, serendipity, or just good timing — two pieces drop from the AI heavens. One is a deep dive into “I<strong>nstruction-Following Evaluation for Large Language Models</strong>,” and the other is Damien Benveniste’s eye-opener on LLM decoding strategies. Talk about perfect timing.</p><p>That lit a spark.</p><p>Why not mix these up? <strong>Why not use these techniques to put DeciLM-7B and Mistral-7B-v0.1 through their paces? I’m a scientist at heart — exploring, experimenting, and evaluating is what I do!</strong></p><h4>🌉Benchmarks vs. Practical Application</h4><p>Reading performance metrics off a leaderboard is one thing; getting your hands dirty with the actual model is quite another.</p><p>Sure, benchmarks are cool, but they don’t give you the feel or the intuition of how a model <strong><em>actually</em></strong><em> </em>works. To get that, you’ve got to hack around with the model and throw real-world prompts at it — like you’d do in day-to-day tasks.</p><p>That’s where the rubber meets the road.</p><p><strong>How do these LLMs perform when it comes to following instructions, and more importantly, how do you accurately measure that ability?</strong></p><h4>🚀 Instruction-Following Evaluation for Large Language Models</h4><p>Evaluating the capability of Large Language Models (LLMs) to follow instructions isn’t a walk in the park. It’s<a href=\"https://www.aisnakeoil.com/p/evaluating-llms-is-a-minefield?r=dtkzn&amp;utm_campaign=post&amp;utm_medium=web\"> more like a stroll through a minefield</a>.</p><p>Jeffrey Zhou and colleagues highlight this intricate issue in their “<a href=\"https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Farxiv.org%2Fabs%2F2311.07911\">Instruction-Following Evaluation for Large Language Models</a>” paper, which provides a more refined, objective, and detailed framework for assessing LLMs’ abilities to follow instructions.</p><p>I like this benchmark because<strong> it’s concrete and representative of how I would use an LLM</strong>. It feels more like a benchmark for AI Engineers building applications using LLMs. The prompts also do a great job of capturing the nuances of human language, which is filled with ambiguities and subjectivities.</p><p>And these are the types of prompts that make consistent LLM evaluation a pain in the a — I mean, a daunting task.</p><h4>🔍 Existing Evaluation Methods</h4><p>The paper identifies three main methods for evaluating LLMs, each with its pitfalls:</p><ol><li><strong>🧑‍🔬 Human Evaluation: </strong>Traditional yet laden with labour and costs. The subjectivity inherent in human judgment adds a layer of unreliability.</li><li><strong>🤖 Model-Based Evaluation:</strong> This relies on another model’s accuracy to judge the LLM…but what if the judge is flawed?</li><li><strong>📈 Quantitative Benchmarks:</strong> Scalable and standardized, yes, but they can miss the forest for the trees in understanding the subtleties of instruction-following.</li></ol><h4>🌟 Introducing IFEval</h4><p>The authors propose a novel approach, <a href=\"https://arxiv.org/abs/2311.07911\">IFEval</a>, focusing on ‘<strong>verifiable instructions.</strong>’</p><p>These crystal-clear commands leave <strong>no room for subjective interpretation </strong>— think word counts or specific formatting like JSON. They’re atomic instructions that allow you to use a “simple, interpretable, deterministic program to verify whether corresponding responses follow the instructions”.</p><p>This methodology aims at making the evaluation process more automated, accurate, and free from ambiguity.</p><h4>👨🏽‍🔬 Setting the Stage for Experimentation</h4><p>Wrapping my head around Jeffrey Zhou and the team’s insights was just the beginning. I wanted to see these models, DeciLM-7B and Mistral-7B-v0.1, not just walk the walk in a controlled environment but also talk the talk — no pun intended.</p><p>And right then, I stumbled upon something that would add a new layer of interestingness to this experiment.</p><h4>🌌 Inspiration from the AI Edge</h4><p>When I thought I was getting a solid grip on how these models tackle real-world prompts, <a href=\"https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fnewsletter.theaiedge.io%2Fp%2Fhow-llms-generate-text\">Damien Benveniste’s piece from the AI Edge Newsletter hit my radar</a>.</p><p>Talk about timing.</p><p>He did a deep dive into the nuts and bolts of decoding strategies for text generation in LLMs — something that could amp up my understanding and experimentation. Benveniste went beyond scratching the surface; he broke down the complex mechanics of how these models weave their magic with words.</p><p>This was exactly what I was looking for!</p><h4>🔍 Overview of LLM Decoding Strategies</h4><p>Benveniste’s exploration into LLMs focuses on how they predict the next token and generate a stream of tokens relevant to a specific prompt or task.</p><p>He details <strong>four distinct strategies for text generation</strong>, each with pros and cons, and examines how these methods influence the quality of the generated text:</p><ol><li>🏹 Greedy Search</li><li>🎲 Multinomial Sampling</li><li>🔦 Beam Search</li><li>🔦 Beam Search w/ Multinomial Sampling</li><li>🔄 Contrastive Search</li></ol><p>I’ll explain these LLM decoding strategies later in this blog.</p><h4>🔮 An Experiment Bridging Theory to Practice</h4><p>There it was, the lightbulb moment.</p><p>Why not mesh these generation techniques I’d just learned with the IFEval benchmark? What if these text-generation strategies could be applied to the models evaluated in the IFEval benchmark? Imagine the insights that could be gleaned from seeing how different methods influence a model’s ability to follow instructions!</p><p>It was a perfect blend of theory and practical experimentation, and I was armed with a bunch of Colab Pro+ units. It felt like the stars aligned- perfect for a hands-on experiment, right?</p><h3>🤖 The Contenders: DeciLM-7B-Instruct and Mistral-7B-v0.1</h3><p>DeciLM-7B-Instruct and Mistral-7B-Instruct-v0.1 — these were my champions.</p><p>Each had its flair and promise. The goal was to see how each model responded to the IFEval benchmark under the influence of the different text generation techniques outlined by Benveniste.</p><p><strong>⏲️ Against the Clock: GPU Generation Time Tracking</strong></p><p>And hey, we’ve been boasting about our model’s speed over Mistral’s.</p><p>So, I figured an interesting twist to this experiment was monitoring the generation time on the GPU for each model and strategy. This wasn’t just about the quality or fidelity of the generated text; it was also a race against time, measuring the efficiency of each approach in a high-performance computing environment. Well, kind of. In this case, it’s an A100 from Google Colab.</p><p>A stopwatch in one hand, a magnifying glass in the other — I was ready to see what these models could do and how quickly they could do it.</p><h4>Preliminaries: Install Dependencies, Import Libraries, and Load Models</h4><pre>%%capture<br />!pip install huggingface_hub<br />!pip install transformers<br />!pip install accelerate<br />!pip install bitsandbytes<br />!pip install ninja<br />!pip install datasets<br /><br /><br /># for IFEval:<br />!pip install langdetect nltk absl-py immutabledict</pre><pre>import os<br /><br />import json<br />import os<br />from pathlib import Path<br />from typing import List<br /># Related third-party imports<br />import nltk<br />import torch<br />from datasets import Dataset, load_dataset<br />from tqdm import tqdm<br />from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline<br />nltk.download('punkt')<br /><br />os.environ['LC_ALL'] = 'en_US.UTF-8'<br />os.environ['LANG'] = 'en_US.UTF-8'<br />os.environ['LC_CTYPE'] = 'en_US.UTF-8'</pre><pre># @title<br />def instantiate_huggingface_model(<br />    model_name,<br />    quantization_config=None,<br />    device_map=&quot;auto&quot;,<br />    use_cache=True,<br />    trust_remote_code=None,<br />    pad_token=None,<br />    padding_side=&quot;left&quot;<br />):<br />    &quot;&quot;&quot;<br />    Instantiate a HuggingFace model with optional quantization using the BitsAndBytes library.<br /><br />    Parameters:<br />    - model_name (str): The name of the model to load from HuggingFace's model hub.<br />    - quantization_config (BitsAndBytesConfig, optional): Configuration for model quantization.<br />      If None, defaults to a pre-defined quantization configuration for 4-bit quantization.<br />    - device_map (str, optional): Device placement strategy for model layers ('auto' by default).<br />    - use_cache (bool, optional): Whether to cache model outputs (False by default).<br />    - trust_remote_code (bool, optional): Whether to trust remote code for custom layers (True by default).<br />    - pad_token (str, optional): The pad token to be used by the tokenizer. If None, uses the EOS token.<br />    - padding_side (str, optional): The side on which to pad the sequences ('left' by default).<br /><br />    Returns:<br />    - model (PreTrainedModel): The instantiated model ready for inference or fine-tuning.<br />    - tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.<br /><br />    The function will throw an exception if model loading fails.<br />    &quot;&quot;&quot;<br /><br />    # If quantization_config is not provided, use the default configuration<br />    if quantization_config is None:<br />        quantization_config = BitsAndBytesConfig(<br />            load_in_4bit=True,<br />            bnb_4bit_use_double_quant=True,<br />            bnb_4bit_quant_type=&quot;nf4&quot;,<br />            bnb_4bit_compute_dtype=torch.bfloat16<br />        )<br /><br />    model = AutoModelForCausalLM.from_pretrained(<br />        model_name,<br />        quantization_config=quantization_config,<br />        device_map=device_map,<br />        use_cache=use_cache,<br />        trust_remote_code=trust_remote_code<br />    )<br /><br />    tokenizer = AutoTokenizer.from_pretrained(model_name)<br /><br />    if pad_token is not None:<br />        tokenizer.pad_token = pad_token<br />    else:<br />        tokenizer.pad_token = tokenizer.eos_token<br />    tokenizer.padding_side = padding_side<br /><br />    return model, tokenizer<br /><br />decilm, decilm_tokenizer = instantiate_huggingface_model(&quot;Deci/DeciLM-7B-instruct&quot;, trust_remote_code=True)<br /><br />mistral, mistral_tokenizer = instantiate_huggingface_model(&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;)<br /><br />deci_generator = pipeline(&quot;text-generation&quot;,<br />                          model=decilm,<br />                          tokenizer=decilm_tokenizer,<br />                          device_map=&quot;auto&quot;,<br />                          max_length=4096,<br />                          return_full_text=False<br />)<br /><br />mistral_generator = pipeline(&quot;text-generation&quot;,<br />                          model=mistral,<br />                          tokenizer=mistral_tokenizer,<br />                          device_map=&quot;auto&quot;,<br />                          max_length=4096,<br />                          return_full_text=False<br />)<br /><br />pipelines = {<br />    'DeciLM': deci_generator,<br />    'Mistral': mistral_generator<br />}</pre><h3>👨🏽‍🎨 LLM Decoding Strategies</h3><h4>⛺️ Baseline</h4><p>This is an empty dictionary, and it will use the default generation parameters from HuggingFace, except for the temperature setting. Which I will set to a small, near-zero value. In my experience with smaller models, I’ve noticed they follow instructions better when the temperature is small.</p><pre>baseline = {'temperature': 1e-4}</pre><h4>🏹 Greedy Search Generation</h4><p><strong>🔧How It Works: </strong>Greedy Search selects the most probable next token at each step of the sequence generation without considering the overall sequence. To enable greedy search, you set the parameters num_beams=1 and do_sample=False.</p><p><strong>⚖️ Pros and Cons: It’s simple and fast, but it tends to lack creativity and, depending on the model, might lead to repetitive text.</strong> This is because it doesn’t look ahead or reconsider past choices.</p><pre>greedy_config = {<br />    'num_beams': 1,<br />    'do_sample': False,<br />    'temperature': 1e-4<br />}</pre><h4>🎲 Multinomial Sampling Generation</h4><p><strong>🔧How It Works:</strong> It samples the next token based on the probability distribution provided by the model rather than just picking the most probable token. To enable multinomial sampling set do_sample=True and num_beams=1.</p><p><strong>⚖️ Pros and Cons:</strong> While this approach will give you varied and interesting outputs, it may sometimes generate less coherent or relevant text.</p><pre>multinomial_config = {<br />    'num_beams': 1,<br />    'do_sample': True,<br />    'temperature': 1e-4<br />}</pre><h4>🔦 Beam Search Generation</h4><p><strong>🔧How It Works:</strong> Beam Search keeps track of a fixed number of the most probable sequences at each step and expands each for the next token. It balances between greedy and exhaustive searches. Beam search is enabled when you specify a value for num_beams (number of hypotheses to keep track of) greater than 1, typically 4 or 5.</p><p><strong>⚖️ Pros and Cons: </strong>This method leads to more coherent and high-quality text but it’s slower and more computationally intensive than greedy search. The quality of the result depends heavily on the size of the “beam” (the number of sequences considered).</p><pre>beam_search_config = {<br />    'num_beams': 5,<br />    'do_sample': False,<br />    'temperature': 1e-4,<br />    'early_stopping':True<br />}</pre><h4>🔦Beam Search w/ 🎲 Multinomial</h4><p><strong>🔧How It Works: </strong>This approach combines Beam Search and Multinomial Sampling. Beam Search is applied with randomness in selecting the next tokens. To use this decoding strategy, you need to set num_beams larger than 1, and do_sample=True.</p><p><strong>⚖️ Pros and Cons:</strong> Balances the benefits of Beam Search in generating coherent sequences with the creativity of Multinomial Sampling. But it’s more complex and computationally demanding.</p><pre>beam_search_multinomial_config = {<br />    'num_beams': 5,<br />    'do_sample': True,<br />    'temperature': 1e-4,<br />    'early_stopping':True<br />}</pre><h4>🔄 Contrastive Search Generation</h4><p><strong>🔧How It Works:</strong> Contrastive Search reduces the typicality of the generated text by contrasting the chosen token with other potential candidates, considering both probability and diversity. The two main parameters that enable and control the behaviour of contrastive search are penalty_alpha and top_k. It assesses the best tokens and applies penalties to reorder them based on criteria like repetition and diversity. Note you can read about this method in the original 2022 paper, <a href=\"https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Farxiv.org%2Fabs%2F2202.06417\">A Contrastive Framework for Neural Text Generation</a></p><p><strong>⚖️ Pros and Cons: </strong>The tradeoff here is the relevance and coherence of generated text in exchange for diversity and novelty. Requires sophisticated mechanisms to balance between contrastiveness and relevance.</p><pre>contrastive_search_config = {<br />    'penalty_alpha': 0.42,<br />    'top_k': 250,<br />    'temperature': 1e-4<br />}</pre><h4>📖 Put all settings in a dictionary</h4><p>This just makes it easier to iterate over everything.</p><pre>gen_configs = {<br />    &quot;baseline&quot;:baseline,<br />    &quot;greedy_search&quot;:greedy_config,<br />    &quot;multinomial_sampling&quot;:multinomial_config,<br />    &quot;beam_search&quot;:beam_search_config,<br />    &quot;beam_search_multinomial&quot;:beam_search_multinomial_config,<br />    &quot;contrastive_search&quot;:contrastive_search_config<br />}</pre><h3>🚀 The Dataset: 540+ Rows</h3><p>We’ve got a dataset here that’s over 540 rows strong. But let’s not get carried away — this is a blog, not a thesis. For simplicity’s sake, we’re narrowing it down to 100 rows.</p><h4>🔢 10 Rounds per Prompt</h4><p>Each prompt gets the full treatment — 12 generations with a different model + config combo. So, that’s 12 variations for every single prompt.</p><h4>⏱ The Reality of Local LLMs: It’s a Time-Eater</h4><p>Anyone who’s dabbled with local LLMs knows this: Text generation takes its sweet time. To keep this post from turning into a marathon, I’m picking 100 prompts at random.</p><h4>👊 For the Hardcore</h4><p>Do you have time and GPUs to burn? Feel free to run the gauntlet with all 540+ prompts.</p><pre>dataset = load_dataset(&quot;harpreetsahota/Instruction-Following-Evaluation-for-Large-Language-Models&quot;, split='train')<br /><br />dataset = dataset.shuffle(seed=42)<br />subset = dataset.select(range(100))<br /><br /># You'll need this as a jsonl file later, so let's save it now<br />subset.to_json('subset_for_eval.jsonl')<br /><br />def convert_kwargs_in_jsonl(file_path):<br />    &quot;&quot;&quot;<br />    Modifies a JSONL file by converting 'kwargs' from a JSON-formatted string to a dictionary.<br /><br />    This adjustment is necessary because the Hugging Face datasets library automatically expands<br />    schemas, potentially altering the original structure of data with variable schema fields like 'kwargs'.<br />    Converting 'kwargs' to a dictionary before loading into Hugging Face datasets avoids this issue.<br /><br />    Parameters:<br />    - file_path (str): Path to the JSONL file to be modified.<br /><br />    The function overwrites the original file with the modified data. Ensure to backup the original file<br />    before running this function.<br /><br />    Example:<br />    convert_kwargs_in_jsonl(&quot;path/to/jsonl_file.jsonl&quot;)<br />    &quot;&quot;&quot;<br />    modified_data = []<br /><br />    with open(file_path, 'r') as file:<br />        for line in file:<br />            line_data = json.loads(line)<br />            if 'kwargs' in line_data and isinstance(line_data['kwargs'], str):<br />                line_data['kwargs'] = json.loads(line_data['kwargs'])<br />            modified_data.append(line_data)<br /><br />    with open(file_path, 'w') as file:<br />        for item in modified_data:<br />            file.write(json.dumps(item) + '\\n')<br /><br /># Replace 'your_file.jsonl' with the path to your JSONL file<br />convert_kwargs_in_jsonl(&quot;/content/subset_for_eval.jsonl&quot;)</pre><blockquote><strong>Author’s Note: </strong>I used the instruction prompt for DeciLM. Mistral’s model didn’t have any specific prompt format, though it mentioned that you should wrap your prompt in &lt;s&gt;[INST] and [/INST]. I didn’t do that here, and it could possibly impact the final results. I’ll put this out there as a challenge to my good friend <a href=\"https://medium.com/u/ae9cae9cbcd2\">Sophia Yang, Ph.D.</a> — take this code, wrap it in the Mistral prompt, and see how it goes! 😉</blockquote><pre>SYSTEM_PROMPT_TEMPLATE = &quot;&quot;&quot;<br />### System:<br />You are an AI assistant that follows instructions extremely well. Help as much as you can.<br />### User:<br />{instruction}<br />### Assistant:<br />&quot;&quot;&quot;<br /><br />def get_prompt_with_template(message: str) -&gt; str:<br />    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)<br /><br />def preprocess_prompt(model_name: str, prompt: str) -&gt; str:<br />    if model_name == 'DeciLM':<br />        return get_prompt_with_template(prompt)<br />    elif model_name == 'Mistral':<br />        return f&quot;&lt;s&gt;[INST]{prompt}[/INST]&quot;<br />    else:<br />        return prompt  # Default case, no special preprocessing<br /><br />def generate_responses(row, model_name, pipeline, config):<br />    &quot;&quot;&quot;<br />    Generates a response using a specified model with a given generation configuration<br />    and measures the time taken for generation on the GPU.<br /><br />    Parameters:<br />    row (dict): A dictionary representing a row in the dataset.<br />    model_name (str): The name of the model.<br />    pipeline (Pipeline): The pipeline object for the model.<br />    config (dict): The generation configuration.<br /><br />    Returns:<br />    dict: Generated text and the time taken for generation.<br />    &quot;&quot;&quot;<br />    # Preprocess prompt based on the model<br />    processed_prompt = preprocess_prompt(model_name, row['prompt'])<br /><br />    # Initialize CUDA events for timing<br />    start_event = torch.cuda.Event(enable_timing=True)<br />    end_event = torch.cuda.Event(enable_timing=True)<br /><br />    # Time the generation process<br />    start_event.record()<br />    try:<br />        response = pipeline(processed_prompt, **config)[0]['generated_text']<br />    except RuntimeError as e:<br />        response = &quot;Failed Generation&quot;<br />    end_event.record()<br /><br />    # Synchronize and calculate time<br />    torch.cuda.synchronize()<br />    time_taken = start_event.elapsed_time(end_event) / 1000.0  # Time in seconds<br />    return {f'{model_name}_response': response, f'{model_name}_time': time_taken}</pre><h3>⏳ Just a heads up, this will take a while</h3><p>The following took ~14 hours to run.</p><pre># Iterate over configurations and models, and generate responses<br />for config_name, config in gen_configs.items():<br />    for model_name, model_pipeline in pipelines.items():<br />        # Prepare new column names<br />        response_column = f'{model_name}_{config_name}_response'<br />        time_column = f'{model_name}_{config_name}_time'<br /><br />        # Initialize lists to store data<br />        responses, times = [], []<br /><br />        # Process dataset<br />        for row in tqdm(subset, desc=f'Processing {model_name} with {config_name}'):<br />            result = generate_responses(row, model_name, model_pipeline, config)<br />            responses.append(result[f'{model_name}_response'])<br />            times.append(result[f'{model_name}_time'])<br /><br />        # Update the dataset with new columns<br />        subset = subset.add_column(response_column, responses)<br />        subset = subset.add_column(time_column, times)</pre><p>Since the above code will take at least a couple of hours to run, running the following cell is a good idea to push your results to HuggingFace immediately.</p><pre>subset.push_to_hub(&quot;&lt;your-hf-username&gt;/IFEval_Experiments&quot;)</pre><p>If you don’t want to run the code yourself, you can download the dataset directly if you wish. Just run the following line of code.</p><pre>data = load_dataset(&quot;harpreetsahota/IFEval_Experiments&quot;, split=&quot;train&quot;)</pre><p>But I encourage you to run this yourself to verify my results independently.</p><h3>⏱️ Timing Generations</h3><p>DeciLM consistently proves to be a superior choice, especially when efficiency and speed are the priorities.</p><p><strong>🏎️ DeciLM’s Remarkable Speed: </strong>In my comparative analysis across various strategies, DeciLM consistently outperforms Mistral. Its ability to process tasks swiftly is unmatched, making it the go-to option for time-sensitive applications.</p><p><strong>✅Advantages Over Mistral: </strong>While Mistral has its merits in handling complex strategies, it falls behind in processing time. DeciLM’s lead in speed is evident and substantial, demonstrating its robustness and superior optimization.</p><p><strong>📲 Ideal for a Range of Applications: </strong>Whether it’s simple or complex tasks, DeciLM shows remarkable versatility, handling them with greater efficiency than Mistral. This makes DeciLM a more flexible and reliable choice in various scenarios.</p><p>⤵️ <strong>The Bottom Line:</strong> DeciLM is the undisputed choice for anyone looking for a model that delivers quick and efficient results.</p><pre>import pandas as pd<br />import matplotlib.pyplot as plt<br />import seaborn as sns<br /><br />def prepare_average_time_data(dataset, models, strategies):<br />    &quot;&quot;&quot;<br />    Prepare the average time data for plotting.<br /><br />    Args:<br />        dataset (Dataset): HuggingFace dataset object.<br />        models (list): List of model names.<br />        strategies (list): List of strategies.<br /><br />    Returns:<br />        pd.DataFrame: A DataFrame with the average time data.<br />    &quot;&quot;&quot;<br />    # Initialize a dictionary to hold cumulative time and counts for averaging<br />    time_data = {(model, strategy): {'total_time': 0, 'count': 0}<br />                 for model in models for strategy in strategies}<br /><br />    # Process each record in the dataset<br />    for record in dataset:<br />        for model in models:<br />            for strategy in strategies:<br />                time_column = f&quot;{model}_{strategy}_time&quot;<br />                if time_column in record and record[time_column] is not None:<br />                    time_data[(model, strategy)]['total_time'] += record[time_column]<br />                    time_data[(model, strategy)]['count'] += 1<br /><br />    # Calculate the average time and prepare data for DataFrame<br />    data_for_df = []<br />    for (model, strategy), time_info in time_data.items():<br />        if time_info['count'] &gt; 0:<br />            avg_time = time_info['total_time'] / time_info['count']<br />            data_for_df.append({'Model': model, 'Strategy': strategy, 'Average Time': avg_time})<br /><br />    return pd.DataFrame(data_for_df)<br /><br />def plot_average_time_by_strategy(df):<br />    &quot;&quot;&quot;<br />    Plots the average time by strategy for different models.<br /><br />    Parameters:<br />    df (DataFrame): A pandas DataFrame containing the columns 'Model', 'Strategy', and 'Average Time'.<br />    &quot;&quot;&quot;<br />    plt.figure(figsize=(12, 6))<br />    sns.barplot(x='Strategy', y='Average Time', hue='Model', data=df, palette=['blue', 'orange'])<br />    plt.title('Average Time by Strategy and Model')<br />    plt.xticks(rotation=45)<br />    plt.ylabel('Average Time (Seconds)')<br />    plt.xlabel('Strategy')<br />    plt.legend(title='Model')<br />    plt.tight_layout()<br />    plt.show()<br /><br />models = ['DeciLM', 'Mistral']<br />strategies = ['baseline', 'greedy_search', 'multinomial_sampling', 'beam_search', 'beam_search_multinomial', 'contrastive_search']<br /><br />df_avg_time = prepare_average_time_data(subset, models, strategies)<br /><br />plot_average_time_by_strategy(df_avg_time)</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*N_EoAY8at79HZWUi.png\" /><figcaption>By Author</figcaption></figure><h3>👨🏾‍🍳 Prepare for Evaluation with IFEval</h3><p>To run IFEval we’ll need to prepare our environment and datasets.</p><p>Here’s what needs to be done:</p><ul><li>Clone the repo and install dependencies</li><li>Format data: Save each generation + strategy combo from the dataset we created as a jsonl file with two entries: prompt and response</li></ul><h4>Clone the repo</h4><p>The IFEval code is in the Google Research repo, which is massive. The below code will download the entire repo but only extract the IFEval code.</p><pre>%%bash<br />curl -L https://github.com/google-research/google-research/archive/master.zip -o google-research-master.zip<br />unzip google-research-master.zip &quot;google-research-master/instruction_following_eval/*&quot;</pre><h4>👩🏿‍🔧 Install dependencies</h4><pre>!pip install absl-py<br />!pip install -r /content/google-research-master/instruction_following_eval/requirements.txt<br />!mkdir /content/output</pre><h4>🔷 Format Data</h4><pre>def save_as_jsonl(dataset: Dataset, prompt_column: str, response_column: str, output_dir: Path) -&gt; None:<br />    &quot;&quot;&quot;<br />    Saves specific columns of a Hugging Face dataset as JSONL files.<br /><br />    Args:<br />        dataset (Dataset): The Hugging Face dataset to process.<br />        prompt_column (str): The name of the column in the dataset that contains the prompts.<br />        response_column (str): The name of the column in the dataset that contains the responses.<br />        output_dir (Path): The directory where the JSONL files will be saved.<br /><br />    Returns:<br />        None<br />    &quot;&quot;&quot;<br />    output_file = output_dir / f'{response_column}.jsonl'<br />    with open(output_file, 'w') as f:<br />        for example in dataset:  # Replace 'train' with the appropriate dataset split<br />            json_object = {<br />                &quot;prompt&quot;: example[prompt_column],<br />                &quot;response&quot;: example[response_column]<br />            }<br />            f.write(json.dumps(json_object) + '\\n')<br /><br /># Define your prompt and response columns<br />prompt_column = 'prompt'<br /><br />response_columns = ['DeciLM_baseline_response',<br />                    'Mistral_baseline_response',<br />                    'DeciLM_greedy_search_response',<br />                    'Mistral_greedy_search_response',<br />                    'DeciLM_multinomial_sampling_response',<br />                    'Mistral_multinomial_sampling_response',<br />                    'DeciLM_beam_search_response',<br />                    'Mistral_beam_search_response',<br />                    'DeciLM_beam_search_multinomial_response',<br />                    'Mistral_beam_search_multinomial_response',<br />                    'DeciLM_contrastive_search_response',<br />                    'Mistral_contrastive_search_response']<br /><br />output_dir = Path('/content/google-research-master/instruction_following_eval/data')<br /><br />for response_column in response_columns:<br />    save_as_jsonl(subset, prompt_column, response_column, output_dir)</pre><h4>🛠 IFEval in Action</h4><p>The implementation involved crafting 25 verifiable instructions into 541 diverse prompts, a process that was both meticulous and rigorous:</p><ul><li><strong>Generating base prompts:</strong> This step involves creating the initial set of prompts.</li><li><strong>Adding variety in phrasing:</strong> To ensure a comprehensive evaluation, each prompt is rephrased to increase the diversity of its presentation, avoiding monotonous or repetitive phrasing.</li><li><strong>A final manual review:</strong> This step involved thoroughly inspecting the rephrased prompts to guarantee their quality, relevance, and alignment with the intended instructional goals.</li></ul><p><strong>The table below describes the 25 verifiable instructions and their descriptions.</strong></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lAVJ6Pbs_1cMJ5xrXIqyWQ.png\" /></figure><h4>Example prompt, instruction, and generations</h4><p>You’ll notice the complete prompt that should be provided as input to the model and a list of instructions and keyword arguments that the framework uses to assess adherence to the prompt.</p><p>You can also see an example of the completion from each model's baseline configuration.</p><pre>subset[42]['prompt'], subset[42]['instruction_id_list'], subset[42]['kwargs']</pre><pre>('Can you give me an example for a journal entry about stress management? Tell me how you come up with the example. Your entire response should contain less than 6 sentences.',<br /> ['length_constraints:number_sentences'],<br /> '[{&quot;relation&quot;: &quot;less than&quot;, &quot;num_sentences&quot;: 6}]')<br /></pre><pre>print(subset[42]['DeciLM_baseline_response'])</pre><pre>Sure, here's an example of a journal entry about stress management:<br /><br />&quot;Today, I experienced a lot of stress due to a heavy workload and personal issues. To manage my stress, I decided to take a break and go for a walk. As I walked, I focused on my breath and the beauty of nature around me. This helped me feel more relaxed and recharged, allowing me to approach my tasks with a clearer mind.&quot;<br /><br />I came up with this example by thinking about common stressors and how people might manage them. I then used my own experience to create a relatable and helpful entry.</pre><pre>print(subset[42]['Mistral_baseline_response'])</pre><pre> Example: &quot;Today, I practiced stress management by taking a break from technology and spending time in nature. I went for a walk in the park and listened to the sound of the birds and the rustling of the leaves. This helped me to clear my mind and reduce feelings of anxiety and stress.&quot;<br /><br />I came up with this example by reflecting on my own experiences with stress management. I know that taking a break from technology and spending time in nature can be helpful in reducing stress levels. So, I decided to write about a specific instance where I practiced this technique and the positive effects it had on me.</pre><h3>How does IFEval work?</h3><p>This evaluation framework is designed to comprehensively test and compare LLMs based on their ability to understand and follow instructions. It combines strict and loose accuracy assessments at both the prompt and instruction levels, supported by a robust testing system to ensure the accuracy and reliability of the evaluation process.</p><ol><li><strong>Preparation: </strong>The system sets instructions using the instruction framework and loads input data.</li><li><strong>Evaluation</strong>: The evaluation_main.py script processes these inputs and compares the system’s responses against the expected outcomes defined by the instructions.</li><li><strong>Result Analysis:</strong> The system generates results that detail how well the instructions were followed under different evaluation criteria (strict vs. loose).</li><li><strong>Testing and Validation: </strong>Test scripts are used throughout to ensure the integrity of the instruction framework and the reliability of the evaluation process.</li></ol><h3>🏃🏽‍♂️💨 Run Evaluation</h3><p>I appreciate its objective approach to evaluating language models.</p><p>Unlike other methods that might require subjective interpretation or additional language models for evaluation, IFEval offers a clear-cut, binary system: the model either follows the given instruction or doesn’t. This simplicity is invaluable, especially when running on a CPU without high-powered resources. Such a straightforward evaluation is a vital sanity check in comparing language models, ensuring they meet specific criteria crucial in high-stakes scenarios.</p><p>This objectivity in assessment provides a reliable baseline for measuring a model’s instruction-following capabilities, an essential factor in its practical application.</p><p>The code below sets up IFEval in your environment and then runs the evaluation. All of this happens on the CPU, and it’s really quick. The evaluation results are written to a text file, so we have to parse them ourselves. Once the results are parsed, we can analyze them and plot them.</p><pre>import sys<br />sys.path.append('/content/google-research-master/instruction_following_eval')</pre><pre>%cd /content/google-research-master<br /><br />log_file = &quot;/content/output/all_responses_log.txt&quot;  # Single log file for all responses<br />os.makedirs(&quot;/content/output&quot;, exist_ok=True)<br /><br />for response_column in response_columns:<br />    response_file = f&quot;/content/google-research-master/instruction_following_eval/data/{response_column}.jsonl&quot;<br />    output_dir = f&quot;/content/output/{response_column}&quot;<br /><br />    # Create the output directory if it doesn't exist<br />    os.makedirs(output_dir, exist_ok=True)<br /><br />    # Run the script for each response file and append output to the log file<br />    !python -m instruction_following_eval.evaluation_main \\<br />      --input_data=/content/subset_for_eval.jsonl \\<br />      --input_response_data={response_file} \\<br />      --output_dir={output_dir} &amp;&gt;&gt; {log_file}<br /><br />    print(f&quot;Completed evaluation for {response_column}&quot;)<br /><br /># Print a message when all processing is complete<br />print(f&quot;All evaluations complete. Check the log file: {log_file} for details.&quot;)</pre><pre>import re<br />import pandas as pd<br />def parse_log(file_path):<br />    &quot;&quot;&quot;<br />    Parses the log file to extract baseline response names and their associated<br />    accuracy scores under both strict and loose evaluation regimes.<br /><br />    Parameters:<br />    file_path (str): Path to the log file.<br /><br />    Returns:<br />    dict: A dictionary with the results for strict and loose<br />    &quot;&quot;&quot;<br />    # Regular expression to match the relevant lines in the log data<br />    pattern = r&quot;/content/output/(?P&lt;response&gt;[\\w_]+)/eval_results_(?P&lt;type&gt;\\w+).jsonl Accuracy Scores:\\n&quot; \\<br />              r&quot;prompt-level: (?P&lt;prompt_level&gt;[\\d.]+)\\n&quot; \\<br />              r&quot;instruction-level: (?P&lt;instruction_level&gt;[\\d.]+)\\n&quot;<br /><br />    # Dictionary to store the parsed data<br />    results = {}<br /><br />    # Read the log file<br />    with open(file_path, 'r') as file:<br />        log_data = file.read()<br /><br />        # Find all matches and process them<br />        for match in re.finditer(pattern, log_data, re.MULTILINE):<br />            response = match.group(&quot;response&quot;)<br />            eval_type = f&quot;eval_results_{match.group('type')}&quot;<br />            prompt_level = float(match.group(&quot;prompt_level&quot;))<br />            instruction_level = float(match.group(&quot;instruction_level&quot;))<br /><br />            # Initialize the response dictionary if not already present<br />            if response not in results:<br />                results[response] = {}<br />            # Store the scores in the corresponding dictionary<br />            results[response][eval_type] = {<br />                &quot;prompt-level&quot;: prompt_level,<br />                &quot;instruction-level&quot;: instruction_level<br />            }<br /><br />    return results<br /><br />parsed_log = parse_log(&quot;/content/output/all_responses_log.txt&quot;)<br /><br /># Create a list of dictionaries for DataFrame construction<br />data_for_df = []<br />for response_name, response_data in parsed_log.items():<br />    # Split the response_name to separate the model and the strategy<br />    parts = response_name.split('_')<br />    model_name = parts[0]<br />    strategy = '_'.join(parts[1:-1])  # Join all parts except the first and last<br /><br />    # Now create a row for each eval_type<br />    for eval_type, scores in response_data.items():<br />        row = {<br />            'Model': model_name,<br />            'Strategy': strategy,<br />            'Evaluation Type': eval_type.split('_')[-1],  # Get 'strict' or 'loose'<br />            'Prompt Level': scores['prompt-level'],<br />            'Instruction Level': scores['instruction-level']<br />        }<br />        data_for_df.append(row)<br /><br /># Create the DataFrame<br />df = pd.DataFrame(data_for_df)</pre><pre><br />import matplotlib.pyplot as plt<br />import pandas as pd<br /><br />def get_strategy_data(df, strategy):<br />    &quot;&quot;&quot;<br />    Filter the DataFrame for a specific strategy.<br /><br />    Args:<br />        df (pd.DataFrame): The DataFrame to filter.<br />        strategy (str): The strategy to filter by.<br /><br />    Returns:<br />        pd.DataFrame: Filtered DataFrame.<br />    &quot;&quot;&quot;<br />    return df[df['Strategy'] == strategy]<br /><br />def plot_model_bars(ax, strategy_data, model_colors, bar_positions, strategy_index):<br />    &quot;&quot;&quot;<br />    Plot bars for each model in the given strategy data.<br /><br />    Args:<br />        ax (matplotlib.axes.Axes): The axis to plot on.<br />        strategy_data (pd.DataFrame): Data for the specific strategy.<br />        model_colors (dict): A dictionary mapping models to colors.<br />        bar_positions (list): Positions for each bar in the plot.<br />        strategy_index (int): Index of the current strategy.<br /><br />    Returns:<br />        None<br />    &quot;&quot;&quot;<br />    for j, (model, color) in enumerate(model_colors.items()):<br />        prompt_level = strategy_data[strategy_data['Model'] == model]['Prompt Level'].values<br />        if prompt_level.size &gt; 0:<br />            ax.bar(bar_positions[j], prompt_level[0], color=color, width=0.4, label=model if strategy_index == 0 else &quot;&quot;)<br /><br />def plot_evaluation(df, evaluation_type, model_colors, ax, strategies, column_to_plot):<br />    &quot;&quot;&quot;<br />    Plot the evaluation data as a bar chart for a specified column.<br /><br />    Args:<br />        df (pd.DataFrame): The DataFrame containing evaluation data.<br />        evaluation_type (str): The type of evaluation ('strict' or 'loose').<br />        model_colors (dict): A dictionary mapping models to colors.<br />        ax (matplotlib.axes.Axes): The matplotlib axis to plot on.<br />        strategies (list): A list of strategies to evaluate.<br />        column_to_plot (str): The column name to plot.<br /><br />    Returns:<br />        None<br />    &quot;&quot;&quot;<br />    for i, strategy in enumerate(strategies):<br />        strategy_data = df[df['Strategy'] == strategy]<br />        bar_positions = [i - 0.2, i + 0.2]<br /><br />        for j, (model, color) in enumerate(model_colors.items()):<br />            values = strategy_data[strategy_data['Model'] == model][column_to_plot].values<br />            if values.size &gt; 0:<br />                ax.bar(bar_positions[j], values[0], color=color, width=0.4, label=model if i == 0 else &quot;&quot;)<br /><br />    ax.set_xticks(range(len(strategies)))<br />    ax.set_xticklabels(strategies, rotation=45, ha='right')<br />    ax.set_ylabel(column_to_plot.replace('_', ' ').title())<br />    ax.set_title(f&quot;{evaluation_type.capitalize()} Evaluation: {column_to_plot.replace('_', ' ').title()}&quot;)<br /><br />    if evaluation_type == 'strict':<br />        handles, labels = ax.get_legend_handles_labels()<br />        ax.legend(handles, labels, loc='best')<br /><br />def plot_strict_and_loose(df, model_colors, column_to_plot):<br />    &quot;&quot;&quot;<br />    Set up and plot for strict and loose evaluations for a specified column.<br /><br />    Args:<br />        df (pd.DataFrame): The DataFrame containing the evaluation data.<br />        model_colors (dict): A dictionary mapping models to colors.<br />        column_to_plot (str): The column name to plot.<br /><br />    Returns:<br />        None<br />    &quot;&quot;&quot;<br />    df_strict = df[df['Evaluation Type'] == 'strict']<br />    df_loose = df[df['Evaluation Type'] == 'loose']<br />    strategies = sorted(df['Strategy'].unique())<br /><br />    fig, axs = plt.subplots(1, 2, figsize=(14, 7), sharey=True)<br />    plot_evaluation(df_strict, 'strict', model_colors, axs[0], strategies, column_to_plot)<br />    plot_evaluation(df_loose, 'loose', model_colors, axs[1], strategies, column_to_plot)<br /><br />    plt.tight_layout()<br />    plt.show()<br /><br /># Example usage<br />model_colors = {'DeciLM': 'blue', 'Mistral': 'orange'}</pre><h3>📏 Clear Metrics for Evaluation</h3><p>IFEval introduces two key metrics for evaluating the adherence of large language models (LLMs) to instructions:</p><ol><li><strong>✅ Strict Accuracy:</strong> This metric is a straightforward assessment: did the LLM follow the instructions exactly as stated? It’s a binary evaluation — either the instruction was followed (True), or it wasn’t (False).</li><li><strong>🔄 Loose Accuracy:</strong> It’s a more lenient metric that aims to reduce false negatives by accounting for variations in how an LLM might execute the instructions.</li></ol><h3>🔍 Bringing It All Together</h3><p>The evaluation results in four distinct accuracy scores, combining strict and loose assessments at both the prompt and instruction levels:</p><ol><li>🎯 Prompt-level strict-accuracy</li><li>📏 Instruction-level strict-accuracy</li><li>🌐 Prompt-level loose-accuracy</li><li>🔍 Instruction-level loose-accuracy</li></ol><h3>Prompt-Level Evaluation: Comparing the Models with Different Decoding Strategies</h3><h4>🎯 Prompt-level strict-accuracy</h4><p>This metric calculates the percentage of prompts in which all verifiable instructions are followed. It assesses the model’s ability to adhere to every instruction within a single prompt.</p><ul><li>DeciLM consistently outperforms Mistral in strict accuracy across all strategies. Its highest strict accuracy is observed with the ‘beam_search’ strategy (0.43), indicating its capability to follow instructions precisely.</li><li>Mistral shows lower strict accuracy, with its highest being 0.34 under the ‘baseline’ and ‘greedy_search’ strategies. This suggests a relative weakness in following instructions exactly as stated compared to DeciLM.</li></ul><h4>🌐 Prompt-level loose-accuracy</h4><p>This is the prompt-level accuracy computed with a loose criterion. The ‘loose’ criterion involves more lenient standards for following instructions, allowing for variations in the responses that still capture the essence of the instructions.</p><ul><li>DeciLM also leads in loose accuracy, demonstrating a better understanding of the essence of instructions. Its highest score is 0.46, achieved with both ‘baseline’ and ‘greedy_search’ LLM decoding strategies.</li><li>While trailing behind DeciLM, Mistral performs well in loose accuracy with the ‘baseline’ and ‘greedy_search’ LLM decoding strategies (0.40). This indicates a reasonable ability to capture the intent of instructions, though not as effectively as DeciLM.</li></ul><p>DeciLM outperforms Mistral in strict and loose accuracy metrics at the prompt level, regardless of the strategy used. This suggests that DeciLM is better equipped to understand and adhere to instructions in various contexts. Mistral, while showing reasonable capability, particularly in loose accuracy, falls behind in strictly adhering to instructions, indicating potential areas for improvement.</p><p>DeciLM demonstrates more balance in understanding the literal and nuanced aspects of instructions. It looks like a more effective model for tasks which require precision and adaptability in language understanding.</p><pre># column_to_plot =   # Specify the column you want to plot<br />plot_strict_and_loose(df, model_colors, column_to_plot='Prompt Level')</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*xRkcqDYHH8FWLXMZLcRcfg.png\" /><figcaption>By Author</figcaption></figure><h3>Instruction-Level Evaluation: Comparing the Models with Different Decoding Strategies</h3><h4>📏 Instruction-level strict-accuracy</h4><p>This metric measures the percentage of individual verifiable instructions that are followed, regardless of the prompt they appear in. It focuses on the model’s consistency in following each type of instruction across different prompts.</p><ul><li>DeciLM’s scores are higher, indicating a better ability to follow instructions exactly as stated.</li><li>Mistral’s lower scores suggest a need for improvement in strict adherence to instructions.</li></ul><h4>🔍 Instruction-level loose-accuracy</h4><p>Similar to prompt-level loose accuracy, this metric is computed at the instruction level with a loose criterion. It evaluates how well the model follows each instruction across different prompts, with some leniency in how strictly the instructions must be followed.</p><ul><li>DeciLM again leads, showing its capability to interpret instructions with some flexibility.</li><li>Mistral performs better in loose accuracy than strict but still trails behind DeciLM.</li></ul><p>DeciLM demonstrates stronger performance in instruction adherence at the instruction level, both strictly and loosely, across all evaluated strategies. Mistral, while showing some capability, especially in loose accuracy, needs improvements to match DeciLM's performance.</p><p>The choice of strategy significantly affects the models’ performance, highlighting the importance of strategy selection in model deployment for specific tasks.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Jw12iRm5nwlIrjXPcmdmaA.png\" /><figcaption>By Author</figcaption></figure><h3>🚀 Wrapping Up</h3><p>Juggling the intricacies of LLM evaluation and decoding strategies has been challenging and immensely enjoyable. A big shout out to the authors whose genius lit the fuse for this firecracker of a project. Your work more than sparked my curiosity; it set off a full-blown fireworks show in my brain!</p><p><strong>The IFEval framework is a gem and has rightly earned its place in my evals toolkit.</strong> It offers a fresh perspective on how we assess instruction-following in language models. To my fellow AI enthusiasts, I hope my explorations shed light on this framework’s potential. Keep the conversation going in the <a href=\"https://www.deeplearningdaily.community/\">Deep Learning Daily Discord</a> community for deeper dives and lively discussions.</p><p>Starting this project, I had no clue what to expect. Mistral v0.1 has a lot of hype behind it, and DeciLM is the underdog. But in my experiments, DeciLM was nailing task after task. Didn’t see that coming!</p><p>The next thing to do is compare Mistral v0.2 with and without the appropriate prompt template. I invite my counterpart (and my good friend) <a href=\"https://medium.com/u/ae9cae9cbcd2\">Sophia Yang, Ph.D.</a>, to do this! There is nothing wrong with a little friendly competition!</p><p>My invitation to the community is to take my work, build on it, and challenge it. Independent exploration by others in the community is not just welcome; it’s essential for validating and enriching our collective understanding.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9c6ac57a8e22\" width=\"1\" /><hr /><p><a href=\"https://medium.com/artificialis/how-to-evaluate-an-llms-ability-to-follow-instructions-9c6ac57a8e22\">How to Evaluate an LLM's Ability to Follow Instructions</a> was originally published in <a href=\"https://medium.com/artificialis\">Artificialis</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",[{'name': 'Harpreet Sahota'}],"Thu, 08 Feb 2024 22:10:22 GMT"
1186,AI Assistants via OpenAI and Hugging Face API,https://medium.com/artificialis/ai-assistants-via-openai-and-hugging-face-api-332fc945837c?source=rss----a81c8d170222---4,"<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LR2G23zOG0Pjp3wxlXLxAQ.jpeg\" /><figcaption>image created by Lexica AI</figcaption></figure><h4><strong>quick hands-on tutorial</strong></h4><h3>Introduction</h3><p>In this guide, we’ll explore the Assistant APIs from OpenAI. We will learn about the primary features of the Assistants API, including the Code Interpreter, Knowledge Retrieval, and Function Calling capabilities.</p><p>This hand-on will show how to equip Assistant’s with tools and function that would enable it to provide technical solutions by executing Python code, retrieve knowledge from the database and many more.</p><p>You’ll be also introduced to other advanced technologies from OpenAI, such as Whisper, Dalle-3, Speech to Text, and the GPT-4 vision API. These tools are essential for anyone looking to develop sophisticated AI assistants using a variety of APIs.</p><p>Then, you’ll learn how to use the free Hugging Face Inference API to get access to the thousands of models hosted on their platform.</p><p>By the end of this tutorial, you’ll gain a solid understanding on how to apply these technologies in your AI projects.</p><h3>Open AI Assistant’s Built-in Functionalities</h3><p><a href=\"https://platform.openai.com/docs/assistants/overview\">The OpenAI Assistants API</a> includes three main functionalities: Code Interpreter, Retrieval, and Function Calling.</p><p><strong>[Code Interpreter]</strong></p><p>The Assistant can use Code Interpreter automatically when you upload a file with data. It’s a tool that transforms the LLM into a more accurate computational problem-solver that can handle tasks like solving complex math equations. It can also generate files with data and images of graphs from the same Python code. It’s a useful way to trust the output from the assistant and a great tool when analyzing data.</p><p><strong>[Knowledge Retrieval]</strong></p><p>Knowledge Retrieval is OpenAI’s own <a href=\"https://arxiv.org/abs/2005.11401\">RAG</a> system offered as part of the Assistants API. KR allows multiple uploads. Once the files are uploaded and passed to the Assistant, OpenAI will automatically chunk your documents, index them, store the embeddings, and implement vector search to retrieve relevant content to answer user queries.</p><p><strong>[Function Calling]</strong></p><p>Function calling allows you to describe functions or tools to the Assistant and have it return the functions that need to be called along with their arguments. It's a powerful way to add new capabilities to your Assistant.</p><h3>How To Set Up an Assistant</h3><p>You have two distinct pathways depending on your needs and expertise:</p><ul><li><a href=\"https://platform.openai.com/playground\"><strong>Assistants Playground</strong></a>: Ideal for those looking to get a feel for the Assistant’s capabilities without going into complex integrations.</li><li><a href=\"https://platform.openai.com/docs/assistants/overview\"><strong>Detailed Integration through the API</strong></a>: Best suited for those who require a more customized and in-depth setup.</li></ul><p><strong>STEP-BY-STEP ASSISTANT CREATION:</strong></p><p><strong>Creating an </strong><strong>Assistant</strong>:</p><p><strong>Purpose</strong>: An Assistant object represents an entity/agent that can be configured to respond to users’ messages in different ways using several parameters.</p><p><strong>Model Selection</strong>: you can specify any version of GPT-3.5 or GPT-4 models, including fine-tuned models. OpenAI recommends using its <a href=\"https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\">latest models</a> with the Assistants API for best results and maximum compatibility with tools. Thus, choose between <strong>gpt-3.5-turbo-1106</strong> or <strong>gpt-4-1106-preview</strong> models.</p><p><strong>Tools:</strong> The Assistant supports the <strong>Code Interpreter</strong> for technical queries that require Python code execution or <strong>Knowledge Retrieval</strong> to augment the Assistant with proprietary external information.</p><p><strong>Setting up a </strong><strong>Thread</strong>:</p><p><strong>Role</strong>: A Thread acts as the foundational unit of user interaction. It can be seen as a single <strong>conversation</strong>. Pass any user-specific context and files in this thread by <a href=\"https://platform.openai.com/docs/api-reference/messages/createMessage\">creating Messages</a>.</p><pre>thread = client.beta.threads.create()</pre><p><strong>Customization</strong>: In Thread, ingest user-specific contexts or attach necessary files so each conversation is unique and personalized.</p><p>Threads don’t have a size limit. You can add as many messages as you want to a conversation/Thread. The Assistant will ensure that requests to the model fit within the maximum context window, using relevant optimization techniques used in ChatGPT, such as truncation.</p><p><strong>Adding a </strong><strong>Message</strong>:</p><p><strong>Definition</strong>: Messages are user inputs, and the Assistant’s answers are appended to a Thread. User inputs can be questions or commands.</p><p><strong>Function</strong>: They serve as the primary mode of communication between the user and the Assistant.</p><pre>message = client.beta.threads.messages.create(<br />    thread_id=thread.id,<br />    role=&quot;user&quot;,<br />    content=&quot;I need to solve the equation `3x + 11 = 14`. Please, help!&quot;<br />)</pre><p>Messages can include <strong>text, images, and other files</strong>. Messages are stored as a list on the Thread. Using GPT-4 with Vision is not supported here. You can upload images and have them <a href=\"https://platform.openai.com/docs/assistants/tools/knowledge-retrieval\">processed via retrieval</a>.</p><p><strong>Activation</strong>: For the Assistant to respond to the user message, you must <a href=\"https://platform.openai.com/docs/api-reference/runs/createRun\">create a Run</a>. The Assistant will then automatically decide what previous Messages to include in the context window for the model.</p><blockquote>⚠️ NOTE: You can <a href=\"https://platform.openai.com/docs/api-reference/runs/createRun#runs-createrun-instructions\">optionally pass additional instructions</a> to the Assistant while creating the Run, but these <strong>will override</strong> the default instructions of the Assistant!</blockquote><p><strong>Process</strong>: The Assistant processes the entire Thread, employs its tools if required, and formulates an appropriate response.</p><p>During its run, the Assistant can call tools or create Messages. Examining Run Steps allows you to check how the Assistant is getting to its final results.</p><p>The assistant’s response to a Run:</p><pre>messages = client.beta.threads.messages.list(thread_id=thread.id)</pre><p>These responses are displayed to the user! During this Run, the Assistant added two new Messages to the Thread.</p><h3>ASSISTANT’S CORE MECHANISM:</h3><p>Creating an Assistant only requires specifying the <strong>model</strong>.</p><p>You can further customize the behavior of the Assistant:</p><ol><li>Use the <strong>instructions</strong> parameter to guide the personality of the Assistant and define its goals. Instructions are similar to system messages in the Chat Completions API.</li><li>Use the <strong>tools</strong> parameter to give the Assistant access to up to 128 tools in parallel. You can give it access to OpenAI-hosted tools (Conde Interpreter, Knowledge Retrieval) or call third-party tools via <strong>function calling</strong>.</li><li>Use the <strong>file_ids</strong> parameter to give the tools access to files. Files are uploaded using the <strong>File</strong> U<a href=\"https://platform.openai.com/docs/api-reference/files/create\">pload endpoint</a>.</li></ol><h3>Example demonstration:</h3><p>Imagine you’re developing an AI assistant for a tech company. This assistant needs to provide detailed product support using a comprehensive knowledge base.</p><pre>mkdir openai-assistants &amp;&amp; cd openai-assistants<br />python3 -m venv openai-assistants-env<br />source openai-assistants-env/bin/activate</pre><pre>pip3 install python-dotenv<br />pip3 install --upgrade openai<br /># fire up VSCode and let's get rolling!<br />code .</pre><p>Replace the text with your OpenAI API key, which you can get from your <a href=\"https://platform.openai.com/api-keys\">OpenAI developer account</a>.</p><pre>OPENAI_API_KEY=&quot;sh-xxx&quot;</pre><pre>$ pip install -U -q openai</pre><h3>Upload Files to a Knowledge Base:</h3><p>First, make a folder to store all the files you’ll create. <strong>Upload</strong> a detailed PDF manual of a product line (e.g., “tech_manual.pdf”) using the API:</p><pre>from openai import OpenAI<br /><br /><br />client = OpenAI()<br />file = client.beta.files.upload(<br /> file=open(&quot;tech_manual.pdf&quot;, &quot;rb&quot;),<br /> filetype=&quot;application/pdf&quot;,<br /> description=&quot;Tech product manual&quot;<br />)</pre><p>Now you can create the assistant with an uploaded file and with the ability to retrieve: tools=[{&quot;type&quot;: &quot;retrieval&quot;}]</p><pre>assistant = client.beta.assistants.create(<br />  instructions=&quot;You are a tech support chatbot. Use the product manual to respond accurately to customer inquiries.&quot;,<br />  model=&quot;gpt-4-1106-preview&quot;,<br />  tools=[{&quot;type&quot;: &quot;retrieval&quot;}],<br />  file_ids=[file.id]<br />)</pre><p><strong>User Interaction</strong>: To interact with the assistant, you need a <strong>thread and a </strong><strong>message:</strong></p><p>The message should contain the customer's question. Here's an example:</p><pre>thread = client.beta.threads.create()<br />   message = client.beta.threads.messages.create(<br />       thread_id=thread.id,<br />       role=&quot;user&quot;,<br />       content=&quot;How do I reset my Model X device?&quot;,<br />   )</pre><h3>RUN Thread:</h3><ul><li>A customer asks, “<em>How do I reset my Model X device?</em>”</li><li>The assistant accesses the uploaded manual, performs a vector search to find the relevant section, and provides clear, step-by-step reset instructions.</li></ul><pre>run = client.beta.threads.runs.create(<br />   thread_id=thread.id,<br />   assistant_id=assistant.id,<br />)<br /># the run will enter the **queued** state before it continues it's execution.</pre><h3>Information retrieval:</h3><p>After the run is complete, you can retrieve the assistant’s response:</p><pre>messages = client.beta.threads.messages.list(<br />	thread_id=thread.id<br />)</pre><pre>assistant_response = messages.data[0].content[0].text.value</pre><p>The output result should contain the assistant’s response to the customer’s question based on knowledge from the uploaded manual.</p><p>The full code and more examples are in the Colab notebook attached in the Resources section.</p><h3>OpenAI’s Other Advanced Models</h3><p>OpenAI also offers different types of models that are not yet integrated into the Assistants API but are accessible. These models offer voice processing, image understanding, and image generation capabilities.</p><h3><a href=\"https://huggingface.co/openai/whisper-large-v3\">Whisper-v3</a></h3><p>Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. It is a transformer-based encoder-decoder model, which is a type of <em>sequence-to-sequence</em> model. The latest large-v3 model shows improved performance over various languages compared to Whisper large-v2. OpenAI released the model’s weights with an <strong>Apache License 2.0.</strong> The model is available on <a href=\"https://huggingface.co/openai/whisper-large-v3\">Hugging Face</a>.</p><h3><a href=\"https://platform.openai.com/docs/guides/text-to-speech\">Text to Speech</a></h3><p>TTS is an AI model that converts text to natural-sounding spoken text. They offer two different model variates: <strong>tts-1</strong> is optimized for real-time text-to-speech use cases, and <strong>tts-1-hd</strong> is optimized for quality. These models can be used with the <a href=\"https://platform.openai.com/docs/guides/text-to-speech\">Speech endpoint in the Audio API</a>.</p><h3><a href=\"https://openai.com/dall-e-3\">Dall-E 3</a></h3><p>A newer iteration of the DALL-E model is designed for image generation. It can create images based on user prompts, making it a valuable tool for graphic designers, artists, and anyone to generate images quickly and efficiently. You can access the model through the <a href=\"https://platform.openai.com/docs/guides/images/generations\">image generation endpoint</a>.</p><h3><a href=\"https://openai.com/research/gpt-4v-system-card\">GPT-4 Vision</a></h3><p>GPT-4 with Vision enables you to ask questions about the contents of images. Visual question answering (VQA) is an important computer vision research field. You can also perform other vision tasks, such as Optical Character Recognition (OCR), where a model reads text in an image.</p><p>Using GPT-4 with Vision, you can ask questions about what is or is not in an image, how objects relate in an image, the spatial relationships between two objects (is one object to the left or right of another), the color of an object, and more.</p><p>GPT-4V is available through the <a href=\"https://openai.com/?ref=blog.roboflow.com\">OpenAI web interface for ChatGPT Plus</a> subscribers and through <a href=\"https://platform.openai.com/docs/guides/vision/vision\">their API</a>. This expands the model’s utility beyond the traditional text-only inputs, enabling it to be applied in a wider range of contexts. It handles images through the Chat Completions API, but note that the Assistants API does not support GPT-4V at this time.</p><p>GPT4-V supports advanced use cases like creating image captions, in-depth analysis of visual content, and interpreting text and graphics in documents.</p><h3>Hugging Face Inference API</h3><p>Hugging Face (HF) offers a free service for testing and evaluating over 150,000 publicly available machine learning models hosted on their platform through their <a href=\"https://huggingface.co/docs/api-inference/index\">Inference API.</a></p><p>HF provides a wide range of models, including transformer and diffusion-based models, that can help solve various NLP or vision tasks such as text classification, sentiment analysis, named entity recognition, etc.</p><blockquote>💡 Note that these free Inference APIs are <strong>rate-limited</strong> and not meant for production use. You can check out their <a href=\"https://huggingface.co/inference-endpoints\">Inference Endpoint service if you want good performance</a>.</blockquote><h3>Steps to use the Inference API</h3><ol><li><a href=\"https://huggingface.co/login\">Login</a> to Hugging Face.</li><li>Navigate to your profile on the top right navigation bar, then click “Edit profile.”</li><li>Click on the “Access Tokens” menu item.</li><li>Set the HF HUB API token:</li></ol><pre>export HUGGINGFACEHUB_API_TOKEN=your-token</pre><p>Use the HUGGINGFACEHUB_API_TOKEN as an environment variable</p><pre>import os<br />from huggingface_hub import HfApi</pre><pre> hf_api = HfApi(token=os.getenv(&quot;HUGGINGFACEHUB_API_TOKEN&quot;))</pre><p><strong>Run the Inference API</strong></p><p>Inference is the process of using a trained model to predict new data. The huggingface_hub library provides an easy way to call a service that runs inference for hosted models. As described above, you have two types of services available.</p><ul><li><a href=\"https://huggingface.co/docs/api-inference/index\"><strong>Inference API</strong></a>: run accelerated inference on Hugging Face’s infrastructure <strong>for free</strong>.</li><li><a href=\"https://huggingface.co/docs/inference-endpoints/index\"><strong>Inference Endpoints</strong></a>: easily deploy models to production <strong>(paid)</strong></li></ul><h4>Choose a model from the <a href=\"https://huggingface.co/models\">Model Hub</a></h4><p>The model checkpoints are stored in the Model Hub; you can search and share them. Note that not all models are available on the Inference API. Once the endpoint has been created, you should see a URL endpoint of it like the following:</p><pre>ENDPOINT = &lt;https://api-inference.huggingface.co/models/&gt;&lt;MODEL_ID&gt;</pre><p><strong>Run the inference:</strong></p><pre>import requests<br />API_URL = &quot;&lt;https://api-inference.huggingface.co/models/&gt;&lt;MODEL_ID&gt;&quot;<br />headers = {&quot;Authorization&quot;: f&quot;Bearer {API_TOKEN}&quot;}<br />def query(payload):<br />    response = requests.post(API_URL, headers=headers, json=payload)<br />    return response.json()<br />data = query(&quot;Can you please let us know more&quot;)</pre><h3>Hugging Face Tasks</h3><p>The team at <a href=\"https://huggingface.co/tasks\">Hugging Face has categorized</a> several models into the different tasks they can solve. You can find models for popular NLP tasks: Question Answering, Sentence Similarity, Summarization, Table Question Answering, and more.</p><p>Here is another example of using the Inference API for a summarization task:</p><pre>API_TOKEN = 'your_api_token_here'<br />model_name = 'facebook/bart-large-cnn'<br />text_to_summarize = &quot;Hugging Face's API simplifies accessing powerful NLP models for tasks like summarization, transforming verbose texts into concise, insightful summaries.&quot;<br />endpoint = f'&lt;https://api-inference.huggingface.co/models/{model_name}&gt;'<br />headers = {'Authorization': f'Bearer {API_TOKEN}'}<br />data = {'inputs': text_to_summarize}<br />response = requests.post(endpoint, headers=headers, json=data)<br />summarized_text = response.json()[0]['summary_text']<br />print(summarized_text)</pre><p>The pre-trained model used above is<strong>[facebook/bart-large-cnn](&lt;https://huggingface.co/facebook/bart-large-cnn&gt;) </strong>trained by Meta demonstrates the ability to produce clear and concise summaries.</p><blockquote><strong>Note:</strong> Not all models are available in this Inference API. Verify if the model is available by reviewing its ‘<em>Model card</em>’.</blockquote><h4>Sentiment analysis task:</h4><pre>headers = {&quot;Authorization&quot;: f&quot;Bearer {API_TOKEN}&quot;}<br />API_URL = &quot;&lt;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&gt;&quot;</pre><pre>def query(payload):<br />    response = requests.post(API_URL, headers=headers, json=payload)<br />    return response.json()</pre><pre>data = query({&quot;inputs&quot;: &quot;I love how this app simplifies complex tasks effortlessly . I'm frustrated by the frequent errors in the software's latest update&quot;})<br />print(data)</pre><h4>Text-to-image task:</h4><pre>model_id = &quot;runwayml/stable-diffusion-v1-5&quot;<br />pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)<br />pipe = pipe.to(&quot;cuda&quot;)</pre><pre>prompt = &quot;Create an image of a futuristic cityscape on an alien planet, featuring towering skyscrapers with glowing neon lights, a sky filled with multiple moons, and inhabitants of various alien species walking through vibrant market streets&quot;<br />image = pipe(prompt).images[0]</pre><pre>image.save(&quot;generated-image.png&quot;)</pre><blockquote>Image generated with stable diffusion is saved to your environment!</blockquote><p>You can also encode a sentence and get text embeddings.</p><pre>from sentence_transformers import SentenceTransformer<br />sentences = [&quot;GAIA's questions are rooted in practical use cases, requiring AI systems to interact with a diverse and uncertain world, reflecting real-world applications.&quot;, &quot; GAIA questions require accurate execution of complex sequences of actions, akin to the Proof of Work concept, where the solution is simple to verify but challenging to generate.&quot;]</pre><pre>model = SentenceTransformer('Equall/english-beta-0.3', use_auth_token=API_TOKEN)<br />embeddings = model.encode(sentences)<br />print(embeddings)</pre><blockquote><em>[[ 0.76227915 -0.5500489 -1.5719271 … -0.34034422 -0.27251056 0.12204967] [ 0.29783687 0.6476462 -2.0379746 … -0.28033397 -1.3997376 0.25214267]]</em></blockquote><p>You can also experiment with image-captioning models:</p><pre>from transformers import pipeline</pre><pre>image_to_text = pipeline(&quot;image-to-text&quot;, model=&quot;nlpconnect/vit-gpt2-image-captioning&quot;)</pre><pre>image_to_text(&quot;&lt;https://ankur3107.github.io/assets/images/image-captioning-example.png&gt;&quot;)</pre><pre># [{'generated_text': 'a soccer game with a player jumping to catch the ball '}]</pre><p>And also perform experiments with classification and image-to-text models, pre-trained on ImageNet:</p><pre>from transformers import ViTImageProcessor, ViTForImageClassification<br />from PIL import Image<br />import requests</pre><pre>url = '&lt;http://images.cocodataset.org/val2017/000000039769.jpg&gt;'<br />image = Image.open(requests.get(url, stream=True).raw)</pre><pre>processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')<br />model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')</pre><pre>inputs = processor(images=image, return_tensors=&quot;pt&quot;)<br />outputs = model(**inputs)<br />logits = outputs.logits<br /># model predicts one of the 1000 ImageNet classes<br />predicted_class_idx = logits.argmax(-1).item()<br />print(&quot;Predicted class:&quot;, model.config.id2label[predicted_class_idx])</pre><blockquote>preprocessor_config.json: 100%</blockquote><blockquote>160/160 [00:00&lt;00:00, 10.5kB/s]</blockquote><blockquote>config.json: 100%</blockquote><blockquote>69.7k/69.7k [00:00&lt;00:00, 3.60MB/s]</blockquote><blockquote>model.safetensors: 100%</blockquote><blockquote>346M/346M [00:02&lt;00:00, 162MB/s]</blockquote><p>the output calculated give us the prediction of an image provided:</p><pre>Predicted class: Egyptian cat</pre><p>Now we’ll scrape a web page using Rapid API to get the articles and summarize them with a huggingface model using HF model inference API.</p><pre># Function to fetch text from the API<br />def fetch_text_from_api():<br />    url = &quot;&lt;https://lexper.p.rapidapi.com/v1.1/extract&gt;&quot;<br />    querystring = {<br />        &quot;url&quot;: &quot;&lt;https://techcrunch.com/2023/11/25/neuralink-elon-musks-brain-implant-startup-quietly-raises-an-additional-43m/&gt;&quot;,<br />        &quot;js_timeout&quot;: &quot;30&quot;,<br />        &quot;media&quot;: &quot;true&quot;<br />    }<br />    headers = {<br />        &quot;X-RapidAPI-Key&quot;: &quot;xxx&quot;,<br />        &quot;X-RapidAPI-Host&quot;: &quot;lexper.p.rapidapi.com&quot;<br />    }<br />    response = requests.get(url, headers=headers, params=querystring)<br />    data = response.json()<br />    # Extract the relevant text from the API response<br />    # Adjust the following line according to the structure of your API response<br />    return data.get('article', {}).get('text', '')</pre><pre># Function to summarize the text using Hugging Face API<br />def query_huggingface(payload):<br />    API_URL = &quot;&lt;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&gt;&quot;<br />    headers = {&quot;Authorization&quot;: f&quot;Bearer {API_TOKEN}&quot;}<br />    response = requests.post(API_URL, headers=headers, json=payload)<br />    return response.json()</pre><pre># Fetch the text<br />text_to_summarize = fetch_text_from_api()</pre><pre># Summarize the text<br />summarization_payload = {<br />    &quot;inputs&quot;: text_to_summarize,<br />    &quot;parameters&quot;: {&quot;do_sample&quot;: False},<br />}</pre><pre>summary_response = query_huggingface(summarization_payload)<br />print(summary_response)</pre><blockquote><em>[{‘summary_text’: ‘Elon Musk-founded company raises $43 million in new venture capital. The company is developing implantable chips that can read brain waves. Critics say the company has a toxic workplace culture and unethical research practices. In June, Reuters reported that the company was valued at about $5 billion.’}]</em></blockquote><h3>Conclusion</h3><p>You have learned how to use the OpenAI Assistants API, and its essential components like Threads and Messages. On a concrete example its shown how the AI assistant can be deployed as a tech support, empowering customer interaction.</p><p>In Hugging Face’s free Inference API section, through practical implementations, you’ve seen how to authenticate, access models via the Model Hub, and perform various NLP tasks.</p><p>Hope this quick walkthrough was helpuful and gave you additonal perspective on Assistants construct, especially the ones you can construct via API — .</p><p>Cheers!</p><p>RESOURCES<br />the Google colab notebook:</p><p><a href=\"https://colab.research.google.com/drive/1GkjoTZUd2rpwIvxmCZreKo5ZUZN6i7vJ#scrollTo=9FDX0V0el1_T\">Google Colaboratory</a></p><blockquote><em>Note: This article is previously published on Notion, you can read the version written as the lesson for Towards-AI courses on RAG SYSTEMS for Activeloop.</em></blockquote><blockquote><a href=\"https://signalism.notion.site/Crafting-AI-Assistants-via-OpenAI-and-Hugging-Face-API-92c7331225c54eb1966856bbc662b808?pvs=4\"><em>https://signalism.notion.site/Crafting-AI-Assistants-via-OpenAI-and-Hugging-Face-API-92c7331225c54eb1966856bbc662b808?pvs=4</em></a></blockquote><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=332fc945837c\" width=\"1\" /><hr /><p><a href=\"https://medium.com/artificialis/ai-assistants-via-openai-and-hugging-face-api-332fc945837c\">AI Assistants via OpenAI and Hugging Face API</a> was originally published in <a href=\"https://medium.com/artificialis\">Artificialis</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",[{'name': 'Iva @ AkashaDB'}],"Wed, 10 Jan 2024 11:23:15 GMT"
1187,Detecting ships in satellite imagery: five years later…,https://medium.com/artificialis/detecting-ships-in-satellite-imagery-five-years-later-28df2e83f987?source=rss----a81c8d170222---4,"<p>In 2018, when I was still working at Airbus Defence and Space, I organised a challenge on Kaggle to detect ships in Airbus SPOT satellite imagery (@ 1.5 meters resolution).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yJDbxqHJzzeN855YNNXBvQ.png\" /><figcaption>Home page of the Airbus Ship Detection Challenge on Kaggle</figcaption></figure><p><a href=\"https://medium.com/artificialis/important-things-you-should-know-before-organizing-a-kaggle-competition-3911b71701fb\">Check my article</a> about the successes and issues that we encountered. That’s a whole story in itself!</p><p>One of the interesting characteristics of the challenge was the <strong>oriented bounding boxes annotations</strong>. When we annotated the dataset, we decided to create rotated rectangles to closely fit the ships that we saw on the imagery. Intuitively, we thought that classic bounding boxes were including too much background. Of course, this was not perfect as well for small speed boats which we could not clearly separate from their wake.</p><p>But at the time, there was very little literature about oriented bounding boxes. So the Kaggle team decided to encode our rectangle bounding boxes into RLE which is a format suited for creating binary masks. And actually most teams used deep learning architectures suited to mask prediction. I will analyse the best practices for segmentation used by the participants and described on Kaggle forums. And then present a completely different approach based on new oriented bounding box architectures.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/913/1*wpQxH6QhtZIlzMutF139Lw.png\" /><figcaption>Bounding box around a ship and corresponding mask (encoded in RLE format)</figcaption></figure><h3>Analysis of winner solutions in 2018</h3><p>But before developing a segmentation model to identify the ships, the participants needed to overcome a first problem of unbalanced classes.</p><p><strong>The sea is mostly void of ships</strong>. And waves or clouds can look like small ships on a satellite image. So we wanted the dataset to include some images of sea without any ships and with various conditions of weather and sea formation.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/612/1*t7KTpllegyf7o7U3eUfg3Q.png\" /><figcaption>Image of sea with no ships — there can be a lot a variation!</figcaption></figure><p>But, unfortunately, the way we generated the imagery created a LOT of images with no ships ; actually out of 231,723 imagery extracts of 768 x 768 pixels, only 81,723 had ships on them. Models usually do not like to be trained on too many empty images, so it made sense to reduce the number of empty images during the training.</p><p>But if the segmenter had not seen a lot of empty images during training, this meant that it would be prone to over-detection during inference. So most of the participants created a two stage detector with first a classifier and then segmenter. The purpose of the classifier was to detect if the image was likely to contain ships and only send these likely images to the segmenter.</p><p>The other reason to create a classifier was that the chosen metric for scoring participants was very biased against detecting a single ship when there was none. This in itself will be the subject for another post. It definitely pushed participants to create this two-stage detector.</p><blockquote>This is not a bad thing because classifiers are much faster then segmentation models and, as I said earlier, sea is mostly void of ships — so this means that the detector was going faster on vast areas of open sea.</blockquote><h3>First step : Ship &lt;&gt; No Ship Classifier</h3><p>Training a good Ship &lt;&gt; No Ship classifier was more important for this competition than having a very good performing segmentation model. The objective was to fight fight false positives (bright spots, wave glare, etc). The classifier should, of course, to be as good as possible and to avoid classifying images with ships as images without ships. But note, that there is an asymmetry in this process: if an image with ship was classified without ship it was lost forever although an error in the other direction could potentially be corrected by the segmenter.</p><p>Back in 2018, the architectures selected for this task where mostly all flavours of ResNet (ResNet-18, ResNet-34, ResNet-50, ResNet-101 and ResNet-152) with some others like DPN-68, DPN-92 or DenseNet-121. A simple training of a ResNet-50 over 10 epochs leads to a 0.975 accuracy which was good but not enough to be in the top 3.</p><p>All the winners decided to train an «ensemble» of models for the classifier. Either by training different architectures or the same architecture on different split of the training data (using cross-validation). Then the results from the various models would just be summed and averaged.</p><h3>Second step : Segmenter</h3><p>Segmentation models are neural networks which produces a mask from an input image. They are usually U-shaped and for this called U-Net. The left side of the U is the encoder which encode the semantic information while reducing the spatial dimension. Then, the right side of the U acts as a decoder to generate the mask containing the semantic information at the correct location. To make sure that the reconstruction is fine-grained, there are shortcuts connections from the encoding branch to the decoding branch to convey the spatial information which has been lost in the semantic encoding.</p><p>All winners used U-Net architecture with various backbones (i.e. encoders). Some participants used FPN architectures and other Mask-RCNN architectures (see below). Larger backbones are not always a must. Smaller architectures like ResNet-34 could work well if trained long enough.</p><p>Since that images where pretty large (768x768 pixels) to fit in large batches in the GPU memory, one smart trick was to use 256x256 crops, randomly selected around the center of the ships. Of course, resizing images was out of questions because it will make the small ships disappear.</p><p>Of course, since the dataset was unbalanced and had a lot of empty images, it was important to reduce number of images with no ships. But one should make sure not to remove them completely. A mix of 20% of images with no ships was important to help the model understand that this could be a normal situation. This could also be done by augmenting the images with ships compared to the images without ships.</p><p>Most participants used several U-Net either with different architecture or specialised on various tasks (like large ships and small ships). The final “ensembling” step was mostly to sum or average the various generated masks. In some case, TTA (or Test Time Augmentation) was also used. This means that the same image was sent to the model after being flipped and/or rotated. The results will then be averaged again.</p><p><a href=\"https://www.kaggle.com/competitions/airbus-ship-detection/discussion/71667\"><em>https://www.kaggle.com/competitions/airbus-ship-detection/discussion/71667</em></a></p><p><a href=\"https://www.kaggle.com/competitions/airbus-ship-detection/discussion/71664\"><em>https://www.kaggle.com/competitions/airbus-ship-detection/discussion/71664</em></a></p><h3>Final step : Post processing</h3><p>After the U-Net had produced a mask, the the last step was to vectorise this mask and create distinct polygons for each individual ship. But, in fact, to deal with ships which were docked side by side, an extra trick was necessary.</p><p>The exterior boundary of the ship had to be treated as a second class. Some participants also decided to treat the contact zone between two ships as a third class. In any case, the trick was to make sure to identify this area as it was key in being able to reconstruct the individual shape of each ship when they were side by side.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/768/0*qBYFCWvdJVFb1khV.png\" /><figcaption>Image extracted from discussion below</figcaption></figure><p><a href=\"https://www.kaggle.com/competitions/airbus-ship-detection/discussion/71659\"><em>https://www.kaggle.com/competitions/airbus-ship-detection/discussion/7165</em></a><a href=\"https://www.kaggle.com/competitions/airbus-ship-detection/discussion/71659\"><em>9</em></a></p><p>The rest of the processing is then done by using python Computer Vision like rasterio or ski-image. Using morphological tools like erosion and dilatation to remove objects under a specific threshold, using a convex hull function, creating an image of distance from border of objects, extracting local peaks or centroid, using watershed techniques and finally vectorisation.</p><p>One interesting technique was definitely to use <strong>pseudo-labelling</strong>. The dataset included a large number of unlabelled tiles which was designed to be used to test the model. Participants were expected to predict ships on the test dataset and submit the resulting predictions as RLE masks. But the predictions could also be used to further train the model and potentially increase slightly the performance of the model.</p><p>When all these steps were done the best possible way, it was possible to reach the highest score on the leaderboard. Typically above 0.854. <a href=\"https://www.youtube.com/watch?v=0Opb8gB1p4w\">Here is a nice video</a> of how trials and errors can lead to an honorable submission.</p><h3>Rotated rectangle or Oriented bounding boxes?</h3><p>Starting from vectorised annotations to generate masks, then using segmentation architectures to generate raster masks and finally vectorising the predicted masks to deliver oriented bounding boxes made a few participants explore other techniques.</p><p>Some participants were quite successful using Mask-RCNN although it was based on simple axis-aligned bounding boxes. <a href=\"https://www.kaggle.com/competitions/airbus-ship-detection/discussion/71607\">Check the following Kaggle post</a>.</p><p>At the time, we would also see the first papers on ArXiv about oriented bounding box architectures. The main idea was to add an extra parameter to the model to predict the orientation of the bounding box either as a discreet parameter or a continuous parameter. In the later case, there was some added complexity to the code as an angle is non-continuous between 359.99° and 0°.</p><p>At the time, a few participants <a href=\"https://www.kaggle.com/competitions/airbus-ship-detection/discussion/71875\">experimented with oriented bounding boxes</a> detector which seemed like a reasonable thing to do but they did not get very good results.</p><p><strong><em>Let’s explore what we can do in 2023 in that respect!</em></strong></p><h3>How to get oriented bounding boxes annotation?</h3><p>Before being able to implement new type of solution, we need to have oriented bounding boxes annotations. And although Airbus created the annotation as rectangles, these are not available in the Kaggle dataset.</p><p>So we need to recreate them from the RLE encodings. Kaggle grandmaster iafoss created an new version of the dataset with oriented bounding boxes but, unfortunately, it is based on the original version of the dataset. Actually, there are two version of the dataset and only the last one is available. Check <a href=\"https://medium.com/artificialis/important-things-you-should-know-before-organizing-a-kaggle-competition-3911b71701fb\">my previous post</a> for a longer story about why this is the case.</p><p>Again we will be using some fantastic python packages like rasterio and shapely. Here is the code to convert RLE to oriented bounding boxes. You can check this Kaggle notebook to see it in action.</p><pre>    # convert RLE encoded pixels into binary mask<br />    mask = encode_mask(str(row.EncodedPixels), shape=(768,768))<br /><br />    # vectorize mask into GeoJSON<br />    value = 0.0<br />    for polygon, value in list(features.shapes(mask)):<br />        if value == 1.0:<br />            break<br />    if value != 1.0:<br />        print('Error while vectorizing mask')<br /><br />    # get oriented bounding box around shape<br />    coords = polygon['coordinates'][0]<br />    obbox = MultiPoint(coords).minimum_rotated_rectangle<br /><br />    # get center of bounding box and correct for half a pixel<br />    xc, yc = list(obbox.centroid.coords)[0]<br />    xc, yc = xc - 0.5, yc - 0.5<br />    <br />    # get external coordinates of oriented rectangle<br />    # compute length, width and angle<br />    p1, p2, p3, p4, p5 = list((obbox.exterior.coords))<br />    dx = math.sqrt((p3[0] - p2[0])**2 + (p3[1] - p2[1])**2)<br />    dy = math.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)<br />    angle = math.atan2((p3[1] - p2[1]), (p3[0] - p2[0]))<br />    #length = max(d1, d2)<br />    #height = min(d1, d2)</pre><p>Some examples to check for quality. The visualisation scripts are mostly based on iafoss work that you can found <a href=\"https://www.kaggle.com/code/iafoss/rotating-bounding-boxes-ship-localization\">here</a>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0VKl0S72AkleghP_266okw.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*G24pb9l-pHkrsSw2ssQIEg.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YSAG7a6LAY3-Fs_pGU-sIA.png\" /></figure><p>These examples show that although some ships are easy to detect, there are also many cases where the boundaries are not clear, the acquisition angle not vertical or the colour very similar to the sea colour.</p><blockquote>Let’s now dive into a new fantastic framework to work with oriented bounding boxes detectors. It is called MMRotate from Open MMLab.</blockquote><h3>Introducing OpenMMLab and MMRotate</h3><p>In one of <a href=\"https://medium.com/artificialis/how-to-choose-a-deep-learning-model-to-detect-aircrafts-in-satellite-imagery-cd7d106e76ad\">my previous post</a>, I presented and used <a href=\"https://airctic.com/\">IceVision</a> to test various model architectures on the same task. Another fantastic framework to experiment with various computer vision deep learning architectures is the <a href=\"https://openmmlab.com/\">OpenMMLab</a> framework.</p><blockquote>OpenMMLab is an open-source algorithm platform for computer vision. It aims to provide a solid benchmark and promote reproducibility for academic research. We have released more than 30 high-quality projects and toolboxes in various research areas such as image classification, object detection, semantic segmentation, action recognition, etc. OpenMMLab has made public more than 300 algorithms and 2,400 checkpoints. Over the past years, OpenMMLab has gained popularity in both academia and industry. It receives over 78,000 stars on GitHub and involves more than 1,700 contributors in the community.</blockquote><p>The code base is available on GitHub and is organised in various independent task-specific packages which all rely on foundation libraries <strong>MMEngine</strong> for training loops, <strong>MMCV</strong> for computer vision functions and <strong>PyTorch</strong> as Deep Learning library.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*R_zN35FHXj4_KsL7.png\" /><figcaption>From <a href=\"https://github.com/open-mmlab\">OpenLab GitHub page</a></figcaption></figure><p>Aside <strong>MMDetection</strong> which offers model architectures and tools for standard bounding box detection, there a specific package for oriented bounding boxes which is called <strong>MMRotate.</strong></p><p>The code was published and very actively updated one year ago in spring 2022. Recently the team has focused on introducing the MMEngine module which centralise the deep learning related code and reorganising the various modules accordingly.</p><h4>Install the MMRotate package and its dependencies</h4><p>In the following code, we will stick with the version 0.3.4 from 2022. <br />The corresponding documentation is <a href=\"https://mmrotate.readthedocs.io/en/v0.3.4/get_started.html\">here</a>. Note that we make sure to install the corresponding MMCV and MMDet versions.</p><pre># Install MMCV and MMDetection<br />RUN pip install -U openmim<br />RUN mim install mmcv-full==1.7.1<br />RUN mim install mmdet==2.28.2<br /><br /># Install MMRotate v0.3.4<br />RUN git clone --depth 1 --branch v0.3.4 https://github.com/open-mmlab/mmrotate.git<br />WORKDIR mmrotate</pre><h4>Create a custom Data Loader for our data</h4><p>A simple way to adapt the code to a specific use case is to convert the training data to the DOTA format as described <a href=\"https://mmrotate.readthedocs.io/en/v0.3.4/tutorials/customize_dataset.html\">here</a>.</p><p>I usually like to preserve the source data as it is so that I can more easily manage multiple version of it. For MMRotate, this mostly involves creating a new <strong>Dataset Type</strong> and loading annotations directly from where they are stored. In our case, since the source annotation are provided as a CSV file, I am using a pandas DataFrame to read and store the annotations.</p><p>Here is the code that will help you create your own Dataset Type:</p><pre>from mmrotate.datasets.builder import ROTATED_DATASETS, PIPELINES<br />from mmrotate.datasets.dota import DOTADataset<br />import glob<br />import numpy as np<br />import pandas as pd<br />from mmrotate.core import poly2obb_np<br />import os.path as osp<br />import os<br /><br />from mmdet.datasets.pipelines import LoadImageFromFile<br />from PIL import Image<br /><br />def convert_rbb2polygon(bbox):<br />    xc, yc, w, h, ag = bbox<br />    wx, wy = w / 2 * np.cos(ag), w / 2 * np.sin(ag)<br />    hx, hy = -h / 2 * np.sin(ag), h / 2 * np.cos(ag)<br />    p1 = (xc - wx - hx, yc - wy - hy)<br />    p2 = (xc + wx - hx, yc + wy - hy)<br />    p3 = (xc + wx + hx, yc + wy + hy)<br />    p4 = (xc - wx + hx, yc - wy + hy)<br />    poly = np.array([p1[0], p1[1], p2[0], p2[1], p3[0], p3[1], p4[0], p4[1]])<br />    return poly<br />        <br />def convert_rbb2polygons(bboxes):<br />    polygons = []<br />    for i, bbox in enumerate(bboxes):<br />        poly = convert_rbb2polygon(bbox)<br />        polygons.append(poly)<br />    return polygons<br /><br />@ROTATED_DATASETS.register_module()<br />class AirbusShipDataset(DOTADataset):<br />    &quot;&quot;&quot;Airbus Ship dataset for detection.&quot;&quot;&quot;<br /><br />    CLASSES = ('ship',)<br /><br />    def load_annotations(self, ann_file):<br />        &quot;&quot;&quot;<br />            Args:<br />                ann_folder: folder which contains the CSV file witn annotations<br />        &quot;&quot;&quot;<br />        cls_map = {c: i<br />           for i, c in enumerate(self.CLASSES)<br />           }<br />        <br />        data_infos = []<br />        if not os.path.isfile(ann_file) :  # test phase<br />            img_files = glob.glob(self.img_prefix + '*.jpg')<br />            for img_file in img_files:<br />                data_info = {}<br />                img_id = osp.split(img_file)[1][:-4]<br />                img_name = img_id + '.jpg'<br />                data_info['filename'] = img_name<br />                data_info['ann'] = {}<br />                data_info['ann']['bboxes'] = []<br />                data_info['ann']['labels'] = []<br />                data_infos.append(data_info)<br />        else:<br />            box_df = pd.read_csv(ann_file)<br />            <br />            # parse all images<br />            for ImageId, ann_df in box_df.groupby('ImageId'):<br />                data_info = {}<br />                img_id = osp.split(ImageId)[1][:-4]<br />                img_name = ImageId #img_id + '.jpg'<br />                data_info['filename'] = img_name<br />                data_info['ann'] = {}<br />                gt_bboxes = []<br />                gt_labels = []<br />                gt_polygons = []<br />                gt_bboxes_ignore = []<br />                gt_labels_ignore = []<br />                gt_polygons_ignore = []<br /><br />                if len(ann_df) == 0 and self.filter_empty_gt:<br />                    continue<br />                    <br />                # parse all annotations in this image<br />                for row_index, row in ann_df.iterrows():<br />                    <br />                    # read annotation<br />                    x, y, w, h, a = row['xc'], row['yc'], row['dx'], row['dy'], row['angle'], <br />                    label = cls_map['ship']<br />                    <br />                    # add to lists of annotations for this image<br />                    gt_bboxes.append([x, y, w, h, a])<br />                    gt_labels.append(label)<br />                    poly = convert_rbb2polygon([x, y, w, h, a])<br />                    gt_polygons.append(poly)<br /><br />                if gt_bboxes:<br />                    data_info['ann']['bboxes'] = np.array(<br />                        gt_bboxes, dtype=np.float32)<br />                    data_info['ann']['labels'] = np.array(<br />                        gt_labels, dtype=np.int64)<br />                    data_info['ann']['polygons'] = np.array(<br />                        gt_polygons, dtype=np.float32)<br />                else:<br />                    data_info['ann']['bboxes'] = np.zeros((0, 5),<br />                                                          dtype=np.float32)<br />                    data_info['ann']['labels'] = np.array([], dtype=np.int64)<br />                    data_info['ann']['polygons'] = np.zeros((0, 8),<br />                                                            dtype=np.float32)<br /><br />                if gt_polygons_ignore:<br />                    data_info['ann']['bboxes_ignore'] = np.array(<br />                        gt_bboxes_ignore, dtype=np.float32)<br />                    data_info['ann']['labels_ignore'] = np.array(<br />                        gt_labels_ignore, dtype=np.int64)<br />                    data_info['ann']['polygons_ignore'] = np.array(<br />                        gt_polygons_ignore, dtype=np.float32)<br />                else:<br />                    data_info['ann']['bboxes_ignore'] = np.zeros(<br />                        (0, 5), dtype=np.float32)<br />                    data_info['ann']['labels_ignore'] = np.array(<br />                        [], dtype=np.int64)<br />                    data_info['ann']['polygons_ignore'] = np.zeros(<br />                        (0, 8), dtype=np.float32)<br /><br />                data_infos.append(data_info)<br /><br />        self.img_ids = [*map(lambda x: x['filename'][:-3], data_infos)]<br />        return data_infos</pre><h4>Configuration file</h4><p>MMRotate is all <a href=\"https://mmrotate.readthedocs.io/en/v0.3.4/tutorials/customize_config.html\">about configuration files</a> as the other OpenMMLab components. You can certainly use MMRotate without but they are the best way to manage various experiments. The idea behind this is that all the parameters for your dataset, you model and your training loop are actually stored in a configuration file (as a Python dictionary or a YAML file).</p><p>This makes it very easy to compared parameters between two experiments (i.e. training of a model). This also enables you to start from one of the existing configuration files and just modify the parameters that you need. OpenMMLab provides a <a href=\"https://mmrotate.readthedocs.io/en/v0.3.4/model_zoo.html\">benchmark and model zoo page</a> where you will find all the base configuration files that you can tweak to your needs.</p><p>For exemple, here, we select <strong>ReDet</strong> which is described in this paper:<br /><a href=\"https://arxiv.org/abs/2103.07733\"><em>ReDet: A Rotation-equivariant Detector for Aerial Object Detection</em></a><em>. </em>The following code download the configuration file for the model with a ResNet-50 backbone, the training loop as well as the associated pre-trained weights on the DOTA dataset.</p><pre># We use mim to download the pre-trained checkpoints for inference and finetuning.<br />!mim download mmrotate --config redet_re50_refpn_1x_dota_ms_rr_le90 --dest .<br /><br />CONFIG_FILE = 'redet_re50_refpn_1x_dota_ms_rr_le90.py'<br /><br />cfg = Config.fromfile(CONFIG_FILE)<br />print(f'Config:\\n{cfg.pretty_text}')</pre><p>We need to make a few changes to this configuration file. We can copy and modify the file or we can make the changes programmatically to the configuration in memory. So we can use Python rather than YAML.</p><p>Here we define the source data:</p><pre># Change the dataset type from DOTADataset to AirbusShipDataset<br />cfg.dataset_type = 'AirbusShipDataset'<br /><br /># Change the location of the root folder for training data<br />cfg.data_root = '/data/share/airbus-ship-detection/'<br /><br /># Change the size of the image (instead of 1024 with DOTA)<br />cfg.img_size = 768<br /><br /># Adapt the number of images accordingly with your GPU memory<br />cfg.data.samples_per_gpu=20<br /><br /># Define the value for normalization<br /># This needs to be computed in the EDA<br /># MMRotate uses cv2 to read imagery so it expects BGR and not RGB<br /># Re-order the channels to RGB after normalization<br />cfg.img_norm_cfg = dict(<br />    mean=[52.29048625, 73.2539164, 80.97759001], <br />    std=[53.09640994, 47.58987537, 42.15418378], <br />    to_rgb=True)</pre><p>And next, the data pre-processing:</p><pre>cfg.train_pipeline = [<br />    # Read images from file with MMDet (cv2)<br />    dict(type='LoadImageFromFile'),<br />    # Read annotations. This is the function that we overloaded earlier<br />    dict(type='LoadAnnotations', with_bbox=True),<br />    # Resize to the initial size of the model<br />    # For satellite images, we want to avoid any downscaling<br />    dict(type='RResize', img_scale=(cfg.img_size, cfg.img_size)),<br />    # Define a 'flip' augmentation<br />    dict(<br />        type='RRandomFlip',<br />        flip_ratio=[0.25, 0.25, 0.25],<br />        direction=['horizontal', 'vertical', 'diagonal'],<br />        version='le90'),<br />    # Define a 'rotation' augmentation<br />    dict(<br />        type='PolyRandomRotate',<br />        rotate_ratio=0.5,<br />        angles_range=180,<br />        auto_bound=False,<br />        version='le90'),<br />    # Normalize the radiometry<br />    dict(type='Normalize', **cfg.img_norm_cfg),<br />    # Pad the image to be divisible by 32<br />    dict(type='Pad', size_divisor=32),<br />    # Return images, bboxes and labels as tensors<br />    dict(type='DefaultFormatBundle'),<br />    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])<br />]</pre><p>Then, we initialise the training data source with our previous parameters: the dataset type, the dataset root, the train.csv file with train annotations and the train data pipeline.</p><pre>cfg.data.train=dict(<br />    type=cfg.dataset_type,<br />    ann_file='train.csv',<br />    img_prefix='train_v2/',<br />    pipeline=cfg.train_pipeline,<br />    version='le90',<br />    data_root=cfg.data_root<br />)</pre><p>We also need to tweak the final layers of the model because DOTA has 15 classes and we have only one class (for ships)</p><pre># modify num classes of the model in two bbox head<br />cfg.model.roi_head.bbox_head[0].num_classes = len(AirbusShipDataset.CLASSES)<br />cfg.model.roi_head.bbox_head[1].num_classes = len(AirbusShipDataset.CLASSES)</pre><p>There are many more modifications that you can make, like modifying the learning rate, the learning rate policy, the anchor generator, etc… Here we just define the maximum epochs and the logging interval.</p><pre>cfg.runner.max_epochs = 50<br />cfg.log_config.interval = 200</pre><p>Finally, I like to organise the logs by architecture and date and I make sure to save the configuration file with the logs so that I can retrieve it later if needed.</p><pre># Get architecture name from config file name<br />config_name = os.path.basename(CONFIG_FILE)<br />keyname = config_name.split('_')[0]<br /><br /># Get current time<br />now = datetime.datetime.now()<br />date_time = now.strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;)<br /><br /># Set up folder to save config file and logs<br />cfg.work_dir = os.path.join('./logs', keyname, date_time)<br /><br /># Create folder<br />mmcv.mkdir_or_exist(os.path.abspath(cfg.work_dir))<br /><br /># Save updated configuration file in logs folder<br />cfg.dump(os.path.join(cfg.work_dir, config_name))</pre><p>After we have finished modifying the configuration file, the next step is to train the model.</p><pre>from mmdet.models import build_detector<br />from mmdet.apis import train_detector<br /><br /># Build the train dataset (and optionaly validation dataset)<br />datasets = [build_dataset(cfg.data.train)]<br /><br /># Build the detection model<br />model = build_detector(cfg.model)<br /><br /># Launch training<br />train_detector(model, datasets, cfg, distributed=False, validate=False)</pre><p>After 20 epochs, the metrics (mAP) reaches a plateau and we can stop the training to display some outputs.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/524/1*xwGRR6k_H7Q-tqeYyHN1pw.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/437/1*Cr2dR59Rjm5bRj7gDBe-kg.png\" /></figure><p>The models performs pretty well in most case and deals pretty well with ships close to each other. The models runs faster than UNet and delivers directly oriented bounding boxes.</p><p>Of course, this is not directly an exceptional model like the ones which were on top of the leaderboard. Some improvement points are:</p><ul><li>Add a classifier before the detector to remove false detections</li><li>Modify the anchor generator to fit the specific shape of ships</li><li>Add TTA (Test-Time Augmentation)</li><li>Increase the dataset by doing pseudo-labelling on the test data</li></ul><blockquote>But globally it is very easy to apply MMRotate to this dataset and the OpenMMLab framework should have its place in most R&amp;D work for object detection on satellite and aerial imagery.</blockquote><h3>Conclusion</h3><p>If you have not already experimented with OpenMMLab and MMRotate, I hope that this blog post will motivate you to test it. I am using it in many of my projects and I am impressed. It requires somewhat of a learning curve but it definitely worth it as it is a powerful yet flexible tool.</p><p>If you like this post, please <strong>clap</strong> for it, <strong>follow</strong> me and <strong>read</strong> my other articles:</p><ul><li><a href=\"https://medium.com/artificialis/is-yolov8-suitable-for-satellite-imagery-d9a2659a50ab\">Is YOLOv8 suitable for satellite imagery?</a></li><li><a href=\"https://medium.com/artificialis/how-to-choose-a-deep-learning-model-to-detect-aircrafts-in-satellite-imagery-cd7d106e76ad\">How to choose a deep learning architecture to detect aircrafts in satellite imagery?</a></li><li><a href=\"https://medium.com/artificialis/oil-storage-detection-on-airbus-imagery-with-yolox-9e38eb6f7e62\">Oil Storage Detection on Airbus Imagery with YOLOX</a></li><li><a href=\"https://medium.com/artificialis/detecting-aircrafts-on-airbus-pleiades-imagery-with-yolov5-5f3d464b75ad\">Detecting aircraft on Airbus Pleiades imagery with YOLOv5</a></li><li><a href=\"https://medium.com/artificialis/important-things-you-should-know-before-organizing-a-kaggle-competition-3911b71701fb\">Things you should know before organising a Kaggle Competition</a></li></ul><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=28df2e83f987\" width=\"1\" /><hr /><p><a href=\"https://medium.com/artificialis/detecting-ships-in-satellite-imagery-five-years-later-28df2e83f987\">Detecting ships in satellite imagery: five years later…</a> was originally published in <a href=\"https://medium.com/artificialis\">Artificialis</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",[{'name': 'Jeff Faudi'}],"Thu, 16 Nov 2023 08:40:20 GMT"
1188,Sound Bytes Part 1: The ABCs of Sound and Digitization,https://medium.com/artificialis/sound-bytes-part-1-the-abcs-of-sound-and-digitization-94423e756969?source=rss----a81c8d170222---4,<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/artificialis/sound-bytes-part-1-the-abcs-of-sound-and-digitization-94423e756969?source=rss----a81c8d170222---4\"><img src=\"https://cdn-images-1.medium.com/max/1792/0*8NE47vcCOfwCt3KI\" width=\"1792\" /></a></p><p class=\"medium-feed-snippet\">Diving into the world of sound and how Deep Learning enhances our audio experience</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/artificialis/sound-bytes-part-1-the-abcs-of-sound-and-digitization-94423e756969?source=rss----a81c8d170222---4\">Continue reading on Artificialis »</a></p></div>,[{'name': 'Alessandro Lamberti'}],"Tue, 31 Oct 2023 08:28:06 GMT"
1189,Building Music Recommendation System,https://medium.com/artificialis/music-recommendation-system-with-scikit-learn-30f4d07c60b3?source=rss----a81c8d170222---4,"<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*-2g1Ue_GUvNkCaSb\" /><figcaption>Photo by <a href=\"https://unsplash.com/@alpridephoto?utm_source=medium&amp;utm_medium=referral\">Андрей Сизов</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>If you are a user of any social media or any websites, I am sure you could be caught in the tracking net of one of them. Do you wonder how those systems are made? I mean, the recommendation system can make you obsess with your time. Or if you are curious about how recommendation systems are made, let’s create a simple top ten music recommendation system with Scikit-Learn.</p><p>There are three types of systems. <strong>Collaborative filtering</strong> and <strong>Content-based filtering</strong> are the two main systems, and <strong>Hybrid</strong> recommendation is the combination of those two systems. Most recommendation systems now use the hybrid one.</p><p>If you want to read more without continuing my blog,</p><blockquote>Recommender system. (2023, April 17). In <em>Wikipedia</em>. <a href=\"https://en.wikipedia.org/wiki/Recommender_system\">https://en.wikipedia.org/wiki/Recommender_system</a></blockquote><h3>Collaborative filtering</h3><p>Collaborative filtering recommends a user based on their past user histories, their preferences, and their interests. There are many types of filtering methods in collaborative filtering. But the most popular one is <strong>user-based collaborative filtering</strong>.</p><p><strong>User-Based collaborative filtering</strong> depends on similarity between users. It looks at one’s preferences and recommends another user based on what the first user did. For example, if you are a programmer, recommend the videos that are viewed by other programmers too.</p><figure><img alt=\"Collaborative filtering (User-Based)\" src=\"https://cdn-images-1.medium.com/max/1024/1*3-5ilXK1ZTjcUzjXRPLANg.png\" /><figcaption>Collaborative filtering (User-Based)</figcaption></figure><h3>Content-Based filtering</h3><p>Content-based filtering is pretty simple. Depending on what he liked in the past, recommend an item that is similar to his preferences. Unlike user-based filtering, content-based filtering only focuses on a single user.</p><figure><img alt=\"Content-Based filtering\" src=\"https://cdn-images-1.medium.com/max/1024/1*MI0WwFxPQzf8ZGdfeVwj9w.png\" /><figcaption>Content-Based filtering</figcaption></figure><p>The main disadvantage of this filtering system is, “<strong>What if for a new user?</strong>” How can we handle this? Yes, we have “user-based filtering”. But let’s talk about <strong>Hybrid</strong> recommendations in the next section.</p><h3>Hybrid recommendation</h3><p>This is the most commonly used recommendation method now a days. It is the combination of two or more filtering systems. The more common are, as described above, user-based filtering and content-based filtering systems. For example, you won’t see the most-viewed videos in your country if you don’t sign in to YouTube. But after signing in, you will see the recommended videos based on your past history.</p><h3>Building recommendation system in Python</h3><p>It is time to build our own recommendation system using Python. Although we can write from scratch using math and just Python, we can get help from a library called Scikit-Learn. And there are some other libraries that are specific designed for building recommendation systems.</p><p>In this blog, we will take a look at how “content-based filtering” systems are made.</p><h4>CountVectorizer</h4><p>CountVectorizer converts a collection of documents into a matrix of word counts. Let’s see how CountVectorizer works.</p><p>Here is a sentence: “Nikola Tesla invented the alternating current”. CountVectorizer will convert that sentence into something like this:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ToZRDVumaeZ42kCOsiBZ1w.png\" /></figure><p>You can see that CountVectorizer counts the word frequency. If there are more than two the in that sentence, you will see 2 instead of 1.</p><p>To be clear, let’s add one more sentence to that same vector. “Thomas Edison invented the direct current”</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*52ywx6NevmpluWTlDYSgqg.png\" /></figure><p>It is saying it will create a new column with a word if the new inserted word in the new sentence doesn’t exist. The limitation of CountVectorizer is that it also counts less important words like a , the , and . But CountVectorizer is suitable for our project. We don’t need to use it in complete sentences.</p><h4>Python codes</h4><p>We will import the required dependencies and read the data. We will only use the first 1000 simples to load faster in development.</p><pre># import requried dependencies<br />import pandas as pd<br />from sklearn.feature_extraction.text import CountVectorizer<br />from sklearn.metrics.pairwise import cosine_similarity<br /><br /># read the data<br />df = pd.read_csv(&quot;data/song-dataset.csv&quot;, low_memory=False)[:1000]</pre><p>Clean the data frame. We will remove the non-required columns from the data frame.</p><pre># remove duplicates<br />df = df.drop_duplicates(subset=&quot;Song Name&quot;)<br /><br /># drop Null values<br />df = df.dropna(axis=0)<br /><br /># Drop the non-required columns<br />df = df.drop(df.columns[3:], axis=1)</pre><p>Removing spaces from the “Artist Name” column. Because if there is a space between the names, CountVectorizer will count that single name as one word.</p><pre># Removing space from &quot;Artist Name&quot; column<br />df[&quot;Artist Name&quot;] = df[&quot;Artist Name&quot;].str.replace(&quot; &quot;, &quot;&quot;)</pre><p>Combine all the column values into a single new column value,</p><pre># Combine all columns and assgin as new column<br />df[&quot;data&quot;] = df.apply(lambda value: &quot; &quot;.join(value.astype(&quot;str&quot;)), axis=1)</pre><p>Initialize the CountVectorizer,</p><pre># models<br />vectorizer = CountVectorizer()<br />vectorized = vectorizer.fit_transform(df[&quot;data&quot;])<br />similarities = cosine_similarity(vectorized)<br /><br /># Assgin the new dataframe with `similarities` values<br />df_tmp = pd.DataFrame(similarities, columns=df[&quot;Song Name&quot;], index=df[&quot;Song Name&quot;]).reset_index()</pre><p>Finally,</p><pre>true = True<br />while true:<br />    print(&quot;The Top 10 Song Recommendation System&quot;)<br />    print(&quot;-------------------------------------&quot;)<br />    print(&quot;This will generate the 10 songs from the database thoese are similar to the song you entered.&quot;)<br /><br />    # Asking the user for a song, it will loop until the song name is in our database.<br />    while True:<br />        input_song = input(&quot;Please enter the name of song: &quot;)<br /><br />        if input_song in df_tmp.columns:<br />            recommendation = df_tmp.nlargest(11, input_song)[&quot;Song Name&quot;]<br />            break<br />        <br />        else:<br />            print(&quot;Sorry, there is no song name in our database. Please try another one.&quot;)<br />    <br />    print(&quot;You should check out these songs: \\n&quot;)<br />    for song in recommendation.values[1:]:<br />        print(song)<br /><br />    print(&quot;\\n&quot;)<br />    # Asking the user for the next command, it will loop until the right command.<br />    while True:<br />        next_command = input(&quot;Do you want to generate again for the next song? [yes, no] &quot;)<br /><br />        if next_command == &quot;yes&quot;:<br />            break<br /><br />        elif next_command == &quot;no&quot;:<br />            # `true` will be false. It will stop the whole script<br />            true = False<br />            break<br /><br />        else:<br />            print(&quot;Please type 'yes' or 'no'&quot;)</pre><p>What does the above code work? Here is the brief explanation,</p><ul><li>Asking the user for a song, it will loop until the song name is in our database.</li><li>Asking the user for the next command, it will loop until the right command.</li><li>true will be false. It will stop the whole script.</li></ul><h4>Final result</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/746/1*Yg88XBa9nLiQFkI5VSH-CQ.gif\" /><figcaption>Final result</figcaption></figure><h4>Full codes and CSV files are on GitHub</h4><p>The codes and get the CSV files I used in this blog are uploaded to my GitHub account,</p><p><a href=\"https://github.com/sithukhantai/music-recommendation-system\">GitHub - sithukhantai/music-recommendation-system: Building the top 10 music recommendation system using Scikit-Learn</a></p><p>If you want to see the above figures in live-action, <a href=\"https://whimsical.com/music-recommendation-sytem-RVp7KL4oCAt74UsZVfr8vC\">see here</a>.</p><h3>Conclusion</h3><p>In this article, we implemented a simple music recommendation system using Scikit-Learn. You can explore more and more. The thing is, the more it is good, the more it is complicated. Here is <a href=\"https://365datascience.com/tutorials/how-to-build-recommendation-system-in-python/\">a great blog</a> by <a href=\"https://www.natasshaselvaraj.com\">Natassha Selvaraj</a> on implementing the book recommendation system if you are willing to know about the book recommendation system.</p><p>I hope this blog will help you, and see you in the next blog.</p><h3>References</h3><ul><li><a href=\"https://365datascience.com/tutorials/how-to-build-recommendation-system-in-python/\">How to Build a Recommendation System in Python</a> by Natassha Selvaraj</li></ul><p><em>If you like to read my blogs, you can follow me on </em><a href=\"https://sithukhantai.medium.com\"><strong><em>Medium</em></strong></a></p><p><em>If you have any problem or want to contact me, </em><a href=\"mailto:sithukhantai@gmail.com\"><strong><em>email me</em></strong></a></p><p><em>You can also follow the </em><a href=\"https://medium.com/artificialis\"><strong><em>Artificialis</em></strong></a><em> publication where I mostly publish my blogs.</em></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=30f4d07c60b3\" width=\"1\" /><hr /><p><a href=\"https://medium.com/artificialis/music-recommendation-system-with-scikit-learn-30f4d07c60b3\">Building Music Recommendation System</a> was originally published in <a href=\"https://medium.com/artificialis\">Artificialis</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",[{'name': 'Sithu Khant'}],"Tue, 11 Jul 2023 07:26:47 GMT"
1190,The Future is Self-Supervised: An Introduction to DINOv2,https://medium.com/artificialis/the-future-is-self-supervised-an-introduction-to-dinov2-c3c37f0dfb6f?source=rss----a81c8d170222---4,<div class=\"medium-feed-item\"><p class=\"medium-feed-image\"><a href=\"https://medium.com/artificialis/the-future-is-self-supervised-an-introduction-to-dinov2-c3c37f0dfb6f?source=rss----a81c8d170222---4\"><img src=\"https://cdn-images-1.medium.com/max/1453/1*86nxzctYEr19NPQ6PWYl0A.png\" width=\"1453\" /></a></p><p class=\"medium-feed-snippet\">Self-supervised learning is a type of machine learning where systems are trained to predict or solve tasks using only a raw dataset&#x2026;</p><p class=\"medium-feed-link\"><a href=\"https://medium.com/artificialis/the-future-is-self-supervised-an-introduction-to-dinov2-c3c37f0dfb6f?source=rss----a81c8d170222---4\">Continue reading on Artificialis »</a></p></div>,[{'name': 'Alessandro Lamberti'}],"Tue, 27 Jun 2023 09:18:58 GMT"
1191,Easily build a Web App with Haar Cascades and OpenCV.js,https://medium.com/artificialis/easily-build-a-web-app-with-haar-cascades-and-opencv-js-aa46be637096?source=rss----a81c8d170222---4,"<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*HPvIOX9kLTp4-YOFNC9cLQ.png\" /><figcaption>Fig — 01 Image By Author Inspired By (<a href=\"https://www.freepik.com/free-vector/face-recognition-biometric-scan-cyber-security-technology-blue-tone_16406566.htm#query=facial%20recognition&amp;position=33&amp;from_view=keyword&amp;track=ais\">rawpixel</a>)</figcaption></figure><p>I’ve been building computer vision based applications for some hands on practice and curious indulgences, but they were mostly desktop native. However, just recently I came across a client who needed an application built using opencv.js. It was a fairly complex problem, which needed days to figure out considering my average java-scripting experience.</p><p>Although, I pulled through but it was a challenging ordeal. So I thought to work on my web-development skills before landing another order. After some experience, I learned that the most effective method to enhance my skills was to adopt a project-based approach and assist fellow learners in the process.</p><p>Here I am, starting the journey by trying to build a face detection web application using none other then Haar cascade and our very own OpenCV.js.</p><p>So without wasting any time, lets dive into the coding part!</p><h3>Setting up the VS code editor</h3><p>Before actually commencing our coding expedition, we need to setup our editor. Here’s what we’ll do</p><ol><li>Install the Live Server extension.</li><li>Install Auto Rename Tag</li><li>Install Prettier</li></ol><p>We don’t need to hard code the installation process, for those who are new to the platform just navigate to the top left — just below the Run and Debug you’d see the “<strong>Extensions”</strong> icon. From there you’d know what to do!</p><h3>Downloading Requisites</h3><p>After setting the code editor, we need to download opencv.js package and the Haar cascade xml files. To download the opencv.js package files, copy the entire script from <a href=\"https://docs.opencv.org/3.4.0/opencv.js\">here</a> and save it on a notepad as a opencv.js file. Move the saved opencv.js file to your project directory. Now download the Haar-cascade trained xml’s for face and eyes detection from this <a href=\"https://github.com/opencv/opencv/tree/master/data/haarcascades\">git repo</a>. Similarly, move the downloaded xml file to the project directory.</p><h3>JavaScripting</h3><h3>Utility.js</h3><p>Now we’re all set to start the coding process. We’ll be dividing the java-scripting in two files. In the first file named “Utils.js”, we’ll be setting up the library, file fetching, canvas display and camera initialization process. The second file would focus solely on defining the logic of Haar cascade and related operations.</p><pre>function Utils(errorOutputId) { <br />    let self = this;<br />    this.errorOutput = document.getElementById(errorOutputId);</pre><p>Initializing a Utils(utility) Class and assigning an error output element to display the error messages.</p><pre>const OPENCV_URL = 'opencv.js';<br />    this.loadOpenCv = function(onloadCallback) {<br />        let script = document.createElement('script');<br />        script.setAttribute('async', '');<br />        script.setAttribute('type', 'text/javascript');<br />        script.addEventListener('load', () =&gt; {<br />                // Checking the loading of the buildInformation<br />           if (cv.getBuildInformation)<br />            {<br />                console.log(cv.getBuildInformation());<br />                onloadCallback();<br />            }<br />            else<br />            {<br />                // Web Assembly check<br />                cv['onRuntimeInitialized']=()=&gt;{<br />                    console.log(cv.getBuildInformation());<br />                    onloadCallback();<br />                }<br />            }<br />        });<br />        script.addEventListener('error', () =&gt; {<br />            self.printError('Failed to load ' + OPENCV_URL);<br />        });<br />        script.src = OPENCV_URL;<br />        let node = document.getElementsByTagName('script')[0];<br />        node.parentNode.insertBefore(script, node);<br />    };</pre><p>Dynamically loading the OpenCV.js library in the browser. It creates a Script element and sets some required attributes. Event Listeners are added to handle the error reports and the loading of the script. The conditional statement <strong>“if-else”</strong> checks the “<strong><em>cv.getBuildInformation”</em></strong> if it exists or not. If it checks positive, the <strong>“<em>onloadCallback()</em>”</strong> function is invoked else it indicates the library is using Web-Assembly(WASP) so it creates an <strong><em>“onRuntimeInitialized”</em></strong> callback to be called when Runtime is initialized. In short the above snippets ensures appropriate loading of the opencv.js library.</p><pre>this.createFileFromUrl = function(path, url, callback) {<br />        let request = new XMLHttpRequest();<br />        request.open('GET', url, true);<br />        request.responseType = 'arraybuffer';<br />        request.onload = function(ev) {<br />            if (request.readyState === 4) {<br />                if (request.status === 200) {<br />                    let data = new Uint8Array(request.response);<br />                    cv.FS_createDataFile('/', path, data, true, false, false);<br />                    callback();<br />                } else {<br />                    self.printError('Failed to load ' + url + ' status: ' + request.status);<br />                }<br />            }<br />        };<br />        request.send();<br />    };</pre><p>This snippet is useful for fetching image or video data from external sources, but we’ll be using the live webcam to capture the results in Realtime. The <strong><em>createFileFromUrl</em></strong> method fetches a file from a given URL and creates a virtual file in the virtual file system. It uses an <strong>XMLHttpRequest</strong> to retrieve the file as an array buffer. On successful retrieval, it converts the response to a <strong>Uint8Array </strong>and creates the virtual file using <strong><em>“cv.FS_createDataFile”</em></strong>. The callback function is invoked once the file creation is complete.</p><pre>this.loadImageToCanvas = function(url, cavansId) {<br />        let canvas = document.getElementById(cavansId);<br />        let ctx = canvas.getContext('2d');<br />        let img = new Image();<br />        img.crossOrigin = 'anonymous';<br />        img.onload = function() {<br />            canvas.width = img.width;<br />            canvas.height = img.height;<br />            ctx.drawImage(img, 0, 0, img.width, img.height);<br />        };<br />        img.src = url;<br />    };</pre><p>The above snippet is responsible for creating a canvas object to display the image. The <strong><em>“loadImageToCanvas”</em></strong> function takes two elements i.e., a url and a canvas Id. The let canvas variable fetches the canvas element from the html document using the <strong><em>“canvasId”</em></strong> passed as a parameter. To draw on the canvas, we’ll use the <strong><em>.getContext(‘2d’)</em></strong> method to define the rendering context. We’ll then create an image object to represent the image that’ll be loaded, followed by defining crossOrigin property to <strong><em>‘anonymous’</em>. </strong>This ensures that images can be accessed from different domains. Then we have an onload event handler, that defines the post-loading events for an image. The succeeding lines equal the image width and height to that of the canvas, followed by a <strong><em>drawImage</em></strong> function to overlay the image on a specified coordinate and shape. Finally the <strong><em>img.src </em></strong>fetches the image from the url.</p><pre>this.executeCode = function(textAreaId) {<br />        try {<br />            this.clearError();<br />            let code = document.getElementById(textAreaId).value;<br />            eval(code);<br />        } catch (err) {<br />            this.printError(err);<br />        }<br />    };<br />this.clearError = function() {<br />        this.errorOutput.innerHTML = '';<br />    };<br />    this.printError = function(err) {<br />        if (typeof err === 'undefined') {<br />            err = '';<br />        } else if (typeof err === 'number') {<br />            if (!isNaN(err)) {<br />                if (typeof cv !== 'undefined') {<br />                    err = 'Exception: ' + cv.exceptionFromPtr(err).msg;<br />                }<br />            }<br />        } else if (typeof err === 'string') {<br />            let ptr = Number(err.split(' ')[0]);<br />            if (!isNaN(ptr)) {<br />                if (typeof cv !== 'undefined') {<br />                    err = 'Exception: ' + cv.exceptionFromPtr(ptr).msg;<br />                }<br />            }<br />        } else if (err instanceof Error) {<br />            err = err.stack.replace(/\\n/g, '&lt;br&gt;');<br />        }<br />        this.errorOutput.innerHTML = err;<br />    };</pre><p>This part of the code is intended for exception handling. The excuteCode function takes <strong><em>“textAreaId”</em></strong> as a parameters and executes the code while the catch block holds any error associated with the execution. The <strong><em>clearError</em></strong> function set the <strong><em>errorOutput</em></strong> element to an empty string. The entire snippet follows the same evaluation format by catching any errors that occur, and formats the error messages based on the type of error encountered before displaying them in the error output element.</p><pre>this.loadCode = function(scriptId, textAreaId) {<br />        let scriptNode = document.getElementById(scriptId);<br />        let textArea = document.getElementById(textAreaId);<br />        if (scriptNode.type !== 'text/code-snippet') {<br />            throw Error('Unknown code snippet type');<br />        }<br />        textArea.value = scriptNode.text.replace(/^\\n/, '');<br />    };</pre><p>The above code snippet defines a function that loads code from a script element and populates <strong><em>textArea</em></strong> element with that code. the <strong><em>loadCode</em></strong> function takes two parameters i.e., the <strong><em>scriptId</em></strong> and the <strong><em>textAreaId</em></strong> which are later fetched from html and stored in <strong><em>scriptNode</em></strong> and textArea using the <strong><em>.getElementById</em></strong>. Later an exception check is initiated by ensuring the <strong><em>scriptNode.type</em></strong> to be code-snippet. The text content of the script element after successful conformation gets assigned to the value property of the <strong><em>textArea</em></strong>.</p><pre>function onVideoCanPlay() {<br />        if (self.onCameraStartedCallback) {<br />            self.onCameraStartedCallback(self.stream, self.video);<br />        }<br />    };</pre><p>This snippet defines a function <strong><em>onVideoCanPlay()</em></strong> that is typically used as an event handler. When the video can be played, it checks if the<strong><em> “onCamerStartedCallback”</em></strong> function is defined. If it is, the <strong><em>“onCamerStartedCallback”</em></strong> function is called, passing the <strong><em>self.stream</em></strong> and <strong><em>self.video</em></strong> as arguments. The purpose of this function is to trigger a callback function when the video is ready to play, allowing further actions or processing to be performed.</p><pre>this.startCamera = function(resolution, callback, videoId) {<br />        const constraints = {<br />            'qvga': {width: {exact: 320}, height: {exact: 240}},<br />            'vga': {width: {exact: 640}, height: {exact: 480}}};<br />        let video = document.getElementById(videoId);<br />        if (!video) {<br />            video = document.createElement('video');<br />        }<br />let videoConstraint = constraints[resolution];<br />        if (!videoConstraint) {<br />            videoConstraint = true;<br />        }<br />        navigator.mediaDevices.getUserMedia({video: videoConstraint, audio: false})<br />            .then(function(stream) {<br />                video.srcObject = stream;<br />                video.play();<br />                self.video = video;<br />                self.stream = stream;<br />                self.onCameraStartedCallback = callback;<br />                video.addEventListener('canplay', onVideoCanPlay, false);<br />            })<br />            .catch(function(err) {<br />                self.printError('Camera Error: ' + err.name + ' ' + err.message);<br />            });<br />    };<br />    this.stopCamera = function() {<br />        if (this.video) {<br />            this.video.pause();<br />            this.video.srcObject = null;<br />            this.video.removeEventListener('canplay', onVideoCanPlay);<br />        }<br />        if (this.stream) {<br />            this.stream.getVideoTracks()[0].stop();<br />        }<br />    };<br />};</pre><p>The code defines two functions, “<strong><em>startCamera”</em></strong> and “<strong><em>stopCamera”</em></strong>, which are responsible for controlling the camera stream.</p><p>The “<strong><em>startCamera”</em></strong> function takes three parameters: “<strong><em>resolution”</em></strong>, “<strong><em>callback”</em></strong>, and “<strong><em>videoId”</em></strong>. It sets up the desired resolution options and creates a video element for displaying the camera stream. It then uses the “<strong><em>navigator.mediaDevices.getUserMedia”</em></strong> method to request access to the camera’s video stream. If the request is successful, the stream is assigned to the video element’s “<strong><em>srcObject”</em></strong> property, and the stream starts playing. The function also assigns the video element, stream, and callback to corresponding properties for later use. Additionally, an event listener is added to the video element for the <strong><em>“canplay”</em></strong> event, which triggers the <strong><em>onVideoCanPlay</em></strong> function.</p><p>On the other hand, the “<strong>stopCamera”</strong> function doesn’t take any parameters. It checks if the video element exists and, if so, pauses the video playback, clears the <strong><em>srcObject</em></strong> property to stop the camera stream, and removes the <strong>“<em>canplay</em>”</strong> event listener. Furthermore, if the stream exists, it stops the video tracks associated with the stream.</p><p>With this comes the end of our utils.js file, now we’ll move towards the haar.js file to finalize our Haar detector. But before that, here’s the entire snippet of the Utils.js.</p><pre>function Utils(errorOutputId) { <br />    let self = this;<br />    this.errorOutput = document.getElementById(errorOutputId);<br />const OPENCV_URL = 'opencv.js';<br />    this.loadOpenCv = function(onloadCallback) {<br />        let script = document.createElement('script');<br />        script.setAttribute('async', '');<br />        script.setAttribute('type', 'text/javascript');<br />        script.addEventListener('load', () =&gt; {<br />            if (cv.getBuildInformation)<br />            {<br />                console.log(cv.getBuildInformation());<br />                onloadCallback();<br />            }<br />            else<br />            {<br />                cv['onRuntimeInitialized']=()=&gt;{<br />                    console.log(cv.getBuildInformation());<br />                    onloadCallback();<br />                }<br />            }<br />        });<br />        script.addEventListener('error', () =&gt; {<br />            self.printError('Failed to load ' + OPENCV_URL);<br />        });<br />        script.src = OPENCV_URL;<br />        let node = document.getElementsByTagName('script')[0];<br />        node.parentNode.insertBefore(script, node);<br />    };<br />    this.createFileFromUrl = function(path, url, callback) {<br />        let request = new XMLHttpRequest();<br />        request.open('GET', url, true);<br />        request.responseType = 'arraybuffer';<br />        request.onload = function(ev) {<br />            if (request.readyState === 4) {<br />                if (request.status === 200) {<br />                    let data = new Uint8Array(request.response);<br />                    cv.FS_createDataFile('/', path, data, true, false, false);<br />                    callback();<br />                } else {<br />                    self.printError('Failed to load ' + url + ' status: ' + request.status);<br />                }<br />            }<br />        };<br />        request.send();<br />    };<br />    this.loadImageToCanvas = function(url, cavansId) {<br />        let canvas = document.getElementById(cavansId);<br />        let ctx = canvas.getContext('2d');<br />        let img = new Image();<br />        img.crossOrigin = 'anonymous';<br />        img.onload = function() {<br />            canvas.width = img.width;<br />            canvas.height = img.height;<br />            ctx.drawImage(img, 0, 0, img.width, img.height);<br />        };<br />        img.src = url;<br />    };<br />    this.executeCode = function(textAreaId) {<br />        try {<br />            this.clearError();<br />            let code = document.getElementById(textAreaId).value;<br />            eval(code);<br />        } catch (err) {<br />            this.printError(err);<br />        }<br />    };<br />    this.clearError = function() {<br />        this.errorOutput.innerHTML = '';<br />    };<br />    this.printError = function(err) {<br />        if (typeof err === 'undefined') {<br />            err = '';<br />        } else if (typeof err === 'number') {<br />            if (!isNaN(err)) {<br />                if (typeof cv !== 'undefined') {<br />                    err = 'Exception: ' + cv.exceptionFromPtr(err).msg;<br />                }<br />            }<br />        } else if (typeof err === 'string') {<br />            let ptr = Number(err.split(' ')[0]);<br />            if (!isNaN(ptr)) {<br />                if (typeof cv !== 'undefined') {<br />                    err = 'Exception: ' + cv.exceptionFromPtr(ptr).msg;<br />                }<br />            }<br />        } else if (err instanceof Error) {<br />            err = err.stack.replace(/\\n/g, '&lt;br&gt;');<br />        }<br />        this.errorOutput.innerHTML = err;<br />    };<br />    this.loadCode = function(scriptId, textAreaId) {<br />        let scriptNode = document.getElementById(scriptId);<br />        let textArea = document.getElementById(textAreaId);<br />        if (scriptNode.type !== 'text/code-snippet') {<br />            throw Error('Unknown code snippet type');<br />        }<br />        textArea.value = scriptNode.text.replace(/^\\n/, '');<br />    };<br />    this.addFileInputHandler = function(fileInputId, canvasId) {<br />        let inputElement = document.getElementById(fileInputId);<br />        inputElement.addEventListener('change', (e) =&gt; {<br />            let files = e.target.files;<br />            if (files.length &gt; 0) {<br />                let imgUrl = URL.createObjectURL(files[0]);<br />                self.loadImageToCanvas(imgUrl, canvasId);<br />            }<br />        }, false);<br />    };<br />    function onVideoCanPlay() {<br />        if (self.onCameraStartedCallback) {<br />            self.onCameraStartedCallback(self.stream, self.video);<br />        }<br />    };<br />    this.startCamera = function(resolution, callback, videoId) {<br />        const constraints = {<br />            'qvga': {width: {exact: 320}, height: {exact: 240}},<br />            'vga': {width: {exact: 640}, height: {exact: 480}}};<br />        let video = document.getElementById(videoId);<br />        if (!video) {<br />            video = document.createElement('video');<br />        }<br />        let videoConstraint = constraints[resolution];<br />        if (!videoConstraint) {<br />            videoConstraint = true;<br />        }<br />        navigator.mediaDevices.getUserMedia({video: videoConstraint, audio: false})<br />            .then(function(stream) {<br />                video.srcObject = stream;<br />                video.play();<br />                self.video = video;<br />                self.stream = stream;<br />                self.onCameraStartedCallback = callback;<br />                video.addEventListener('canplay', onVideoCanPlay, false);<br />            })<br />            .catch(function(err) {<br />                self.printError('Camera Error: ' + err.name + ' ' + err.message);<br />            });<br />    };<br />    this.stopCamera = function() {<br />        if (this.video) {<br />            this.video.pause();<br />            this.video.srcObject = null;<br />            this.video.removeEventListener('canplay', onVideoCanPlay);<br />        }<br />        if (this.stream) {<br />            this.stream.getVideoTracks()[0].stop();<br />        }<br />    };<br />};</pre><h3>Haar.js</h3><p>As we’re done with the utility setup, we’ll code the detection part by creating a file named haar.js. We’ll start the coding now, without any delays!</p><pre>let isFaceDetection = true; // Flag to indicate whether face detection is enabled<br />function switchDetection() {<br />    isFaceDetection = !isFaceDetection; // Toggle the flag<br />}</pre><p>The above code snippet initializes with a detection check for the face strictly followed by a toggle function <strong><em>switchDetection()</em></strong>. This switch function is used to add an additional toggling feature in the application, such that the user can navigate between the face detection or the face and eyes detection by clicking the switch button. The function of the switch button is thoroughly explained in the code below.</p><pre>function addNavigationButtons() {<br />    // Create a button for switching detection<br />    let switchButton = document.createElement('button');<br />    switchButton.textContent = 'Switch Detection';<br />    switchButton.addEventListener('click', switchDetection);<br />// Append the button to the body<br />    // Get the button container element<br />    let buttonContainer = document.getElementById('buttonContainer');<br />    // Append the button to the button container<br />    buttonContainer.appendChild(switchButton);<br />}</pre><p>This code is quite simple to follow. The <strong><em>addNavigationButtons() </em></strong>function first creates a “<strong><em>switchButton”</em></strong> variable to create a button element and add in the text content of choice. The next line is the event handling for the switch button by passing in the event name <strong><em>‘click’ </em></strong>and the callback function <strong><em>‘switchDetection()’</em> </strong>that we created above. Next we fetch the button container through the id defined in the html and store it inside the <strong><em>buttonContainer</em></strong> variable. Don’t worry we’ll get to see the html part in the final stages of this project. The last thing we’ll do is add our <strong><em>switchButton</em></strong> as a child element to the fetched <strong><em>“buttonContainer”</em></strong> element from the html. This is done to allow the access inside the html editor for style or other required changes to this button element.</p><pre>function openCvReady() {<br />    cv['onRuntimeInitialized'] = () =&gt; {<br />        let video = document.getElementById(&quot;cam_input&quot;); // video is the id of video tag<br />        navigator.mediaDevices.getUserMedia({ video: true, audio: false })<br />        .then(function(stream) {<br />            video.srcObject = stream;<br />            video.play();<br />        })<br />        .catch(function(err) {<br />            console.log(&quot;An error occurred! &quot; + err);<br />        });<br />        let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);<br />        let dst = new cv.Mat(video.height, video.width, cv.CV_8UC1);<br />        let gray = new cv.Mat();<br />        let cap = new cv.VideoCapture(cam_input);<br />        let faces = new cv.RectVector();<br />        let eyes = new cv.RectVector();<br />        let faceClassifier = new cv.CascadeClassifier();<br />        let eyeClassifier = new cv.CascadeClassifier();<br />        let utils = new Utils('errorMessage');<br />        let faceCascadeFile = 'haarcascade_frontalface_default.xml'; // path to face cascade xml<br />        let eyeCascadeFile = 'haarcascade_eye.xml'; // path to eye cascade xml<br />        utils.createFileFromUrl(faceCascadeFile, faceCascadeFile, () =&gt; {<br />            faceClassifier.load(faceCascadeFile); // in the callback, load the face cascade from file<br />        });<br />        utils.createFileFromUrl(eyeCascadeFile, eyeCascadeFile, () =&gt; {<br />            eyeClassifier.load(eyeCascadeFile); // in the callback, load the eye cascade from file<br />        });<br />        const FPS = 24;</pre><p>Here the <strong><em>opencvReady()</em></strong> function is a callback which initiates with the loading of the <strong><em>OpenCV</em></strong> library. <strong><em>onRuntimeInitialized </em></strong>is an event that is triggered when the library is initialized and a class-back function is assigned to it using an arrow. Inside the callback the video element is retrieved using the ID <strong><em>“cam_input”</em></strong>. Then a promise based function <strong><em>“navigator.mediaDevices.getUserMedia” </em></strong>is used to request access to user’s camera which upon access returns a promise using <strong><em>then()</em></strong> function. It assigns the stream to the <strong><em>“srcObject”</em></strong> property of the video element and starts playing the video. The <strong><em>catch( ) </em></strong>method is used to log errors. Next, several variables and objects are initialized. These include <strong><em>src</em></strong> , <strong><em>dst </em></strong>and <strong><em>gray</em></strong>, which are instances of OpenCV matrices used for image processing. <strong><em>cap</em></strong> is a <strong><em>VideoCapture </em></strong>object that provides access to video frames. <strong><em>faces</em></strong> and <strong><em>eyes </em></strong>are <strong><em>RectVector </em></strong>objects that will store the detected faces and eyes, respectively. <strong><em>faceClassifier</em></strong> and <strong><em>eyesClassifier </em></strong>are instances of <strong><em>CascadeClassifier </em></strong>, which are used to load and apply trained cascade classifiers for face and eye detection. An instance of the <strong><em>Utils </em></strong>class is created, which provides utility functions. The constructor of <strong><em>Utils</em></strong> takes an argument, “<strong><em>errorMessage”</em></strong>, which is the ID of an HTML element used to display error messages. The <strong><em>utils.createFileFromUrl()</em></strong> function is called to asynchronously load the cascade classifier XML files. It takes the file path, a callback function, and optional parameters. In the callback function, the XML file is loaded using <strong><em>faceClassifer.load() </em></strong>and <strong>eyeClassifier.load() </strong>methods. Finally, <strong><em>FPS </em></strong>is set to 24, indicating the desired frames per second for video processing.</p><pre>function processVideo() {<br />            let begin = Date.now();<br />            cap.read(src);<br />            src.copyTo(dst);<br />            cv.cvtColor(dst, gray, cv.COLOR_RGBA2GRAY, 0);<br />if (isFaceDetection) {<br />                try {<br />                    faceClassifier.detectMultiScale(gray, faces, 1.1, 3, 0);<br />                    console.log(faces.size());<br />                } catch (err) {<br />                    console.log(err);<br />                }<br />                for (let i = 0; i &lt; faces.size(); ++i) {<br />                    let face = faces.get(i);<br />                    let point1 = new cv.Point(face.x, face.y);<br />                    let point2 = new cv.Point(face.x + face.width, face.y + face.height);<br />                    cv.rectangle(dst, point1, point2, [255, 0, 0, 255]);<br />                }<br />            } else {<br />                try {<br />                    faceClassifier.detectMultiScale(gray, faces, 1.1, 3, 0);<br />                    eyeClassifier.detectMultiScale(gray, eyes, 1.1, 3, 0);<br />                    console.log(eyes.size());<br />                } catch (err) {<br />                    console.log(err);<br />                }<br />                for (let i = 0; i &lt; faces.size(); ++i) {<br />                    let face = faces.get(i);<br />                    let point1 = new cv.Point(face.x, face.y);<br />                    let point2 = new cv.Point(face.x + face.width, face.y + face.height);<br />                    cv.rectangle(dst, point1, point2, [255, 0, 0, 255]);<br />                }<br />                for (let i = 0; i &lt; eyes.size(); ++i) {<br />                    let eye = eyes.get(i);<br />                    let point1 = new cv.Point(eye.x, eye.y);<br />                    let point2 = new cv.Point(eye.x + eye.width, eye.y + eye.height);<br />                    cv.rectangle(dst, point1, point2, [0, 255, 0, 255]);<br />                }<br />            }<br />            cv.imshow(&quot;canvas_output&quot;, dst);<br />            // schedule next one.<br />            let delay = 1000 / FPS - (Date.now() - begin);<br />            setTimeout(processVideo, delay);<br />        }<br />        // Add navigation buttons<br />        addNavigationButtons();<br />        // schedule first one.<br />        setTimeout(processVideo, 0);<br />    };</pre><p>The <strong><em>“processVideo()” </em></strong>function is responsible for processing video frames in real-time. It starts by capturing a frame from the video source and converting it to grayscale. The function then determines whether to perform face detection or both face and eye detection based on the <strong><em>“isFaceDetection”</em></strong> flag. If face detection is enabled, it uses a face classifier to detect faces in the frame and draws rectangles around them. If both face and eye detection are enabled, it also uses an eye classifier to detect eyes within the detected faces and draws rectangles around them. The processed frame with the drawn rectangles is displayed on a canvas. The function then calculates the delay required to achieve a desired frame rate and schedules the next frame processing using <strong><em>setTimeout()</em></strong>. Additionally, the <strong><em>addNavigationButtons()</em></strong> function is called to add buttons for toggling between face and eye detection. Overall, this code allows for real-time video processing with face and eye detection capabilities.</p><p>With this comes the end of the JavaScripting part. Before moving forward to the html let’s just take a look at the entire Haar.js part.</p><pre>let isFaceDetection = true; // Flag to indicate whether face detection is enabled<br />function switchDetection() {<br />    isFaceDetection = !isFaceDetection; // Toggle the flag<br />}<br />function addNavigationButtons() {<br />    // Create a button for switching detection<br />    let switchButton = document.createElement('button');<br />    switchButton.textContent = 'Switch Detection';<br />    switchButton.addEventListener('click', switchDetection);<br />    // Append the button to the body<br />    // Get the button container element<br />    let buttonContainer = document.getElementById('buttonContainer');<br />    // Append the button to the button container<br />    buttonContainer.appendChild(switchButton);<br />}<br />function openCvReady() {<br />    cv['onRuntimeInitialized'] = () =&gt; {<br />        let video = document.getElementById(&quot;cam_input&quot;); // video is the id of video tag<br />        navigator.mediaDevices.getUserMedia({ video: true, audio: false })<br />        .then(function(stream) {<br />            video.srcObject = stream;<br />            video.play();<br />        })<br />        .catch(function(err) {<br />            console.log(&quot;An error occurred! &quot; + err);<br />        });<br />        let src = new cv.Mat(video.height, video.width, cv.CV_8UC4);<br />        let dst = new cv.Mat(video.height, video.width, cv.CV_8UC1);<br />        let gray = new cv.Mat();<br />        let cap = new cv.VideoCapture(cam_input);<br />        let faces = new cv.RectVector();<br />        let eyes = new cv.RectVector();<br />        let faceClassifier = new cv.CascadeClassifier();<br />        let eyeClassifier = new cv.CascadeClassifier();<br />        let utils = new Utils('errorMessage');<br />        let faceCascadeFile = 'haarcascade_frontalface_default.xml'; // path to face cascade xml<br />        let eyeCascadeFile = 'haarcascade_eye.xml'; // path to eye cascade xml<br />        utils.createFileFromUrl(faceCascadeFile, faceCascadeFile, () =&gt; {<br />            faceClassifier.load(faceCascadeFile); // in the callback, load the face cascade from file<br />        });<br />        utils.createFileFromUrl(eyeCascadeFile, eyeCascadeFile, () =&gt; {<br />            eyeClassifier.load(eyeCascadeFile); // in the callback, load the eye cascade from file<br />        });<br />        const FPS = 24;<br />        function processVideo() {<br />            let begin = Date.now();<br />            cap.read(src);<br />            src.copyTo(dst);<br />            cv.cvtColor(dst, gray, cv.COLOR_RGBA2GRAY, 0);<br />            if (isFaceDetection) {<br />                try {<br />                    faceClassifier.detectMultiScale(gray, faces, 1.1, 3, 0);<br />                    console.log(faces.size());<br />                } catch (err) {<br />                    console.log(err);<br />                }<br />                for (let i = 0; i &lt; faces.size(); ++i) {<br />                    let face = faces.get(i);<br />                    let point1 = new cv.Point(face.x, face.y);<br />                    let point2 = new cv.Point(face.x + face.width, face.y + face.height);<br />                    cv.rectangle(dst, point1, point2, [255, 0, 0, 255]);<br />                }<br />            } else {<br />                try {<br />                    faceClassifier.detectMultiScale(gray, faces, 1.1, 3, 0);<br />                    eyeClassifier.detectMultiScale(gray, eyes, 1.1, 3, 0);<br />                    console.log(eyes.size());<br />                } catch (err) {<br />                    console.log(err);<br />                }<br />                for (let i = 0; i &lt; faces.size(); ++i) {<br />                    let face = faces.get(i);<br />                    let point1 = new cv.Point(face.x, face.y);<br />                    let point2 = new cv.Point(face.x + face.width, face.y + face.height);<br />                    cv.rectangle(dst, point1, point2, [255, 0, 0, 255]);<br />                }<br />                for (let i = 0; i &lt; eyes.size(); ++i) {<br />                    let eye = eyes.get(i);<br />                    let point1 = new cv.Point(eye.x, eye.y);<br />                    let point2 = new cv.Point(eye.x + eye.width, eye.y + eye.height);<br />                    cv.rectangle(dst, point1, point2, [0, 255, 0, 255]);<br />                }<br />            }<br />            cv.imshow(&quot;canvas_output&quot;, dst);<br />            // schedule next one.<br />            let delay = 1000 / FPS - (Date.now() - begin);<br />            setTimeout(processVideo, delay);<br />        }<br />        // Add navigation buttons<br />        addNavigationButtons();<br />        // schedule first one.<br />        setTimeout(processVideo, 0);<br />    };<br />}</pre><h3>Html</h3><p>We’ll now be coding the html part for basic interfacing and getting things together.</p><pre>&lt;!DOCTYPE html&gt;<br />&lt;html lang=&quot;en&quot;&gt;<br />&lt;head&gt;<br />    &lt;meta charset=&quot;UTF-8&quot;&gt;<br />    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;<br />    &lt;title&gt;Opencv JS&lt;/title&gt;<br />    &lt;style&gt;<br />        body {<br />            font-family: Arial, sans-serif;<br />            background-color: #f4f4f4;<br />            margin: 0;<br />            padding: 0;<br />            display: flex;<br />            justify-content: center;<br />            align-items: center;<br />            height: 100vh;<br />            flex-direction: column;<br />        }<br />        h2, h3 {<br />            text-align: center;<br />            color: #333;<br />            margin: 0;<br />            padding: 10px;<br />        }<br />        video, canvas {<br />            display: block;<br />            margin: 0 auto;<br />            background-color: #000;<br />            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.3);<br />        }<br />        #buttonContainer {<br />            padding: 7px 15px;<br />            font-size: 10px;<br />            background-color: #4CAF50;<br />            color: #fff;<br />            border: none;<br />            border-radius: 2px;<br />            cursor: pointer;<br />            transition: background-color 0.3s ease;<br />        }<br />        #buttonContainer:hover {<br />            background-color: #45a049;<br />        }<br />    &lt;/style&gt;<br />    &lt;script async src=&quot;js/opencv.js&quot; onload=&quot;openCvReady();&quot;&gt;&lt;/script&gt;<br />    &lt;script src=&quot;js/haar.js&quot;&gt;&lt;/script&gt;<br />    &lt;script src=&quot;js/utils.js&quot;&gt;&lt;/script&gt;<br />&lt;/head&gt;<br />&lt;body&gt;<br />    &lt;video id=&quot;cam_input&quot; height=&quot;320&quot; width=&quot;480&quot;&gt;&lt;/video&gt;<br />    &lt;canvas id=&quot;canvas_output&quot;&gt;&lt;/canvas&gt;<br />    &lt;h2&gt;Haar Cascade Based Face and Eyes Detection&lt;/h2&gt;<br />    &lt;h3&gt;Click the button to toggle between Simple Face Detection and Face + Eyes Detection.&lt;/h3&gt;<br />    &lt;div id=&quot;buttonContainer&quot;&gt;<br />    &lt;/div&gt;<br />    &lt;/body&gt;<br />&lt;/html&gt;</pre><p>The code is basic html coding and nothing so complex to need explanation. However, its important to understand the path to the opencv.js library file that we downloaded earlier. As you can see, the opencv.js, haar.js and utils.js files are all in the same folder <strong><em>“js”</em></strong>, whereas<strong><em> </em></strong>the html file for the above code is placed alongside the “<strong><em>js”</em></strong> directory. You can adjust the paths according to your requirements.</p><p>Below is the output for both the face detection and face and eyes detection. Pardon the background</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*J4N66T4cocHmQo-LqTduGg.png\" /><figcaption>Fig — 02 Image By author. (Haar Cascade Face Detection)</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1-XNtPPutS0uph77-T2T0A.png\" /><figcaption>Fig — 03 Image By author. (Haar Cascade Face and Eyes Detection)</figcaption></figure><p>Download the entire code and auxiliary files from my <a href=\"https://github.com/nbeeeel/haar-cascade-web-application-using-opencv.js\">Github repo</a>.</p><h3>Conclusion</h3><p>In this tutorial we implemented Haar cascade face and eye detection using opencv.js on our web application. We also went through the details of the code in order to learn the purpose of each method. This way we’ll be learning both computer vision and web development. I hope this tutorial adds some value to your learning journey and if some stuff is missed or you have some suggestions, do let me know. If you want the code and the related files do visit my <a href=\"https://github.com/nbeeeel/haar-cascade-web-application-using-opencv.js\">Github</a> for this project.</p><p><strong><em>Till next time….. Stay Blessed!</em></strong></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=aa46be637096\" width=\"1\" /><hr /><p><a href=\"https://medium.com/artificialis/easily-build-a-web-app-with-haar-cascades-and-opencv-js-aa46be637096\">Easily build a Web App with Haar Cascades and OpenCV.js</a> was originally published in <a href=\"https://medium.com/artificialis\">Artificialis</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",[{'name': 'Nabeel Khan'}],"Thu, 22 Jun 2023 19:21:44 GMT"
1192,Getting Started with Depth Estimation using MiDaS and Python,https://medium.com/artificialis/getting-started-with-depth-estimation-using-midas-and-python-d0119bfe1159?source=rss----a81c8d170222---4,"<p>Getting Started with Depth Estimation using MiDaS and Python</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GOIdx95IUMsHNP7YnIkzdw.png\" /><figcaption>Fig — 01 Image by Author Inspired by <a href=\"https://pixabay.com/users/sambeetarts-1339110/\">Sambeetarts</a> on <a href=\"https://pixabay.com/illustrations/geometric-design-computer-1732847/\">Pixabay</a></figcaption></figure><p>Measuring<strong> </strong>distance of an object from camera poses a significant challenge within the computer vision domain, due to the lack of inherent depth information in 2D images, perspective distortion, variable object sizes, camera calibration requirements, and occlusion in complex scenes. Distance estimation via perspective projection, for instance, relies on variables such as the size of the sensor, focal length, and the actual height of the object. The computation of these unknown variables adds to the complexity of the task.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/598/1*tTdzp9ChZ6uT47gCXMzAmg.png\" /><figcaption>Fig — 02 (Formula for Distance to Object)</figcaption></figure><p>A series of traditional and deep learning based approaches have been in service for quite some time to provide an effective solution to distance estimation. Solutions involving stereo vision have proven to be effective and accurate in depth calculation, however there’s always a search for a much efficient and inexpensive alternative. Deep learning has shown itself to thrive in such constraints, pushing the boundaries of possibilities to a whole new level and bringing to reality depth estimation models for monocular vision — one of which we’ll be exploring in this article.</p><p>Following my recent <a href=\"https://medium.com/artificialis/swift-and-simple-calculate-object-distance-with-ease-in-just-few-lines-of-code-38889575bb12?source=your_stories_page-------------------------------------\">distance measurement using media-pipe</a> tutorial, I intended to build upon the notion of monocular depth estimation by bringing all possible approaches — to provide you with best options for your fun projects.</p><p>In this article, I’ll be estimating the distance of an object using a hybrid of media-pipe pose estimation module and MiDaS depth estimation model. But before that, let’s just take a quick overview of what we’ll be covering in this article.</p><ol><li><strong>MiDaS overview.</strong></li><li><strong>Distance measurement using Media-pipe landmarks and MiDaS depth map.</strong></li></ol><h3>MiDaS</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*iHcmPCWez148p32CP2nToA.png\" /><figcaption>Fig — 03 (MiDaS Depth Estimation) Image by Author Inspired by <a href=\"https://pixabay.com/users/mikes-photography-1860391/\">Mikes-Photography</a> on <a href=\"https://pixabay.com/videos/cars-motorway-speed-motion-traffic-1900/\">Pixabay</a></figcaption></figure><p>MiDaS(Multiple Depth Estimation Accuracy with Single Network) is a deep learning based residual model built atop Res-Net for monocular depth estimation. MiDaS is known to have shown promising results in depth estimation from single images. Below is the generic overview of the MiDaS architecture:</p><ol><li><strong>Encoder-Decoder Architecture</strong></li></ol><p>MiDaS is based on an encoder-decoder architecture, where the encoder part is responsible for high level feature extraction and the decoder generates the depth map from these features via up-sampling.</p><p>2. <strong>Backbone</strong></p><p>MiDaS typically uses a residual network (ResNet-50 or ResNet-101) for feature extraction because it is robust to vanishing gradients. Allowing MiDaS to extract multi-channeled feature maps from input images, capturing hierarchical information at varying scales.</p><p>3. <strong>Multi-Scale Feature Fusion</strong></p><p>Skip connections and feature fusion is incorporated within MiDaS to allow accurate depth estimation. Feature maps from earlier layers are connected to the later layers via skip connections for accessing low level details during up-sampling. With feature fusion, the multi-scale feature maps are combined to ensure the effective exploitation of both local and global information for depth estimation.</p><p>4. <strong>Up-Sampling and Refinement</strong></p><p>The final depth map is generated using up-sampling. The general techniques used for the up-sampling are bi-linear interpolation or transposed convolutions to increase the spatial resolution of the feature maps. Feature fusion is employed to combine the depth maps with corresponding skip connections in order to refine the depth estimation.</p><h3>Python Code for distance measurement</h3><pre>import cv2<br />import torch<br />import mediapipe as mp<br />import numpy as np<br />from scipy.interpolate import RectBivariateSpline</pre><pre>mp_pose = mp.solutions.pose<br />pose = mp_pose.Pose(static_image_mode=False)</pre><p>Importing required packages and initializing media-pipe pose estimation class <strong><em>mp_pose.Pose</em></strong>.</p><pre>#Downloading the model from TorchHub.<br />midas = torch.hub.load('intel-isl/MiDaS','MiDaS_small')<br />midas.to('cpu')<br />midas.eval()</pre><p>Downloading MiDaS_small model from torch hub. You can download the python executable for MiDaS through <a href=\"https://github.com/isl-org/MiDaS\">Github</a> once, instead. There are three variants of MiDaS available on torch hub and can be downloaded by substituting <strong>‘<em>DPT_Large</em>’</strong> or <strong>‘<em>DPT_Hybrid</em>’ </strong>for<strong> ‘<em>MiDaS_small</em>’</strong>. The general performance of all three variants is given below :</p><ol><li><strong>Small variant </strong>: Lowest accuracy and high inference rate.</li><li><strong>Hybrid variant</strong> : Medium accuracy and medium inference speed</li><li><strong>Large variant </strong>: Highest accuracy with lowest inference speed.</li></ol><p>If you’ve a <strong>Cuda</strong> compatible GPU, then you can replace the <strong><em>midas.to(‘cpu’)</em></strong> with <strong><em>midas.to(“cuda”)</em></strong> to maximize the inference speed.</p><pre>#Performing preprocessing on input for small model <br />transforms = torch.hub.load('intel-isl/MiDaS','transforms')<br />transform = transforms.small_transform</pre><pre>#Converting Depth to distance<br />def depth_to_distance(depth_value,depth_scale):<br />  return -1.0/(depth_value*depth_scale)</pre><p>Applying the requisite preprocessing on the input image/ video frame for MiDaS small model. Followed by a function <strong>‘depth_to_distance’</strong> defined for converting the calculated depth values into the respective distance values.</p><pre>cap = cv2.VideoCapture('')<br />while cap.isOpened():<br />  ret, frame = cap.read()<br /><br />  img = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)<br /><br />  cv2.imshow('Walking',img)<br /><br />  if cv2.waitKey(2) &amp;0xFF == ord('q'):<br />    cap.release()<br />    cv2.destroyAllWindows()</pre><p>Reading the video input and applying color space transformation using <strong>cv2.cvtColor</strong> function. As cv2 reads images in BGR format we need to convert it to RGB for standard visuals. Let run the code to check if its working right so far.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/632/1*tc3crTchEIlC3uUj7crQJA.gif\" /><figcaption>Fig — 04 (A person walking in the woods) Gif by Author Inspired by <a href=\"https://pixabay.com/users/matthias_groeneveld-4535957/\">Matthias Groeneveld</a> on <a href=\"https://pixabay.com/videos/hiking-walking-man-alone-nature-109277/\">Pixabay</a></figcaption></figure><p>Next we will extract the landmarks from the video frame using Media-Pipe using the below code.</p><pre># Detect the body landmarks in the frame<br />    results = pose.process(img)<br /><br />    # Check if landmarks are detected<br />    if results.pose_landmarks is not None:<br />        # Draw Landmarks<br />        mp_drawing = mp.solutions.drawing_utils<br />        mp_drawing.draw_landmarks(img, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/632/1*BAvpgsjucGY1nanjac5BzQ.gif\" /><figcaption>Fig — 05 (Keypoints Detected)</figcaption></figure><p>After successfully drawing the detected landmarks, we’ll now extract the waist landmarks as a reference for the distance estimation. Lets write the code to extract the waist landmarks.</p><pre># Extract Landmark Coordinates<br />        landmarks = []<br />        for landmark in results.pose_landmarks.landmark:<br />            landmarks.append((landmark.x, landmark.y, landmark.z))<br /><br />        # Extract left and right waist Landmarks<br />        waist_landmarks = [results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP],<br />                           results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP]]<br />        #Finding midpoint from waist    <br />        mid_point = ((waist_landmarks[0].x + waist_landmarks[1].x) / 2, (waist_landmarks[0].y + waist_landmarks[1].y) / 2)<br />        mid_x , mid_y = mid_point</pre><p>Extracting the x and y coordinate values for both landmarks and calculating the midpoint. You can pick any landmark of your choice depending on the use-case from the pose-landmark list on <a href=\"https://developers.google.com/mediapipe/solutions/vision/pose_landmarker\">Media-pipe</a>.</p><p>Moving forward, we’ll pass our video through MiDaS depth estimation model to get the depth map.</p><pre>imgbatch = transform(img).to('cpu')<br /><br />    # Making a prediction<br />    with torch.no_grad():<br />        prediction = midas(imgbatch)<br />        prediction = torch.nn.functional.interpolate(<br />            prediction.unsqueeze(1),<br />            size=img.shape[:2],<br />            mode='bicubic',<br />            align_corners=False<br />        ).squeeze()<br /><br />    output = prediction.cpu().numpy()<br />    #Normalizing the output predictions for cv2 to read.<br />    output_norm = cv2.normalize(output, None, 0, 1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)<br /><br />    cv2.imshow('Walking',output_norm)</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/632/1*hWbpd1NzmCVJ1HHG4lJc4A.gif\" /><figcaption>Fig — 06 (MiDaS Depth Map)</figcaption></figure><p>Above is the depth map extracted through MiDaS, also change the <strong><em>waitKey </em></strong>value to<strong><em> </em></strong>1 to decrease the frame delay. You can also change the output of the depth map to a colored map using the code below, but we’ll be using the standard black and white map in this project.</p><pre>#Colored Depth map<br />output_norm = (output_norm*255).astype(np.uint8)<br />output_norm = cv2.applyColorMap(output_norm, cv2.COLORMAP_MAGMA)</pre><p>We’ll then use the from waist landmarks extracted earlier, to calculate the depth value using MiDaS.</p><pre>#Creating a spline array of non-integer grid<br />        h , w = output_norm.shape<br />        x_grid = np.arange(w)<br />        y_grid = np.arange(h)<br /><br />        # Create a spline object using the output_norm array<br />        spline = RectBivariateSpline(y_grid, x_grid, output_norm)</pre><p>The use of a spline array in above code snippet enables the creation of a smooth and continuous representation of the <strong><em>output_norm</em></strong> array on a non-integer grid. The need for a spline array arises from the normalization of the output predictions, resulting in an array with floating-point values. By utilizing a spline object, more accurate and versatile computations or visualizations can be performed by interpolating values based on the given data.</p><pre>#Passing the x and y co-ordinates distance function to calculate distance.<br />#Tweak with the depth scale to see what suits you!<br />depth_scale = 1<br />depth_mid_filt = spline(mid_y,mid_x)<br />depth_midas = depth_to_distance(depth_mid_filt, depth_scale)<br /><br />#Displaying the distance.<br />cv2.putText(img, &quot;Depth in unit: &quot; + str(<br />            np.format_float_positional(depth_mid_filt , precision=3)), (20, 50), cv2.FONT_HERSHEY_SIMPLEX,<br />                    1, (255, 255, 255), 3)</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*V689v_yP8e6aNJ4HK7DOAA.gif\" /><figcaption>Fig — 07 (Depth Calculation)</figcaption></figure><p>The depth values are fluctuating a bit. To stabilize the values we’ll use a exponential mean average filter on the depth values and then see the improvements.</p><pre>#Adjust alpha values to suit your need<br />alpha = 0.2<br />previous_depth = 0.0<br /><br />#Applying exponential moving average filter<br />def apply_ema_filter(current_depth):<br />    global previous_depth<br />    filtered_depth = alpha * current_depth + (1 - alpha) * previous_depth<br />    previous_depth = filtered_depth  # Update the previous depth value<br />    return filtered_depth</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*u82MQbed3TBJJeSCVxl-gg.gif\" /><figcaption>Fig — 08 (Applying EMA Filter)</figcaption></figure><p>Here we can witness considerable difference in the fluctuation after applying Exponential Mean Average Filter on distance value.</p><p>Lets just take a look at the entire code snippet.</p><pre>import cv2<br />import torch<br />import matplotlib.pyplot as plt<br />import mediapipe as mp<br />import numpy as np<br />import shutil<br />from scipy.interpolate import RectBivariateSpline<br /><br />#To Clear the model cache<br /># shutil.rmtree(torch.hub.get_dir(), ignore_errors=True)<br /><br />#Initializing the body landmarks detection module<br />mp_pose = mp.solutions.pose<br />pose = mp_pose.Pose(static_image_mode=False)<br /><br /># #download the model<br />midas = torch.hub.load('intel-isl/MiDaS','MiDaS_small')<br />midas.to('cpu')<br />midas.eval()<br /><br />#Process image<br />transforms = torch.hub.load('intel-isl/MiDaS','transforms')<br />transform = transforms.small_transform<br /><br />alpha = 0.2<br />previous_depth = 0.0<br />depth_scale = 1.0<br /><br />#Applying exponential moving average filter<br />def apply_ema_filter(current_depth):<br />    global previous_depth<br />    filtered_depth = alpha * current_depth + (1 - alpha) * previous_depth<br />    previous_depth = filtered_depth  # Update the previous depth value<br />    return filtered_depth<br /><br /><br />#Define depth to distance<br />def depth_to_distance(depth_value,depth_scale):<br />    return 1.0 / (depth_value*depth_scale)<br /><br />def depth_to_distance1(depth_value,depth_scale):<br />    return -1.0 / (depth_value*depth_scale)<br /><br />cap = cv2.VideoCapture('distance1.mp4')<br />while cap.isOpened():<br />    ret, frame = cap.read()<br /><br />    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)<br /><br />    # Detect the body landmarks in the frame<br />    results = pose.process(img)<br /><br />    # Check if landmarks are detected<br />    if results.pose_landmarks is not None:<br />        # Draw Landmarks<br />        # mp_drawing = mp.solutions.drawing_utils<br />        # mp_drawing.draw_landmarks(img, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)<br /><br />        # Extract Landmark Coordinates<br />        landmarks = []<br />        for landmark in results.pose_landmarks.landmark:<br />            landmarks.append((landmark.x, landmark.y, landmark.z))<br /><br />        waist_landmarks = [results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP],<br />                           results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP]]<br /><br />        mid_point = ((waist_landmarks[0].x + waist_landmarks[1].x) / 2, (waist_landmarks[0].y + waist_landmarks[1].y) / 2,(waist_landmarks[0].z + waist_landmarks[1].z) /2)<br />        mid_x,mid_y = mid_point<br /><br />        <br />        imgbatch = transform(img).to('cpu')<br /><br />        # Making a prediction<br />        with torch.no_grad():<br />            prediction = midas(imgbatch)<br />            prediction = torch.nn.functional.interpolate(<br />                prediction.unsqueeze(1),<br />                size=img.shape[:2],<br />                mode='bicubic',<br />                align_corners=False<br />            ).squeeze()<br /><br />        output = prediction.cpu().numpy()<br />        output_norm = cv2.normalize(output, None, 0, 1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)<br /><br />        # Creating a spline array of non-integer grid<br />        h, w = output_norm.shape<br />        x_grid = np.arange(w)<br />        y_grid = np.arange(h)<br /><br />        # Create a spline object using the output_norm array<br />        spline = RectBivariateSpline(y_grid, x_grid, output_norm)<br />        depth_mid_filt = spline(mid_y,mid_x)<br />        depth_midas = depth_to_distance(depth_mid_filt, depth_scale)<br />        depth_mid_filt = (apply_ema_filter(depth_midas)/10)[0][0]<br />        <br />        cv2.putText(img, &quot;Depth in unit: &quot; + str(<br />            np.format_float_positional(depth_mid_filt , precision=3)) , (20, 50), cv2.FONT_HERSHEY_SIMPLEX,<br />                    1, (255, 255, 255), 3)<br />        <br />        cv2.imshow('Walking',img)<br /><br />    if cv2.waitKey(1) &amp;0xFF == ord('q'):<br />        cap.release()<br />        cv2.destroyAllWindows()</pre><h3>Conclusion</h3><p>In this article we calculated the distance of an object from the camera using the MiDaS depth estimation model while leveraging the reference landmark extracted from Media-pipe. The following approach can be used to detect distance of multiple objects/people and integrated into mini projects based on proximity.</p><p>You can download the source code using <a href=\"https://github.com/nbeeeel/Distance-Measurement-Using-MiDaS\"><strong>Github </strong></a>link<strong>.</strong></p><h3>References</h3><p>[1] Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler and Vladlen Koltun, <a href=\"https://arxiv.org/abs/1907.01341\">Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer</a> (2020), IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).</p><p>[2] Ranftl, Alexey Bochkovskiy and Vladlen Koltun, <a href=\"https://arxiv.org/abs/2103.13413\">Vision Transformers for Dense Prediction</a> (2021), ArXiv preprint.</p><p>[3] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, Wan-Teh Chang, Wei Hua, Manfred Georg and Matthias Grundmann, <a href=\"https://arxiv.org/abs/1906.08172\">MediaPipe: A Framework for Building Perception Pipelines</a> (2019), ArXiv preprint.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d0119bfe1159\" width=\"1\" /><hr /><p><a href=\"https://medium.com/artificialis/getting-started-with-depth-estimation-using-midas-and-python-d0119bfe1159\">Getting Started with Depth Estimation using MiDaS and Python</a> was originally published in <a href=\"https://medium.com/artificialis\">Artificialis</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",[{'name': 'Nabeel Khan'}],"Mon, 12 Jun 2023 11:23:32 GMT"
1193,Navigating Math for Computer Vision: Your Ultimate Roadmap,https://medium.com/artificialis/navigating-math-for-computer-vision-your-ultimate-roadmap-8389a0d7b7be?source=rss----a81c8d170222---4,"<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4wD92Z8uTSzKQ8S9fHmPBQ.png\" /></figure><p>I got myself occupied with developing an understanding of Convolutional Neural Networks, as part of my final year project themed around object detection. However, absorbing the mathematical principles defining the fundamentals of the subject was not an easy feat. I had to joust with the underlying concepts and for the first time see numbers as a representation of images, which to be honest was not easy for me to get my head around. Anyways, I did eventually made sense of the maths behind the representation and manipulation of images albeit after some sleepless nights.</p><p>Coming to the point, learning algebra, calculus or other numerical concepts to develop an understanding of computer vision or image processing would in itself be a debilitating ordeal, unless a clarity in relation between images and their numeric representation is established. Many of us are familiar with the vital role mathematics plays within the computer vision domain while those of you who aren’t, would be vaguely familiarized through this post along with a roadmap to conquer your fear of sophisticated Greek and Latin representations that contribute a sizeable proportion of modern day mathematics. So its been established, conquering mathematical methods is an imperative objective in our quest to becoming a CV expert and there is no other way. Hence we should embrace it as a powerful language to explain densely intricate phenomenon, an archaic tool to decrypt the mysteries of visual data and a pathway to innovation. One of the many effective ways to demystify the language of the Gods for CV whilst being creative and robust in our comprehension of it is to leverage python, as it will aid greatly in closely observing the image response to subsequent mathematical operations while also providing high dimensional visualizations via its arsenal of libraries.</p><p>After introduction to my hard-earned epiphany, let me walk you through a generic overview of mathematical concepts that play a critical role in image processing and computer vision.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/259/1*FjRRyfGC58ua7iBgnU_3Pg.jpeg\" /><figcaption><a href=\"https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html\">credits</a></figcaption></figure><h3>Goal of the article</h3><ol><li>Familiarize with the role of various mathematical methods in Computer Vision.</li><li>Provide a complete mathematical roadmap for Computer Vision research and development.</li></ol><h3><strong>Linear Algebra</strong></h3><p>Linear Algebra comes into play as we represent images in the form of vectors, matrices and tensors.</p><h3><strong>Calculus</strong></h3><p>Calculus helps derive and optimize mathematical models for image processing and computer vision tasks.</p><h3>Probability and Statistics</h3><p>Probability and Statistics help us model and analyse image data, including feature extraction, image segmentation and object detection.</p><h3>Signal Processing</h3><p>To filter and transform images for artifacts and noise removal as well as to extract meaningful information in time-frequency domain, we employ signal processing techniques such as Fourier Analysis and wavelet transforms.</p><h3>Differential Equations</h3><p>Differential Equations are leveraged to model Dynamic Systems such as optical flow, motion estimation and image registration.</p><h3>Geometry</h3><p>Geometry is important for spatial transformations and 3D reconstruction of objects in computer vision.</p><h3>Optimization</h3><p>Optimization is used to develop algorithms and models for image denoising, deblurring, and super-resolution.</p><h3>Mathematics Roadmap For Image Processing and Computer Vision</h3><p>Below is the roadmap of mathematical methods for computer vision that will contribute sufficiently in your computer vision research and development journey.</p><h3><strong>Linear Algebra</strong></h3><ul><li>Vector spaces and subspaces: Understanding properties and operations.</li><li>Matrix factorizations: Singular value decomposition (SVD) and eigenvalue decomposition.</li><li>Linear regression: Modeling relationships between variables for regression tasks.</li><li>Principal Component Analysis (PCA): Dimensionality reduction technique.</li></ul><h3>Calculus</h3><ul><li>Multivariable calculus: Partial derivatives, gradients, and optimization in multiple dimensions.</li><li>Chain rule: Calculating derivatives in composite functions.</li><li>Hessian matrix: Analyzing curvature and optimization in higher dimensions.</li><li>Variational calculus: Euler-Lagrange equations for energy minimization problems.</li></ul><h3>Probability and Statistics</h3><ul><li>Random processes: Modeling temporal and spatial uncertainty in computer vision.</li><li>Markov chains: Analyzing sequential and temporal data.</li><li>Statistical pattern recognition: Statistical techniques for object recognition and classification.</li><li>Bayesian decision theory: Decision-making under uncertainty.</li></ul><h3>Signal Processing</h3><p><strong>Sampling Theory</strong></p><ul><li>Nyquist-Shannon sampling theorem: Principles of converting continuous signals to discrete signals.</li><li>Aliasing: Understanding the effects of under-sampling and frequency folding.</li></ul><p><strong>Image Filtering</strong></p><ul><li>Linear filters: Convolution, correlation, and their applications for noise reduction and image enhancement.</li><li>Non-linear filters: Median filtering, bilateral filtering, and their use in preserving edges and reducing noise.</li></ul><p><strong>Frequency Domain Analysis</strong></p><ul><li>Discrete Fourier Transform (DFT): Transforming signals from time domain to frequency domain.</li><li>Fast Fourier Transform (FFT): Efficient algorithms for computing the DFT.</li><li>Power spectra: Analyzing signal content and identifying dominant frequencies.</li></ul><p><strong>Wavelet Theory</strong></p><ul><li>Continuous Wavelet Transform (CWT): Analysis of signals at different scales and resolutions.</li><li>Discrete Wavelet Transform (DWT): Decomposing signals into wavelet coefficients for efficient representation.</li><li>Wavelet packet analysis: Further analysis and decomposition of signals using wavelet packets.</li></ul><p><strong>Filter Design</strong></p><ul><li>Finite Impulse Response (FIR) filters: Designing filters with finite-duration impulse responses.</li><li>Infinite Impulse Response (IIR) filters: Designing filters with infinite-duration impulse responses.</li><li>Filter banks: Constructing filter banks for multi-resolution analysis and synthesis.</li></ul><p><strong>Image Compression</strong></p><ul><li>Transform coding: Applying transformations like Discrete Cosine Transform (DCT) for efficient data representation.</li><li>Quantization: Reducing precision while preserving essential image information.</li><li>Entropy coding: Techniques like Huffman coding and arithmetic coding for further compression.</li></ul><p><strong>Image Restoration</strong></p><ul><li>Inverse problems: Modeling image deblurring, super-resolution, and image reconstruction.</li><li>Regularization: Balancing fidelity to observed data and prior assumptions in restoration tasks.</li></ul><h3>Differential Equations</h3><ul><li>Partial differential equations (PDEs) in computer vision: Heat equation, wave equation, and diffusion equation for image analysis and restoration.</li><li>Level set methods: Implicit representation of curves and surfaces for segmentation tasks.</li><li>Active contours (Snakes): Contour evolution for object boundary detection.</li></ul><h3>Geometry</h3><ul><li>3D geometry: Representing and transforming 3D objects and scenes.</li><li>Camera geometry: Pinhole camera model, intrinsic and extrinsic camera parameters.</li><li>Structure from motion: Estimating 3D structure from 2D image sequences.</li><li>3D reconstruction: Techniques for building 3D models from multiple images.</li></ul><h3>Optimization</h3><ul><li>Nonlinear optimization: Techniques like Newton’s method and Levenberg — Marquardt algorithm.</li><li>Convex hulls: Convex polytopes and their applications in computer vision.</li><li>Graph cuts: Energy minimization for image segmentation and object recognition.</li><li>Combinatorial optimization: Solving NP-hard problems in computer vision.</li></ul><h3>Conclusion</h3><p>Computer Vision has grown rapidly in the last decade, owing to the vehement endeavors of the research community. To keep pace with the research and publications within the domain, an understanding of governing mathematical principles is imperative. Hence this article, intended to unravel the roadmap required for a career as a researcher or even a developer in Computer Vision domain.</p><p>I hope the aforementioned roadmap would navigate your computer vision journey. If some facets were left unattended, do let me know in the comment section.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8389a0d7b7be\" width=\"1\" /><hr /><p><a href=\"https://medium.com/artificialis/navigating-math-for-computer-vision-your-ultimate-roadmap-8389a0d7b7be\">Navigating Math for Computer Vision: Your Ultimate Roadmap</a> was originally published in <a href=\"https://medium.com/artificialis\">Artificialis</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",[{'name': 'Nabeel Khan'}],"Wed, 31 May 2023 14:34:35 GMT"
1194,Photonic-electronic spiking neuron with multi-modal and multi-wavelength excitatory and inhibitory operation for high-speed neuromorphic sensing and computing,https://arxiv.org/abs/2403.03775,"arXiv:2403.03775v1 Announce Type: cross 
Abstract: We report a multi-modal spiking neuron that allows optical and electronic input and control, and wavelength-multiplexing operation, for use in novel high-speed neuromorphic sensing and computing functionalities. The photonic-electronic neuron is built with a micro-scale, nanostructure resonant tunnelling diode (RTD) with photodetection (PD) capability. Leveraging the advantageous intrinsic properties of this RTD-PD system, namely highly nonlinear characteristics, photo-sensitivity, light-induced I-V curve shift, and the ability to deliver excitable responses under electrical and optical inputs, we successfully achieve flexible neuromorphic spike activation and inhibition regimes through photonic-electrical control. We also demonstrate the ability of this RTD-PD spiking sensing-processing neuron to operate under the simultaneous arrival of multiple wavelength-multiplexed optical signals, due to its large photodetection spectral window (covering the 1310 and 1550 nm telecom wavelength bands). Our results highlight the potential of RTD photonic-electronic neurons to reproduce multiple key excitatory and inhibitory spiking regimes, at high speed (ns-rate spiking responses, with faster sub-ns regimes theoretically predicted) and low energy (requiring only ~10 mV and ~150 microW, electrical and optical input amplitudes, respectively), similar in nature to those commonly found in the biological neurons of the visual system and the brain. This work offers a highly promising approach for the realisation of high-speed, energy-efficient photonic-electronic spiking neurons and spiking neural networks, enabling multi-modal and multi-wavelength operation for sensing and information processing tasks. This work therefore paves the way for innovative high-speed, photonic-electronic, and spike-based neuromorphic sensing and computing systems and artificial intelligence hardware.","[{'name': \"Weikang Zhang, Mat\\\\v{e}j Hejda, Qusay Raghib Ali Al-Taai, Dafydd Owen-Newns, Bruno Romeira, Jos\\\\'e M. L. Figueiredo, Joshua Robertson, Edward Wasige, Antonio Hurtado\"}]",
1195,Compiling Quantum Circuits for Dynamically Field-Programmable Neutral Atoms Array Processors,https://arxiv.org/abs/2306.03487,"arXiv:2306.03487v4 Announce Type: replace-cross 
Abstract: Dynamically field-programmable qubit arrays (DPQA) have recently emerged as a promising platform for quantum information processing. In DPQA, atomic qubits are selectively loaded into arrays of optical traps that can be reconfigured during the computation itself. Leveraging qubit transport and parallel, entangling quantum operations, different pairs of qubits, even those initially far away, can be entangled at different stages of the quantum program execution. Such reconfigurability and non-local connectivity present new challenges for compilation, especially in the layout synthesis step which places and routes the qubits and schedules the gates. In this paper, we consider a DPQA architecture that contains multiple arrays and supports 2D array movements, representing cutting-edge experimental platforms. Within this architecture, we discretize the state space and formulate layout synthesis as a satisfiability modulo theories problem, which can be solved by existing solvers optimally in terms of circuit depth. For a set of benchmark circuits generated by random graphs with complex connectivities, our compiler OLSQ-DPQA reduces the number of two-qubit entangling gates on small problem instances by 1.7x compared to optimal compilation results on a fixed planar architecture. To further improve scalability and practicality of the method, we introduce a greedy heuristic inspired by the iterative peeling approach in classical integrated circuit routing. Using a hybrid approach that combined the greedy and optimal methods, we demonstrate that our DPQA-based compiled circuits feature reduced scaling overhead compared to a grid fixed architecture, resulting in 5.1X less two-qubit gates for 90 qubit quantum circuits. These methods enable programmable, complex quantum circuits with neutral atom quantum computers, as well as informing both future compilers and future hardware choices.","[{'name': 'Daniel Bochen Tan, Dolev Bluvstein, Mikhail D. Lukin, Jason Cong'}]",
1196,Teaching Quantum Computing with Videos,https://research.library.fordham.edu/cs_facultypubs/1,"<p>Quantum computing will likely change our world as we know it today. In bringing together fields such as quantum mechanics, mathematics, and computer science in their construction and application, quantum computers hold a promise of tremendous increase in processing power when compared to present day technology. For an educator, teaching this topic requires that consideration be given to the fact that quantum computing is not only based on hard-to-teach aspects of physics, math, and computing, but that quantum computing itself is still in the early stages of its development. What is certain is that concepts such as Shor’s and Grover’s algorithms have already been developed specifically for use with quantum computing. Both algorithms are the beneficiaries of quantum properties such as superposition and entanglement which are at the core of the new technology. This paper is presented in a bottom up structure starting with the explanation of the basic component, the qubit, before moving to superposition and entanglement and ending with a lucid explanation of both of the algorithms. Due to the complex nature of the topic, teaching this subject requires that certain measures be taken to teach all students at all levels and have them feel comfortable within themselves about the topic. The course is meant for anyone who wants to learn quantum computing. It is furthermore recommended that those attending have a blank slate so we start with new students. We would like to see a variety of students taking and passing the course, with that in mind, the explanations of the topics are done without employing an excessive amount of math common to other courses in quantum computing. The recommended approach for teaching is through the use of videos and a curriculum which is broken down so topics are made more general and the discussion of that effort is the main focus</p>",[{'name': 'Daniel A. Sabol et al.'}],"Thu, 25 Apr 2019 21:30:19 PDT"
1197,Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy,https://arxiv.org/abs/2403.03288,"arXiv:2403.03288v1 Announce Type: new 
Abstract: In the rapidly evolving field of Large Language Models (LLMs), there is a critical need to thoroughly analyze their capabilities and risks. Central to our investigation are two novel elements. Firstly, it is the innovative parallels between the statistical patterns of word relationships within LLMs and Martin Heidegger's concepts of \"ready-to-hand\" and \"present-at-hand,\" which encapsulate the utilitarian and scientific altitudes humans employ in interacting with the world. This comparison lays the groundwork for positioning LLMs as the digital counterpart to the Faculty of Verbal Knowledge, shedding light on their capacity to emulate certain facets of human reasoning. Secondly, a structural analysis of human reasoning, viewed through Heidegger's notion of truth as \"unconcealment\" is conducted This foundational principle enables us to map out the inputs and outputs of the reasoning system and divide reasoning into four distinct categories. Respective cognitive faculties are delineated, allowing us to place LLMs within the broader schema of human reasoning, thus clarifying their strengths and inherent limitations. Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities, due to the current lack of many analogous AI models such as the Faculty of Judgement. The potential and risks of LLMs when they are augmented with other AI technologies are also evaluated. The results indicate that although LLMs have achieved proficiency in some reasoning abilities, the aspiration to match or exceed human intellectual capabilities is yet unattained. This research not only enriches our comprehension of LLMs but also propels forward the discourse on AI's potential and its bounds, paving the way for future explorations into AI's evolving landscape.",[{'name': 'Jianqiiu Zhang'}],
1198,AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis,https://arxiv.org/abs/2403.03293,"arXiv:2403.03293v1 Announce Type: new 
Abstract: This paper discusses the effectiveness of leveraging Chatbot: Generative Pre-trained Transformer (ChatGPT) versions 3.5 and 4 for analyzing research papers for effective writing of scientific literature surveys. The study selected the \\textit{Application of Artificial Intelligence in Breast Cancer Treatment} as the research topic. Research papers related to this topic were collected from three major publication databases Google Scholar, Pubmed, and Scopus. ChatGPT models were used to identify the category, scope, and relevant information from the research papers for automatic identification of relevant papers related to Breast Cancer Treatment (BCT), organization of papers according to scope, and identification of key information for survey paper writing. Evaluations performed using ground truth data annotated using subject experts reveal, that GPT-4 achieves 77.3\\% accuracy in identifying the research paper categories and 50\\% of the papers were correctly identified by GPT-4 for their scopes. Further, the results demonstrate that GPT-4 can generate reasons for its decisions with an average of 27\\% new words, and 67\\% of the reasons given by the model were completely agreeable to the subject experts.","[{'name': 'Anjalee De Silva, Janaka L. Wijekoon, Rashini Liyanarachchi, Rrubaa Panchendrarajan, Weranga Rajapaksha'}]",
1199,"The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa",https://arxiv.org/abs/2403.03357,"arXiv:2403.03357v1 Announce Type: new 
Abstract: With growing application of machine learning (ML) technologies in healthcare, there have been calls for developing techniques to understand and mitigate biases these systems may exhibit. Fair-ness considerations in the development of ML-based solutions for health have particular implications for Africa, which already faces inequitable power imbalances between the Global North and South.This paper seeks to explore fairness for global health, with Africa as a case study. We conduct a scoping review to propose axes of disparities for fairness consideration in the African context and delineate where they may come into play in different ML-enabled medical modalities. We then conduct qualitative research studies with 672 general population study participants and 28 experts inML, health, and policy focused on Africa to obtain corroborative evidence on the proposed axes of disparities. Our analysis focuses on colonialism as the attribute of interest and examines the interplay between artificial intelligence (AI), health, and colonialism. Among the pre-identified attributes, we found that colonial history, country of origin, and national income level were specific axes of disparities that participants believed would cause an AI system to be biased.However, there was also divergence of opinion between experts and general population participants. Whereas experts generally expressed a shared view about the relevance of colonial history for the development and implementation of AI technologies in Africa, the majority of the general population participants surveyed did not think there was a direct link between AI and colonialism. Based on these findings, we provide practical recommendations for developing fairness-aware ML solutions for health in Africa.","[{'name': 'Mercy Asiedu, Awa Dieng, Alexander Haykel, Negar Rostamzadeh, Stephen Pfohl, Chirag Nagpal, Maria Nagawa, Abigail Oppong, Sanmi Koyejo, Katherine Heller'}]",
1200,RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging,https://arxiv.org/abs/2403.03359,"arXiv:2403.03359v1 Announce Type: new 
Abstract: Autonomous parallel-style on-ramp merging in human controlled traffic continues to be an existing issue for autonomous vehicle control. Existing non-learning based solutions for vehicle control rely on rules and optimization primarily. These methods have been seen to present significant challenges. Recent advancements in Deep Reinforcement Learning have shown promise and have received significant academic interest however the available learning based approaches show inadequate attention to other highway vehicles and often rely on inaccurate road traffic assumptions. In addition, the parallel-style case is rarely considered. A novel learning based model for acceleration and lane change decision making that explicitly considers the utility to both the ego vehicle and its surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable is proposed. The novel reward function makes use of Social Value Orientation to weight the vehicle's level of social cooperation and is divided into ego vehicle and surrounding vehicle utility which are weighted according to the model's designated Social Value Orientation. A two-lane highway with an on-ramp divided into a taper-style and parallel-style section is considered. Simulation results indicated the importance of considering surrounding vehicles in reward function design and show that the proposed model matches or surpasses those in literature in terms of collisions while also introducing socially courteous behaviour avoiding near misses and anti-social behaviour through direct consideration of the effect of merging on surrounding vehicles.",[{'name': 'Jordan Poots'}],
1201,Adaptive Discovering and Merging for Incremental Novel Class Discovery,https://arxiv.org/abs/2403.03382,"arXiv:2403.03382v1 Announce Type: new 
Abstract: One important desideratum of lifelong learning aims to discover novel classes from unlabelled data in a continuous manner. The central challenge is twofold: discovering and learning novel classes while mitigating the issue of catastrophic forgetting of established knowledge. To this end, we introduce a new paradigm called Adaptive Discovering and Merging (ADM) to discover novel categories adaptively in the incremental stage and integrate novel knowledge into the model without affecting the original knowledge. To discover novel classes adaptively, we decouple representation learning and novel class discovery, and use Triple Comparison (TC) and Probability Regularization (PR) to constrain the probability discrepancy and diversity for adaptive category assignment. To merge the learned novel knowledge adaptively, we propose a hybrid structure with base and novel branches named Adaptive Model Merging (AMM), which reduces the interference of the novel branch on the old classes to preserve the previous knowledge, and merges the novel branch to the base model without performance loss and parameter growth. Extensive experiments on several datasets show that ADM significantly outperforms existing class-incremental Novel Class Discovery (class-iNCD) approaches. Moreover, our AMM also benefits the class-incremental Learning (class-IL) task by alleviating the catastrophic forgetting problem.","[{'name': 'Guangyao Chen, Peixi Peng, Yangru Huang, Mengyue Geng, Yonghong Tian'}]",
1202,BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving,https://arxiv.org/abs/2403.03401,"arXiv:2403.03401v1 Announce Type: new 
Abstract: Artificial Intelligence for Theorem Proving has given rise to a plethora of benchmarks and methodologies, particularly in Interactive Theorem Proving (ITP). Research in the area is fragmented, with a diverse set of approaches being spread across several ITP systems. This presents a significant challenge to the comparison of methods, which are often complex and difficult to replicate. Addressing this, we present BAIT, a framework for fair and streamlined comparison of learning approaches in ITP. We demonstrate BAIT's capabilities with an in-depth comparison, across several ITP benchmarks, of state-of-the-art architectures applied to the problem of formula embedding. We find that Structure Aware Transformers perform particularly well, improving on techniques associated with the original problem sets. BAIT also allows us to assess the end-to-end proving performance of systems built on interactive environments. This unified perspective reveals a novel end-to-end system that improves on prior work. We also provide a qualitative analysis, illustrating that improved performance is associated with more semantically-aware embeddings. By streamlining the implementation and comparison of Machine Learning algorithms in the ITP context, we anticipate BAIT will be a springboard for future research.","[{'name': 'Sean Lamont, Michael Norrish, Amir Dezfouli, Christian Walder, Paul Montague'}]",
1203,An EnKF-LSTM Assimilation Algorithm for Crop Growth Model,https://arxiv.org/abs/2403.03406,"arXiv:2403.03406v1 Announce Type: new 
Abstract: Accurate and timely prediction of crop growth is of great significance to ensure crop yields and researchers have developed several crop models for the prediction of crop growth. However, there are large difference between the simulation results obtained by the crop models and the actual results, thus in this paper, we proposed to combine the simulation results with the collected crop data for data assimilation so that the accuracy of prediction will be improved. In this paper, an EnKF-LSTM data assimilation method for various crops is proposed by combining ensemble Kalman filter and LSTM neural network, which effectively avoids the overfitting problem of existing data assimilation methods and eliminates the uncertainty of the measured data. The verification of the proposed EnKF-LSTM method and the comparison of the proposed method with other data assimilation methods were performed using datasets collected by sensor equipment deployed on a farm.","[{'name': 'Siqi Zhou, Ling Wang, Jie Liu, Jinshan Tang'}]",
1204,IB-Net: Initial Branch Network for Variable Decision in Boolean Satisfiability,https://arxiv.org/abs/2403.03517,"arXiv:2403.03517v1 Announce Type: new 
Abstract: Boolean Satisfiability problems are vital components in Electronic Design Automation, particularly within the Logic Equivalence Checking process. Currently, SAT solvers are employed for these problems and neural network is tried as assistance to solvers. However, as SAT problems in the LEC context are distinctive due to their predominantly unsatisfiability nature and a substantial proportion of UNSAT-core variables, existing neural network assistance has proven unsuccessful in this specialized domain. To tackle this challenge, we propose IB-Net, an innovative framework utilizing graph neural networks and novel graph encoding techniques to model unsatisfiable problems and interact with state-of-the-art solvers. Extensive evaluations across solvers and datasets demonstrate IB-Net's acceleration, achieving an average runtime speedup of 5.0% on industrial data and 8.3% on SAT competition data empirically. This breakthrough advances efficient solving in LEC workflows.","[{'name': 'Tsz Ho Chan, Wenyi Xiao, Junhua Huang, Huiling Zhen, Guangji Tian, Mingxuan Yuan'}]",
1205,Prompt Mining for Language-based Human Mobility Forecasting,https://arxiv.org/abs/2403.03544,"arXiv:2403.03544v1 Announce Type: new 
Abstract: With the advancement of large language models, language-based forecasting has recently emerged as an innovative approach for predicting human mobility patterns. The core idea is to use prompts to transform the raw mobility data given as numerical values into natural language sentences so that the language models can be leveraged to generate the description for future observations. However, previous studies have only employed fixed and manually designed templates to transform numerical values into sentences. Since the forecasting performance of language models heavily relies on prompts, using fixed templates for prompting may limit the forecasting capability of language models. In this paper, we propose a novel framework for prompt mining in language-based mobility forecasting, aiming to explore diverse prompt design strategies. Specifically, the framework includes a prompt generation stage based on the information entropy of prompts and a prompt refinement stage to integrate mechanisms such as the chain of thought. Experimental results on real-world large-scale data demonstrate the superiority of generated prompts from our prompt mining pipeline. Additionally, the comparison of different prompt variants shows that the proposed prompt refinement process is effective. Our study presents a promising direction for further advancing language-based mobility forecasting.","[{'name': 'Hao Xue, Tianye Tang, Ali Payani, Flora D. Salim'}]",
1206,Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models,https://arxiv.org/abs/2403.03550,"arXiv:2403.03550v1 Announce Type: new 
Abstract: This study investigates the generation of synthetic disinformation by OpenAI's Large Language Models (LLMs) through prompt engineering and explores their responsiveness to emotional prompting. Leveraging various LLM iterations using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed experiments to assess their success in producing disinformation. Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation. When prompted politely, all examined LLMs consistently generate disinformation at a high frequency. Conversely, when prompted impolitely, the frequency of disinformation production diminishes, as the models often refuse to generate disinformation and instead caution users that the tool is not intended for such purposes. This research contributes to the ongoing discourse surrounding responsible development and application of AI technologies, particularly in mitigating the spread of disinformation and promoting transparency in AI-generated content.","[{'name': 'Rasita Vinay, Giovanni Spitale, Nikola Biller-Andorno, Federico Germani'}]",
1207,Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision: Insights from Group and Individual Assessments,https://arxiv.org/abs/2403.03594,"arXiv:2403.03594v1 Announce Type: new 
Abstract: Recently, it has been recognized that large language models demonstrate high performance on various intellectual tasks. However, few studies have investigated alignment with humans in behaviors that involve sensibility, such as aesthetic evaluation. This study investigates the performance of GPT-4 with Vision, a state-of-the-art language model that can handle image input, on the task of aesthetic evaluation of images. We employ two tasks, prediction of the average evaluation values of a group and an individual's evaluation values. We investigate the performance of GPT-4 with Vision by exploring prompts and analyzing prediction behaviors. Experimental results reveal GPT-4 with Vision's superior performance in predicting aesthetic evaluations and the nature of different responses to beauty and ugliness. Finally, we discuss developing an AI system for aesthetic evaluation based on scientific knowledge of the human perception of beauty, employing agent technologies that integrate traditional deep learning models with large language models.","[{'name': 'Yoshia Abe, Tatsuya Daikoku, Yasuo Kuniyoshi'}]",
1208,A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation,https://arxiv.org/abs/2403.03600,"arXiv:2403.03600v1 Announce Type: new 
Abstract: Cross-domain recommendation (CDR) aims to enhance recommendation accuracy in a target domain with sparse data by leveraging rich information in a source domain, thereby addressing the data-sparsity problem. Some existing CDR methods highlight the advantages of extracting domain-common and domain-specific features to learn comprehensive user and item representations. However, these methods can't effectively disentangle these components as they often rely on simple user-item historical interaction information (such as ratings, clicks, and browsing), neglecting the rich multi-modal features. Additionally, they don't protect user-sensitive data from potential leakage during knowledge transfer between domains. To address these challenges, we propose a Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation, called P2M2-CDR. Specifically, we first design a multi-modal disentangled encoder that utilizes multi-modal information to disentangle more informative domain-common and domain-specific embeddings. Furthermore, we introduce a privacy-preserving decoder to mitigate user privacy leakage during knowledge transfer. Local differential privacy (LDP) is utilized to obfuscate the disentangled embeddings before inter-domain exchange, thereby enhancing privacy protection. To ensure both consistency and differentiation among these obfuscated disentangled embeddings, we incorporate contrastive learning-based domain-inter and domain-intra losses. Extensive Experiments conducted on four real-world datasets demonstrate that P2M2-CDR outperforms other state-of-the-art single-domain and cross-domain baselines.","[{'name': 'Li Wang, Lei Sang, Quangui Zhang, Qiang Wu, Min Xu'}]",
1209,The Geometric Structure of Topic Models,https://arxiv.org/abs/2403.03607,"arXiv:2403.03607v1 Announce Type: new 
Abstract: Topic models are a popular tool for clustering and analyzing textual data. They allow texts to be classified on the basis of their affiliation to the previously calculated topics. Despite their widespread use in research and application, an in-depth analysis of topic models is still an open research topic. State-of-the-art methods for interpreting topic models are based on simple visualizations, such as similarity matrices, top-term lists or embeddings, which are limited to a maximum of three dimensions. In this paper, we propose an incidence-geometric method for deriving an ordinal structure from flat topic models, such as non-negative matrix factorization. These enable the analysis of the topic model in a higher (order) dimension and the possibility of extracting conceptual relationships between several topics at once. Due to the use of conceptual scaling, our approach does not introduce any artificial topical relationships, such as artifacts of feature compression. Based on our findings, we present a new visualization paradigm for concept hierarchies based on ordinal motifs. These allow for a top-down view on topic spaces. We introduce and demonstrate the applicability of our approach based on a topic model derived from a corpus of scientific papers taken from 32 top machine learning venues.","[{'name': 'Johannes Hirth, Tom Hanika'}]",
1210,SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models,https://arxiv.org/abs/2403.03636,"arXiv:2403.03636v1 Announce Type: new 
Abstract: Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: $\\textit{Planner}$, $\\textit{Informer}$, and $\\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20-30% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at https://sheetagent.github.io.","[{'name': 'Yibin Chen, Yifu Yuan, Zeyu Zhang, Yan Zheng, Jinyi Liu, Fei Ni, Jianye Hao'}]",
1211,K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data,https://arxiv.org/abs/2403.03645,"arXiv:2403.03645v1 Announce Type: new 
Abstract: Sourced from various sensors and organized chronologically, Multivariate Time-Series (MTS) data involves crucial spatial-temporal dependencies, e.g., correlations among sensors. To capture these dependencies, Graph Neural Networks (GNNs) have emerged as powerful tools, yet their effectiveness is restricted by the quality of graph construction from MTS data. Typically, existing approaches construct graphs solely from MTS signals, which may introduce bias due to a small training dataset and may not accurately represent underlying dependencies. To address this challenge, we propose a novel framework named K-Link, leveraging Large Language Models (LLMs) to encode extensive general knowledge and thereby providing effective solutions to reduce the bias. Leveraging the knowledge embedded in LLMs, such as physical principles, we extract a \\textit{Knowledge-Link graph}, capturing vast semantic knowledge of sensors and the linkage of the sensor-level knowledge. To harness the potential of the knowledge-link graph in enhancing the graph derived from MTS data, we propose a graph alignment module, facilitating the transfer of semantic knowledge within the knowledge-link graph into the MTS-derived graph. By doing so, we can improve the graph quality, ensuring effective representation learning with GNNs for MTS data. Extensive experiments demonstrate the efficacy of our approach for superior performance across various MTS-related downstream tasks.","[{'name': 'Yucheng Wang, Ruibing Jin, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen'}]",
1212,Towards Safe and Aligned Large Language Models for Medicine,https://arxiv.org/abs/2403.03744,"arXiv:2403.03744v1 Announce Type: new 
Abstract: The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical LLMs. Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale approaches used by the machine learning community to develop safe and aligned LLMs. We hope that this work casts light on the safety and alignment of medical LLMs and motivates future work to study it and develop additional mitigation strategies, minimizing the risks of harm of LLMs in medicine.","[{'name': 'Tessa Han, Aounon Kumar, Chirag Agarwal, Himabindu Lakkaraju'}]",
1213,DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models,https://arxiv.org/abs/2403.03768,"arXiv:2403.03768v1 Announce Type: new 
Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater effectiveness than a comparator set of two approved drug in 5/8 colorectal cancer (CRC) organoids. This highlights DeepCRE's ability to identify a collection of drug candidates with superior therapeutic effects, underscoring its potential to revolutionize the field of therapeutic development.",[{'name': 'Yushuai Wu'}],
1214,From Clicks to Security: Investigating Continuous Authentication via Mouse Dynamics,https://arxiv.org/abs/2403.03828,"arXiv:2403.03828v1 Announce Type: new 
Abstract: In the realm of computer security, the importance of efficient and reliable user authentication methods has become increasingly critical. This paper examines the potential of mouse movement dynamics as a consistent metric for continuous authentication. By analyzing user mouse movement patterns in two contrasting gaming scenarios, \"Team Fortress\" and Poly Bridge we investigate the distinctive behavioral patterns inherent in high-intensity and low-intensity UI interactions. The study extends beyond conventional methodologies by employing a range of machine learning models. These models are carefully selected to assess their effectiveness in capturing and interpreting the subtleties of user behavior as reflected in their mouse movements. This multifaceted approach allows for a more nuanced and comprehensive understanding of user interaction patterns. Our findings reveal that mouse movement dynamics can serve as a reliable indicator for continuous user authentication. The diverse machine learning models employed in this study demonstrate competent performance in user verification, marking an improvement over previous methods used in this field. This research contributes to the ongoing efforts to enhance computer security and highlights the potential of leveraging user behavior, specifically mouse dynamics, in developing robust authentication systems.","[{'name': 'Rushit Dave, Marcho Handoko, Ali Rashid, Cole Schoenbauer'}]",
1215,Your device may know you better than you know yourself -- continuous authentication on novel dataset using machine learning,https://arxiv.org/abs/2403.03832,"arXiv:2403.03832v1 Announce Type: new 
Abstract: This research aims to further understanding in the field of continuous authentication using behavioral biometrics. We are contributing a novel dataset that encompasses the gesture data of 15 users playing Minecraft with a Samsung Tablet, each for a duration of 15 minutes. Utilizing this dataset, we employed machine learning (ML) binary classifiers, being Random Forest (RF), K-Nearest Neighbors (KNN), and Support Vector Classifier (SVC), to determine the authenticity of specific user actions. Our most robust model was SVC, which achieved an average accuracy of approximately 90%, demonstrating that touch dynamics can effectively distinguish users. However, further studies are needed to make it viable option for authentication systems","[{'name': 'Pedro Gomes do Nascimento, Pidge Witiak, Tucker MacCallum, Zachary Winterfeldt, Rushit Dave'}]",
1216,IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators,https://arxiv.org/abs/2403.03894,"arXiv:2403.03894v1 Announce Type: new 
Abstract: Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.
  To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled with respective intermediate representations. Next, starting from various base Code-LMs (ranging in size from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages. Our resulting models, dubbed IRCoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.","[{'name': 'Indraneil Paul, Jun Luo, Goran Glava\\\\v{s}, Iryna Gurevych'}]",
1217,Enhancing Instructional Quality: Leveraging Computer-Assisted Textual Analysis to Generate In-Depth Insights from Educational Artifacts,https://arxiv.org/abs/2403.03920,"arXiv:2403.03920v1 Announce Type: new 
Abstract: This paper explores the transformative potential of computer-assisted textual analysis in enhancing instructional quality through in-depth insights from educational artifacts. We integrate Richard Elmore's Instructional Core Framework to examine how artificial intelligence (AI) and machine learning (ML) methods, particularly natural language processing (NLP), can analyze educational content, teacher discourse, and student responses to foster instructional improvement. Through a comprehensive review and case studies within the Instructional Core Framework, we identify key areas where AI/ML integration offers significant advantages, including teacher coaching, student support, and content development. We unveil patterns that indicate AI/ML not only streamlines administrative tasks but also introduces novel pathways for personalized learning, providing actionable feedback for educators and contributing to a richer understanding of instructional dynamics. This paper emphasizes the importance of aligning AI/ML technologies with pedagogical goals to realize their full potential in educational settings, advocating for a balanced approach that considers ethical considerations, data quality, and the integration of human expertise.","[{'name': 'Zewei Tian, Min Sun, Alex Liu, Shawon Sarkar, Jing Liu'}]",
1218,Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing,https://arxiv.org/abs/2402.16627,"arXiv:2402.16627v2 Announce Type: cross 
Abstract: Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing. In each task, our ContextDiff achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations. Our code is available at https://github.com/YangLing0818/ContextDiff","[{'name': 'Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, Bin Cui'}]",
1219,Structure-Guided Adversarial Training of Diffusion Models,https://arxiv.org/abs/2402.17563,"arXiv:2402.17563v2 Announce Type: cross 
Abstract: Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively.","[{'name': 'Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui'}]",
1220,Knowledge-guided EEG Representation Learning,https://arxiv.org/abs/2403.03222,"arXiv:2403.03222v1 Announce Type: cross 
Abstract: Self-supervised learning has produced impressive results in multimedia domains of audio, vision and speech. This paradigm is equally, if not more, relevant for the domain of biosignals, owing to the scarcity of labelled data in such scenarios. The ability to leverage large-scale unlabelled data to learn robust representations could help improve the performance of numerous inference tasks on biosignals. Given the inherent domain differences between multimedia modalities and biosignals, the established objectives for self-supervised learning may not translate well to this domain. Hence, there is an unmet need to adapt these methods to biosignal analysis. In this work we propose a self-supervised model for EEG, which provides robust performance and remarkable parameter efficiency by using state space-based deep learning architecture. We also propose a novel knowledge-guided pre-training objective that accounts for the idiosyncrasies of the EEG signal. The results indicate improved embedding representation learning and downstream performance compared to prior works on exemplary tasks. Also, the proposed objective significantly reduces the amount of pre-training data required to obtain performance equivalent to prior works.","[{'name': 'Aditya Kommineni, Kleanthis Avramidis, Richard Leahy, Shrikanth Narayanan'}]",
1221,Reinforcement Learning Jazz Improvisation: When Music Meets Game Theory,https://arxiv.org/abs/2403.03224,"arXiv:2403.03224v1 Announce Type: cross 
Abstract: Live performances of music are always charming, with the unpredictability of improvisation due to the dynamic between musicians and interactions with the audience. Jazz improvisation is a particularly noteworthy example for further investigation from a theoretical perspective. Here, we introduce a novel mathematical game theory model for jazz improvisation, providing a framework for studying music theory and improvisational methodologies. We use computational modeling, mainly reinforcement learning, to explore diverse stochastic improvisational strategies and their paired performance on improvisation. We find that the most effective strategy pair is a strategy that reacts to the most recent payoff (Stepwise Changes) with a reinforcement learning strategy limited to notes in the given chord (Chord-Following Reinforcement Learning). Conversely, a strategy that reacts to the partner's last note and attempts to harmonize with it (Harmony Prediction) strategy pair yields the lowest non-control payoff and highest standard deviation, indicating that picking notes based on immediate reactions to the partner player can yield inconsistent outcomes. On average, the Chord-Following Reinforcement Learning strategy demonstrates the highest mean payoff, while Harmony Prediction exhibits the lowest. Our work lays the foundation for promising applications beyond jazz: including the use of artificial intelligence (AI) models to extract data from audio clips to refine musical reward systems, and training machine learning (ML) models on existing jazz solos to further refine strategies within the game.","[{'name': 'Vedant Tapiavala, Joshua Piesner, Sourjyamoy Barman, Feng Fu'}]",
1222,Large language models surpass human experts in predicting neuroscience results,https://arxiv.org/abs/2403.03230,"arXiv:2403.03230v1 Announce Type: cross 
Abstract: Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.","[{'name': \"Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe Y\\\\'a\\\\~nez, Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina Borghesani, Anton Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro Salatiello, Ilia Sucholutsky, Pasquale Minervini, Sepehr Razavi, Roberta Rocca, Elkhan Yusifov, Tereza Okalova, Nianlong Gu, Martin Ferianc, Mikail Khona, Kaustubh R. Patil, Pui-Shee Lee, Rui Mata, Nicholas E. Myers, Jennifer K Bizley, Sebastian Musslick, Isil Poyraz Bilgin, Guiomar Niso, Justin M. Ales, Michael Gaebler, N Apurva Ratan Murty, Chloe M. Hall, Jessica Dafflon, Sherry Dongqi Bao, Bradley C. Love\"}]",
1223,"Note: Harnessing Tellurium Nanoparticles in the Digital Realm Plasmon Resonance, in the Context of Brewster's Angle and the Drude Model for Fake News Adsorption in Incomplete Information Games",https://arxiv.org/abs/2403.03239,"arXiv:2403.03239v1 Announce Type: cross 
Abstract: This note explores the innovative application of soliton theory and plasmonic phenomena in modeling user behavior and engagement within digital health platforms. By introducing the concept of soliton solutions, we present a novel approach to understanding stable patterns of health improvement behaviors over time. Additionally, we delve into the role of tellurium nanoparticles and their plasmonic properties in adsorbing fake news, thereby influencing user interactions and engagement levels. Through a theoretical framework that combines nonlinear dynamics with the unique characteristics of tellurium nanoparticles, we aim to provide new insights into the dynamics of user engagement in digital health environments. Our analysis highlights the potential of soliton theory in capturing the complex, nonlinear dynamics of user behavior, while the application of plasmonic phenomena offers a promising avenue for enhancing the sensitivity and effectiveness of digital health platforms. This research ventures into an uncharted territory where optical phenomena such as Brewster's Angle and Snell's Law, along with the concept of spin solitons, are metaphorically applied to address the challenge of fake news dissemination. By exploring the analogy between light refraction, reflection, and the propagation of information in digital platforms, we unveil a novel perspective on how the 'angle' at which information is presented can significantly affect its acceptance and spread. Additionally, we propose the use of tellurium nanoparticles to manage 'information waves' through mechanisms akin to plasmonic resonance and soliton dynamics. This theoretical exploration aims to bridge the gap between physical sciences and digital communication, offering insights into the development of strategies for mitigating misinformation.",[{'name': 'Yasuko Kawahata'}],
1224,From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE,https://arxiv.org/abs/2403.03274,"arXiv:2403.03274v1 Announce Type: cross 
Abstract: Digital health technologies (DHT), such as wearable devices, provide personalized, continuous, and real-time monitoring of patient. These technologies are contributing to the development of novel therapies and personalized medicine. Gaining insight from these technologies requires appropriate modeling techniques to capture clinically-relevant changes in disease state. The data generated from these devices is characterized by being stochastic in nature, may have missing elements, and exhibits considerable inter-individual variability - thereby making it difficult to analyze using traditional longitudinal modeling techniques. We present a novel pharmacology-informed neural stochastic differential equation (SDE) model capable of addressing these challenges. Using synthetic data, we demonstrate that our approach is effective in identifying treatment effects and learning causal relationships from stochastic data, thereby enabling counterfactual simulation.","[{'name': 'Samira Pakravan, Nikolaos Evangelou, Maxime Usdin, Logan Brooks, James Lu'}]",
1225,ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures,https://arxiv.org/abs/2403.03276,"arXiv:2403.03276v1 Announce Type: cross 
Abstract: We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneous datasets, including the CHB-MIT and UPenn and Mayos Clinic, CHB-MIT datasets. The empirical findings suggest that the ARNN model outperforms baseline methods such as LSTM, Vision Transformer (ViT), Compact Convolution Transformer (CCT), and R-Transformer (RT), showcasing superior performance and faster processing capabilities across a wide range of tasks. The code has been made publicly accessible at \\url{https://github.com/Salim-Lysiun/ARNN}.","[{'name': 'Salim Rukhsar, Anil Kumar Tiwari'}]",
1226,Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits,https://arxiv.org/abs/2403.03281,"arXiv:2403.03281v1 Announce Type: cross 
Abstract: We consider the problem of late multi-modal fusion for discriminative learning. Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of multi-modal fusion. We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities. We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC. Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while maintaining competitive performance with the state-of-the-art.","[{'name': 'Sahil Sidheekh, Pranuthi Tenali, Saurabh Mathur, Erik Blasch, Kristian Kersting, Sriraam Natarajan'}]",
1227,Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification,https://arxiv.org/abs/2403.03305,"arXiv:2403.03305v1 Announce Type: cross 
Abstract: This paper introduces a novel neuro-symbolic architecture for relation classification (RC) that combines rule-based methods with contemporary deep learning techniques. This approach capitalizes on the strengths of both paradigms: the adaptability of rule-based systems and the generalization power of neural networks. Our architecture consists of two components: a declarative rule-based model for transparent classification and a neural component to enhance rule generalizability through semantic text matching. Notably, our semantic matcher is trained in an unsupervised domain-agnostic way, solely with synthetic data. Further, these components are loosely coupled, allowing for rule modifications without retraining the semantic matcher. In our evaluation, we focused on two few-shot relation classification datasets: Few-Shot TACRED and a Few-Shot version of NYT29. We show that our proposed method outperforms previous state-of-the-art models in three out of four settings, despite not seeing any human-annotated training data. Further, we show that our approach remains modular and pliable, i.e., the corresponding rules can be locally modified to improve the overall model. Human interventions to the rules for the TACRED relation \\texttt{org:parents} boost the performance on that relation by as much as 26\\% relative improvement, without negatively impacting the other relations, and without retraining the semantic matching component.","[{'name': 'Robert Vacareanu, Fahmida Alam, Md Asiful Islam, Haris Riaz, Mihai Surdeanu'}]",
1228,Deep Configuration Performance Learning: A Systematic Survey and Taxonomy,https://arxiv.org/abs/2403.03322,"arXiv:2403.03322v1 Announce Type: cross 
Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.
  In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evaluated and how they are exploited in different tasks related to software configuration. We also identify the good practice and the potentially problematic phenomena from the studies surveyed, together with insights on future opportunities for the field. To promote open science, all the raw results of this survey can be accessed at our repository: https://github.com/ideas-labo/DCPL-SLR.","[{'name': 'Jingzhi Gong, Tao Chen'}]",
1229,"DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification",https://arxiv.org/abs/2403.03334,"arXiv:2403.03334v1 Announce Type: cross 
Abstract: Stance detection of social media text is a key component of downstream tasks involving the identification of groups of users with opposing opinions on contested topics such as vaccination and within arguments. In particular, stance provides an indication of an opinion towards an entity. This paper introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated for their stance towards videos of the U.S. military. The stance is annotated through a human-guided, machine-assisted labeling methodology that makes use of weak signals of tone within the sentence as supporting indicators, as opposed to using manual annotations by humans. These weak signals consist of the presence of hate speech and sarcasm, the presence of specific keywords, the sentiment of the text, and the stance inference from two Large Language Models. The weak signals are then consolidated using a data programming model before each comment is annotated with a final stance label. On average, the videos have 200 comments each, and the stance of the comments skews slightly towards the \"against\" characterization for both the U.S. Army and the videos posted on the channel.","[{'name': 'Iain J. Cruickshank, Lynnette Hui Xian Ng'}]",
1230,Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation,https://arxiv.org/abs/2403.03344,"arXiv:2403.03344v1 Announce Type: cross 
Abstract: The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generate codes considered in this study are produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code's \"green capacity\", based on certain sustainability metrics. We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development.","[{'name': 'Tina Vartziotis, Ippolyti Dellatolas, George Dasoulas, Maximilian Schmidt, Florian Schneider, Tim Hoffmann, Sotirios Kotsopoulos, Michael Keckeisen'}]",
1231,Learning to Maximize Mutual Information for Chain-of-Thought Distillation,https://arxiv.org/abs/2403.03348,"arXiv:2403.03348v1 Announce Type: cross 
Abstract: Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve this optimization problem using a learning-based method. Our experimental results across four datasets demonstrate that our method outperforms the state-of-the-art DSS. Our findings offer insightful guidance for future research on language model distillation as well as applications involving CoT. Code and models will be released soon.","[{'name': 'Xin Chen, Hanxian Huang, Yanjun Gao, Yi Wang, Jishen Zhao, Ke Ding'}]",
1232,Multi-modal Deep Learning,https://arxiv.org/abs/2403.03385,"arXiv:2403.03385v1 Announce Type: cross 
Abstract: This article investigates deep learning methodologies for single-modality clinical data analysis, as a crucial precursor to multi-modal medical research. Building on Guo JingYuan's work, the study refines clinical data processing through Compact Convolutional Transformer (CCT), Patch Up, and the innovative CamCenterLoss technique, establishing a foundation for future multimodal investigations. The proposed methodology demonstrates improved prediction accuracy and at tentiveness to critically ill patients compared to Guo JingYuan's ResNet and StageNet approaches. Novelty that using image-pretrained vision transformer backbone to perform transfer learning time-series clinical data.The study highlights the potential of CCT, Patch Up, and novel CamCenterLoss in processing single modality clinical data within deep learning frameworks, paving the way for future multimodal medical research and promoting precision and personalized healthcare",[{'name': 'Chen Yuhua'}],
1233,Interactive Melody Generation System for Enhancing the Creativity of Musicians,https://arxiv.org/abs/2403.03395,"arXiv:2403.03395v1 Announce Type: cross 
Abstract: This study proposes a system designed to enumerate the process of collaborative composition among humans, using automatic music composition technology. By integrating multiple Recurrent Neural Network (RNN) models, the system provides an experience akin to collaborating with several composers, thereby fostering diverse creativity. Through dynamic adaptation to the user's creative intentions, based on feedback, the system enhances its capability to generate melodies that align with user preferences and creative needs. The system's effectiveness was evaluated through experiments with composers of varying backgrounds, revealing its potential to facilitate musical creativity and suggesting avenues for further refinement. The study underscores the importance of interaction between the composer and AI, aiming to make music composition more accessible and personalized. This system represents a step towards integrating AI into the creative process, offering a new tool for composition support and collaborative artistic exploration.","[{'name': 'So Hirawata, Noriko Otani'}]",
1234,Human vs. Machine: Language Models and Wargames,https://arxiv.org/abs/2403.03407,"arXiv:2403.03407v1 Announce Type: cross 
Abstract: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.","[{'name': 'Max Lamparth, Anthony Corso, Jacob Ganz, Oriana Skylar Mastro, Jacquelyn Schneider, Harold Trinkunas'}]",
1235,Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN,https://arxiv.org/abs/2403.03409,"arXiv:2403.03409v1 Announce Type: cross 
Abstract: Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task, and, then, pruning neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning a large randomly initialized model. We introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from a randomly initialized RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained for different tasks, such as image classification and temporal prediction. We experimentally show that, in spite of being task-agnostic, LNP increases computational efficiency (fewer neurons and synapses) and prediction performance of RSNNs compared to traditional activity-based pruning of trained dense models.","[{'name': 'Biswadeep Chakraborty, Beomseok Kang, Harshit Kumar, Saibal Mukhopadhyay'}]",
1236,Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization,https://arxiv.org/abs/2403.03419,"arXiv:2403.03419v1 Announce Type: cross 
Abstract: Large language models (LLMs) have revolutionized the role of AI, yet also pose potential risks of propagating unethical content. Alignment technologies have been introduced to steer LLMs towards human preference, gaining increasing attention. Despite notable breakthroughs in this direction, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research focus: achieving alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness. For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between the generated responses and the dispreferred ones to effectively eschew harmful information. We theoretically demonstrate that D$^2$O is equivalent to learning a distributional instead of instance-level preference model reflecting human dispreference against the distribution of negative responses. Besides, D$^2$O integrates an implicit Jeffrey Divergence regularization to balance the exploitation and exploration of reference policies and converges to a non-negative one during training. Extensive experiments demonstrate that our method achieves comparable generation quality and surpasses the latest baselines in producing less harmful and more informative responses with better training stability and faster convergence.","[{'name': 'Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu'}]",
1237,LEAD: Learning Decomposition for Source-free Universal Domain Adaptation,https://arxiv.org/abs/2403.03421,"arXiv:2403.03421v1 Announce Type: cross 
Abstract: Universal Domain Adaptation (UniDA) targets knowledge transfer in the presence of both covariate and label shifts. Recently, Source-free Universal Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to source data, which tends to be more practical due to data protection policies. The main challenge lies in determining whether covariate-shifted samples belong to target-private unknown categories. Existing methods tackle this either through hand-crafted thresholding or by developing time-consuming iterative clustering strategies. In this paper, we propose a new idea of LEArning Decomposition (LEAD), which decouples features into source-known and -unknown components to identify target-private data. Technically, LEAD initially leverages the orthogonal decomposition analysis for feature decomposition. Then, LEAD builds instance-level decision boundaries to adaptively identify target-private data. Extensive experiments across various UniDA scenarios have demonstrated the effectiveness and superiority of LEAD. Notably, in the OPDA scenario on VisDA dataset, LEAD outperforms GLC by 3.5% overall H-score and reduces 75% time to derive pseudo-labeling decision boundaries. Besides, LEAD is also appealing in that it is complementary to most existing methods. The code is available at https://github.com/ispc-lab/LEAD.","[{'name': 'Sanqing Qu, Tianpei Zou, Lianghua He, Florian R\\\\\"ohrbein, Alois Knoll, Guang Chen, Changjun Jiang'}]",
1238,Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models,https://arxiv.org/abs/2403.03432,"arXiv:2403.03432v1 Announce Type: cross 
Abstract: Instruction Tuning has the potential to stimulate or enhance specific capabilities of large language models (LLMs). However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks. To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and parameter-efficient tuning method designed for multi-task learning with LLMs. In this paper, we start by individually training multiple domain-specific LoRA modules using corresponding supervised corpus data. These LoRA modules can be aligned with the expert design principles observed in Mixture-of-Experts (MoE). Subsequently, we combine the multiple LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which help prevent interference between tasks and ultimately enhances the performance of each individual task. Furthermore, each LoRA model can be iteratively adapted to a new domain, allowing for quick domain-specific adaptation. Experiments on diverse tasks demonstrate superior and robust performance, which can further promote the wide application of domain-specific LLMs.","[{'name': 'Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Yu Han, Hao Wang'}]",
1239,Uncertainty quantification for deeponets with ensemble kalman inversion,https://arxiv.org/abs/2403.03444,"arXiv:2403.03444v1 Announce Type: cross 
Abstract: In recent years, operator learning, particularly the DeepONet, has received much attention for efficiently learning complex mappings between input and output functions across diverse fields. However, in practical scenarios with limited and noisy data, accessing the uncertainty in DeepONet predictions becomes essential, especially in mission-critical or safety-critical applications. Existing methods, either computationally intensive or yielding unsatisfactory uncertainty quantification, leave room for developing efficient and informative uncertainty quantification (UQ) techniques tailored for DeepONets. In this work, we proposed a novel inference approach for efficient UQ for operator learning by harnessing the power of the Ensemble Kalman Inversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and highly parallelizable feature, has demonstrated its advantages for UQ for physics-informed neural networks [28]. Our innovative application of EKI enables us to efficiently train ensembles of DeepONets while obtaining informative uncertainty estimates for the output of interest. We deploy a mini-batch variant of EKI to accommodate larger datasets, mitigating the computational demand due to large datasets during the training stage. Furthermore, we introduce a heuristic method to estimate the artificial dynamics covariance, thereby improving our uncertainty estimates. Finally, we demonstrate the effectiveness and versatility of our proposed methodology across various benchmark problems, showcasing its potential to address the pressing challenges of uncertainty quantification in DeepONets, especially for practical applications with limited and noisy data.","[{'name': 'Andrew Pensoneault, Xueyu Zhu'}]",
1240,DLP-GAN: Learning to Draw Modern Chinese Landscape Photos with Generative Adversarial Network,https://arxiv.org/abs/2403.03456,"arXiv:2403.03456v1 Announce Type: cross 
Abstract: Chinese landscape painting has a unique and artistic style, and its drawing technique is highly abstract in both the use of color and the realistic representation of objects. Previous methods focus on transferring from modern photos to ancient ink paintings. However, little attention has been paid to translating landscape paintings into modern photos. To solve such problems, in this paper, we (1) propose DLP-GAN (\\textbf{D}raw Modern Chinese \\textbf{L}andscape \\textbf{P}hotos with \\textbf{G}enerative \\textbf{A}dversarial \\textbf{N}etwork), an unsupervised cross-domain image translation framework with a novel asymmetric cycle mapping, and (2) introduce a generator based on a dense-fusion module to match different translation directions. Moreover, a dual-consistency loss is proposed to balance the realism and abstraction of model painting. In this way, our model can draw landscape photos and sketches in the modern sense. Finally, based on our collection of modern landscape and sketch datasets, we compare the images generated by our model with other benchmarks. Extensive experiments including user studies show that our model outperforms state-of-the-art methods.","[{'name': 'Xiangquan Gui, Binxuan Zhang, Li Li, Yi Yang'}]",
1241,Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts,https://arxiv.org/abs/2403.03506,"arXiv:2403.03506v1 Announce Type: cross 
Abstract: This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical findings highlight (1) detecting AI-generated sentences in hybrid texts is overall a challenging task because (1.1) human writers' selecting and even editing AI-generated sentences based on personal preferences adds difficulty in identifying the authorship of segments; (1.2) the frequent change of authorship between neighboring sentences within the hybrid text creates difficulties for segment detectors in identifying authorship-consistent segments; (1.3) the short length of text segments within hybrid texts provides limited stylistic cues for reliable authorship determination; (2) before embarking on the detection process, it is beneficial to assess the average length of segments within the hybrid text. This assessment aids in deciding whether (2.1) to employ a text segmentation-based strategy for hybrid texts with longer segments, or (2.2) to adopt a direct sentence-by-sentence classification strategy for those with shorter segments.","[{'name': \"Zijie Zeng, Shiqi Liu, Lele Sha, Zhuang Li, Kaixun Yang, Sannyuya Liu, Dragan Ga\\\\v{s}evi\\\\'c, Guanliang Chen\"}]",
1242,Towards Efficient and Effective Unlearning of Large Language Models for Recommendation,https://arxiv.org/abs/2403.03536,"arXiv:2403.03536v1 Announce Type: cross 
Abstract: The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning. In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \\textit{inefficiency} and \\textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \\textbf{E2URec}, the first \\underline{E}fficient and \\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our proposed E2URec enhances the unlearning efficiency by updating only a few additional LoRA parameters, and improves the unlearning effectiveness by employing a teacher-student framework, where we maintain multiple teacher networks to guide the unlearning process. Extensive experiments show that E2URec outperforms state-of-the-art baselines on two real-world datasets. Specifically, E2URec can efficiently forget specific data without affecting recommendation performance. The source code is at \\url{https://github.com/justarter/E2URec}.","[{'name': 'Hangyu Wang, Jianghao Lin, Bo Chen, Yang Yang, Ruiming Tang, Weinan Zhang, Yong Yu'}]",
1243,RADIA -- Radio Advertisement Detection with Intelligent Analytics,https://arxiv.org/abs/2403.03538,"arXiv:2403.03538v1 Announce Type: cross 
Abstract: Radio advertising remains an integral part of modern marketing strategies, with its appeal and potential for targeted reach undeniably effective. However, the dynamic nature of radio airtime and the rising trend of multiple radio spots necessitates an efficient system for monitoring advertisement broadcasts. This study investigates a novel automated radio advertisement detection technique incorporating advanced speech recognition and text classification algorithms. RadIA's approach surpasses traditional methods by eliminating the need for prior knowledge of the broadcast content. This contribution allows for detecting impromptu and newly introduced advertisements, providing a comprehensive solution for advertisement detection in radio broadcasting. Experimental results show that the resulting model, trained on carefully segmented and tagged text data, achieves an F1-macro score of 87.76 against a theoretical maximum of 89.33. This paper provides insights into the choice of hyperparameters and their impact on the model's performance. This study demonstrates its potential to ensure compliance with advertising broadcast contracts and offer competitive surveillance. This groundbreaking research could fundamentally change how radio advertising is monitored and open new doors for marketing optimization.","[{'name': \"Jorge \\\\'Alvarez, Juan Carlos Armenteros, Camilo Torr\\\\'on, Miguel Ortega-Mart\\\\'in, Alfonso Ardoiz, \\\\'Oscar Garc\\\\'ia, Ignacio Arranz, \\\\'I\\\\~nigo Galdeano, Ignacio Garrido, Adri\\\\'an Alonso, Fernando Bay\\\\'on, Oleg Vorontsov\"}]",
1244,gaHealth: An English-Irish Bilingual Corpus of Health Data,https://arxiv.org/abs/2403.03575,"arXiv:2403.03575v1 Announce Type: cross 
Abstract: Machine Translation is a mature technology for many high-resource language pairs. However in the context of low-resource languages, there is a paucity of parallel data datasets available for developing translation models. Furthermore, the development of datasets for low-resource languages often focuses on simply creating the largest possible dataset for generic translation. The benefits and development of smaller in-domain datasets can easily be overlooked. To assess the merits of using in-domain data, a dataset for the specific domain of health was developed for the low-resource English to Irish language pair. Our study outlines the process used in developing the corpus and empirically demonstrates the benefits of using an in-domain dataset for the health domain. In the context of translating health-related data, models developed using the gaHealth corpus demonstrated a maximum BLEU score improvement of 22.2 points (40%) when compared with top performing models from the LoResMT2021 Shared Task. Furthermore, we define linguistic guidelines for developing gaHealth, the first bilingual corpus of health data for the Irish language, which we hope will be of use to other creators of low-resource data sets. gaHealth is now freely available online and is ready to be explored for further research.","[{'name': \"S\\\\'eamus Lankford, Haithem Afli, \\\\'Orla N\\\\'i Loinsigh, Andy Way\"}]",
1245,Causal Disentanglement for Regulating Social Influence Bias in Social Recommendation,https://arxiv.org/abs/2403.03578,"arXiv:2403.03578v1 Announce Type: cross 
Abstract: Social recommendation systems face the problem of social influence bias, which can lead to an overemphasis on recommending items that friends have interacted with. Addressing this problem is crucial, and existing methods often rely on techniques such as weight adjustment or leveraging unbiased data to eliminate this bias. However, we argue that not all biases are detrimental, i.e., some items recommended by friends may align with the user's interests. Blindly eliminating such biases could undermine these positive effects, potentially diminishing recommendation accuracy. In this paper, we propose a Causal Disentanglement-based framework for Regulating Social influence Bias in social recommendation, named CDRSB, to improve recommendation performance. From the perspective of causal inference, we find that the user social network could be regarded as a confounder between the user and item embeddings (treatment) and ratings (outcome). Due to the presence of this social network confounder, two paths exist from user and item embeddings to ratings: a non-causal social influence path and a causal interest path. Building upon this insight, we propose a disentangled encoder that focuses on disentangling user and item embeddings into interest and social influence embeddings. Mutual information-based objectives are designed to enhance the distinctiveness of these disentangled embeddings, eliminating redundant information. Additionally, a regulatory decoder that employs a weight calculation module to dynamically learn the weights of social influence embeddings for effectively regulating social influence bias has been designed. Experimental results on four large-scale real-world datasets Ciao, Epinions, Dianping, and Douban book demonstrate the effectiveness of CDRSB compared to state-of-the-art baselines.","[{'name': 'Li Wang, Min Xu, Quangui Zhang, Yunxiao Shi, Qiang Wu'}]",
1246,Design of an Open-Source Architecture for Neural Machine Translation,https://arxiv.org/abs/2403.03582,"arXiv:2403.03582v1 Announce Type: cross 
Abstract: adaptNMT is an open-source application that offers a streamlined approach to the development and deployment of Recurrent Neural Networks and Transformer models. This application is built upon the widely-adopted OpenNMT ecosystem, and is particularly useful for new entrants to the field, as it simplifies the setup of the development environment and creation of train, validation, and test splits. The application offers a graphing feature that illustrates the progress of model training, and employs SentencePiece for creating subword segmentation models. Furthermore, the application provides an intuitive user interface that facilitates hyperparameter customization. Notably, a single-click model development approach has been implemented, and models developed by adaptNMT can be evaluated using a range of metrics. To encourage eco-friendly research, adaptNMT incorporates a green report that flags the power consumption and kgCO${_2}$ emissions generated during model development. The application is freely available.","[{'name': \"S\\\\'eamus Lankford, Haithem Afli, Andy Way\"}]",
1247,RouteExplainer: An Explanation Framework for Vehicle Routing Problem,https://arxiv.org/abs/2403.03585,"arXiv:2403.03585v1 Announce Type: cross 
Abstract: The Vehicle Routing Problem (VRP) is a widely studied combinatorial optimization problem and has been applied to various practical problems. While the explainability for VRP is significant for improving the reliability and interactivity in practical VRP applications, it remains unexplored. In this paper, we propose RouteExplainer, a post-hoc explanation framework that explains the influence of each edge in a generated route. Our framework realizes this by rethinking a route as the sequence of actions and extending counterfactual explanations based on the action influence model to VRP. To enhance the explanation, we additionally propose an edge classifier that infers the intentions of each edge, a loss function to train the edge classifier, and explanation-text generation by Large Language Models (LLMs). We quantitatively evaluate our edge classifier on four different VRPs. The results demonstrate its rapid computation while maintaining reasonable accuracy, thereby highlighting its potential for deployment in practical applications. Moreover, on the subject of a tourist route, we qualitatively evaluate explanations generated by our framework. This evaluation not only validates our framework but also shows the synergy between explanation frameworks and LLMs. See https://ntt-dkiku.github.io/xai-vrp for our code, datasets, models, and demo.","[{'name': 'Daisuke Kikuta, Hiroki Ikeuchi, Kengo Tajiri, Yuusuke Nakano'}]",
1248,Wildest Dreams: Reproducible Research in Privacy-preserving Neural Network Training,https://arxiv.org/abs/2403.03592,"arXiv:2403.03592v1 Announce Type: cross 
Abstract: Machine Learning (ML), addresses a multitude of complex issues in multiple disciplines, including social sciences, finance, and medical research. ML models require substantial computing power and are only as powerful as the data utilized. Due to high computational cost of ML methods, data scientists frequently use Machine Learning-as-a-Service (MLaaS) to outsource computation to external servers. However, when working with private information, like financial data or health records, outsourcing the computation might result in privacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have enabled ML training and inference over protected data through the use of Privacy-Preserving Machine Learning (PPML). However, these techniques are still at a preliminary stage and their application in real-world situations is demanding. In order to comprehend discrepancy between theoretical research suggestions and actual applications, this work examines the past and present of PPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party Computation (SMPC) applied to ML. This work primarily focuses on the ML model's training phase, where maintaining user data privacy is of utmost importance. We provide a solid theoretical background that eases the understanding of current approaches and their limitations. In addition, we present a SoK of the most recent PPML frameworks for model training and provide a comprehensive comparison in terms of the unique properties and performances on standard benchmarks. Also, we reproduce the results for some of the papers and examine at what level existing works in the field provide support for open science. We believe our work serves as a valuable contribution by raising awareness about the current gap between theoretical advancements and real-world applications in PPML, specifically regarding open-source availability, reproducibility, and usability.","[{'name': 'Tanveer Khan, Mindaugas Budzys, Khoa Nguyen, Antonis Michalas'}]",
1249,Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem,https://arxiv.org/abs/2403.03593,"arXiv:2403.03593v1 Announce Type: cross 
Abstract: Training high-quality deep learning models is a challenging task due to computational and technical requirements. A growing number of individuals, institutions, and companies increasingly rely on pre-trained, third-party models made available in public repositories. These models are often used directly or integrated in product pipelines with no particular precautions, since they are effectively just data in tensor form and considered safe. In this paper, we raise awareness of a new machine learning supply chain threat targeting neural networks. We introduce MaleficNet 2.0, a novel technique to embed self-extracting, self-executing malware in neural networks. MaleficNet 2.0 uses spread-spectrum channel coding combined with error correction techniques to inject malicious payloads in the parameters of deep neural networks. MaleficNet 2.0 injection technique is stealthy, does not degrade the performance of the model, and is robust against removal techniques. We design our approach to work both in traditional and distributed learning settings such as Federated Learning, and demonstrate that it is effective even when a reduced number of bits is used for the model parameters. Finally, we implement a proof-of-concept self-extracting neural network malware using MaleficNet 2.0, demonstrating the practicality of the attack against a widely adopted machine learning framework. Our aim with this work is to raise awareness against these new, dangerous attacks both in the research community and industry, and we hope to encourage further research in mitigation techniques against such threats.","[{'name': 'Dorjan Hitaj, Giulio Pagnotta, Fabio De Gaspari, Sediola Ruko, Briland Hitaj, Luigi V. Mancini, Fernando Perez-Cruz'}]",
1250,Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators,https://arxiv.org/abs/2403.03606,"arXiv:2403.03606v1 Announce Type: cross 
Abstract: This study presents an innovative approach for predicting cryptocurrency time series, specifically focusing on Bitcoin, Ethereum, and Litecoin. The methodology integrates the use of technical indicators, a Performer neural network, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal dynamics and extract significant features from raw cryptocurrency data. The application of technical indicators, such facilitates the extraction of intricate patterns, momentum, volatility, and trends. The Performer neural network, employing Fast Attention Via positive Orthogonal Random features (FAVOR+), has demonstrated superior computational efficiency and scalability compared to the traditional Multi-head attention mechanism in Transformer models. Additionally, the integration of BiLSTM in the feedforward network enhances the model's capacity to capture temporal dynamics in the data, processing it in both forward and backward directions. This is particularly advantageous for time series data where past and future data points can influence the current state. The proposed method has been applied to the hourly and daily timeframes of the major cryptocurrencies and its performance has been benchmarked against other methods documented in the literature. The results underscore the potential of the proposed method to outperform existing models, marking a significant progression in the field of cryptocurrency price prediction.","[{'name': 'Mohammad Ali Labbaf Khaniki, Mohammad Manthouri'}]",
1251,GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding,https://arxiv.org/abs/2403.03608,"arXiv:2403.03608v1 Announce Type: cross 
Abstract: Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified.","[{'name': 'Zi-Ting Chou, Sheng-Yu Huang, I-Jieh Liu, Yu-Chiang Frank Wang'}]",
1252,Multimodal Large Language Models to Support Real-World Fact-Checking,https://arxiv.org/abs/2403.03627,"arXiv:2403.03627v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) carry the potential to support humans in processing vast amounts of information. While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied. Here is aim to bridge this gap. In particular, we propose a framework for systematically assessing the capacity of current multimodal models to facilitate real-world fact-checking. Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities. By designing prompts that extract models' predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure. We empirically find that (1) GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing open-source models exhibit strong biases and are highly sensitive to the prompt. Our study offers insights into combating false multimodal information and building secure, trustworthy multimodal models. To the best of our knowledge, we are the first to evaluate MLLMs for real-world fact-checking.","[{'name': 'Jiahui Geng, Yova Kementchedjhieva, Preslav Nakov, Iryna Gurevych'}]",
1253,Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People,https://arxiv.org/abs/2403.03640,"arXiv:2403.03640v1 Announce Type: cross 
Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a proxy-tuning fashion. We will open-source training corpora, code, model weights and evaluation benchmark.","[{'name': 'Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, Benyou Wang'}]",
1254,A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation,https://arxiv.org/abs/2403.03643,"arXiv:2403.03643v1 Announce Type: cross 
Abstract: The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefore, this paper aims to summarize and review recent theoretical methods and applied research utilizing reinforcement learning to address spatial resource allocation problems. It provides a summary and comprehensive overview of its fundamental principles, related methodologies, and applied research. Additionally, it highlights several unresolved issues that urgently require attention in this direction for the future.","[{'name': 'Di Zhang, Moyang Wang, Joseph Mango, Xiang Li'}]",
1255,General2Specialized LLMs Translation for E-commerce,https://arxiv.org/abs/2403.03689,"arXiv:2403.03689v1 Announce Type: cross 
Abstract: Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and robustness of our G2ST approach, as compared with state-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4.","[{'name': 'Kaidi Chen, Ben Chen, Dehong Gao, Huangyu Dai, Wen Jiang, Wei Ning, Shanqing Yu, Libin Yang, Xiaoyan Cai'}]",
1256,Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese,https://arxiv.org/abs/2403.03690,"arXiv:2403.03690v1 Announce Type: cross 
Abstract: The creation of instruction data and evaluation benchmarks for serving Large language models often involves enormous human annotation. This issue becomes particularly pronounced when rapidly developing such resources for a non-English language like Japanese. Instead of following the popular practice of directly translating existing English resources into Japanese (e.g., Japanese-Alpaca), we propose an efficient self-instruct method based on GPT-4. We first translate a small amount of English instructions into Japanese and post-edit them to obtain native-level quality. GPT-4 then utilizes them as demonstrations to automatically generate Japanese instruction data. We also construct an evaluation benchmark containing 80 questions across 8 categories, using GPT-4 to automatically assess the response quality of LLMs without human references. The empirical results suggest that the models fine-tuned on our GPT-4 self-instruct data significantly outperformed the Japanese-Alpaca across all three base pre-trained models. Our GPT-4 self-instruct data allowed the LLaMA 13B model to defeat GPT-3.5 (Davinci-003) with a 54.37\\% win-rate. The human evaluation exhibits the consistency between GPT-4's assessments and human preference. Our high-quality instruction data and evaluation benchmark have been released here.","[{'name': 'Yikun Sun, Zhen Wan, Nobuhiro Ueda, Sakiko Yahata, Fei Cheng, Chenhui Chu, Sadao Kurohashi'}]",
1257,MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition,https://arxiv.org/abs/2403.03691,"arXiv:2403.03691v1 Announce Type: cross 
Abstract: In the field of chemical structure recognition, the task of converting molecular images into graph structures and SMILES string stands as a significant challenge, primarily due to the varied drawing styles and conventions prevalent in chemical literature. To bridge this gap, we proposed MolNexTR, a novel image-to-graph deep learning model that collaborates to fuse the strengths of ConvNext, a powerful Convolutional Neural Network variant, and Vision-TRansformer. This integration facilitates a more nuanced extraction of both local and global features from molecular images. MolNexTR can predict atoms and bonds simultaneously and understand their layout rules. It also excels at flexibly integrating symbolic chemistry principles to discern chirality and decipher abbreviated structures. We further incorporate a series of advanced algorithms, including improved data augmentation module, image contamination module, and a post-processing module to get the final SMILES output. These modules synergistically enhance the model's robustness against the diverse styles of molecular imagery found in real literature. In our test sets, MolNexTR has demonstrated superior performance, achieving an accuracy rate of 81-97%, marking a significant advancement in the domain of molecular structure recognition. Scientific contribution: MolNexTR is a novel image-to-graph model that incorporates a unique dual-stream encoder to extract complex molecular image features, and combines chemical rules to predict atoms and bonds while understanding atom and bond layout rules. In addition, it employs a series of novel augmentation algorithms to significantly enhance the robustness and performance of the model.","[{'name': 'Yufan Chen, Ching Ting Leung, Yong Huang, Jianwei Sun, Hao Chen, Hanyu Gao'}]",
1258,Towards Controllable Time Series Generation,https://arxiv.org/abs/2403.03698,"arXiv:2403.03698v1 Announce Type: cross 
Abstract: Time Series Generation (TSG) has emerged as a pivotal technique in synthesizing data that accurately mirrors real-world time series, becoming indispensable in numerous applications. Despite significant advancements in TSG, its efficacy frequently hinges on having large training datasets. This dependency presents a substantial challenge in data-scarce scenarios, especially when dealing with rare or unique conditions. To confront these challenges, we explore a new problem of Controllable Time Series Generation (CTSG), aiming to produce synthetic time series that can adapt to various external conditions, thereby tackling the data scarcity issue.
  In this paper, we propose \\textbf{C}ontrollable \\textbf{T}ime \\textbf{S}eries (\\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG. A key feature of \\textsf{CTS} is that it decouples the mapping process from standard VAE training, enabling precise learning of a complex interplay between latent features and external conditions. Moreover, we develop a comprehensive evaluation scheme for CTSG. Extensive experiments across three real-world time series datasets showcase \\textsf{CTS}'s exceptional capabilities in generating high-quality, controllable outputs. This underscores its adeptness in seamlessly integrating latent features with external conditions. Extending \\textsf{CTS} to the image domain highlights its remarkable potential for explainability and further reinforces its versatility across different modalities.","[{'name': 'Yifan Bao, Yihao Ang, Qiang Huang, Anthony K. H. Tung, Zhiyong Huang'}]",
1259,Diffusion on language model embeddings for protein sequence generation,https://arxiv.org/abs/2403.03726,"arXiv:2403.03726v1 Announce Type: cross 
Abstract: Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accurately reflect the inherent structural and functional diversity of the protein space. This work advances the field of protein design and sets the stage for conditional models by providing a robust framework for scalable and high-quality protein sequence generation.","[{'name': 'Viacheslav Meshchaninov, Pavel Strashnov, Andrey Shevtsov, Fedor Nikolaev, Nikita Ivanisenko, Olga Kardymon, Dmitry Vetrov'}]",
1260,Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training,https://arxiv.org/abs/2403.03728,"arXiv:2403.03728v1 Announce Type: cross 
Abstract: This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models. We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies. Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes.","[{'name': 'Paul Doucet, Benjamin Estermann, Till Aczel, Roger Wattenhofer'}]",
1261,Learning 3D object-centric representation through prediction,https://arxiv.org/abs/2403.03730,"arXiv:2403.03730v1 Announce Type: cross 
Abstract: As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic reasoning. While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking. Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion. The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes. This results in object representations being learned as an essential byproduct of learning to predict.","[{'name': 'John Day, Tushar Arora, Jirui Liu, Li Erran Li, Ming Bo Cai'}]",
1262,A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network,https://arxiv.org/abs/2403.03739,"arXiv:2403.03739v1 Announce Type: cross 
Abstract: Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&amp;B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10, CIFAR-100, and ImageNet datasets, respectively, which are competitive with the state-of-the-art. Ablation studies have verified the efficacy of the quantized RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to using a fixed slope RLeakyReLU. The proposed add&amp;bit-operation-only BNN offers an innovative approach for hardware-friendly network architecture.","[{'name': 'Ruichen Ma, Guanchao Qiao, Yian Liu, Liwei Meng, Ning Ning, Yang Liu, Shaogang Hu'}]",
1263,SUPClust: Active Learning at the Boundaries,https://arxiv.org/abs/2403.03741,"arXiv:2403.03741v1 Announce Type: cross 
Abstract: Active learning is a machine learning paradigm designed to optimize model performance in a setting where labeled data is expensive to acquire. In this work, we propose a novel active learning method called SUPClust that seeks to identify points at the decision boundary between classes. By targeting these points, SUPClust aims to gather information that is most informative for refining the model's prediction of complex decision regions. We demonstrate experimentally that labeling these points leads to strong model performance. This improvement is observed even in scenarios characterized by strong class imbalance.","[{'name': 'Yuta Ono, Till Aczel, Benjamin Estermann, Roger Wattenhofer'}]",
1264,German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset,https://arxiv.org/abs/2403.03750,"arXiv:2403.03750v1 Announce Type: cross 
Abstract: The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings. We open-source and release the absinth dataset to foster further research on hallucination detection in German.","[{'name': 'Laura Mascarell, Ribin Chalumattu, Annette Rios'}]",
1265,ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport,https://arxiv.org/abs/2403.03777,"arXiv:2403.03777v1 Announce Type: cross 
Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our method, called Expectile-Regularised Neural Optimal Transport (ENOT). ENOT outperforms previous state-of-the-art approaches on the Wasserstein-2 benchmark tasks by a large margin (up to a 3-fold improvement in quality and up to a 10-fold improvement in runtime).","[{'name': 'Nazar Buzun, Maksim Bobrin, Dmitry V. Dylov'}]",
1266,Neural Architecture Search using Particle Swarm and Ant Colony Optimization,https://arxiv.org/abs/2403.03781,"arXiv:2403.03781v1 Announce Type: cross 
Abstract: Neural network models have a number of hyperparameters that must be chosen along with their architecture. This can be a heavy burden on a novice user, choosing which architecture and what values to assign to parameters. In most cases, default hyperparameters and architectures are used. Significant improvements to model accuracy can be achieved through the evaluation of multiple architectures. A process known as Neural Architecture Search (NAS) may be applied to automatically evaluate a large number of such architectures. A system integrating open source tools for Neural Architecture Search (OpenNAS), in the classification of images, has been developed as part of this research. OpenNAS takes any dataset of grayscale, or RBG images, and generates Convolutional Neural Network (CNN) architectures based on a range of metaheuristics using either an AutoKeras, a transfer learning or a Swarm Intelligence (SI) approach. Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO) are used as the SI algorithms. Furthermore, models developed through such metaheuristics may be combined using stacking ensembles. In the context of this paper, we focus on training and optimizing CNNs using the Swarm Intelligence (SI) components of OpenNAS. Two major types of SI algorithms, namely PSO and ACO, are compared to see which is more effective in generating higher model accuracies. It is shown, with our experimental design, that the PSO algorithm performs better than ACO. The performance improvement of PSO is most notable with a more complex dataset. As a baseline, the performance of fine-tuned pre-trained models is also evaluated.","[{'name': \"S\\\\'eamus Lankford, Diarmuid Grimes\"}]",
1267,KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs,https://arxiv.org/abs/2403.03791,"arXiv:2403.03791v1 Announce Type: cross 
Abstract: Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes. Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data. To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships. KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average improvement of 7% in Area under the ROC Curve (AUC) and 9% in Influence Function-based Precision of Estimating Heterogeneous Effects (IF-PEHE). The effectiveness of our estimated treatment effects is further affirmed by alignment with established randomized clinical trial findings.","[{'name': 'Ruoqi Liu, Lingfei Wu, Ping Zhang'}]",
1268,Confidence-Aware Decision-Making and Control for Tool Selection,https://arxiv.org/abs/2403.03808,"arXiv:2403.03808v1 Announce Type: cross 
Abstract: Self-reflecting about our performance (e.g., how confident we are) before doing a task is essential for decision making, such as selecting the most suitable tool or choosing the best route to drive. While this form of awareness -- thinking about our performance or metacognitive performance -- is well-known in humans, robots still lack this cognitive ability. This reflective monitoring can enhance their embodied decision power, robustness and safety. Here, we take a step in this direction by introducing a mathematical framework that allows robots to use their control self-confidence to make better-informed decisions. We derive a mathematical closed-form expression for control confidence for dynamic systems (i.e., the posterior inverse covariance of the control action). This control confidence seamlessly integrates within an objective function for decision making, that balances the: i) performance for task completion, ii) control effort, and iii) self-confidence. To evaluate our theoretical account, we framed the decision-making within the tool selection problem, where the agent has to select the best robot arm for a particular control task. The statistical analysis of the numerical simulations with randomized 2DOF arms shows that using control confidence during tool selection improves both real task performance, and the reliability of the tool for performance under unmodelled perturbations (e.g., external forces). Furthermore, our results indicate that control confidence is an early indicator of performance and thus, it can be used as a heuristic for making decisions when computation power is restricted or decision-making is intractable. Overall, we show the advantages of using confidence-aware decision-making and control scheme for dynamic systems.","[{'name': 'Ajith Anil Meera, Pablo Lanillos'}]",
1269,ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing,https://arxiv.org/abs/2403.03812,"arXiv:2403.03812v1 Announce Type: cross 
Abstract: Used car pricing is a critical aspect of the automotive industry, influenced by many economic factors and market dynamics. With the recent surge in online marketplaces and increased demand for used cars, accurate pricing would benefit both buyers and sellers by ensuring fair transactions. However, the transition towards automated pricing algorithms using machine learning necessitates the comprehension of model uncertainties, specifically the ability to flag predictions that the model is unsure about. Although recent literature proposes the use of boosting algorithms or nearest neighbor-based approaches for swift and precise price predictions, encapsulating model uncertainties with such algorithms presents a complex challenge. We introduce ProbSAINT, a model that offers a principled approach for uncertainty quantification of its price predictions, along with accurate point predictions that are comparable to state-of-the-art boosting techniques. Furthermore, acknowledging that the business prefers pricing used cars based on the number of days the vehicle was listed for sale, we show how ProbSAINT can be used as a dynamic forecasting model for predicting price probabilities for different expected offer duration. Our experiments further indicate that ProbSAINT is especially accurate on instances where it is highly certain. This proves the applicability of its probabilistic predictions in real-world scenarios where trustworthiness is crucial.","[{'name': 'Kiran Madhusudhanan, Gunnar Behrens, Maximilian Stubbemann, Lars Schmidt-Thieme'}]",
1270,Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ,https://arxiv.org/abs/2403.03814,"arXiv:2403.03814v1 Announce Type: cross 
Abstract: Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\\ whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages beyond their intended use. Most models are more accurate when they respond faithfully. However, differences across models are large, and there is a long tail of languages where models are neither accurate nor faithful. We explore differences in tokenization as a potential explanation for our findings, identifying possible correlations that warrant further investigation.","[{'name': 'Carolin Holtermann, Paul R\\\\\"ottger, Timm Dill, Anne Lauscher'}]",
1271,Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning,https://arxiv.org/abs/2403.03835,"arXiv:2403.03835v1 Announce Type: cross 
Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.","[{'name': 'Xin Lian, Sashank Varma, Christopher J. MacLellan'}]",
1272,"Accelerating Convergence of Score-Based Diffusion Models, Provably",https://arxiv.org/abs/2403.03852,"arXiv:2403.03852v1 Announce Type: cross 
Abstract: Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\\sqrt{T})$ for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like the DPM-Solver-2. Our theory accommodates $\\ell_2$-accurate score estimates, and does not require log-concavity or smoothness on the target distribution.","[{'name': 'Gen Li, Yu Huang, Timofey Efimov, Yuting Wei, Yuejie Chi, Yuxin Chen'}]",
1273,Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning,https://arxiv.org/abs/2403.03864,"arXiv:2403.03864v1 Announce Type: cross 
Abstract: This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size. Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles. The findings emphasize the challenges of integrating visual, language, and algorithmic knowledge for solving complex reasoning problems.","[{'name': 'Deepanway Ghosal, Vernon Toh Yan Han, Chia Yew Ken, Soujanya Poria'}]",
1274,Impoverished Language Technology: The Lack of (Social) Class in NLP,https://arxiv.org/abs/2403.03874,"arXiv:2403.03874v1 Announce Type: cross 
Abstract: Since Labov's (1964) foundational work on the social stratification of language, linguistics has dedicated concerted efforts towards understanding the relationships between socio-demographic factors and language production and perception. Despite the large body of evidence identifying significant relationships between socio-demographic factors and language production, relatively few of these factors have been investigated in the context of NLP technology. While age and gender are well covered, Labov's initial target, socio-economic class, is largely absent. We survey the existing Natural Language Processing (NLP) literature and find that only 20 papers even mention socio-economic status. However, the majority of those papers do not engage with class beyond collecting information of annotator-demographics. Given this research lacuna, we provide a definition of class that can be operationalised by NLP researchers, and argue for including socio-economic class in future language technologies.","[{'name': 'Amanda Cercas Curry, Zeerak Talat, Dirk Hovy'}]",
1275,Redefining cystoscopy with ai: bladder cancer diagnosis using an efficient hybrid cnn-transformer model,https://arxiv.org/abs/2403.03879,"arXiv:2403.03879v1 Announce Type: cross 
Abstract: Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and is among the most expensive cancers to treat due to the high recurrence rates which require lifetime follow-ups. The primary tool for diagnosis is cystoscopy, which heavily relies on doctors' expertise and interpretation. Therefore, annually, numerous cases are either undiagnosed or misdiagnosed and treated as urinary infections. To address this, we suggest a deep learning approach for bladder cancer detection and segmentation which combines CNNs with a lightweight positional-encoding-free transformer and dual attention gates that fuse self and spatial attention for feature enhancement. The architecture suggested in this paper is efficient making it suitable for medical scenarios that require real time inference. Experiments have proven that this model addresses the critical need for a balance between computational efficiency and diagnostic accuracy in cystoscopic imaging as despite its small size it rivals large models in performance.","[{'name': 'Meryem Amaouche, Ouassim Karrakchou, Mounir Ghogho, Anouar El Ghazzaly, Mohamed Alami, Ahmed Ameur'}]",
1276,Latent Dataset Distillation with Diffusion Models,https://arxiv.org/abs/2403.03881,"arXiv:2403.03881v1 Announce Type: cross 
Abstract: The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both challenges. LD3M incorporates a novel diffusion process tailored for dataset distillation, which improves the gradient norms for learning synthetic images. By adjusting the number of diffusion steps, LD3M also offers a straightforward way of controlling the trade-off between speed and accuracy. We evaluate our approach in several ImageNet subsets and for high-resolution images (128x128 and 256x256). As a result, LD3M consistently outperforms state-of-the-art distillation techniques by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.","[{'name': 'Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel'}]",
1277,Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation,https://arxiv.org/abs/2403.03890,"arXiv:2403.03890v1 Announce Type: cross 
Abstract: This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation. HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories. The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions. To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint position diffuser via differentiable kinematics. Empirically, we show that HDP achieves a significantly higher success rate than the state-of-the-art methods in both simulation and real-world.","[{'name': 'Xiao Ma, Sumit Patidar, Iain Haughton, Stephen James'}]",
1278,From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models,https://arxiv.org/abs/2403.03893,"arXiv:2403.03893v1 Announce Type: cross 
Abstract: To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it's crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages. Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field. Code and data are available at https://github.com/for-ai/goodtriever.","[{'name': 'Luiza Pozzobon, Patrick Lewis, Sara Hooker, Beyza Ermis'}]",
1279,Consciousness qua Mortal Computation,https://arxiv.org/abs/2403.03925,"arXiv:2403.03925v1 Announce Type: cross 
Abstract: Computational functionalism posits that consciousness is a computation. Here we show, perhaps surprisingly, that it cannot be a Turing computation. Rather, computational functionalism implies that consciousness is a novel type of computation that has recently been proposed by Geoffrey Hinton, called mortal computation.",[{'name': 'Johannes Kleiner'}],
1280,Extreme Precipitation Nowcasting using Transformer-based Generative Models,https://arxiv.org/abs/2403.03929,"arXiv:2403.03929v1 Announce Type: cross 
Abstract: This paper presents an innovative approach to extreme precipitation nowcasting by employing Transformer-based generative models, namely NowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a comprehensive dataset from the Royal Netherlands Meteorological Institute (KNMI), our study focuses on predicting short-term precipitation with high accuracy. We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events. We present both qualitative and quantitative analyses, demonstrating the superior performance of the proposed NowcastingGPT-EVL in generating accurate precipitation forecasts, especially when dealing with extreme precipitation events. The code is available at \\url{https://github.com/Cmeo97/NowcastingGPT}.","[{'name': 'Cristian Meo, Ankush Roy, Mircea Lic\\\\u{a}, Junzhe Yin, Zeineb Bou Che, Yanbo Wang, Ruben Imhoff, Remko Uijlenhoet, Justin Dauwels'}]",
1281,Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation,https://arxiv.org/abs/2403.03949,"arXiv:2403.03949v1 Announce Type: cross 
Abstract: Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in \"digital twin\" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel \"inverse distillation\" procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required. We evaluate RialTo across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf, and six other tasks. RialTo increases (over 67%) in policy robustness without requiring extensive human data collection. Project website and videos at https://real-to-sim-to-real.github.io/RialTo/","[{'name': 'Marcel Torne, Anthony Simeonov, Zechu Li, April Chan, Tao Chen, Abhishek Gupta, Pulkit Agrawal'}]",
1282,Stop Regressing: Training Value Functions via Classification for Scalable Deep RL,https://arxiv.org/abs/2403.03950,"arXiv:2403.03950v1 Announce Type: cross 
Abstract: Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity. Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost.","[{'name': 'Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Ta\\\\\"iga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Aviral Kumar, Rishabh Agarwal'}]",
1283,Decision-making with Speculative Opponent Models,https://arxiv.org/abs/2211.11940,"arXiv:2211.11940v2 Announce Type: replace 
Abstract: Opponent modeling has benefited a controlled agent's decision-making by constructing models of other agents. Existing methods commonly assume access to opponents' observations and actions, which is infeasible when opponents' behaviors are unobservable or hard to obtain. We propose a novel multi-agent distributional actor-critic algorithm to achieve speculative opponent modeling with purely local information (i.e., the controlled agent's observations, actions, and rewards). Specifically, the actor maintains a speculated belief of the opponents, which we call the speculative opponent models, to predict opponent actions using local observations and makes decisions accordingly. Further, the distributional critic models the return distribution of the policy. It reflects the quality of the actor and thus can guide the training of the speculative opponent model that the actor relies on. Extensive experiments confirm that our method successfully models opponents' behaviors without their data and delivers superior performance against baseline methods with a faster convergence speed.","[{'name': 'Jing Sun, Shuo Chen, Cong Zhang, Jie Zhang'}]",
1284,Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model,https://arxiv.org/abs/2307.08424,"arXiv:2307.08424v3 Announce Type: replace 
Abstract: Model inversion attacks (MIAs) aim to recover private data from inaccessible training sets of deep learning models, posing a privacy threat. MIAs primarily focus on the white-box scenario where attackers have full access to the model's structure and parameters. However, practical applications are usually in black-box scenarios or label-only scenarios, i.e., the attackers can only obtain the output confidence vectors or labels by accessing the model. Therefore, the attack models in existing MIAs are difficult to effectively train with the knowledge of the target model, resulting in sub-optimal attacks. To the best of our knowledge, we pioneer the research of a powerful and practical attack model in the label-only scenario.
  In this paper, we develop a novel MIA method, leveraging a conditional diffusion model (CDM) to recover representative samples under the target label from the training set. Two techniques are introduced: selecting an auxiliary dataset relevant to the target model task and using predicted labels as conditions to guide training CDM; and inputting target label, pre-defined guidance strength, and random noise into the trained attack model to generate and correct multiple results for final selection. This method is evaluated using Learned Perceptual Image Patch Similarity as a new metric and as a judgment basis for deciding the values of hyper-parameters. Experimental results show that this method can generate similar and accurate samples to the target label, outperforming generators of previous approaches.","[{'name': 'Rongke Liu, Dong Wang, Yizhi Ren, Zhen Wang, Kaitian Guo, Qianqian Qin, Xiaolei Liu'}]",
1285,Arithmetic with Language Models: from Memorization to Computation,https://arxiv.org/abs/2308.01154,"arXiv:2308.01154v3 Announce Type: replace 
Abstract: A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypothesis that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation.","[{'name': 'Davide Maltoni, Matteo Ferrara'}]",
1286,PyGraft: Configurable Generation of Synthetic Schemas and Knowledge Graphs at Your Fingertips,https://arxiv.org/abs/2309.03685,"arXiv:2309.03685v2 Announce Type: replace 
Abstract: Knowledge graphs (KGs) have emerged as a prominent data representation and management paradigm. Being usually underpinned by a schema (e.g., an ontology), KGs capture not only factual information but also contextual knowledge. In some tasks, a few KGs established themselves as standard benchmarks. However, recent works outline that relying on a limited collection of datasets is not sufficient to assess the generalization capability of an approach. In some data-sensitive fields such as education or medicine, access to public datasets is even more limited. To remedy the aforementioned issues, we release PyGraft, a Python-based tool that generates highly customized, domain-agnostic schemas and KGs. The synthesized schemas encompass various RDFS and OWL constructs, while the synthesized KGs emulate the characteristics and scale of real-world KGs. Logical consistency of the generated resources is ultimately ensured by running a description logic (DL) reasoner. By providing a way of generating both a schema and KG in a single pipeline, PyGraft's aim is to empower the generation of a more diverse array of KGs for benchmarking novel approaches in areas such as graph-based machine learning (ML), or more generally KG processing. In graph-based ML in particular, this should foster a more holistic evaluation of model performance and generalization capability, thereby going beyond the limited collection of available benchmarks. PyGraft is available at: https://github.com/nicolas-hbt/pygraft.","[{'name': \"Nicolas Hubert, Pierre Monnin, Mathieu d'Aquin, Davy Monticolo, Armelle Brun\"}]",
1287,On Generating Explanations for Reinforcement Learning Policies: An Empirical Study,https://arxiv.org/abs/2309.16960,"arXiv:2309.16960v2 Announce Type: replace 
Abstract: Understanding a \\textit{reinforcement learning} policy, which guides state-to-action mappings to maximize rewards, necessitates an accompanying explanation for human comprehension. In this paper, we introduce a set of \\textit{linear temporal logic} (LTL) formulae designed to provide explanations for policies, and an algorithm for searching through those formulae for the one that best explains a given policy. Our focus is on crafting explanations that elucidate both the ultimate objectives accomplished by the policy and the prerequisite conditions it upholds throughout its execution. These LTL-based explanations feature a structured representation, which is particularly well-suited for local-search techniques. The effectiveness of our proposed approach is illustrated through a simulated game of capture the flag and a car-parking environment. The paper concludes with suggested directions for future","[{'name': 'Mikihisa Yuasa, Huy T. Tran, Ramavarapu S. Sreenivas'}]",
1288,A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models,https://arxiv.org/abs/2310.00194,"arXiv:2310.00194v3 Announce Type: replace 
Abstract: Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. To address this, we take inspiration from the human brain, in which planning is accomplished via the recurrent interaction of specialized modules in the prefrontal cortex (PFC). These modules perform functions such as conflict monitoring, state prediction, state evaluation, task decomposition, and task coordination. We find that LLMs are sometimes capable of carrying out these functions in isolation, but struggle to autonomously coordinate them in the service of a goal. Therefore, we propose a black box architecture with multiple LLM-based (GPT-4) modules. The architecture improves planning through the interaction of specialized PFC-inspired modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate the combined architecture on three challenging planning tasks -- graph traversal, Tower of Hanoi, and logistics -- finding that it yields significant improvements over standard LLM methods (e.g., zero-shot prompting, in-context learning, and chain-of-thought). These results demonstrate the benefit of utilizing knowledge from cognitive neuroscience to improve planning in LLMs.","[{'name': 'Taylor Webb, Shanka Subhra Mondal, Chi Wang, Brian Krabach, Ida Momennejad'}]",
1289,Self-Supervised Deconfounding Against Spatio-Temporal Shifts: Theory and Modeling,https://arxiv.org/abs/2311.12472,"arXiv:2311.12472v2 Announce Type: replace 
Abstract: As an important application of spatio-temporal (ST) data, ST traffic forecasting plays a crucial role in improving urban travel efficiency and promoting sustainable development. In practice, the dynamics of traffic data frequently undergo distributional shifts attributed to external factors such as time evolution and spatial differences. This entails forecasting models to handle the out-of-distribution (OOD) issue where test data is distributed differently from training data. In this work, we first formalize the problem by constructing a causal graph of past traffic data, future traffic data, and external ST contexts. We reveal that the failure of prior arts in OOD traffic data is due to ST contexts acting as a confounder, i.e., the common cause for past data and future ones. Then, we propose a theoretical solution named Disentangled Contextual Adjustment (DCA) from a causal lens. It differentiates invariant causal correlations against variant spurious ones and deconfounds the effect of ST contexts. On top of that, we devise a Spatio-Temporal sElf-superVised dEconfounding (STEVE) framework. It first encodes traffic data into two disentangled representations for associating invariant and variant ST contexts. Then, we use representative ST contexts from three conceptually different perspectives (i.e., temporal, spatial, and semantic) as self-supervised signals to inject context information into both representations. In this way, we improve the generalization ability of the learned context-oriented representations to OOD ST traffic forecasting. Comprehensive experiments on four large-scale benchmark datasets demonstrate that our STEVE consistently outperforms the state-of-the-art baselines across various ST OOD scenarios.","[{'name': 'Jiahao Ji, Wentao Zhang, Jingyuan Wang, Yue He, Chao Huang'}]",
1290,Explicitly explainable AI solution to the AI black box problem,https://arxiv.org/abs/2401.03093,"arXiv:2401.03093v2 Announce Type: replace 
Abstract: Artificial intelligence based on neural networks has made significant progress. However, there are concerns about the reliability and security of this approach due to its lack of transparency. This is the black box problem of AI. Here we show how this problem can be solved using symbolic AI, which has a transparent white box nature. The widespread use of symbolic AI is hindered by the opacity of mathematical models and natural language terms, the lack of a unified ontology, and the combinatorial explosion of search options. To solve the AI black box problem and to implement general-purpose symbolic AI, we propose to use deterministic logic cellular automata with rules based on first principles of the general theory of the relevant domain. In this case, the general theory of the relevant domain plays the role of a knowledge base for the cellular automaton inference. A cellular automaton implements automatic parallel logical inference at three levels of organization of a complex system. Our verification of several ecological hypotheses provides a successful precedent for the implementation of white-box AI. Finally, we discuss a program for creating a general-purpose symbolic AI capable of processing knowledge and ensuring the reliability and safety of automated decisions.","[{'name': 'V. L. Kalmykov, L. V. Kalmykov'}]",
1291,Integration of cognitive tasks into artificial general intelligence test for large models,https://arxiv.org/abs/2402.02547,"arXiv:2402.02547v2 Announce Type: replace 
Abstract: During the evolution of large models, performance evaluation is necessarily performed to assess their capabilities and ensure safety before practical application. However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models. In this perspective, we advocate for a comprehensive framework of cognitive science-inspired artificial general intelligence (AGI) tests, aimed at fulfilling the testing needs of large models with enhanced capabilities. The cognitive science-inspired AGI tests encompass the full spectrum of intelligence facets, including crystallized intelligence, fluid intelligence, social intelligence, and embodied intelligence. To assess the multidimensional intelligence of large models, the AGI tests consist of a battery of well-designed cognitive tests adopted from human intelligence tests, and then naturally encapsulates into an immersive virtual community. We propose increasing the complexity of AGI testing tasks commensurate with advancements in large models and emphasizing the necessity for the interpretation of test results to avoid false negatives and false positives. We believe that cognitive science-inspired AGI tests will effectively guide the targeted improvement of large models in specific dimensions of intelligence and accelerate the integration of large models into human society.","[{'name': 'Youzhi Qu, Chen Wei, Penghui Du, Wenxin Che, Chi Zhang, Wanli Ouyang, Yatao Bian, Feiyang Xu, Bin Hu, Kai Du, Haiyan Wu, Jia Liu, Quanying Liu'}]",
1292,Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction,https://arxiv.org/abs/2402.04154,"arXiv:2402.04154v3 Announce Type: replace 
Abstract: Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning. However, these works encounter challenges in extending their capabilities to new tasks. Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction. However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks. This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a \"read-to-play\" capability. Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer. Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities.","[{'name': 'Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu Xiang, Shawn Yue, Stephen W. Huang, Wenhu Chen, Zhaofeng He, Jie Fu'}]",
1293,Prompt Learning on Temporal Interaction Graphs,https://arxiv.org/abs/2402.06326,"arXiv:2402.06326v2 Announce Type: replace 
Abstract: Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict'' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios.
  Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straightforward. The application of prompting in static graph contexts falls short in temporal settings due to a lack of consideration for time-sensitive dynamics and a deficiency in expressive power. To address this issue, we introduce Temporal Interaction Graph Prompting (TIGPrompt), a versatile framework that seamlessly integrates with TIG models, bridging both the temporal and semantic gaps. In detail, we propose a temporal prompt generator to offer temporally-aware prompts for different tasks. These prompts stand out for their minimalistic design, relying solely on the tuning of the prompt generator with very little supervision data. To cater to varying computational resource demands, we propose an extended ``pre-train, prompt-based fine-tune'' paradigm, offering greater flexibility. Through extensive experiments, the TIGPrompt demonstrates the SOTA performance and remarkable efficiency advantages.","[{'name': 'Xi Chen, Siwei Zhang, Yun Xiong, Xixi Wu, Jiawei Zhang, Xiangguo Sun, Yao Zhang, Feng Zhao, Yulin Kang'}]",
1294,The Who in XAI: How AI Background Shapes Perceptions of AI Explanations,https://arxiv.org/abs/2107.13509,"arXiv:2107.13509v2 Announce Type: replace-cross 
Abstract: Explainability of AI systems is critical for users to take informed actions. Understanding \"who\" opens the black-box of AI is just as important as opening it. We conduct a mixed-methods study of how two different groups--people with and without AI background--perceive different types of AI explanations. Quantitatively, we share user perceptions along five dimensions. Qualitatively, we describe how AI background can influence interpretations, elucidating the differences through lenses of appropriation and cognitive heuristics. We find that (1) both groups showed unwarranted faith in numbers for different reasons and (2) each group found value in different explanations beyond their intended design. Carrying critical implications for the field of XAI, our findings showcase how AI generated explanations can have negative consequences despite best intentions and how that could lead to harmful manipulation of trust. We propose design interventions to mitigate them.","[{'name': 'Upol Ehsan, Samir Passi, Q. Vera Liao, Larry Chan, I-Hsiang Lee, Michael Muller, Mark O. Riedl'}]",
1295,SemSegDepth: A Combined Model for Semantic Segmentation and Depth Completion,https://arxiv.org/abs/2209.00381,"arXiv:2209.00381v2 Announce Type: replace-cross 
Abstract: Holistic scene understanding is pivotal for the performance of autonomous machines. In this paper we propose a new end-to-end model for performing semantic segmentation and depth completion jointly. The vast majority of recent approaches have developed semantic segmentation and depth completion as independent tasks. Our approach relies on RGB and sparse depth as inputs to our model and produces a dense depth map and the corresponding semantic segmentation image. It consists of a feature extractor, a depth completion branch, a semantic segmentation branch and a joint branch which further processes semantic and depth information altogether. The experiments done on Virtual KITTI 2 dataset, demonstrate and provide further evidence, that combining both tasks, semantic segmentation and depth completion, in a multi-task network can effectively improve the performance of each task. Code is available at https://github.com/juanb09111/semantic depth.","[{'name': 'Juan Pablo Lagos, Esa Rahtu'}]",
1296,Optimal Settings for Cryptocurrency Trading Pairs,https://arxiv.org/abs/2210.10971,"arXiv:2210.10971v3 Announce Type: replace-cross 
Abstract: The goal of cryptocurrencies is decentralization. In principle, all currencies have equal status. Unlike traditional stock markets, there is no default currency of denomination (fiat), thus the trading pairs can be set freely. However, it is impractical to set up a trading market between every two currencies. In order to control management costs and ensure sufficient liquidity, we must give priority to covering those large-volume trading pairs and ensure that all coins are reachable. We note that this is an optimization problem. Its particularity lies in: 1) the trading volume between most (>99.5%) possible trading pairs cannot be directly observed. 2) It satisfies the connectivity constraint, that is, all currencies are guaranteed to be tradable.
  To solve this problem, we use a two-stage process: 1) Fill in missing values based on a regularized, truncated eigenvalue decomposition, where the regularization term is used to control what extent missing values should be limited to zero. 2) Search for the optimal trading pairs, based on a branch and bound process, with heuristic search and pruning strategies.
  The experimental results show that: 1) If the number of denominated coins is not limited, we will get a more decentralized trading pair settings, which advocates the establishment of trading pairs directly between large currency pairs. 2) There is a certain room for optimization in all exchanges. The setting of inappropriate trading pairs is mainly caused by subjectively setting small coins to quote, or failing to track emerging big coins in time. 3) Too few trading pairs will lead to low coverage; too many trading pairs will need to be adjusted with markets frequently. Exchanges should consider striking an appropriate balance between them.","[{'name': 'Di Zhang, Youzhou Zhou'}]",
1297,Seamful XAI: Operationalizing Seamful Design in Explainable AI,https://arxiv.org/abs/2211.06753,"arXiv:2211.06753v2 Announce Type: replace-cross 
Abstract: Mistakes in AI systems are inevitable, arising from both technical limitations and sociotechnical gaps. While black-boxing AI systems can make the user experience seamless, hiding the seams risks disempowering users to mitigate fallouts from AI mistakes. Instead of hiding these AI imperfections, can we leverage them to help the user? While Explainable AI (XAI) has predominantly tackled algorithmic opaqueness, we propose that seamful design can foster AI explainability by revealing and leveraging sociotechnical and infrastructural mismatches. We introduce the concept of Seamful XAI by (1) conceptually transferring \"seams\" to the AI context and (2) developing a design process that helps stakeholders anticipate and design with seams. We explore this process with 43 AI practitioners and real end-users, using a scenario-based co-design activity informed by real-world use cases. We found that the Seamful XAI design process helped users foresee AI harms, identify underlying reasons (seams), locate them in the AI's lifecycle, learn how to leverage seamful information to improve XAI and user agency. We share empirical insights, implications, and reflections on how this process can help practitioners anticipate and craft seams in AI, how seamfulness can improve explainability, empower end-users, and facilitate Responsible AI.","[{'name': 'Upol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, Hal Daume III'}]",
1298,Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction,https://arxiv.org/abs/2212.04475,"arXiv:2212.04475v2 Announce Type: replace-cross 
Abstract: Robust prediction of citywide traffic flows at different time periods plays a crucial role in intelligent transportation systems. While previous work has made great efforts to model spatio-temporal correlations, existing methods still suffer from two key limitations: i) Most models collectively predict all regions' flows without accounting for spatial heterogeneity, i.e., different regions may have skewed traffic flow distributions. ii) These models fail to capture the temporal heterogeneity induced by time-varying traffic patterns, as they typically model temporal correlations with a shared parameterized space for all time periods. To tackle these challenges, we propose a novel Spatio-Temporal Self-Supervised Learning (ST-SSL) traffic prediction framework which enhances the traffic pattern representations to be reflective of both spatial and temporal heterogeneity, with auxiliary self-supervised learning paradigms. Specifically, our ST-SSL is built over an integrated module with temporal and spatial convolutions for encoding the information across space and time. To achieve the adaptive spatio-temporal self-supervised learning, our ST-SSL first performs the adaptive augmentation over the traffic flow graph data at both attribute- and structure-levels. On top of the augmented traffic graph, two SSL auxiliary tasks are constructed to supplement the main traffic prediction task with spatial and temporal heterogeneity-aware augmentation. Experiments on four benchmark datasets demonstrate that ST-SSL consistently outperforms various state-of-the-art baselines. Since spatio-temporal heterogeneity widely exists in practical datasets, the proposed framework may also cast light on other spatial-temporal applications. Model implementation is available at https://github.com/Echo-Ji/ST-SSL.","[{'name': 'Jiahao Ji, Jingyuan Wang, Chao Huang, Junjie Wu, Boren Xu, Zhenhe Wu, Junbo Zhang, Yu Zheng'}]",
1299,THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression,https://arxiv.org/abs/2302.08545,"arXiv:2302.08545v2 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) are the de facto standard for essential use cases, such as image classification, computer vision, and natural language processing. As DNNs and datasets get larger, they require distributed training on increasingly larger clusters. A main bottleneck is the resulting communication overhead where workers exchange model updates (i.e., gradients) on a per-round basis. To address this bottleneck and accelerate training, a widely-deployed approach is compression. However, previous deployments often apply bi-directional compression schemes by simply using a uni-directional gradient compression scheme in each direction. This results in significant computational overheads at the parameter server and increased compression error, leading to longer training and lower accuracy. We introduce Tensor Homomorphic Compression (THC), a novel bi-directional compression framework that enables the direct aggregation of compressed values and thus eliminating the aforementioned computational overheads. Moreover, THC is compatible with in-network aggregation (INA), which allows for further acceleration. Our evaluation shows that training representative vision and language models with THC reaches target accuracy by 1.40x to 1.47x faster using INA and 1.28x to 1.33x faster using a software PS compared with state-of-the-art systems.","[{'name': 'Minghao LiHarvard University, Ran Ben BasatUniversity College London, Shay VargaftikVMware Research, ChonLam LaoHarvard University, Kevin XuHarvard University, Michael MitzenmacherHarvard University, Minlan YuHarvard University'}]",
1300,Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction,https://arxiv.org/abs/2303.00286,"arXiv:2303.00286v4 Announce Type: replace-cross 
Abstract: Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that consider batches of true and false triples. However, different kinds of false triples exist and recent works suggest that they should not be valued equally, leading to specific negative sampling procedures. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. signatures of relations (domain and range) are high-quality negatives. Hence, we enrich the three main loss functions for link prediction such that all kinds of negatives are sampled but treated differently based on their semantic validity. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results which demonstrates both the generality and superiority of our proposed approach. In fact, the proposed loss functions (1) lead to better MRR and Hits@10 values, and (2) drive KGEMs towards better semantic correctness as measured by the Sem@K metric. This highlights that relation signatures globally improve KGEMs, and thus should be incorporated into loss functions. Domains and ranges of relations being largely available in schema-defined KGs, this makes our approach both beneficial and widely usable in practice.","[{'name': 'Nicolas Hubert, Pierre Monnin, Armelle Brun, Davy Monticolo'}]",
1301,CWTM: Leveraging Contextualized Word Embeddings from BERT for Neural Topic Modeling,https://arxiv.org/abs/2305.09329,"arXiv:2305.09329v3 Announce Type: replace-cross 
Abstract: Most existing topic models rely on bag-of-words (BOW) representation, which limits their ability to capture word order information and leads to challenges with out-of-vocabulary (OOV) words in new documents. Contextualized word embeddings, however, show superiority in word sense disambiguation and effectively address the OOV issue. In this work, we introduce a novel neural topic model called the Contextlized Word Topic Model (CWTM), which integrates contextualized word embeddings from BERT. The model is capable of learning the topic vector of a document without BOW information. In addition, it can also derive the topic vectors for individual words within a document based on their contextualized word embeddings. Experiments across various datasets show that CWTM generates more coherent and meaningful topics compared to existing topic models, while also accommodating unseen words in newly encountered documents.","[{'name': 'Zheng Fang, Yulan He, Rob Procter'}]",
1302,Interpretable Stereotype Identification through Reasoning,https://arxiv.org/abs/2308.00071,"arXiv:2308.00071v2 Announce Type: replace-cross 
Abstract: Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.","[{'name': 'Jacob-Junqi Tian, Omkar Dige, David Emerson, Faiza Khan Khattak'}]",
1303,VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs,https://arxiv.org/abs/2308.02117,"arXiv:2308.02117v3 Announce Type: replace-cross 
Abstract: GNN-to-MLP distillation aims to utilize knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (student MLP) on graph data by mimicking the output representations of teacher GNN. Existing methods mainly make the MLP to mimic the GNN predictions over a few class labels. However, the class space may not be expressive enough for covering numerous diverse local graph structures, thus limiting the performance of knowledge transfer from GNN to MLP. To address this issue, we propose to learn a new powerful graph representation space by directly labeling nodes' diverse local structures for GNN-to-MLP distillation. Specifically, we propose a variant of VQ-VAE to learn a structure-aware tokenizer on graph data that can encode each node's local substructure as a discrete code. The discrete codes constitute a codebook as a new graph representation space that is able to identify different local graph structures of nodes with the corresponding code indices. Then, based on the learned codebook, we propose a new distillation target, namely soft code assignments, to directly transfer the structural knowledge of each node from GNN to MLP. The resulting framework VQGraph achieves new state-of-the-art performance on GNN-to-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828x, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average, respectively. Code: https://github.com/YangLing0818/VQGraph.","[{'name': 'Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, Bin Cui, Muhan Zhang, Jure Leskovec'}]",
1304,Beyond MD17: the reactive xxMD dataset,https://arxiv.org/abs/2308.11155,"arXiv:2308.11155v3 Announce Type: replace-cross 
Abstract: System specific neural force fields (NFFs) have gained popularity in computational chemistry. One of the most popular datasets as a bencharmk to develop NFFs models is the MD17 dataset and its subsequent extension. These datasets comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampled from direct adiabatic dynamics. However, many chemical reactions involve significant molecular geometrical deformations, for example, bond breaking. Therefore, MD17 is inadequate to represent a chemical reaction. To address this limitation in MD17, we introduce a new dataset, called Extended Excited-state Molecular Dynamics (xxMD) dataset. The xxMD dataset involves geometries sampled from direct non-adiabatic dynamics, and the energies are computed at both multireference wavefunction theory and density functional theory. We show that the xxMD dataset involves diverse geometries which represent chemical reactions. Assessment of NFF models on xxMD dataset reveals significantly higher predictive errors than those reported for MD17 and its variants. This work underscores the challenges faced in crafting a generalizable NFF model with extrapolation capability.","[{'name': 'Zihan Pengmei, Junyu Liu, Yinan Shu'}]",
1305,Projected Task-Specific Layers for Multi-Task Reinforcement Learning,https://arxiv.org/abs/2309.08776,"arXiv:2309.08776v2 Announce Type: replace-cross 
Abstract: Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm.","[{'name': 'Josselin Somerville Roberts, Julia Di'}]",
1306,Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis,https://arxiv.org/abs/2309.09553,"arXiv:2309.09553v4 Announce Type: replace-cross 
Abstract: The excellent text-to-image synthesis capability of diffusion models has driven progress in synthesizing coherent visual stories. The current state-of-the-art method combines the features of historical captions, historical frames, and the current captions as conditions for generating the current frame. However, this method treats each historical frame and caption as the same contribution. It connects them in order with equal weights, ignoring that not all historical conditions are associated with the generation of the current frame. To address this issue, we propose Causal-Story. This model incorporates a local causal attention mechanism that considers the causal relationship between previous captions, frames, and current captions. By assigning weights based on this relationship, Causal-Story generates the current frame, thereby improving the global consistency of story generation. We evaluated our model on the PororoSV and FlintstonesSV datasets and obtained state-of-the-art FID scores, and the generated frames also demonstrate better storytelling in visuals.","[{'name': 'Tianyi Song, Jiuxin Cao, Kun Wang, Bo Liu, Xiaofeng Zhang'}]",
1307,Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification,https://arxiv.org/abs/2309.13734,"arXiv:2309.13734v2 Announce Type: replace-cross 
Abstract: Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology that can reduce or even eliminate the need for manual annotations. We investigate 10 open-source models and 7 prompting schemes, finding that LLMs are competitive with in-domain supervised models but are not necessarily consistent in their performance. We also fine-tuned the LLMs, but discovered that fine-tuning process does not necessarily lead to better performance. In general, we discover that LLMs do not routinely outperform their smaller supervised machine learning models, and thus call for stance detection to be a benchmark for which LLMs also optimize for. The code used in this study is available at \\url{https://github.com/ijcruic/LLM-Stance-Labeling}","[{'name': 'Iain J. Cruickshank, Lynnette Hui Xian Ng'}]",
1308,Continual Driving Policy Optimization with Closed-Loop Individualized Curricula,https://arxiv.org/abs/2309.14209,"arXiv:2309.14209v3 Announce Type: replace-cross 
Abstract: The safety of autonomous vehicles (AV) has been a long-standing top concern, stemming from the absence of rare and safety-critical scenarios in the long-tail naturalistic driving distribution. To tackle this challenge, a surge of research in scenario-based autonomous driving has emerged, with a focus on generating high-risk driving scenarios and applying them to conduct safety-critical testing of AV models. However, limited work has been explored on the reuse of these extensive scenarios to iteratively improve AV models. Moreover, it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement. Therefore, we develop a continual driving policy optimization framework featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into a set of standardized sub-modules for flexible implementation choices: AV Evaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a collision prediction task, where it estimates the chance of AV failures in these scenarios at each iteration. Subsequently, by re-sampling from historical scenarios based on these failure probabilities, CLIC tailors individualized curricula for downstream training, aligning them with the evaluated capability of AV. Accordingly, CLIC not only maximizes the utilization of the vast pre-collected scenario library for closed-loop driving policy optimization but also facilitates AV improvement by individualizing its training with more challenging cases out of those poorly organized scenarios. Experimental results clearly indicate that CLIC surpasses other curriculum-based training strategies, showing substantial improvement in managing risky scenarios, while still maintaining proficiency in handling simpler cases.","[{'name': 'Haoyi Niu, Yizhou Xu, Xingjian Jiang, Jianming Hu'}]",
1309,AI-Dentify: Deep learning for proximal caries detection on bitewing x-ray -- HUNT4 Oral Health Study,https://arxiv.org/abs/2310.00354,"arXiv:2310.00354v2 Announce Type: replace-cross 
Abstract: Background: Dental caries diagnosis requires the manual inspection of diagnostic bitewing images of the patient, followed by a visual inspection and probing of the identified dental pieces with potential lesions. Yet the use of artificial intelligence, and in particular deep-learning, has the potential to aid in the diagnosis by providing a quick and informative analysis of the bitewing images.
  Methods: A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were annotated individually by six different experts, and used to train three different object detection deep-learning architectures: RetinaNet (ResNet50), YOLOv5 (M size), and EfficientDet (D0 and D1 sizes). A consensus dataset of 197 images, annotated jointly by the same six dentist, was used for evaluation. A five-fold cross validation scheme was used to evaluate the performance of the AI models.
  Results: he trained models show an increase in average precision and F1-score, and decrease of false negative rate, with respect to the dental clinicians. When compared against the dental clinicians, the YOLOv5 model shows the largest improvement, reporting 0.647 mean average precision, 0.548 mean F1-score, and 0.149 mean false negative rate. Whereas the best annotators on each of these metrics reported 0.299, 0.495, and 0.164 respectively.
  Conclusion: Deep-learning models have shown the potential to assist dental professionals in the diagnosis of caries. Yet, the task remains challenging due to the artifacts natural to the bitewing images.","[{'name': \"Javier P\\\\'erez de Frutos, Ragnhild Holden Helland, Shreya Desai, Line Cathrine Nymoen, Thomas Lang{\\\\o}, Theodor Remman, Abhijit Sen\"}]",
1310,Improving Adversarial Attacks on Latent Diffusion Model,https://arxiv.org/abs/2310.04687,"arXiv:2310.04687v3 Announce Type: replace-cross 
Abstract: Adversarial attacks on Latent Diffusion Model (LDM), the state-of-the-art image generative model, have been adopted as effective protection against malicious finetuning of LDM on unauthorized images. We show that these attacks add an extra error to the score function of adversarial examples predicted by LDM. LDM finetuned on these adversarial examples learns to lower the error by a bias, from which the model is attacked and predicts the score function with biases.
  Based on the dynamics, we propose to improve the adversarial attack on LDM by Attacking with Consistent score-function Errors (ACE). ACE unifies the pattern of the extra error added to the predicted score function. This induces the finetuned LDM to learn the same pattern as a bias in predicting the score function. We then introduce a well-crafted pattern to improve the attack. Our method outperforms state-of-the-art methods in adversarial attacks on LDM.","[{'name': 'Boyang Zheng, Chumeng Liang, Xiaoyu Wu, Yan Liu'}]",
1311,Scalable Neural Network Kernels,https://arxiv.org/abs/2310.13225,"arXiv:2310.13225v2 Announce Type: replace-cross 
Abstract: We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universal random features (or URFs), applied to instantiate several SNNK variants, and interesting on its own in the context of scalable kernel methods. We provide rigorous theoretical analysis of all these concepts as well as an extensive empirical evaluation, ranging from point-wise kernel estimation to Transformers' fine-tuning with novel adapter layers inspired by SNNKs. Our mechanism provides up to 5x reduction in the number of trainable parameters, while maintaining competitive accuracy.","[{'name': 'Arijit Sehanobish, Krzysztof Choromanski, Yunfan Zhao, Avinava Dubey, Valerii Likhosherstov'}]",
1312,Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias,https://arxiv.org/abs/2310.14814,"arXiv:2310.14814v3 Announce Type: replace-cross 
Abstract: Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, although they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on classification datasets of various data modalities. The code is available at https://github.com/ambroiseodt/tsim.","[{'name': 'Ambroise Odonnat, Vasilii Feofanov, Ievgen Redko'}]",
1313,From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach,https://arxiv.org/abs/2311.03260,"arXiv:2311.03260v2 Announce Type: replace-cross 
Abstract: We propose the Kuramoto Graph Neural Network (KuramotoGNN), a novel class of continuous-depth graph neural networks (GNNs) that employs the Kuramoto model to mitigate the over-smoothing phenomenon, in which node features in GNNs become indistinguishable as the number of layers increases. The Kuramoto model captures the synchronization behavior of non-linear coupled oscillators. Under the view of coupled oscillators, we first show the connection between Kuramoto model and basic GNN and then over-smoothing phenomenon in GNNs can be interpreted as phase synchronization in Kuramoto model. The KuramotoGNN replaces this phase synchronization with frequency synchronization to prevent the node features from converging into each other while allowing the system to reach a stable synchronized state. We experimentally verify the advantages of the KuramotoGNN over the baseline GNNs and existing methods in reducing over-smoothing on various graph deep learning benchmark tasks.","[{'name': 'Tuan Nguyen, Hirotada Honda, Takashi Sano, Vinh Nguyen, Shugo Nakamura, Tan M. Nguyen'}]",
1314,Fair Text-to-Image Diffusion via Fair Mapping,https://arxiv.org/abs/2311.17695,"arXiv:2311.17695v2 Announce Type: replace-cross 
Abstract: In this paper, we address the limitations of existing text-to-image diffusion models in generating demographically fair results when given human-related descriptions. These models often struggle to disentangle the target language context from sociocultural biases, resulting in biased image generation. To overcome this challenge, we propose Fair Mapping, a flexible, model-agnostic, and lightweight approach that modifies a pre-trained text-to-image diffusion model by controlling the prompt to achieve fair image generation. One key advantage of our approach is its high efficiency. It only requires updating an additional linear network with few parameters at a low computational cost. By developing a linear network that maps conditioning embeddings into a debiased space, we enable the generation of relatively balanced demographic results based on the specified text condition. With comprehensive experiments on face image generation, we show that our method significantly improves image generation fairness with almost the same image quality compared to conventional diffusion models when prompted with descriptions related to humans. By effectively addressing the issue of implicit language bias, our method produces more fair and diverse image outputs.","[{'name': 'Jia Li, Lijie Hu, Jingfeng Zhang, Tianhang Zheng, Hua Zhang, Di Wang'}]",
1315,Large-scale Training of Foundation Models for Wearable Biosignals,https://arxiv.org/abs/2312.05409,"arXiv:2312.05409v2 Announce Type: replace-cross 
Abstract: Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch. We curated PPG and ECG datasets from AHMS that include data from ~141K participants spanning ~3 years. Our self-supervised learning framework includes participant level positive pair selection, stochastic augmentation module and a regularized contrastive loss optimized with momentum training, and generalizes well to both PPG and ECG modalities. We show that the pre-trained foundation models readily encode information regarding participants' demographics and health conditions. To the best of our knowledge, this is the first study that builds foundation models using large-scale PPG and ECG data collected via wearable consumer devices $\\unicode{x2013}$ prior works have commonly used smaller-size datasets collected in clinical and experimental settings. We believe PPG and ECG foundation models can enhance future wearable devices by reducing the reliance on labeled data and hold the potential to help the users improve their health.","[{'name': 'Salar Abbaspourazad, Oussama Elachqar, Andrew C. Miller, Saba Emrani, Udhyakumar Nallasamy, Ian Shapiro'}]",
1316,A Novel Image Classification Framework Based on Variational Quantum Algorithms,https://arxiv.org/abs/2312.07932,"arXiv:2312.07932v2 Announce Type: replace-cross 
Abstract: Image classification is a crucial task in machine learning with widespread practical applications. The existing classical framework for image classification typically utilizes a global pooling operation at the end of the network to reduce computational complexity and mitigate overfitting. However, this operation often results in a significant loss of information, which can affect the performance of classification models. To overcome this limitation, we introduce a novel image classification framework that leverages variational quantum algorithms (VQAs)-hybrid approaches combining quantum and classical computing paradigms within quantum machine learning. The major advantage of our framework is the elimination of the need for the global pooling operation at the end of the network. In this way, our approach preserves more discriminative features and fine-grained details in the images, which enhances classification performance. Additionally, employing VQAs enables our framework to have fewer parameters than the classical framework, even in the absence of global pooling, which makes it more advantageous in preventing overfitting. We apply our method to different state-of-the-art image classification models and demonstrate the superiority of the proposed quantum architecture over its classical counterpart through a series of experiments on public datasets. Our experiments show that the proposed quantum framework achieves up to a 9.21% increase in accuracy and up to a 15.79% improvement in F1 score, compared to the classical framework.",[{'name': 'Yixiong Chen'}],
1317,Parameterized Projected Bellman Operator,https://arxiv.org/abs/2312.12869,"arXiv:2312.12869v3 Announce Type: replace-cross 
Abstract: Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projected Bellman operator (PBO). We formulate an optimization problem to learn PBO for generic sequential decision-making problems, and we theoretically analyze its properties in two representative classes of RL problems. Furthermore, we theoretically study our approach under the lens of AVI and devise algorithmic implementations to learn PBO in offline and online settings by leveraging neural network parameterizations. Finally, we empirically showcase the benefits of PBO w.r.t. the regular Bellman operator on several RL problems.","[{'name': \"Th\\\\'eo Vincent, Alberto Maria Metelli, Boris Belousov, Jan Peters, Marcello Restelli, Carlo D'Eramo\"}]",
1318,Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models,https://arxiv.org/abs/2312.14197,"arXiv:2312.14197v2 Announce Type: replace-cross 
Abstract: The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box methods based on prompt learning and a white-box defense method based on fine-tuning with adversarial training accordingly. Experimental results demonstrate that black-box defenses are highly effective in mitigating these attacks, while the white-box defense reduces the attack success rate to near-zero levels. Overall, our work systematically investigates indirect prompt injection attacks by introducing a benchmark, analyzing the underlying reason for the success of the attack, and developing an initial set of defenses.","[{'name': 'Jingwei Yi, Yueqi Xie, Bin Zhu, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu'}]",
1319,Understanding Distributed Representations of Concepts in Deep Neural Networks without Supervision,https://arxiv.org/abs/2312.17285,"arXiv:2312.17285v2 Announce Type: replace-cross 
Abstract: Understanding intermediate representations of the concepts learned by deep learning classifiers is indispensable for interpreting general model behaviors. Existing approaches to reveal learned concepts often rely on human supervision, such as pre-defined concept sets or segmentation processes. In this paper, we propose a novel unsupervised method for discovering distributed representations of concepts by selecting a principal subset of neurons. Our empirical findings demonstrate that instances with similar neuron activation states tend to share coherent concepts. Based on the observations, the proposed method selects principal neurons that construct an interpretable region, namely a Relaxed Decision Region (RDR), encompassing instances with coherent concepts in the feature space. It can be utilized to identify unlabeled subclasses within data and to detect the causes of misclassifications. Furthermore, the applicability of our method across various layers discloses distinct distributed representations over the layers, which provides deeper insights into the internal mechanisms of the deep learning model.","[{'name': 'Wonjoon Chang, Dahee Kwon, Jaesik Choi'}]",
1320,Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding,https://arxiv.org/abs/2401.04575,"arXiv:2401.04575v2 Announce Type: replace-cross 
Abstract: Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgrounds. Our experiments on LGS show that the classifiers trained on existing benchmark datasets do not readily generalize to e-commerce data, while specific self-supervised visual feature extractors can better generalize. Furthermore, LGS's high-quality e-commerce-focused images and bimodal nature make it advantageous for vision-language bi-modal tasks: LGS enables image-captioning models to generate richer captions and helps text-to-image generation models achieve e-commerce style transfer.","[{'name': 'Yatong Bai, Utsav Garg, Apaar Shanker, Haoming Zhang, Samyak Parajuli, Erhan Bas, Isidora Filipovic, Amelia N. Chu, Eugenia D Fomitcheva, Elliot Branson, Aerin Kim, Somayeh Sojoudi, Kyunghyun Cho'}]",
1321,DevEval: Evaluating Code Generation in Practical Software Projects,https://arxiv.org/abs/2401.06401,"arXiv:2401.06401v4 Announce Type: replace-cross 
Abstract: How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experiments. We also discuss the challenges and future directions of code generation in practical projects. We open-source DevEval and hope it can facilitate the development of code generation in practical projects.","[{'name': 'Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Zhi Jin, Hao Zhu, Huanyu Liu, Kaibo Liu, Lecheng Wang, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming Zhang, Yihong Dong, Yuqi Zhu, Bin Gu, Mengfei Yang'}]",
1322,SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding,https://arxiv.org/abs/2401.09340,"arXiv:2401.09340v2 Announce Type: replace-cross 
Abstract: 3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-language pairs derived from both human annotations and our scalable scene-graph-based generation approach. We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pre-training for Scenes (GPS), for 3D vision-language learning. Through extensive experiments, we showcase the effectiveness of GPS by achieving state-of-the-art performance on all existing 3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is unveiled through zero-shot transfer experiments in the challenging 3D vision-language tasks. Project website: https://scene-verse.github.io.","[{'name': 'Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, Siyuan Huang'}]",
1323,Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge,https://arxiv.org/abs/2401.10712,"arXiv:2401.10712v2 Announce Type: replace-cross 
Abstract: With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question generation model. Then, we use an image tagging model to identify various instances and send packaged image-tag pairs into the visual question generation model to generate relevant questions with the extracted image tags as answers. Finally, we encode these generated question-answer pairs as prompts with a visual-aware prompting module and send them into pre-trained multi-modal large language models to reason out the final answers. Experimental results show that, compared with state-of-the-art methods, our Q&amp;A Prompts achieves substantial improvements on the challenging visual question answering datasets requiring reasoning over diverse world knowledge, such as OK-VQA and A-OKVQA.","[{'name': 'Haibi Wang, Weifeng Ge'}]",
1324,MedLM: Exploring Language Models for Medical Question Answering Systems,https://arxiv.org/abs/2401.11389,"arXiv:2401.11389v2 Announce Type: replace-cross 
Abstract: In the face of rapidly expanding online medical literature, automated systems for aggregating and summarizing information are becoming increasingly crucial for healthcare professionals and patients. Large Language Models (LLMs), with their advanced generative capabilities, have shown promise in various NLP tasks, and their potential in the healthcare domain, particularly for Closed-Book Generative QnA, is significant. However, the performance of these models in domain-specific tasks such as medical Q&amp;A remains largely unexplored. This study aims to fill this gap by comparing the performance of general and medical-specific distilled LMs for medical Q&amp;A. We aim to evaluate the effectiveness of fine-tuning domain-specific LMs and compare the performance of different families of Language Models. The study will address critical questions about these models' reliability, comparative performance, and effectiveness in the context of medical Q&amp;A. The findings will provide valuable insights into the suitability of different LMs for specific applications in the medical domain.","[{'name': 'Niraj Yagnik, Jay Jhaveri, Vivek Sharma, Gabriel Pila'}]",
1325,SelectLLM: Can LLMs Select Important Instructions to Annotate?,https://arxiv.org/abs/2401.16553,"arXiv:2401.16553v4 Announce Type: replace-cross 
Abstract: Instruction tuning benefits from large and diverse datasets, however creating such datasets involves a high cost of human labeling. While synthetic datasets generated by large language models (LLMs) have partly solved this issue, they often contain low-quality data. One effective solution is selectively annotating unlabelled instructions, especially given the relative ease of acquiring unlabeled instructions or texts from various sources. However, how to select unlabelled instructions is not well-explored, especially in the context of LLMs. Further, traditional data selection methods, relying on input embedding space density, tend to underestimate instruction sample complexity, whereas those based on model prediction uncertainty often struggle with synthetic label quality. Therefore, we introduce SelectLLM, an alternative framework that leverages the capabilities of LLMs to more effectively select unlabeled instructions. SelectLLM consists of two key steps: Coreset-based clustering of unlabelled instructions for diversity and then prompting a LLM to identify the most beneficial instructions within each cluster. Our experiments demonstrate that SelectLLM matches or outperforms other state-of-the-art methods in instruction tuning benchmarks. It exhibits remarkable consistency across human and synthetic datasets, along with better cross-dataset generalization, as evidenced by a 10% performance improvement on the Cleaned Alpaca test set when trained on Dolly data. All code and data are publicly available (https://github.com/minnesotanlp/select-llm).","[{'name': 'Ritik Sachin Parkar, Jaehyung Kim, Jong Inn Park, Dongyeop Kang'}]",
1326,Error Estimation for Physics-informed Neural Networks Approximating Semilinear Wave Equations,https://arxiv.org/abs/2402.07153,"arXiv:2402.07153v2 Announce Type: replace-cross 
Abstract: This paper provides rigorous error bounds for physics-informed neural networks approximating the semilinear wave equation. We provide bounds for the generalization and training error in terms of the width of the network's layers and the number of training points for a tanh neural network with two hidden layers. Our main result is a bound of the total error in the $H^1([0,T];L^2(\\Omega))$-norm in terms of the training error and the number of training points, which can be made arbitrarily small under some assumptions. We illustrate our theoretical bounds with numerical experiments.","[{'name': 'Beatrice Lorenz, Aras Bacho, Gitta Kutyniok'}]",
1327,Average-Case Analysis of Iterative Voting,https://arxiv.org/abs/2402.08144,"arXiv:2402.08144v2 Announce Type: replace-cross 
Abstract: Iterative voting is a natural model of repeated strategic decision-making in social choice when agents have the opportunity to update their votes prior to finalizing the group decision. Prior work has analyzed the efficacy of iterative plurality on the welfare of the chosen outcome at equilibrium, relative to the truthful vote profile, via an adaptation of the price of anarchy. However, prior analyses have only studied the worst- and average-case performances when agents' preferences are distributed by the impartial culture. This work extends average-case analysis to a wider class of distributions and distinguishes when iterative plurality improves or degrades asymptotic welfare.","[{'name': 'Joshua Kavner, Lirong Xia'}]",
1328,Brant-2: Foundation Model for Brain Signals,https://arxiv.org/abs/2402.10251,"arXiv:2402.10251v3 Announce Type: replace-cross 
Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintain performance in scenarios with scarce labels. The source code and pre-trained weights are available at: https://github.com/yzz673/Brant-2.","[{'name': 'Zhizhang Yuan, Daoze Zhang, Junru Chen, Gefei Gu, Yang Yang'}]",
1329,Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities,https://arxiv.org/abs/2402.16073,"arXiv:2402.16073v2 Announce Type: replace-cross 
Abstract: In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.","[{'name': 'Binyam Gebre, Karoliina Ranta, Stef van den Elzen, Ernst Kuiper, Thijs Baars, Tom Heskes'}]",
1330,A priori Estimates for Deep Residual Network in Continuous-time Reinforcement Learning,https://arxiv.org/abs/2402.16899,"arXiv:2402.16899v2 Announce Type: replace-cross 
Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \\emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \\emph{a priori} generalization error without the curse of dimensionality.","[{'name': 'Shuyu Yin, Qixuan Zhou, Fei Wen, Tao Luo'}]",
1331,On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction,https://arxiv.org/abs/2402.18061,"arXiv:2402.18061v2 Announce Type: replace-cross 
Abstract: The superior performance of supervised classification methods in the information extraction (IE) area heavily relies on a large amount of gold standard data. Recent zero-shot classification methods converted the task to other NLP tasks (e.g., textual entailment) and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks. However, there is no further investigation into the use of these data. In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) Finetuning the off-the-shelf model using clean data; (4) Inference on the test data. The experimental results show that Clean-LaVe can outperform the baseline by 5% and 6% on TACRED and Wiki80 dataset in the zero-shot relation classification task, and by 3%-7% on Smile (Korean and Polish) in the zero-shot cross-lingual relation classification task, and by 8% on ACE05-E+ in the zero-shot event argument classification task. The code is share in https://github.com/wjw136/Clean_LaVe.git.","[{'name': 'Jianwei Wang, Tianyin Wang, Ziqian Zeng'}]",
1332,Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards,https://arxiv.org/abs/2402.18571,"arXiv:2402.18571v3 Announce Type: replace-cross 
Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO).","[{'name': 'Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang'}]",
1333,Learning with Language-Guided State Abstractions,https://arxiv.org/abs/2402.18759,"arXiv:2402.18759v2 Announce Type: replace-cross 
Abstract: We describe a framework for using natural language to design state abstractions for imitation learning. Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones. These state representations are typically manually specified, or derived from other labor-intensive labeling procedures. Our method, LGA (language-guided abstraction), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks. In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demonstrations and LGA-generated abstract states. Experiments on simulated robotic tasks show that LGA yields state abstractions similar to those designed by humans, but in a fraction of the time, and that these abstractions improve generalization and robustness in the presence of spurious correlations and ambiguous specifications. We illustrate the utility of the learned abstractions on mobile manipulation tasks with a Spot robot.","[{'name': 'Andi Peng, Ilia Sucholutsky, Belinda Z. Li, Theodore R. Sumers, Thomas L. Griffiths, Jacob Andreas, Julie A. Shah'}]",
1334,Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy,https://arxiv.org/abs/2402.19379,"arXiv:2402.19379v2 Announce Type: replace-cross 
Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our preregistered main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is not statistically different from the human crowd. In exploratory analyses, we find that these two approaches are equivalent with respect to medium-effect-size equivalence bounds. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even split of positive and negative resolutions. Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2) can be improved by drawing on human cognitive output. We find that both models' forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between 17% and 28%: though this leads to less accurate predictions than simply averaging human and machine forecasts. Our results suggest that LLMs can achieve forecasting accuracy rivaling that of human crowd forecasting tournaments: via the simple, practically applicable method of forecast aggregation. This replicates the 'wisdom of the crowd' effect for LLMs, and opens up their use for a variety of applications throughout society.","[{'name': 'Philipp Schoenegger, Indre Tuminauskaite, Peter S. Park, Philip E. Tetlock'}]",
1335,AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation,https://arxiv.org/abs/2403.01818,"arXiv:2403.01818v2 Announce Type: replace-cross 
Abstract: Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate the burden of time-consuming pixel-level manual labeling, which leverages limited labeled data along with larger amounts of unlabeled data. Current state-of-the-art methods train the labeled data with ground truths and unlabeled data with pseudo labels. However, the two training flows are separate, which allows labeled data to dominate the training process, resulting in low-quality pseudo labels and, consequently, sub-optimal results. To alleviate this issue, we present AllSpark, which reborns the labeled features from unlabeled ones with the channel-wise cross-attention mechanism. We further introduce a Semantic Memory along with a Channel Semantic Grouping strategy to ensure that unlabeled features adequately represent labeled features. The AllSpark shed new light on the architecture level designs of SSSS rather than framework level, which avoids increasingly complicated training pipeline designs. It can also be regarded as a flexible bottleneck module that can be seamlessly integrated into a general transformer-based segmentation model. The proposed AllSpark outperforms existing methods across all evaluation protocols on Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and model weights are available at: https://github.com/xmed-lab/AllSpark.","[{'name': 'Haonan Wang, Qixiang Zhang, Yi Li, Xiaomeng Li'}]",
1336,Towards Implicit Prompt For Text-To-Image Models,https://arxiv.org/abs/2403.02118,"arXiv:2403.02118v2 Announce Type: replace-cross 
Abstract: Recent text-to-image (T2I) models have had great success, and many benchmarks have been proposed to evaluate their performance and safety. However, they only consider explicit prompts while neglecting implicit prompts (hint at a target without explicitly mentioning it). These prompts may get rid of safety constraints and pose potential threats to the applications of these models. This position paper highlights the current state of T2I models toward implicit prompts. We present a benchmark named ImplicitBench and conduct an investigation on the performance and impacts of implicit prompts with popular T2I models. Specifically, we design and collect more than 2,000 implicit prompts of three aspects: General Symbols, Celebrity Privacy, and Not-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models' capabilities under these implicit prompts. Experiment results show that (1) T2I models are able to accurately create various target symbols indicated by implicit prompts; (2) Implicit prompts bring potential risks of privacy leakage for T2I models. (3) Constraints of NSFW in most of the evaluated T2I models can be bypassed with implicit prompts. We call for increased attention to the potential and risks of implicit prompts in the T2I community and further investigation into the capabilities and impacts of implicit prompts, advocating for a balanced approach that harnesses their benefits while mitigating their risks.","[{'name': 'Yue Yang, Yuqi lin, Hong Liu, Wenqi Shao, Runjian Chen, Hailong Shang, Yu Wang, Yu Qiao, Kaipeng Zhang, Ping Luo'}]",
1337,Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution,https://arxiv.org/abs/2403.02131,"arXiv:2403.02131v2 Announce Type: replace-cross 
Abstract: Evolutionary algorithms, such as Differential Evolution, excel in solving real-parameter optimization challenges. However, the effectiveness of a single algorithm varies across different problem instances, necessitating considerable efforts in algorithm selection or configuration. This paper aims to address the limitation by leveraging the complementary strengths of a group of algorithms and dynamically scheduling them throughout the optimization progress for specific problems. We propose a deep reinforcement learning-based dynamic algorithm selection framework to accomplish this task. Our approach models the dynamic algorithm selection a Markov Decision Process, training an agent in a policy gradient manner to select the most suitable algorithm according to the features observed during the optimization process. To empower the agent with the necessary information, our framework incorporates a thoughtful design of landscape and algorithmic features. Meanwhile, we employ a sophisticated deep neural network model to infer the optimal action, ensuring informed algorithm selections. Additionally, an algorithm context restoration mechanism is embedded to facilitate smooth switching among different algorithms. These mechanisms together enable our framework to seamlessly select and switch algorithms in a dynamic online fashion. Notably, the proposed framework is simple and generic, offering potential improvements across a broad spectrum of evolutionary algorithms. As a proof-of-principle study, we apply this framework to a group of Differential Evolution algorithms. The experimental results showcase the remarkable effectiveness of the proposed framework, not only enhancing the overall optimization performance but also demonstrating favorable generalization ability across different problem classes.","[{'name': 'Hongshu Guo, Yining Ma, Zeyuan Ma, Jiacheng Chen, Xinglin Zhang, Zhiguang Cao, Jun Zhang, Yue-Jiao Gong'}]",
1338,NeuroVoz: a Castillian Spanish corpus of parkinsonian speech,https://arxiv.org/abs/2403.02371,"arXiv:2403.02371v2 Announce Type: replace-cross 
Abstract: The advancement of Parkinson's Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research.
  In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state. This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues. The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, and the first in Castillian Spanish.
  NeuroVoz is composed by 2,903 audio recordings averaging $26.88 \\pm 3.35$ recordings per participant, offering a substantial resource for the scientific exploration of PD's impact on speech. This dataset has already underpinned several studies, achieving a benchmark accuracy of 89% in PD speech pattern identification, indicating marked speech alterations attributable to PD. Despite these advances, the broader challenge of conducting a language-agnostic, cross-corpora analysis of Parkinsonian speech patterns remains an open area for future research. This contribution not only fills a critical void in PD speech analysis resources but also sets a new standard for the global research community in leveraging speech as a diagnostic tool for neurodegenerative diseases.","[{'name': \"Jana\\\\'ina Mendes-Laureano, Jorge A. G\\\\'omez-Garc\\\\'ia, Alejandro Guerrero-L\\\\'opez, Elisa Luque-Buzo, Juli\\\\'an D. Arias-Londo\\\\~no, Francisco J. Grandas-P\\\\'erez, Juan I. Godino-Llorente\"}]",
1339,ImgTrojan: Jailbreaking Vision-Language Models with ONE Image,https://arxiv.org/abs/2403.02910,"arXiv:2403.02910v2 Announce Type: replace-cross 
Abstract: There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.","[{'name': 'Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong'}]",
1340,Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation,https://arxiv.org/abs/2403.02951,"arXiv:2403.02951v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions. To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer valuable insights for enhancing the development of LLM-based Text-to-SQL systems.","[{'name': 'Bin Zhang, Yuxiao Ye, Guoqing Du, Xiaoru Hu, Zhishuai Li, Sun Yang, Chi Harold Liu, Rui Zhao, Ziyue Li, Hangyu Mao'}]",
1341,Large language models surpass human experts in predicting neuroscience results,https://arxiv.org/abs/2403.03230,"arXiv:2403.03230v1 Announce Type: new 
Abstract: Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.","[{'name': \"Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe Y\\\\'a\\\\~nez, Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina Borghesani, Anton Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro Salatiello, Ilia Sucholutsky, Pasquale Minervini, Sepehr Razavi, Roberta Rocca, Elkhan Yusifov, Tereza Okalova, Nianlong Gu, Martin Ferianc, Mikail Khona, Kaustubh R. Patil, Pui-Shee Lee, Rui Mata, Nicholas E. Myers, Jennifer K Bizley, Sebastian Musslick, Isil Poyraz Bilgin, Guiomar Niso, Justin M. Ales, Michael Gaebler, N Apurva Ratan Murty, Chloe M. Hall, Jessica Dafflon, Sherry Dongqi Bao, Bradley C. Love\"}]",
1342,Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy,https://arxiv.org/abs/2403.03288,"arXiv:2403.03288v1 Announce Type: new 
Abstract: In the rapidly evolving field of Large Language Models (LLMs), there is a critical need to thoroughly analyze their capabilities and risks. Central to our investigation are two novel elements. Firstly, it is the innovative parallels between the statistical patterns of word relationships within LLMs and Martin Heidegger's concepts of \"ready-to-hand\" and \"present-at-hand,\" which encapsulate the utilitarian and scientific altitudes humans employ in interacting with the world. This comparison lays the groundwork for positioning LLMs as the digital counterpart to the Faculty of Verbal Knowledge, shedding light on their capacity to emulate certain facets of human reasoning. Secondly, a structural analysis of human reasoning, viewed through Heidegger's notion of truth as \"unconcealment\" is conducted This foundational principle enables us to map out the inputs and outputs of the reasoning system and divide reasoning into four distinct categories. Respective cognitive faculties are delineated, allowing us to place LLMs within the broader schema of human reasoning, thus clarifying their strengths and inherent limitations. Our findings reveal that while LLMs possess the capability for Direct Explicative Reasoning and Pseudo Rational Reasoning, they fall short in authentic rational reasoning and have no creative reasoning capabilities, due to the current lack of many analogous AI models such as the Faculty of Judgement. The potential and risks of LLMs when they are augmented with other AI technologies are also evaluated. The results indicate that although LLMs have achieved proficiency in some reasoning abilities, the aspiration to match or exceed human intellectual capabilities is yet unattained. This research not only enriches our comprehension of LLMs but also propels forward the discourse on AI's potential and its bounds, paving the way for future explorations into AI's evolving landscape.",[{'name': 'Jianqiiu Zhang'}],
1343,AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis,https://arxiv.org/abs/2403.03293,"arXiv:2403.03293v1 Announce Type: new 
Abstract: This paper discusses the effectiveness of leveraging Chatbot: Generative Pre-trained Transformer (ChatGPT) versions 3.5 and 4 for analyzing research papers for effective writing of scientific literature surveys. The study selected the \\textit{Application of Artificial Intelligence in Breast Cancer Treatment} as the research topic. Research papers related to this topic were collected from three major publication databases Google Scholar, Pubmed, and Scopus. ChatGPT models were used to identify the category, scope, and relevant information from the research papers for automatic identification of relevant papers related to Breast Cancer Treatment (BCT), organization of papers according to scope, and identification of key information for survey paper writing. Evaluations performed using ground truth data annotated using subject experts reveal, that GPT-4 achieves 77.3\\% accuracy in identifying the research paper categories and 50\\% of the papers were correctly identified by GPT-4 for their scopes. Further, the results demonstrate that GPT-4 can generate reasons for its decisions with an average of 27\\% new words, and 67\\% of the reasons given by the model were completely agreeable to the subject experts.","[{'name': 'Anjalee De Silva, Janaka L. Wijekoon, Rashini Liyanarachchi, Rrubaa Panchendrarajan, Weranga Rajapaksha'}]",
1344,"The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism, AI, and Health in Africa",https://arxiv.org/abs/2403.03357,"arXiv:2403.03357v1 Announce Type: new 
Abstract: With growing application of machine learning (ML) technologies in healthcare, there have been calls for developing techniques to understand and mitigate biases these systems may exhibit. Fair-ness considerations in the development of ML-based solutions for health have particular implications for Africa, which already faces inequitable power imbalances between the Global North and South.This paper seeks to explore fairness for global health, with Africa as a case study. We conduct a scoping review to propose axes of disparities for fairness consideration in the African context and delineate where they may come into play in different ML-enabled medical modalities. We then conduct qualitative research studies with 672 general population study participants and 28 experts inML, health, and policy focused on Africa to obtain corroborative evidence on the proposed axes of disparities. Our analysis focuses on colonialism as the attribute of interest and examines the interplay between artificial intelligence (AI), health, and colonialism. Among the pre-identified attributes, we found that colonial history, country of origin, and national income level were specific axes of disparities that participants believed would cause an AI system to be biased.However, there was also divergence of opinion between experts and general population participants. Whereas experts generally expressed a shared view about the relevance of colonial history for the development and implementation of AI technologies in Africa, the majority of the general population participants surveyed did not think there was a direct link between AI and colonialism. Based on these findings, we provide practical recommendations for developing fairness-aware ML solutions for health in Africa.","[{'name': 'Mercy Asiedu, Awa Dieng, Alexander Haykel, Negar Rostamzadeh, Stephen Pfohl, Chirag Nagpal, Maria Nagawa, Abigail Oppong, Sanmi Koyejo, Katherine Heller'}]",
1345,RACE-SM: Reinforcement Learning Based Autonomous Control for Social On-Ramp Merging,https://arxiv.org/abs/2403.03359,"arXiv:2403.03359v1 Announce Type: new 
Abstract: Autonomous parallel-style on-ramp merging in human controlled traffic continues to be an existing issue for autonomous vehicle control. Existing non-learning based solutions for vehicle control rely on rules and optimization primarily. These methods have been seen to present significant challenges. Recent advancements in Deep Reinforcement Learning have shown promise and have received significant academic interest however the available learning based approaches show inadequate attention to other highway vehicles and often rely on inaccurate road traffic assumptions. In addition, the parallel-style case is rarely considered. A novel learning based model for acceleration and lane change decision making that explicitly considers the utility to both the ego vehicle and its surrounding vehicles which may be cooperative or uncooperative to produce behaviour that is socially acceptable is proposed. The novel reward function makes use of Social Value Orientation to weight the vehicle's level of social cooperation and is divided into ego vehicle and surrounding vehicle utility which are weighted according to the model's designated Social Value Orientation. A two-lane highway with an on-ramp divided into a taper-style and parallel-style section is considered. Simulation results indicated the importance of considering surrounding vehicles in reward function design and show that the proposed model matches or surpasses those in literature in terms of collisions while also introducing socially courteous behaviour avoiding near misses and anti-social behaviour through direct consideration of the effect of merging on surrounding vehicles.",[{'name': 'Jordan Poots'}],
1346,Adaptive Discovering and Merging for Incremental Novel Class Discovery,https://arxiv.org/abs/2403.03382,"arXiv:2403.03382v1 Announce Type: new 
Abstract: One important desideratum of lifelong learning aims to discover novel classes from unlabelled data in a continuous manner. The central challenge is twofold: discovering and learning novel classes while mitigating the issue of catastrophic forgetting of established knowledge. To this end, we introduce a new paradigm called Adaptive Discovering and Merging (ADM) to discover novel categories adaptively in the incremental stage and integrate novel knowledge into the model without affecting the original knowledge. To discover novel classes adaptively, we decouple representation learning and novel class discovery, and use Triple Comparison (TC) and Probability Regularization (PR) to constrain the probability discrepancy and diversity for adaptive category assignment. To merge the learned novel knowledge adaptively, we propose a hybrid structure with base and novel branches named Adaptive Model Merging (AMM), which reduces the interference of the novel branch on the old classes to preserve the previous knowledge, and merges the novel branch to the base model without performance loss and parameter growth. Extensive experiments on several datasets show that ADM significantly outperforms existing class-incremental Novel Class Discovery (class-iNCD) approaches. Moreover, our AMM also benefits the class-incremental Learning (class-IL) task by alleviating the catastrophic forgetting problem.","[{'name': 'Guangyao Chen, Peixi Peng, Yangru Huang, Mengyue Geng, Yonghong Tian'}]",
1347,BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving,https://arxiv.org/abs/2403.03401,"arXiv:2403.03401v1 Announce Type: new 
Abstract: Artificial Intelligence for Theorem Proving has given rise to a plethora of benchmarks and methodologies, particularly in Interactive Theorem Proving (ITP). Research in the area is fragmented, with a diverse set of approaches being spread across several ITP systems. This presents a significant challenge to the comparison of methods, which are often complex and difficult to replicate. Addressing this, we present BAIT, a framework for fair and streamlined comparison of learning approaches in ITP. We demonstrate BAIT's capabilities with an in-depth comparison, across several ITP benchmarks, of state-of-the-art architectures applied to the problem of formula embedding. We find that Structure Aware Transformers perform particularly well, improving on techniques associated with the original problem sets. BAIT also allows us to assess the end-to-end proving performance of systems built on interactive environments. This unified perspective reveals a novel end-to-end system that improves on prior work. We also provide a qualitative analysis, illustrating that improved performance is associated with more semantically-aware embeddings. By streamlining the implementation and comparison of Machine Learning algorithms in the ITP context, we anticipate BAIT will be a springboard for future research.","[{'name': 'Sean Lamont, Michael Norrish, Amir Dezfouli, Christian Walder, Paul Montague'}]",
1348,An EnKF-LSTM Assimilation Algorithm for Crop Growth Model,https://arxiv.org/abs/2403.03406,"arXiv:2403.03406v1 Announce Type: new 
Abstract: Accurate and timely prediction of crop growth is of great significance to ensure crop yields and researchers have developed several crop models for the prediction of crop growth. However, there are large difference between the simulation results obtained by the crop models and the actual results, thus in this paper, we proposed to combine the simulation results with the collected crop data for data assimilation so that the accuracy of prediction will be improved. In this paper, an EnKF-LSTM data assimilation method for various crops is proposed by combining ensemble Kalman filter and LSTM neural network, which effectively avoids the overfitting problem of existing data assimilation methods and eliminates the uncertainty of the measured data. The verification of the proposed EnKF-LSTM method and the comparison of the proposed method with other data assimilation methods were performed using datasets collected by sensor equipment deployed on a farm.","[{'name': 'Siqi Zhou, Ling Wang, Jie Liu, Jinshan Tang'}]",
1349,IB-Net: Initial Branch Network for Variable Decision in Boolean Satisfiability,https://arxiv.org/abs/2403.03517,"arXiv:2403.03517v1 Announce Type: new 
Abstract: Boolean Satisfiability problems are vital components in Electronic Design Automation, particularly within the Logic Equivalence Checking process. Currently, SAT solvers are employed for these problems and neural network is tried as assistance to solvers. However, as SAT problems in the LEC context are distinctive due to their predominantly unsatisfiability nature and a substantial proportion of UNSAT-core variables, existing neural network assistance has proven unsuccessful in this specialized domain. To tackle this challenge, we propose IB-Net, an innovative framework utilizing graph neural networks and novel graph encoding techniques to model unsatisfiable problems and interact with state-of-the-art solvers. Extensive evaluations across solvers and datasets demonstrate IB-Net's acceleration, achieving an average runtime speedup of 5.0% on industrial data and 8.3% on SAT competition data empirically. This breakthrough advances efficient solving in LEC workflows.","[{'name': 'Tsz Ho Chan, Wenyi Xiao, Junhua Huang, Huiling Zhen, Guangji Tian, Mingxuan Yuan'}]",
1350,Prompt Mining for Language-based Human Mobility Forecasting,https://arxiv.org/abs/2403.03544,"arXiv:2403.03544v1 Announce Type: new 
Abstract: With the advancement of large language models, language-based forecasting has recently emerged as an innovative approach for predicting human mobility patterns. The core idea is to use prompts to transform the raw mobility data given as numerical values into natural language sentences so that the language models can be leveraged to generate the description for future observations. However, previous studies have only employed fixed and manually designed templates to transform numerical values into sentences. Since the forecasting performance of language models heavily relies on prompts, using fixed templates for prompting may limit the forecasting capability of language models. In this paper, we propose a novel framework for prompt mining in language-based mobility forecasting, aiming to explore diverse prompt design strategies. Specifically, the framework includes a prompt generation stage based on the information entropy of prompts and a prompt refinement stage to integrate mechanisms such as the chain of thought. Experimental results on real-world large-scale data demonstrate the superiority of generated prompts from our prompt mining pipeline. Additionally, the comparison of different prompt variants shows that the proposed prompt refinement process is effective. Our study presents a promising direction for further advancing language-based mobility forecasting.","[{'name': 'Hao Xue, Tianye Tang, Ali Payani, Flora D. Salim'}]",
1351,Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models,https://arxiv.org/abs/2403.03550,"arXiv:2403.03550v1 Announce Type: new 
Abstract: This study investigates the generation of synthetic disinformation by OpenAI's Large Language Models (LLMs) through prompt engineering and explores their responsiveness to emotional prompting. Leveraging various LLM iterations using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed experiments to assess their success in producing disinformation. Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation. When prompted politely, all examined LLMs consistently generate disinformation at a high frequency. Conversely, when prompted impolitely, the frequency of disinformation production diminishes, as the models often refuse to generate disinformation and instead caution users that the tool is not intended for such purposes. This research contributes to the ongoing discourse surrounding responsible development and application of AI technologies, particularly in mitigating the spread of disinformation and promoting transparency in AI-generated content.","[{'name': 'Rasita Vinay, Giovanni Spitale, Nikola Biller-Andorno, Federico Germani'}]",
1352,Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision: Insights from Group and Individual Assessments,https://arxiv.org/abs/2403.03594,"arXiv:2403.03594v1 Announce Type: new 
Abstract: Recently, it has been recognized that large language models demonstrate high performance on various intellectual tasks. However, few studies have investigated alignment with humans in behaviors that involve sensibility, such as aesthetic evaluation. This study investigates the performance of GPT-4 with Vision, a state-of-the-art language model that can handle image input, on the task of aesthetic evaluation of images. We employ two tasks, prediction of the average evaluation values of a group and an individual's evaluation values. We investigate the performance of GPT-4 with Vision by exploring prompts and analyzing prediction behaviors. Experimental results reveal GPT-4 with Vision's superior performance in predicting aesthetic evaluations and the nature of different responses to beauty and ugliness. Finally, we discuss developing an AI system for aesthetic evaluation based on scientific knowledge of the human perception of beauty, employing agent technologies that integrate traditional deep learning models with large language models.","[{'name': 'Yoshia Abe, Tatsuya Daikoku, Yasuo Kuniyoshi'}]",
1353,A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation,https://arxiv.org/abs/2403.03600,"arXiv:2403.03600v1 Announce Type: new 
Abstract: Cross-domain recommendation (CDR) aims to enhance recommendation accuracy in a target domain with sparse data by leveraging rich information in a source domain, thereby addressing the data-sparsity problem. Some existing CDR methods highlight the advantages of extracting domain-common and domain-specific features to learn comprehensive user and item representations. However, these methods can't effectively disentangle these components as they often rely on simple user-item historical interaction information (such as ratings, clicks, and browsing), neglecting the rich multi-modal features. Additionally, they don't protect user-sensitive data from potential leakage during knowledge transfer between domains. To address these challenges, we propose a Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation, called P2M2-CDR. Specifically, we first design a multi-modal disentangled encoder that utilizes multi-modal information to disentangle more informative domain-common and domain-specific embeddings. Furthermore, we introduce a privacy-preserving decoder to mitigate user privacy leakage during knowledge transfer. Local differential privacy (LDP) is utilized to obfuscate the disentangled embeddings before inter-domain exchange, thereby enhancing privacy protection. To ensure both consistency and differentiation among these obfuscated disentangled embeddings, we incorporate contrastive learning-based domain-inter and domain-intra losses. Extensive Experiments conducted on four real-world datasets demonstrate that P2M2-CDR outperforms other state-of-the-art single-domain and cross-domain baselines.","[{'name': 'Li Wang, Lei Sang, Quangui Zhang, Qiang Wu, Min Xu'}]",
1354,The Geometric Structure of Topic Models,https://arxiv.org/abs/2403.03607,"arXiv:2403.03607v1 Announce Type: new 
Abstract: Topic models are a popular tool for clustering and analyzing textual data. They allow texts to be classified on the basis of their affiliation to the previously calculated topics. Despite their widespread use in research and application, an in-depth analysis of topic models is still an open research topic. State-of-the-art methods for interpreting topic models are based on simple visualizations, such as similarity matrices, top-term lists or embeddings, which are limited to a maximum of three dimensions. In this paper, we propose an incidence-geometric method for deriving an ordinal structure from flat topic models, such as non-negative matrix factorization. These enable the analysis of the topic model in a higher (order) dimension and the possibility of extracting conceptual relationships between several topics at once. Due to the use of conceptual scaling, our approach does not introduce any artificial topical relationships, such as artifacts of feature compression. Based on our findings, we present a new visualization paradigm for concept hierarchies based on ordinal motifs. These allow for a top-down view on topic spaces. We introduce and demonstrate the applicability of our approach based on a topic model derived from a corpus of scientific papers taken from 32 top machine learning venues.","[{'name': 'Johannes Hirth, Tom Hanika'}]",
1355,SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models,https://arxiv.org/abs/2403.03636,"arXiv:2403.03636v1 Announce Type: new 
Abstract: Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: $\\textit{Planner}$, $\\textit{Informer}$, and $\\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20-30% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at https://sheetagent.github.io.","[{'name': 'Yibin Chen, Yifu Yuan, Zeyu Zhang, Yan Zheng, Jinyi Liu, Fei Ni, Jianye Hao'}]",
1356,K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data,https://arxiv.org/abs/2403.03645,"arXiv:2403.03645v1 Announce Type: new 
Abstract: Sourced from various sensors and organized chronologically, Multivariate Time-Series (MTS) data involves crucial spatial-temporal dependencies, e.g., correlations among sensors. To capture these dependencies, Graph Neural Networks (GNNs) have emerged as powerful tools, yet their effectiveness is restricted by the quality of graph construction from MTS data. Typically, existing approaches construct graphs solely from MTS signals, which may introduce bias due to a small training dataset and may not accurately represent underlying dependencies. To address this challenge, we propose a novel framework named K-Link, leveraging Large Language Models (LLMs) to encode extensive general knowledge and thereby providing effective solutions to reduce the bias. Leveraging the knowledge embedded in LLMs, such as physical principles, we extract a \\textit{Knowledge-Link graph}, capturing vast semantic knowledge of sensors and the linkage of the sensor-level knowledge. To harness the potential of the knowledge-link graph in enhancing the graph derived from MTS data, we propose a graph alignment module, facilitating the transfer of semantic knowledge within the knowledge-link graph into the MTS-derived graph. By doing so, we can improve the graph quality, ensuring effective representation learning with GNNs for MTS data. Extensive experiments demonstrate the efficacy of our approach for superior performance across various MTS-related downstream tasks.","[{'name': 'Yucheng Wang, Ruibing Jin, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen'}]",
1357,Towards Safe and Aligned Large Language Models for Medicine,https://arxiv.org/abs/2403.03744,"arXiv:2403.03744v1 Announce Type: new 
Abstract: The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical LLMs. Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale approaches used by the machine learning community to develop safe and aligned LLMs. We hope that this work casts light on the safety and alignment of medical LLMs and motivates future work to study it and develop additional mitigation strategies, minimizing the risks of harm of LLMs in medicine.","[{'name': 'Tessa Han, Aounon Kumar, Chirag Agarwal, Himabindu Lakkaraju'}]",
1358,DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models,https://arxiv.org/abs/2403.03768,"arXiv:2403.03768v1 Announce Type: new 
Abstract: The field of pharmaceutical development and therapeutic application both face substantial challenges. Therapeutic domain calls for more treatment alternatives while numerous promising pre-clinical drugs fail in clinical trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stage of drug development. Although in-silico CRE models offer a solution to this problem, existing methodologies are either limited to early development stages or lack the capacity for a comprehensive CRE analysis. Herein, we introduce a novel computational model named DeepCRE and present the potential of DeepCRE in advancing therapeutic discovery and development. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7\\% in patient-level CRE, and a 5-fold increase in indication-level CRE. Furthermore, DeepCRE has identified six drug candidates that show significantly greater effectiveness than a comparator set of two approved drug in 5/8 colorectal cancer (CRC) organoids. This highlights DeepCRE's ability to identify a collection of drug candidates with superior therapeutic effects, underscoring its potential to revolutionize the field of therapeutic development.",[{'name': 'Yushuai Wu'}],
1359,From Clicks to Security: Investigating Continuous Authentication via Mouse Dynamics,https://arxiv.org/abs/2403.03828,"arXiv:2403.03828v1 Announce Type: new 
Abstract: In the realm of computer security, the importance of efficient and reliable user authentication methods has become increasingly critical. This paper examines the potential of mouse movement dynamics as a consistent metric for continuous authentication. By analyzing user mouse movement patterns in two contrasting gaming scenarios, \"Team Fortress\" and Poly Bridge we investigate the distinctive behavioral patterns inherent in high-intensity and low-intensity UI interactions. The study extends beyond conventional methodologies by employing a range of machine learning models. These models are carefully selected to assess their effectiveness in capturing and interpreting the subtleties of user behavior as reflected in their mouse movements. This multifaceted approach allows for a more nuanced and comprehensive understanding of user interaction patterns. Our findings reveal that mouse movement dynamics can serve as a reliable indicator for continuous user authentication. The diverse machine learning models employed in this study demonstrate competent performance in user verification, marking an improvement over previous methods used in this field. This research contributes to the ongoing efforts to enhance computer security and highlights the potential of leveraging user behavior, specifically mouse dynamics, in developing robust authentication systems.","[{'name': 'Rushit Dave, Marcho Handoko, Ali Rashid, Cole Schoenbauer'}]",
1360,Your device may know you better than you know yourself -- continuous authentication on novel dataset using machine learning,https://arxiv.org/abs/2403.03832,"arXiv:2403.03832v1 Announce Type: new 
Abstract: This research aims to further understanding in the field of continuous authentication using behavioral biometrics. We are contributing a novel dataset that encompasses the gesture data of 15 users playing Minecraft with a Samsung Tablet, each for a duration of 15 minutes. Utilizing this dataset, we employed machine learning (ML) binary classifiers, being Random Forest (RF), K-Nearest Neighbors (KNN), and Support Vector Classifier (SVC), to determine the authenticity of specific user actions. Our most robust model was SVC, which achieved an average accuracy of approximately 90%, demonstrating that touch dynamics can effectively distinguish users. However, further studies are needed to make it viable option for authentication systems","[{'name': 'Pedro Gomes do Nascimento, Pidge Witiak, Tucker MacCallum, Zachary Winterfeldt, Rushit Dave'}]",
1361,IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators,https://arxiv.org/abs/2403.03894,"arXiv:2403.03894v1 Announce Type: new 
Abstract: Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.
  To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled with respective intermediate representations. Next, starting from various base Code-LMs (ranging in size from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages. Our resulting models, dubbed IRCoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.","[{'name': 'Indraneil Paul, Jun Luo, Goran Glava\\\\v{s}, Iryna Gurevych'}]",
1362,Enhancing Instructional Quality: Leveraging Computer-Assisted Textual Analysis to Generate In-Depth Insights from Educational Artifacts,https://arxiv.org/abs/2403.03920,"arXiv:2403.03920v1 Announce Type: new 
Abstract: This paper explores the transformative potential of computer-assisted textual analysis in enhancing instructional quality through in-depth insights from educational artifacts. We integrate Richard Elmore's Instructional Core Framework to examine how artificial intelligence (AI) and machine learning (ML) methods, particularly natural language processing (NLP), can analyze educational content, teacher discourse, and student responses to foster instructional improvement. Through a comprehensive review and case studies within the Instructional Core Framework, we identify key areas where AI/ML integration offers significant advantages, including teacher coaching, student support, and content development. We unveil patterns that indicate AI/ML not only streamlines administrative tasks but also introduces novel pathways for personalized learning, providing actionable feedback for educators and contributing to a richer understanding of instructional dynamics. This paper emphasizes the importance of aligning AI/ML technologies with pedagogical goals to realize their full potential in educational settings, advocating for a balanced approach that considers ethical considerations, data quality, and the integration of human expertise.","[{'name': 'Zewei Tian, Min Sun, Alex Liu, Shawon Sarkar, Jing Liu'}]",
1363,Consciousness qua Mortal Computation,https://arxiv.org/abs/2403.03925,"arXiv:2403.03925v1 Announce Type: new 
Abstract: Computational functionalism posits that consciousness is a computation. Here we show, perhaps surprisingly, that it cannot be a Turing computation. Rather, computational functionalism implies that consciousness is a novel type of computation that has recently been proposed by Geoffrey Hinton, called mortal computation.",[{'name': 'Johannes Kleiner'}],
1364,Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing,https://arxiv.org/abs/2402.16627,"arXiv:2402.16627v2 Announce Type: cross 
Abstract: Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing. In each task, our ContextDiff achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations. Our code is available at https://github.com/YangLing0818/ContextDiff","[{'name': 'Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, Bin Cui'}]",
1365,Structure-Guided Adversarial Training of Diffusion Models,https://arxiv.org/abs/2402.17563,"arXiv:2402.17563v2 Announce Type: cross 
Abstract: Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively.","[{'name': 'Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui'}]",
1366,Knowledge-guided EEG Representation Learning,https://arxiv.org/abs/2403.03222,"arXiv:2403.03222v1 Announce Type: cross 
Abstract: Self-supervised learning has produced impressive results in multimedia domains of audio, vision and speech. This paradigm is equally, if not more, relevant for the domain of biosignals, owing to the scarcity of labelled data in such scenarios. The ability to leverage large-scale unlabelled data to learn robust representations could help improve the performance of numerous inference tasks on biosignals. Given the inherent domain differences between multimedia modalities and biosignals, the established objectives for self-supervised learning may not translate well to this domain. Hence, there is an unmet need to adapt these methods to biosignal analysis. In this work we propose a self-supervised model for EEG, which provides robust performance and remarkable parameter efficiency by using state space-based deep learning architecture. We also propose a novel knowledge-guided pre-training objective that accounts for the idiosyncrasies of the EEG signal. The results indicate improved embedding representation learning and downstream performance compared to prior works on exemplary tasks. Also, the proposed objective significantly reduces the amount of pre-training data required to obtain performance equivalent to prior works.","[{'name': 'Aditya Kommineni, Kleanthis Avramidis, Richard Leahy, Shrikanth Narayanan'}]",
1367,Reinforcement Learning Jazz Improvisation: When Music Meets Game Theory,https://arxiv.org/abs/2403.03224,"arXiv:2403.03224v1 Announce Type: cross 
Abstract: Live performances of music are always charming, with the unpredictability of improvisation due to the dynamic between musicians and interactions with the audience. Jazz improvisation is a particularly noteworthy example for further investigation from a theoretical perspective. Here, we introduce a novel mathematical game theory model for jazz improvisation, providing a framework for studying music theory and improvisational methodologies. We use computational modeling, mainly reinforcement learning, to explore diverse stochastic improvisational strategies and their paired performance on improvisation. We find that the most effective strategy pair is a strategy that reacts to the most recent payoff (Stepwise Changes) with a reinforcement learning strategy limited to notes in the given chord (Chord-Following Reinforcement Learning). Conversely, a strategy that reacts to the partner's last note and attempts to harmonize with it (Harmony Prediction) strategy pair yields the lowest non-control payoff and highest standard deviation, indicating that picking notes based on immediate reactions to the partner player can yield inconsistent outcomes. On average, the Chord-Following Reinforcement Learning strategy demonstrates the highest mean payoff, while Harmony Prediction exhibits the lowest. Our work lays the foundation for promising applications beyond jazz: including the use of artificial intelligence (AI) models to extract data from audio clips to refine musical reward systems, and training machine learning (ML) models on existing jazz solos to further refine strategies within the game.","[{'name': 'Vedant Tapiavala, Joshua Piesner, Sourjyamoy Barman, Feng Fu'}]",
1368,"Note: Harnessing Tellurium Nanoparticles in the Digital Realm Plasmon Resonance, in the Context of Brewster's Angle and the Drude Model for Fake News Adsorption in Incomplete Information Games",https://arxiv.org/abs/2403.03239,"arXiv:2403.03239v1 Announce Type: cross 
Abstract: This note explores the innovative application of soliton theory and plasmonic phenomena in modeling user behavior and engagement within digital health platforms. By introducing the concept of soliton solutions, we present a novel approach to understanding stable patterns of health improvement behaviors over time. Additionally, we delve into the role of tellurium nanoparticles and their plasmonic properties in adsorbing fake news, thereby influencing user interactions and engagement levels. Through a theoretical framework that combines nonlinear dynamics with the unique characteristics of tellurium nanoparticles, we aim to provide new insights into the dynamics of user engagement in digital health environments. Our analysis highlights the potential of soliton theory in capturing the complex, nonlinear dynamics of user behavior, while the application of plasmonic phenomena offers a promising avenue for enhancing the sensitivity and effectiveness of digital health platforms. This research ventures into an uncharted territory where optical phenomena such as Brewster's Angle and Snell's Law, along with the concept of spin solitons, are metaphorically applied to address the challenge of fake news dissemination. By exploring the analogy between light refraction, reflection, and the propagation of information in digital platforms, we unveil a novel perspective on how the 'angle' at which information is presented can significantly affect its acceptance and spread. Additionally, we propose the use of tellurium nanoparticles to manage 'information waves' through mechanisms akin to plasmonic resonance and soliton dynamics. This theoretical exploration aims to bridge the gap between physical sciences and digital communication, offering insights into the development of strategies for mitigating misinformation.",[{'name': 'Yasuko Kawahata'}],
1369,From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE,https://arxiv.org/abs/2403.03274,"arXiv:2403.03274v1 Announce Type: cross 
Abstract: Digital health technologies (DHT), such as wearable devices, provide personalized, continuous, and real-time monitoring of patient. These technologies are contributing to the development of novel therapies and personalized medicine. Gaining insight from these technologies requires appropriate modeling techniques to capture clinically-relevant changes in disease state. The data generated from these devices is characterized by being stochastic in nature, may have missing elements, and exhibits considerable inter-individual variability - thereby making it difficult to analyze using traditional longitudinal modeling techniques. We present a novel pharmacology-informed neural stochastic differential equation (SDE) model capable of addressing these challenges. Using synthetic data, we demonstrate that our approach is effective in identifying treatment effects and learning causal relationships from stochastic data, thereby enabling counterfactual simulation.","[{'name': 'Samira Pakravan, Nikolaos Evangelou, Maxime Usdin, Logan Brooks, James Lu'}]",
1370,ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures,https://arxiv.org/abs/2403.03276,"arXiv:2403.03276v1 Announce Type: cross 
Abstract: We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneous datasets, including the CHB-MIT and UPenn and Mayos Clinic, CHB-MIT datasets. The empirical findings suggest that the ARNN model outperforms baseline methods such as LSTM, Vision Transformer (ViT), Compact Convolution Transformer (CCT), and R-Transformer (RT), showcasing superior performance and faster processing capabilities across a wide range of tasks. The code has been made publicly accessible at \\url{https://github.com/Salim-Lysiun/ARNN}.","[{'name': 'Salim Rukhsar, Anil Kumar Tiwari'}]",
1371,Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits,https://arxiv.org/abs/2403.03281,"arXiv:2403.03281v1 Announce Type: cross 
Abstract: We consider the problem of late multi-modal fusion for discriminative learning. Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of multi-modal fusion. We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities. We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC. Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while maintaining competitive performance with the state-of-the-art.","[{'name': 'Sahil Sidheekh, Pranuthi Tenali, Saurabh Mathur, Erik Blasch, Kristian Kersting, Sriraam Natarajan'}]",
1372,Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification,https://arxiv.org/abs/2403.03305,"arXiv:2403.03305v1 Announce Type: cross 
Abstract: This paper introduces a novel neuro-symbolic architecture for relation classification (RC) that combines rule-based methods with contemporary deep learning techniques. This approach capitalizes on the strengths of both paradigms: the adaptability of rule-based systems and the generalization power of neural networks. Our architecture consists of two components: a declarative rule-based model for transparent classification and a neural component to enhance rule generalizability through semantic text matching. Notably, our semantic matcher is trained in an unsupervised domain-agnostic way, solely with synthetic data. Further, these components are loosely coupled, allowing for rule modifications without retraining the semantic matcher. In our evaluation, we focused on two few-shot relation classification datasets: Few-Shot TACRED and a Few-Shot version of NYT29. We show that our proposed method outperforms previous state-of-the-art models in three out of four settings, despite not seeing any human-annotated training data. Further, we show that our approach remains modular and pliable, i.e., the corresponding rules can be locally modified to improve the overall model. Human interventions to the rules for the TACRED relation \\texttt{org:parents} boost the performance on that relation by as much as 26\\% relative improvement, without negatively impacting the other relations, and without retraining the semantic matching component.","[{'name': 'Robert Vacareanu, Fahmida Alam, Md Asiful Islam, Haris Riaz, Mihai Surdeanu'}]",
1373,Deep Configuration Performance Learning: A Systematic Survey and Taxonomy,https://arxiv.org/abs/2403.03322,"arXiv:2403.03322v1 Announce Type: cross 
Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.
  In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evaluated and how they are exploited in different tasks related to software configuration. We also identify the good practice and the potentially problematic phenomena from the studies surveyed, together with insights on future opportunities for the field. To promote open science, all the raw results of this survey can be accessed at our repository: https://github.com/ideas-labo/DCPL-SLR.","[{'name': 'Jingzhi Gong, Tao Chen'}]",
1374,"DIVERSE: Deciphering Internet Views on the U.S. Military Through Video Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification",https://arxiv.org/abs/2403.03334,"arXiv:2403.03334v1 Announce Type: cross 
Abstract: Stance detection of social media text is a key component of downstream tasks involving the identification of groups of users with opposing opinions on contested topics such as vaccination and within arguments. In particular, stance provides an indication of an opinion towards an entity. This paper introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated for their stance towards videos of the U.S. military. The stance is annotated through a human-guided, machine-assisted labeling methodology that makes use of weak signals of tone within the sentence as supporting indicators, as opposed to using manual annotations by humans. These weak signals consist of the presence of hate speech and sarcasm, the presence of specific keywords, the sentiment of the text, and the stance inference from two Large Language Models. The weak signals are then consolidated using a data programming model before each comment is annotated with a final stance label. On average, the videos have 200 comments each, and the stance of the comments skews slightly towards the \"against\" characterization for both the U.S. Army and the videos posted on the channel.","[{'name': 'Iain J. Cruickshank, Lynnette Hui Xian Ng'}]",
1375,Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation,https://arxiv.org/abs/2403.03344,"arXiv:2403.03344v1 Announce Type: cross 
Abstract: The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generate codes considered in this study are produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code's \"green capacity\", based on certain sustainability metrics. We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development.","[{'name': 'Tina Vartziotis, Ippolyti Dellatolas, George Dasoulas, Maximilian Schmidt, Florian Schneider, Tim Hoffmann, Sotirios Kotsopoulos, Michael Keckeisen'}]",
1376,Learning to Maximize Mutual Information for Chain-of-Thought Distillation,https://arxiv.org/abs/2403.03348,"arXiv:2403.03348v1 Announce Type: cross 
Abstract: Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve this optimization problem using a learning-based method. Our experimental results across four datasets demonstrate that our method outperforms the state-of-the-art DSS. Our findings offer insightful guidance for future research on language model distillation as well as applications involving CoT. Code and models will be released soon.","[{'name': 'Xin Chen, Hanxian Huang, Yanjun Gao, Yi Wang, Jishen Zhao, Ke Ding'}]",
1377,Multi-modal Deep Learning,https://arxiv.org/abs/2403.03385,"arXiv:2403.03385v1 Announce Type: cross 
Abstract: This article investigates deep learning methodologies for single-modality clinical data analysis, as a crucial precursor to multi-modal medical research. Building on Guo JingYuan's work, the study refines clinical data processing through Compact Convolutional Transformer (CCT), Patch Up, and the innovative CamCenterLoss technique, establishing a foundation for future multimodal investigations. The proposed methodology demonstrates improved prediction accuracy and at tentiveness to critically ill patients compared to Guo JingYuan's ResNet and StageNet approaches. Novelty that using image-pretrained vision transformer backbone to perform transfer learning time-series clinical data.The study highlights the potential of CCT, Patch Up, and novel CamCenterLoss in processing single modality clinical data within deep learning frameworks, paving the way for future multimodal medical research and promoting precision and personalized healthcare",[{'name': 'Chen Yuhua'}],
1378,Interactive Melody Generation System for Enhancing the Creativity of Musicians,https://arxiv.org/abs/2403.03395,"arXiv:2403.03395v1 Announce Type: cross 
Abstract: This study proposes a system designed to enumerate the process of collaborative composition among humans, using automatic music composition technology. By integrating multiple Recurrent Neural Network (RNN) models, the system provides an experience akin to collaborating with several composers, thereby fostering diverse creativity. Through dynamic adaptation to the user's creative intentions, based on feedback, the system enhances its capability to generate melodies that align with user preferences and creative needs. The system's effectiveness was evaluated through experiments with composers of varying backgrounds, revealing its potential to facilitate musical creativity and suggesting avenues for further refinement. The study underscores the importance of interaction between the composer and AI, aiming to make music composition more accessible and personalized. This system represents a step towards integrating AI into the creative process, offering a new tool for composition support and collaborative artistic exploration.","[{'name': 'So Hirawata, Noriko Otani'}]",
1379,Human vs. Machine: Language Models and Wargames,https://arxiv.org/abs/2403.03407,"arXiv:2403.03407v1 Announce Type: cross 
Abstract: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.","[{'name': 'Max Lamparth, Anthony Corso, Jacob Ganz, Oriana Skylar Mastro, Jacquelyn Schneider, Harold Trinkunas'}]",
1380,Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN,https://arxiv.org/abs/2403.03409,"arXiv:2403.03409v1 Announce Type: cross 
Abstract: Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task, and, then, pruning neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning a large randomly initialized model. We introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from a randomly initialized RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained for different tasks, such as image classification and temporal prediction. We experimentally show that, in spite of being task-agnostic, LNP increases computational efficiency (fewer neurons and synapses) and prediction performance of RSNNs compared to traditional activity-based pruning of trained dense models.","[{'name': 'Biswadeep Chakraborty, Beomseok Kang, Harshit Kumar, Saibal Mukhopadhyay'}]",
1381,Leveraging The Finite States of Emotion Processing to Study Late-Life Mental Health,https://arxiv.org/abs/2403.03414,"arXiv:2403.03414v1 Announce Type: cross 
Abstract: Traditional approaches in mental health research apply General Linear Models (GLM) to describe the longitudinal dynamics of observed psycho-behavioral measurements (questionnaire summary scores). Similarly, GLMs are also applied to characterize relationships between neurobiological measurements (regional fMRI signals) and perceptual stimuli or other regional signals. While these methods are useful for exploring linear correlations among the isolated signals of those constructs (i.e., summary scores or fMRI signals), these classical frameworks fall short in providing insights into the comprehensive system-level dynamics underlying observable changes. Hidden Markov Models (HMM) are a statistical model that enable us to describe the sequential relations among multiple observable constructs, and when applied through the lens of Finite State Automata (FSA), can provide a more integrated and intuitive framework for modeling and understanding the underlying controller (the prescription for how to respond to inputs) that fundamentally defines any system, as opposed to linearly correlating output signals produced by the controller. We present a simple and intuitive HMM processing pipeline vcHMM (See Preliminary Data) that highlights FSA theory and is applicable for both behavioral analysis of questionnaire data and fMRI data. HMMs offer theoretic promise as they are computationally equivalent to the FSA, the control processor of a Turing Machine (TM) The dynamic programming Viterbi algorithm is used to leverage the HMM model. It efficiently identifies the most likely sequence of hidden states. The vcHMM pipeline leverages this grammar to understand how behavior and neural activity relate to depression.","[{'name': 'Yuanzhe Huang, Saurab Faruque, Minjie Wu, Akiko Mizuno, Eduardo Diniz, Shaolin Yang, George Dewitt Stetten, Noah Schweitzer, Hecheng Jin, Linghai Wang, Howard J. Aizenstein'}]",
1382,Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization,https://arxiv.org/abs/2403.03419,"arXiv:2403.03419v1 Announce Type: cross 
Abstract: Large language models (LLMs) have revolutionized the role of AI, yet also pose potential risks of propagating unethical content. Alignment technologies have been introduced to steer LLMs towards human preference, gaining increasing attention. Despite notable breakthroughs in this direction, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research focus: achieving alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness. For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between the generated responses and the dispreferred ones to effectively eschew harmful information. We theoretically demonstrate that D$^2$O is equivalent to learning a distributional instead of instance-level preference model reflecting human dispreference against the distribution of negative responses. Besides, D$^2$O integrates an implicit Jeffrey Divergence regularization to balance the exploitation and exploration of reference policies and converges to a non-negative one during training. Extensive experiments demonstrate that our method achieves comparable generation quality and surpasses the latest baselines in producing less harmful and more informative responses with better training stability and faster convergence.","[{'name': 'Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu'}]",
1383,LEAD: Learning Decomposition for Source-free Universal Domain Adaptation,https://arxiv.org/abs/2403.03421,"arXiv:2403.03421v1 Announce Type: cross 
Abstract: Universal Domain Adaptation (UniDA) targets knowledge transfer in the presence of both covariate and label shifts. Recently, Source-free Universal Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to source data, which tends to be more practical due to data protection policies. The main challenge lies in determining whether covariate-shifted samples belong to target-private unknown categories. Existing methods tackle this either through hand-crafted thresholding or by developing time-consuming iterative clustering strategies. In this paper, we propose a new idea of LEArning Decomposition (LEAD), which decouples features into source-known and -unknown components to identify target-private data. Technically, LEAD initially leverages the orthogonal decomposition analysis for feature decomposition. Then, LEAD builds instance-level decision boundaries to adaptively identify target-private data. Extensive experiments across various UniDA scenarios have demonstrated the effectiveness and superiority of LEAD. Notably, in the OPDA scenario on VisDA dataset, LEAD outperforms GLC by 3.5% overall H-score and reduces 75% time to derive pseudo-labeling decision boundaries. Besides, LEAD is also appealing in that it is complementary to most existing methods. The code is available at https://github.com/ispc-lab/LEAD.","[{'name': 'Sanqing Qu, Tianpei Zou, Lianghua He, Florian R\\\\\"ohrbein, Alois Knoll, Guang Chen, Changjun Jiang'}]",
1384,Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models,https://arxiv.org/abs/2403.03432,"arXiv:2403.03432v1 Announce Type: cross 
Abstract: Instruction Tuning has the potential to stimulate or enhance specific capabilities of large language models (LLMs). However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks. To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and parameter-efficient tuning method designed for multi-task learning with LLMs. In this paper, we start by individually training multiple domain-specific LoRA modules using corresponding supervised corpus data. These LoRA modules can be aligned with the expert design principles observed in Mixture-of-Experts (MoE). Subsequently, we combine the multiple LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which help prevent interference between tasks and ultimately enhances the performance of each individual task. Furthermore, each LoRA model can be iteratively adapted to a new domain, allowing for quick domain-specific adaptation. Experiments on diverse tasks demonstrate superior and robust performance, which can further promote the wide application of domain-specific LLMs.","[{'name': 'Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Yu Han, Hao Wang'}]",
1385,Uncertainty quantification for deeponets with ensemble kalman inversion,https://arxiv.org/abs/2403.03444,"arXiv:2403.03444v1 Announce Type: cross 
Abstract: In recent years, operator learning, particularly the DeepONet, has received much attention for efficiently learning complex mappings between input and output functions across diverse fields. However, in practical scenarios with limited and noisy data, accessing the uncertainty in DeepONet predictions becomes essential, especially in mission-critical or safety-critical applications. Existing methods, either computationally intensive or yielding unsatisfactory uncertainty quantification, leave room for developing efficient and informative uncertainty quantification (UQ) techniques tailored for DeepONets. In this work, we proposed a novel inference approach for efficient UQ for operator learning by harnessing the power of the Ensemble Kalman Inversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and highly parallelizable feature, has demonstrated its advantages for UQ for physics-informed neural networks [28]. Our innovative application of EKI enables us to efficiently train ensembles of DeepONets while obtaining informative uncertainty estimates for the output of interest. We deploy a mini-batch variant of EKI to accommodate larger datasets, mitigating the computational demand due to large datasets during the training stage. Furthermore, we introduce a heuristic method to estimate the artificial dynamics covariance, thereby improving our uncertainty estimates. Finally, we demonstrate the effectiveness and versatility of our proposed methodology across various benchmark problems, showcasing its potential to address the pressing challenges of uncertainty quantification in DeepONets, especially for practical applications with limited and noisy data.","[{'name': 'Andrew Pensoneault, Xueyu Zhu'}]",
1386,DLP-GAN: Learning to Draw Modern Chinese Landscape Photos with Generative Adversarial Network,https://arxiv.org/abs/2403.03456,"arXiv:2403.03456v1 Announce Type: cross 
Abstract: Chinese landscape painting has a unique and artistic style, and its drawing technique is highly abstract in both the use of color and the realistic representation of objects. Previous methods focus on transferring from modern photos to ancient ink paintings. However, little attention has been paid to translating landscape paintings into modern photos. To solve such problems, in this paper, we (1) propose DLP-GAN (\\textbf{D}raw Modern Chinese \\textbf{L}andscape \\textbf{P}hotos with \\textbf{G}enerative \\textbf{A}dversarial \\textbf{N}etwork), an unsupervised cross-domain image translation framework with a novel asymmetric cycle mapping, and (2) introduce a generator based on a dense-fusion module to match different translation directions. Moreover, a dual-consistency loss is proposed to balance the realism and abstraction of model painting. In this way, our model can draw landscape photos and sketches in the modern sense. Finally, based on our collection of modern landscape and sketch datasets, we compare the images generated by our model with other benchmarks. Extensive experiments including user studies show that our model outperforms state-of-the-art methods.","[{'name': 'Xiangquan Gui, Binxuan Zhang, Li Li, Yi Yang'}]",
1387,Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts,https://arxiv.org/abs/2403.03506,"arXiv:2403.03506v1 Announce Type: cross 
Abstract: This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical findings highlight (1) detecting AI-generated sentences in hybrid texts is overall a challenging task because (1.1) human writers' selecting and even editing AI-generated sentences based on personal preferences adds difficulty in identifying the authorship of segments; (1.2) the frequent change of authorship between neighboring sentences within the hybrid text creates difficulties for segment detectors in identifying authorship-consistent segments; (1.3) the short length of text segments within hybrid texts provides limited stylistic cues for reliable authorship determination; (2) before embarking on the detection process, it is beneficial to assess the average length of segments within the hybrid text. This assessment aids in deciding whether (2.1) to employ a text segmentation-based strategy for hybrid texts with longer segments, or (2.2) to adopt a direct sentence-by-sentence classification strategy for those with shorter segments.","[{'name': \"Zijie Zeng, Shiqi Liu, Lele Sha, Zhuang Li, Kaixun Yang, Sannyuya Liu, Dragan Ga\\\\v{s}evi\\\\'c, Guanliang Chen\"}]",
1388,FingerNet: EEG Decoding of A Fine Motor Imagery with Finger-tapping Task Based on A Deep Neural Network,https://arxiv.org/abs/2403.03526,"arXiv:2403.03526v1 Announce Type: cross 
Abstract: Brain-computer interface (BCI) technology facilitates communication between the human brain and computers, primarily utilizing electroencephalography (EEG) signals to discern human intentions. Although EEG-based BCI systems have been developed for paralysis individuals, ongoing studies explore systems for speech imagery and motor imagery (MI). This study introduces FingerNet, a specialized network for fine MI classification, departing from conventional gross MI studies. The proposed FingerNet could extract spatial and temporal features from EEG signals, improving classification accuracy within the same hand. The experimental results demonstrated that performance showed significantly higher accuracy in classifying five finger-tapping tasks, encompassing thumb, index, middle, ring, and little finger movements. FingerNet demonstrated dominant performance compared to the conventional baseline models, EEGNet and DeepConvNet. The average accuracy for FingerNet was 0.3049, whereas EEGNet and DeepConvNet exhibited lower accuracies of 0.2196 and 0.2533, respectively. Statistical validation also demonstrates the predominance of FingerNet over baseline networks. For biased predictions, particularly for thumb and index classes, we led to the implementation of weighted cross-entropy and also adapted the weighted cross-entropy, a method conventionally employed to mitigate class imbalance. The proposed FingerNet involves optimizing network structure, improving performance, and exploring applications beyond fine MI. Moreover, the weighted Cross Entropy approach employed to address such biased predictions appears to have broader applicability and relevance across various domains involving multi-class classification tasks. We believe that effective execution of motor imagery can be achieved not only for fine MI, but also for local muscle MI","[{'name': 'Young-Min Go, Seong-Hyun Yu, Hyeong-Yeong Park, Minji Lee, Ji-Hoon Jeong'}]",
1389,Towards Efficient and Effective Unlearning of Large Language Models for Recommendation,https://arxiv.org/abs/2403.03536,"arXiv:2403.03536v1 Announce Type: cross 
Abstract: The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning. In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \\textit{inefficiency} and \\textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \\textbf{E2URec}, the first \\underline{E}fficient and \\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our proposed E2URec enhances the unlearning efficiency by updating only a few additional LoRA parameters, and improves the unlearning effectiveness by employing a teacher-student framework, where we maintain multiple teacher networks to guide the unlearning process. Extensive experiments show that E2URec outperforms state-of-the-art baselines on two real-world datasets. Specifically, E2URec can efficiently forget specific data without affecting recommendation performance. The source code is at \\url{https://github.com/justarter/E2URec}.","[{'name': 'Hangyu Wang, Jianghao Lin, Bo Chen, Yang Yang, Ruiming Tang, Weinan Zhang, Yong Yu'}]",
1390,RADIA -- Radio Advertisement Detection with Intelligent Analytics,https://arxiv.org/abs/2403.03538,"arXiv:2403.03538v1 Announce Type: cross 
Abstract: Radio advertising remains an integral part of modern marketing strategies, with its appeal and potential for targeted reach undeniably effective. However, the dynamic nature of radio airtime and the rising trend of multiple radio spots necessitates an efficient system for monitoring advertisement broadcasts. This study investigates a novel automated radio advertisement detection technique incorporating advanced speech recognition and text classification algorithms. RadIA's approach surpasses traditional methods by eliminating the need for prior knowledge of the broadcast content. This contribution allows for detecting impromptu and newly introduced advertisements, providing a comprehensive solution for advertisement detection in radio broadcasting. Experimental results show that the resulting model, trained on carefully segmented and tagged text data, achieves an F1-macro score of 87.76 against a theoretical maximum of 89.33. This paper provides insights into the choice of hyperparameters and their impact on the model's performance. This study demonstrates its potential to ensure compliance with advertising broadcast contracts and offer competitive surveillance. This groundbreaking research could fundamentally change how radio advertising is monitored and open new doors for marketing optimization.","[{'name': \"Jorge \\\\'Alvarez, Juan Carlos Armenteros, Camilo Torr\\\\'on, Miguel Ortega-Mart\\\\'in, Alfonso Ardoiz, \\\\'Oscar Garc\\\\'ia, Ignacio Arranz, \\\\'I\\\\~nigo Galdeano, Ignacio Garrido, Adri\\\\'an Alonso, Fernando Bay\\\\'on, Oleg Vorontsov\"}]",
1391,gaHealth: An English-Irish Bilingual Corpus of Health Data,https://arxiv.org/abs/2403.03575,"arXiv:2403.03575v1 Announce Type: cross 
Abstract: Machine Translation is a mature technology for many high-resource language pairs. However in the context of low-resource languages, there is a paucity of parallel data datasets available for developing translation models. Furthermore, the development of datasets for low-resource languages often focuses on simply creating the largest possible dataset for generic translation. The benefits and development of smaller in-domain datasets can easily be overlooked. To assess the merits of using in-domain data, a dataset for the specific domain of health was developed for the low-resource English to Irish language pair. Our study outlines the process used in developing the corpus and empirically demonstrates the benefits of using an in-domain dataset for the health domain. In the context of translating health-related data, models developed using the gaHealth corpus demonstrated a maximum BLEU score improvement of 22.2 points (40%) when compared with top performing models from the LoResMT2021 Shared Task. Furthermore, we define linguistic guidelines for developing gaHealth, the first bilingual corpus of health data for the Irish language, which we hope will be of use to other creators of low-resource data sets. gaHealth is now freely available online and is ready to be explored for further research.","[{'name': \"S\\\\'eamus Lankford, Haithem Afli, \\\\'Orla N\\\\'i Loinsigh, Andy Way\"}]",
1392,Causal Disentanglement for Regulating Social Influence Bias in Social Recommendation,https://arxiv.org/abs/2403.03578,"arXiv:2403.03578v1 Announce Type: cross 
Abstract: Social recommendation systems face the problem of social influence bias, which can lead to an overemphasis on recommending items that friends have interacted with. Addressing this problem is crucial, and existing methods often rely on techniques such as weight adjustment or leveraging unbiased data to eliminate this bias. However, we argue that not all biases are detrimental, i.e., some items recommended by friends may align with the user's interests. Blindly eliminating such biases could undermine these positive effects, potentially diminishing recommendation accuracy. In this paper, we propose a Causal Disentanglement-based framework for Regulating Social influence Bias in social recommendation, named CDRSB, to improve recommendation performance. From the perspective of causal inference, we find that the user social network could be regarded as a confounder between the user and item embeddings (treatment) and ratings (outcome). Due to the presence of this social network confounder, two paths exist from user and item embeddings to ratings: a non-causal social influence path and a causal interest path. Building upon this insight, we propose a disentangled encoder that focuses on disentangling user and item embeddings into interest and social influence embeddings. Mutual information-based objectives are designed to enhance the distinctiveness of these disentangled embeddings, eliminating redundant information. Additionally, a regulatory decoder that employs a weight calculation module to dynamically learn the weights of social influence embeddings for effectively regulating social influence bias has been designed. Experimental results on four large-scale real-world datasets Ciao, Epinions, Dianping, and Douban book demonstrate the effectiveness of CDRSB compared to state-of-the-art baselines.","[{'name': 'Li Wang, Min Xu, Quangui Zhang, Yunxiao Shi, Qiang Wu'}]",
1393,Design of an Open-Source Architecture for Neural Machine Translation,https://arxiv.org/abs/2403.03582,"arXiv:2403.03582v1 Announce Type: cross 
Abstract: adaptNMT is an open-source application that offers a streamlined approach to the development and deployment of Recurrent Neural Networks and Transformer models. This application is built upon the widely-adopted OpenNMT ecosystem, and is particularly useful for new entrants to the field, as it simplifies the setup of the development environment and creation of train, validation, and test splits. The application offers a graphing feature that illustrates the progress of model training, and employs SentencePiece for creating subword segmentation models. Furthermore, the application provides an intuitive user interface that facilitates hyperparameter customization. Notably, a single-click model development approach has been implemented, and models developed by adaptNMT can be evaluated using a range of metrics. To encourage eco-friendly research, adaptNMT incorporates a green report that flags the power consumption and kgCO${_2}$ emissions generated during model development. The application is freely available.","[{'name': \"S\\\\'eamus Lankford, Haithem Afli, Andy Way\"}]",
1394,RouteExplainer: An Explanation Framework for Vehicle Routing Problem,https://arxiv.org/abs/2403.03585,"arXiv:2403.03585v1 Announce Type: cross 
Abstract: The Vehicle Routing Problem (VRP) is a widely studied combinatorial optimization problem and has been applied to various practical problems. While the explainability for VRP is significant for improving the reliability and interactivity in practical VRP applications, it remains unexplored. In this paper, we propose RouteExplainer, a post-hoc explanation framework that explains the influence of each edge in a generated route. Our framework realizes this by rethinking a route as the sequence of actions and extending counterfactual explanations based on the action influence model to VRP. To enhance the explanation, we additionally propose an edge classifier that infers the intentions of each edge, a loss function to train the edge classifier, and explanation-text generation by Large Language Models (LLMs). We quantitatively evaluate our edge classifier on four different VRPs. The results demonstrate its rapid computation while maintaining reasonable accuracy, thereby highlighting its potential for deployment in practical applications. Moreover, on the subject of a tourist route, we qualitatively evaluate explanations generated by our framework. This evaluation not only validates our framework but also shows the synergy between explanation frameworks and LLMs. See https://ntt-dkiku.github.io/xai-vrp for our code, datasets, models, and demo.","[{'name': 'Daisuke Kikuta, Hiroki Ikeuchi, Kengo Tajiri, Yuusuke Nakano'}]",
1395,Wildest Dreams: Reproducible Research in Privacy-preserving Neural Network Training,https://arxiv.org/abs/2403.03592,"arXiv:2403.03592v1 Announce Type: cross 
Abstract: Machine Learning (ML), addresses a multitude of complex issues in multiple disciplines, including social sciences, finance, and medical research. ML models require substantial computing power and are only as powerful as the data utilized. Due to high computational cost of ML methods, data scientists frequently use Machine Learning-as-a-Service (MLaaS) to outsource computation to external servers. However, when working with private information, like financial data or health records, outsourcing the computation might result in privacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have enabled ML training and inference over protected data through the use of Privacy-Preserving Machine Learning (PPML). However, these techniques are still at a preliminary stage and their application in real-world situations is demanding. In order to comprehend discrepancy between theoretical research suggestions and actual applications, this work examines the past and present of PPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party Computation (SMPC) applied to ML. This work primarily focuses on the ML model's training phase, where maintaining user data privacy is of utmost importance. We provide a solid theoretical background that eases the understanding of current approaches and their limitations. In addition, we present a SoK of the most recent PPML frameworks for model training and provide a comprehensive comparison in terms of the unique properties and performances on standard benchmarks. Also, we reproduce the results for some of the papers and examine at what level existing works in the field provide support for open science. We believe our work serves as a valuable contribution by raising awareness about the current gap between theoretical advancements and real-world applications in PPML, specifically regarding open-source availability, reproducibility, and usability.","[{'name': 'Tanveer Khan, Mindaugas Budzys, Khoa Nguyen, Antonis Michalas'}]",
1396,Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem,https://arxiv.org/abs/2403.03593,"arXiv:2403.03593v1 Announce Type: cross 
Abstract: Training high-quality deep learning models is a challenging task due to computational and technical requirements. A growing number of individuals, institutions, and companies increasingly rely on pre-trained, third-party models made available in public repositories. These models are often used directly or integrated in product pipelines with no particular precautions, since they are effectively just data in tensor form and considered safe. In this paper, we raise awareness of a new machine learning supply chain threat targeting neural networks. We introduce MaleficNet 2.0, a novel technique to embed self-extracting, self-executing malware in neural networks. MaleficNet 2.0 uses spread-spectrum channel coding combined with error correction techniques to inject malicious payloads in the parameters of deep neural networks. MaleficNet 2.0 injection technique is stealthy, does not degrade the performance of the model, and is robust against removal techniques. We design our approach to work both in traditional and distributed learning settings such as Federated Learning, and demonstrate that it is effective even when a reduced number of bits is used for the model parameters. Finally, we implement a proof-of-concept self-extracting neural network malware using MaleficNet 2.0, demonstrating the practicality of the attack against a widely adopted machine learning framework. Our aim with this work is to raise awareness against these new, dangerous attacks both in the research community and industry, and we hope to encourage further research in mitigation techniques against such threats.","[{'name': 'Dorjan Hitaj, Giulio Pagnotta, Fabio De Gaspari, Sediola Ruko, Briland Hitaj, Luigi V. Mancini, Fernando Perez-Cruz'}]",
1397,Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators,https://arxiv.org/abs/2403.03606,"arXiv:2403.03606v1 Announce Type: cross 
Abstract: This study presents an innovative approach for predicting cryptocurrency time series, specifically focusing on Bitcoin, Ethereum, and Litecoin. The methodology integrates the use of technical indicators, a Performer neural network, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal dynamics and extract significant features from raw cryptocurrency data. The application of technical indicators, such facilitates the extraction of intricate patterns, momentum, volatility, and trends. The Performer neural network, employing Fast Attention Via positive Orthogonal Random features (FAVOR+), has demonstrated superior computational efficiency and scalability compared to the traditional Multi-head attention mechanism in Transformer models. Additionally, the integration of BiLSTM in the feedforward network enhances the model's capacity to capture temporal dynamics in the data, processing it in both forward and backward directions. This is particularly advantageous for time series data where past and future data points can influence the current state. The proposed method has been applied to the hourly and daily timeframes of the major cryptocurrencies and its performance has been benchmarked against other methods documented in the literature. The results underscore the potential of the proposed method to outperform existing models, marking a significant progression in the field of cryptocurrency price prediction.","[{'name': 'Mohammad Ali Labbaf Khaniki, Mohammad Manthouri'}]",
1398,GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding,https://arxiv.org/abs/2403.03608,"arXiv:2403.03608v1 Announce Type: cross 
Abstract: Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified.","[{'name': 'Zi-Ting Chou, Sheng-Yu Huang, I-Jieh Liu, Yu-Chiang Frank Wang'}]",
1399,Multimodal Large Language Models to Support Real-World Fact-Checking,https://arxiv.org/abs/2403.03627,"arXiv:2403.03627v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) carry the potential to support humans in processing vast amounts of information. While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied. Here is aim to bridge this gap. In particular, we propose a framework for systematically assessing the capacity of current multimodal models to facilitate real-world fact-checking. Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities. By designing prompts that extract models' predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure. We empirically find that (1) GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing open-source models exhibit strong biases and are highly sensitive to the prompt. Our study offers insights into combating false multimodal information and building secure, trustworthy multimodal models. To the best of our knowledge, we are the first to evaluate MLLMs for real-world fact-checking.","[{'name': 'Jiahui Geng, Yova Kementchedjhieva, Preslav Nakov, Iryna Gurevych'}]",
1400,Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People,https://arxiv.org/abs/2403.03640,"arXiv:2403.03640v1 Announce Type: cross 
Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a proxy-tuning fashion. We will open-source training corpora, code, model weights and evaluation benchmark.","[{'name': 'Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, Benyou Wang'}]",
1401,A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation,https://arxiv.org/abs/2403.03643,"arXiv:2403.03643v1 Announce Type: cross 
Abstract: The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefore, this paper aims to summarize and review recent theoretical methods and applied research utilizing reinforcement learning to address spatial resource allocation problems. It provides a summary and comprehensive overview of its fundamental principles, related methodologies, and applied research. Additionally, it highlights several unresolved issues that urgently require attention in this direction for the future.","[{'name': 'Di Zhang, Moyang Wang, Joseph Mango, Xiang Li'}]",
1402,General2Specialized LLMs Translation for E-commerce,https://arxiv.org/abs/2403.03689,"arXiv:2403.03689v1 Announce Type: cross 
Abstract: Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and robustness of our G2ST approach, as compared with state-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4.","[{'name': 'Kaidi Chen, Ben Chen, Dehong Gao, Huangyu Dai, Wen Jiang, Wei Ning, Shanqing Yu, Libin Yang, Xiaoyan Cai'}]",
1403,Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese,https://arxiv.org/abs/2403.03690,"arXiv:2403.03690v1 Announce Type: cross 
Abstract: The creation of instruction data and evaluation benchmarks for serving Large language models often involves enormous human annotation. This issue becomes particularly pronounced when rapidly developing such resources for a non-English language like Japanese. Instead of following the popular practice of directly translating existing English resources into Japanese (e.g., Japanese-Alpaca), we propose an efficient self-instruct method based on GPT-4. We first translate a small amount of English instructions into Japanese and post-edit them to obtain native-level quality. GPT-4 then utilizes them as demonstrations to automatically generate Japanese instruction data. We also construct an evaluation benchmark containing 80 questions across 8 categories, using GPT-4 to automatically assess the response quality of LLMs without human references. The empirical results suggest that the models fine-tuned on our GPT-4 self-instruct data significantly outperformed the Japanese-Alpaca across all three base pre-trained models. Our GPT-4 self-instruct data allowed the LLaMA 13B model to defeat GPT-3.5 (Davinci-003) with a 54.37\\% win-rate. The human evaluation exhibits the consistency between GPT-4's assessments and human preference. Our high-quality instruction data and evaluation benchmark have been released here.","[{'name': 'Yikun Sun, Zhen Wan, Nobuhiro Ueda, Sakiko Yahata, Fei Cheng, Chenhui Chu, Sadao Kurohashi'}]",
1404,MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition,https://arxiv.org/abs/2403.03691,"arXiv:2403.03691v1 Announce Type: cross 
Abstract: In the field of chemical structure recognition, the task of converting molecular images into graph structures and SMILES string stands as a significant challenge, primarily due to the varied drawing styles and conventions prevalent in chemical literature. To bridge this gap, we proposed MolNexTR, a novel image-to-graph deep learning model that collaborates to fuse the strengths of ConvNext, a powerful Convolutional Neural Network variant, and Vision-TRansformer. This integration facilitates a more nuanced extraction of both local and global features from molecular images. MolNexTR can predict atoms and bonds simultaneously and understand their layout rules. It also excels at flexibly integrating symbolic chemistry principles to discern chirality and decipher abbreviated structures. We further incorporate a series of advanced algorithms, including improved data augmentation module, image contamination module, and a post-processing module to get the final SMILES output. These modules synergistically enhance the model's robustness against the diverse styles of molecular imagery found in real literature. In our test sets, MolNexTR has demonstrated superior performance, achieving an accuracy rate of 81-97%, marking a significant advancement in the domain of molecular structure recognition. Scientific contribution: MolNexTR is a novel image-to-graph model that incorporates a unique dual-stream encoder to extract complex molecular image features, and combines chemical rules to predict atoms and bonds while understanding atom and bond layout rules. In addition, it employs a series of novel augmentation algorithms to significantly enhance the robustness and performance of the model.","[{'name': 'Yufan Chen, Ching Ting Leung, Yong Huang, Jianwei Sun, Hao Chen, Hanyu Gao'}]",
1405,Towards Controllable Time Series Generation,https://arxiv.org/abs/2403.03698,"arXiv:2403.03698v1 Announce Type: cross 
Abstract: Time Series Generation (TSG) has emerged as a pivotal technique in synthesizing data that accurately mirrors real-world time series, becoming indispensable in numerous applications. Despite significant advancements in TSG, its efficacy frequently hinges on having large training datasets. This dependency presents a substantial challenge in data-scarce scenarios, especially when dealing with rare or unique conditions. To confront these challenges, we explore a new problem of Controllable Time Series Generation (CTSG), aiming to produce synthetic time series that can adapt to various external conditions, thereby tackling the data scarcity issue.
  In this paper, we propose \\textbf{C}ontrollable \\textbf{T}ime \\textbf{S}eries (\\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG. A key feature of \\textsf{CTS} is that it decouples the mapping process from standard VAE training, enabling precise learning of a complex interplay between latent features and external conditions. Moreover, we develop a comprehensive evaluation scheme for CTSG. Extensive experiments across three real-world time series datasets showcase \\textsf{CTS}'s exceptional capabilities in generating high-quality, controllable outputs. This underscores its adeptness in seamlessly integrating latent features with external conditions. Extending \\textsf{CTS} to the image domain highlights its remarkable potential for explainability and further reinforces its versatility across different modalities.","[{'name': 'Yifan Bao, Yihao Ang, Qiang Huang, Anthony K. H. Tung, Zhiyong Huang'}]",
1406,Diffusion on language model embeddings for protein sequence generation,https://arxiv.org/abs/2403.03726,"arXiv:2403.03726v1 Announce Type: cross 
Abstract: Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accurately reflect the inherent structural and functional diversity of the protein space. This work advances the field of protein design and sets the stage for conditional models by providing a robust framework for scalable and high-quality protein sequence generation.","[{'name': 'Viacheslav Meshchaninov, Pavel Strashnov, Andrey Shevtsov, Fedor Nikolaev, Nikita Ivanisenko, Olga Kardymon, Dmitry Vetrov'}]",
1407,Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training,https://arxiv.org/abs/2403.03728,"arXiv:2403.03728v1 Announce Type: cross 
Abstract: This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models. We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies. Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes.","[{'name': 'Paul Doucet, Benjamin Estermann, Till Aczel, Roger Wattenhofer'}]",
1408,Learning 3D object-centric representation through prediction,https://arxiv.org/abs/2403.03730,"arXiv:2403.03730v1 Announce Type: cross 
Abstract: As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic reasoning. While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking. Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion. The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes. This results in object representations being learned as an essential byproduct of learning to predict.","[{'name': 'John Day, Tushar Arora, Jirui Liu, Li Erran Li, Ming Bo Cai'}]",
1409,A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network,https://arxiv.org/abs/2403.03739,"arXiv:2403.03739v1 Announce Type: cross 
Abstract: Binary neural networks utilize 1-bit quantized weights and activations to reduce both the model's storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&amp;B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the quantized RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The quantized RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10, CIFAR-100, and ImageNet datasets, respectively, which are competitive with the state-of-the-art. Ablation studies have verified the efficacy of the quantized RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to using a fixed slope RLeakyReLU. The proposed add&amp;bit-operation-only BNN offers an innovative approach for hardware-friendly network architecture.","[{'name': 'Ruichen Ma, Guanchao Qiao, Yian Liu, Liwei Meng, Ning Ning, Yang Liu, Shaogang Hu'}]",
1410,SUPClust: Active Learning at the Boundaries,https://arxiv.org/abs/2403.03741,"arXiv:2403.03741v1 Announce Type: cross 
Abstract: Active learning is a machine learning paradigm designed to optimize model performance in a setting where labeled data is expensive to acquire. In this work, we propose a novel active learning method called SUPClust that seeks to identify points at the decision boundary between classes. By targeting these points, SUPClust aims to gather information that is most informative for refining the model's prediction of complex decision regions. We demonstrate experimentally that labeling these points leads to strong model performance. This improvement is observed even in scenarios characterized by strong class imbalance.","[{'name': 'Yuta Ono, Till Aczel, Benjamin Estermann, Roger Wattenhofer'}]",
1411,German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset,https://arxiv.org/abs/2403.03750,"arXiv:2403.03750v1 Announce Type: cross 
Abstract: The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings. We open-source and release the absinth dataset to foster further research on hallucination detection in German.","[{'name': 'Laura Mascarell, Ribin Chalumattu, Annette Rios'}]",
1412,ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport,https://arxiv.org/abs/2403.03777,"arXiv:2403.03777v1 Announce Type: cross 
Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our method, called Expectile-Regularised Neural Optimal Transport (ENOT). ENOT outperforms previous state-of-the-art approaches on the Wasserstein-2 benchmark tasks by a large margin (up to a 3-fold improvement in quality and up to a 10-fold improvement in runtime).","[{'name': 'Nazar Buzun, Maksim Bobrin, Dmitry V. Dylov'}]",
1413,Neural Architecture Search using Particle Swarm and Ant Colony Optimization,https://arxiv.org/abs/2403.03781,"arXiv:2403.03781v1 Announce Type: cross 
Abstract: Neural network models have a number of hyperparameters that must be chosen along with their architecture. This can be a heavy burden on a novice user, choosing which architecture and what values to assign to parameters. In most cases, default hyperparameters and architectures are used. Significant improvements to model accuracy can be achieved through the evaluation of multiple architectures. A process known as Neural Architecture Search (NAS) may be applied to automatically evaluate a large number of such architectures. A system integrating open source tools for Neural Architecture Search (OpenNAS), in the classification of images, has been developed as part of this research. OpenNAS takes any dataset of grayscale, or RBG images, and generates Convolutional Neural Network (CNN) architectures based on a range of metaheuristics using either an AutoKeras, a transfer learning or a Swarm Intelligence (SI) approach. Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO) are used as the SI algorithms. Furthermore, models developed through such metaheuristics may be combined using stacking ensembles. In the context of this paper, we focus on training and optimizing CNNs using the Swarm Intelligence (SI) components of OpenNAS. Two major types of SI algorithms, namely PSO and ACO, are compared to see which is more effective in generating higher model accuracies. It is shown, with our experimental design, that the PSO algorithm performs better than ACO. The performance improvement of PSO is most notable with a more complex dataset. As a baseline, the performance of fine-tuned pre-trained models is also evaluated.","[{'name': \"S\\\\'eamus Lankford, Diarmuid Grimes\"}]",
1414,KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs,https://arxiv.org/abs/2403.03791,"arXiv:2403.03791v1 Announce Type: cross 
Abstract: Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes. Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data. To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships. KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average improvement of 7% in Area under the ROC Curve (AUC) and 9% in Influence Function-based Precision of Estimating Heterogeneous Effects (IF-PEHE). The effectiveness of our estimated treatment effects is further affirmed by alignment with established randomized clinical trial findings.","[{'name': 'Ruoqi Liu, Lingfei Wu, Ping Zhang'}]",
1415,Confidence-Aware Decision-Making and Control for Tool Selection,https://arxiv.org/abs/2403.03808,"arXiv:2403.03808v1 Announce Type: cross 
Abstract: Self-reflecting about our performance (e.g., how confident we are) before doing a task is essential for decision making, such as selecting the most suitable tool or choosing the best route to drive. While this form of awareness -- thinking about our performance or metacognitive performance -- is well-known in humans, robots still lack this cognitive ability. This reflective monitoring can enhance their embodied decision power, robustness and safety. Here, we take a step in this direction by introducing a mathematical framework that allows robots to use their control self-confidence to make better-informed decisions. We derive a mathematical closed-form expression for control confidence for dynamic systems (i.e., the posterior inverse covariance of the control action). This control confidence seamlessly integrates within an objective function for decision making, that balances the: i) performance for task completion, ii) control effort, and iii) self-confidence. To evaluate our theoretical account, we framed the decision-making within the tool selection problem, where the agent has to select the best robot arm for a particular control task. The statistical analysis of the numerical simulations with randomized 2DOF arms shows that using control confidence during tool selection improves both real task performance, and the reliability of the tool for performance under unmodelled perturbations (e.g., external forces). Furthermore, our results indicate that control confidence is an early indicator of performance and thus, it can be used as a heuristic for making decisions when computation power is restricted or decision-making is intractable. Overall, we show the advantages of using confidence-aware decision-making and control scheme for dynamic systems.","[{'name': 'Ajith Anil Meera, Pablo Lanillos'}]",
1416,ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing,https://arxiv.org/abs/2403.03812,"arXiv:2403.03812v1 Announce Type: cross 
Abstract: Used car pricing is a critical aspect of the automotive industry, influenced by many economic factors and market dynamics. With the recent surge in online marketplaces and increased demand for used cars, accurate pricing would benefit both buyers and sellers by ensuring fair transactions. However, the transition towards automated pricing algorithms using machine learning necessitates the comprehension of model uncertainties, specifically the ability to flag predictions that the model is unsure about. Although recent literature proposes the use of boosting algorithms or nearest neighbor-based approaches for swift and precise price predictions, encapsulating model uncertainties with such algorithms presents a complex challenge. We introduce ProbSAINT, a model that offers a principled approach for uncertainty quantification of its price predictions, along with accurate point predictions that are comparable to state-of-the-art boosting techniques. Furthermore, acknowledging that the business prefers pricing used cars based on the number of days the vehicle was listed for sale, we show how ProbSAINT can be used as a dynamic forecasting model for predicting price probabilities for different expected offer duration. Our experiments further indicate that ProbSAINT is especially accurate on instances where it is highly certain. This proves the applicability of its probabilistic predictions in real-world scenarios where trustworthiness is crucial.","[{'name': 'Kiran Madhusudhanan, Gunnar Behrens, Maximilian Stubbemann, Lars Schmidt-Thieme'}]",
1417,Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ,https://arxiv.org/abs/2403.03814,"arXiv:2403.03814v1 Announce Type: cross 
Abstract: Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\\ whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages beyond their intended use. Most models are more accurate when they respond faithfully. However, differences across models are large, and there is a long tail of languages where models are neither accurate nor faithful. We explore differences in tokenization as a potential explanation for our findings, identifying possible correlations that warrant further investigation.","[{'name': 'Carolin Holtermann, Paul R\\\\\"ottger, Timm Dill, Anne Lauscher'}]",
1418,Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning,https://arxiv.org/abs/2403.03835,"arXiv:2403.03835v1 Announce Type: cross 
Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.","[{'name': 'Xin Lian, Sashank Varma, Christopher J. MacLellan'}]",
1419,"Accelerating Convergence of Score-Based Diffusion Models, Provably",https://arxiv.org/abs/2403.03852,"arXiv:2403.03852v1 Announce Type: cross 
Abstract: Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\\sqrt{T})$ for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like the DPM-Solver-2. Our theory accommodates $\\ell_2$-accurate score estimates, and does not require log-concavity or smoothness on the target distribution.","[{'name': 'Gen Li, Yu Huang, Timofey Efimov, Yuting Wei, Yuejie Chi, Yuxin Chen'}]",
1420,Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning,https://arxiv.org/abs/2403.03864,"arXiv:2403.03864v1 Announce Type: cross 
Abstract: This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size. Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles. The findings emphasize the challenges of integrating visual, language, and algorithmic knowledge for solving complex reasoning problems.","[{'name': 'Deepanway Ghosal, Vernon Toh Yan Han, Chia Yew Ken, Soujanya Poria'}]",
1421,Impoverished Language Technology: The Lack of (Social) Class in NLP,https://arxiv.org/abs/2403.03874,"arXiv:2403.03874v1 Announce Type: cross 
Abstract: Since Labov's (1964) foundational work on the social stratification of language, linguistics has dedicated concerted efforts towards understanding the relationships between socio-demographic factors and language production and perception. Despite the large body of evidence identifying significant relationships between socio-demographic factors and language production, relatively few of these factors have been investigated in the context of NLP technology. While age and gender are well covered, Labov's initial target, socio-economic class, is largely absent. We survey the existing Natural Language Processing (NLP) literature and find that only 20 papers even mention socio-economic status. However, the majority of those papers do not engage with class beyond collecting information of annotator-demographics. Given this research lacuna, we provide a definition of class that can be operationalised by NLP researchers, and argue for including socio-economic class in future language technologies.","[{'name': 'Amanda Cercas Curry, Zeerak Talat, Dirk Hovy'}]",
1422,Redefining cystoscopy with ai: bladder cancer diagnosis using an efficient hybrid cnn-transformer model,https://arxiv.org/abs/2403.03879,"arXiv:2403.03879v1 Announce Type: cross 
Abstract: Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and is among the most expensive cancers to treat due to the high recurrence rates which require lifetime follow-ups. The primary tool for diagnosis is cystoscopy, which heavily relies on doctors' expertise and interpretation. Therefore, annually, numerous cases are either undiagnosed or misdiagnosed and treated as urinary infections. To address this, we suggest a deep learning approach for bladder cancer detection and segmentation which combines CNNs with a lightweight positional-encoding-free transformer and dual attention gates that fuse self and spatial attention for feature enhancement. The architecture suggested in this paper is efficient making it suitable for medical scenarios that require real time inference. Experiments have proven that this model addresses the critical need for a balance between computational efficiency and diagnostic accuracy in cystoscopic imaging as despite its small size it rivals large models in performance.","[{'name': 'Meryem Amaouche, Ouassim Karrakchou, Mounir Ghogho, Anouar El Ghazzaly, Mohamed Alami, Ahmed Ameur'}]",
1423,Latent Dataset Distillation with Diffusion Models,https://arxiv.org/abs/2403.03881,"arXiv:2403.03881v1 Announce Type: cross 
Abstract: The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both challenges. LD3M incorporates a novel diffusion process tailored for dataset distillation, which improves the gradient norms for learning synthetic images. By adjusting the number of diffusion steps, LD3M also offers a straightforward way of controlling the trade-off between speed and accuracy. We evaluate our approach in several ImageNet subsets and for high-resolution images (128x128 and 256x256). As a result, LD3M consistently outperforms state-of-the-art distillation techniques by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.","[{'name': 'Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel'}]",
1424,Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation,https://arxiv.org/abs/2403.03890,"arXiv:2403.03890v1 Announce Type: cross 
Abstract: This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation. HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories. The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions. To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and distill the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint position diffuser via differentiable kinematics. Empirically, we show that HDP achieves a significantly higher success rate than the state-of-the-art methods in both simulation and real-world.","[{'name': 'Xiao Ma, Sumit Patidar, Iain Haughton, Stephen James'}]",
1425,From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models,https://arxiv.org/abs/2403.03893,"arXiv:2403.03893v1 Announce Type: cross 
Abstract: To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it's crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages. Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field. Code and data are available at https://github.com/for-ai/goodtriever.","[{'name': 'Luiza Pozzobon, Patrick Lewis, Sara Hooker, Beyza Ermis'}]",
1426,Extreme Precipitation Nowcasting using Transformer-based Generative Models,https://arxiv.org/abs/2403.03929,"arXiv:2403.03929v1 Announce Type: cross 
Abstract: This paper presents an innovative approach to extreme precipitation nowcasting by employing Transformer-based generative models, namely NowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a comprehensive dataset from the Royal Netherlands Meteorological Institute (KNMI), our study focuses on predicting short-term precipitation with high accuracy. We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events. We present both qualitative and quantitative analyses, demonstrating the superior performance of the proposed NowcastingGPT-EVL in generating accurate precipitation forecasts, especially when dealing with extreme precipitation events. The code is available at \\url{https://github.com/Cmeo97/NowcastingGPT}.","[{'name': 'Cristian Meo, Ankush Roy, Mircea Lic\\\\u{a}, Junzhe Yin, Zeineb Bou Che, Yanbo Wang, Ruben Imhoff, Remko Uijlenhoet, Justin Dauwels'}]",
1427,Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation,https://arxiv.org/abs/2403.03949,"arXiv:2403.03949v1 Announce Type: cross 
Abstract: Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in \"digital twin\" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel \"inverse distillation\" procedure for bringing real-world demonstrations into simulated environments for efficient fine-tuning, with minimal human intervention and engineering required. We evaluate RialTo across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf, and six other tasks. RialTo increases (over 67%) in policy robustness without requiring extensive human data collection. Project website and videos at https://real-to-sim-to-real.github.io/RialTo/","[{'name': 'Marcel Torne, Anthony Simeonov, Zechu Li, April Chan, Tao Chen, Abhishek Gupta, Pulkit Agrawal'}]",
1428,Stop Regressing: Training Value Functions via Classification for Scalable Deep RL,https://arxiv.org/abs/2403.03950,"arXiv:2403.03950v1 Announce Type: cross 
Abstract: Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity. Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost.","[{'name': 'Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Ta\\\\\"iga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Aviral Kumar, Rishabh Agarwal'}]",
1429,Decision-making with Speculative Opponent Models,https://arxiv.org/abs/2211.11940,"arXiv:2211.11940v2 Announce Type: replace 
Abstract: Opponent modeling has benefited a controlled agent's decision-making by constructing models of other agents. Existing methods commonly assume access to opponents' observations and actions, which is infeasible when opponents' behaviors are unobservable or hard to obtain. We propose a novel multi-agent distributional actor-critic algorithm to achieve speculative opponent modeling with purely local information (i.e., the controlled agent's observations, actions, and rewards). Specifically, the actor maintains a speculated belief of the opponents, which we call the speculative opponent models, to predict opponent actions using local observations and makes decisions accordingly. Further, the distributional critic models the return distribution of the policy. It reflects the quality of the actor and thus can guide the training of the speculative opponent model that the actor relies on. Extensive experiments confirm that our method successfully models opponents' behaviors without their data and delivers superior performance against baseline methods with a faster convergence speed.","[{'name': 'Jing Sun, Shuo Chen, Cong Zhang, Jie Zhang'}]",
1430,Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model,https://arxiv.org/abs/2307.08424,"arXiv:2307.08424v3 Announce Type: replace 
Abstract: Model inversion attacks (MIAs) aim to recover private data from inaccessible training sets of deep learning models, posing a privacy threat. MIAs primarily focus on the white-box scenario where attackers have full access to the model's structure and parameters. However, practical applications are usually in black-box scenarios or label-only scenarios, i.e., the attackers can only obtain the output confidence vectors or labels by accessing the model. Therefore, the attack models in existing MIAs are difficult to effectively train with the knowledge of the target model, resulting in sub-optimal attacks. To the best of our knowledge, we pioneer the research of a powerful and practical attack model in the label-only scenario.
  In this paper, we develop a novel MIA method, leveraging a conditional diffusion model (CDM) to recover representative samples under the target label from the training set. Two techniques are introduced: selecting an auxiliary dataset relevant to the target model task and using predicted labels as conditions to guide training CDM; and inputting target label, pre-defined guidance strength, and random noise into the trained attack model to generate and correct multiple results for final selection. This method is evaluated using Learned Perceptual Image Patch Similarity as a new metric and as a judgment basis for deciding the values of hyper-parameters. Experimental results show that this method can generate similar and accurate samples to the target label, outperforming generators of previous approaches.","[{'name': 'Rongke Liu, Dong Wang, Yizhi Ren, Zhen Wang, Kaitian Guo, Qianqian Qin, Xiaolei Liu'}]",
1431,Distinctive pupil and microsaccade-rate signatures in self-recognition,https://arxiv.org/abs/2307.15239,"arXiv:2307.15239v3 Announce Type: replace 
Abstract: Visual fixation is an active process with pupil dynamics as well as fixational eye movements and microsaccades that support perception. Measures of both pupil contraction and microsaccades are known to be sensitive to ongoing cognition and emotional processing. Here we present experimental results from a visual fixation task demonstrating that pupil size and microsaccade rate respond differently during self-recognition (when seeing one's own face) than when seeing familiar or unfamiliar faces. First, the pupil response is characterized by an immediate pupil-constriction followed by later dilation in response to stimulus onsets. For one's own face, we observe muted constriction and greater dilation compared to other faces. Second, microsaccades, which generally show an inhibitory response to incoming stimuli, are more strongly inhibited in response to one's own face compared to other faces. Our results lend support to the idea that eye-related physiological measures could be useful for authentication or exposing hidden knowledge.","[{'name': 'Lisa Schwetlick, Hendrik Graupner, Olaf Dimigen, Ralf Engbert'}]",
1432,Arithmetic with Language Models: from Memorization to Computation,https://arxiv.org/abs/2308.01154,"arXiv:2308.01154v3 Announce Type: replace 
Abstract: A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypothesis that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation.","[{'name': 'Davide Maltoni, Matteo Ferrara'}]",
1433,State-transition dynamics of resting-state functional magnetic resonance imaging data: Model comparison and test-to-retest analysis,https://arxiv.org/abs/2308.11910,"arXiv:2308.11910v2 Announce Type: replace 
Abstract: Electroencephalogram (EEG) microstate analysis entails finding dynamics of quasi-stable and generally recurrent discrete states in multichannel EEG time series data and relating properties of the estimated state-transition dynamics to observables such as cognition and behavior. While microstate analysis has been widely employed to analyze EEG data, its use remains less prevalent in functional magnetic resonance imaging (fMRI) data, largely due to the slower timescale of such data. In the present study, we extend various data clustering methods used in EEG microstate analysis to resting-state fMRI data from healthy humans to extract their state-transition dynamics. We show that the quality of clustering is on par with that for various microstate analyses of EEG data. We then develop a method for examining test-retest reliability of the discrete-state transition dynamics between fMRI sessions and show that the within-participant test-retest reliability is higher than between-participant test-retest reliability for different indices of state-transition dynamics, different networks, and different data sets. This result suggests that state-transition dynamics analysis of fMRI data could discriminate between different individuals and is a promising tool for performing fingerprinting analysis of individuals.","[{'name': 'Saiful Islam, Pitambar Khanra, Johan Nakuci, Sarah F. Muldoon, Takamitsu Watanabe, Naoki Masuda'}]",
1434,PyGraft: Configurable Generation of Synthetic Schemas and Knowledge Graphs at Your Fingertips,https://arxiv.org/abs/2309.03685,"arXiv:2309.03685v2 Announce Type: replace 
Abstract: Knowledge graphs (KGs) have emerged as a prominent data representation and management paradigm. Being usually underpinned by a schema (e.g., an ontology), KGs capture not only factual information but also contextual knowledge. In some tasks, a few KGs established themselves as standard benchmarks. However, recent works outline that relying on a limited collection of datasets is not sufficient to assess the generalization capability of an approach. In some data-sensitive fields such as education or medicine, access to public datasets is even more limited. To remedy the aforementioned issues, we release PyGraft, a Python-based tool that generates highly customized, domain-agnostic schemas and KGs. The synthesized schemas encompass various RDFS and OWL constructs, while the synthesized KGs emulate the characteristics and scale of real-world KGs. Logical consistency of the generated resources is ultimately ensured by running a description logic (DL) reasoner. By providing a way of generating both a schema and KG in a single pipeline, PyGraft's aim is to empower the generation of a more diverse array of KGs for benchmarking novel approaches in areas such as graph-based machine learning (ML), or more generally KG processing. In graph-based ML in particular, this should foster a more holistic evaluation of model performance and generalization capability, thereby going beyond the limited collection of available benchmarks. PyGraft is available at: https://github.com/nicolas-hbt/pygraft.","[{'name': \"Nicolas Hubert, Pierre Monnin, Mathieu d'Aquin, Davy Monticolo, Armelle Brun\"}]",
1435,On Generating Explanations for Reinforcement Learning Policies: An Empirical Study,https://arxiv.org/abs/2309.16960,"arXiv:2309.16960v2 Announce Type: replace 
Abstract: Understanding a \\textit{reinforcement learning} policy, which guides state-to-action mappings to maximize rewards, necessitates an accompanying explanation for human comprehension. In this paper, we introduce a set of \\textit{linear temporal logic} (LTL) formulae designed to provide explanations for policies, and an algorithm for searching through those formulae for the one that best explains a given policy. Our focus is on crafting explanations that elucidate both the ultimate objectives accomplished by the policy and the prerequisite conditions it upholds throughout its execution. These LTL-based explanations feature a structured representation, which is particularly well-suited for local-search techniques. The effectiveness of our proposed approach is illustrated through a simulated game of capture the flag and a car-parking environment. The paper concludes with suggested directions for future","[{'name': 'Mikihisa Yuasa, Huy T. Tran, Ramavarapu S. Sreenivas'}]",
1436,A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models,https://arxiv.org/abs/2310.00194,"arXiv:2310.00194v3 Announce Type: replace 
Abstract: Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. To address this, we take inspiration from the human brain, in which planning is accomplished via the recurrent interaction of specialized modules in the prefrontal cortex (PFC). These modules perform functions such as conflict monitoring, state prediction, state evaluation, task decomposition, and task coordination. We find that LLMs are sometimes capable of carrying out these functions in isolation, but struggle to autonomously coordinate them in the service of a goal. Therefore, we propose a black box architecture with multiple LLM-based (GPT-4) modules. The architecture improves planning through the interaction of specialized PFC-inspired modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate the combined architecture on three challenging planning tasks -- graph traversal, Tower of Hanoi, and logistics -- finding that it yields significant improvements over standard LLM methods (e.g., zero-shot prompting, in-context learning, and chain-of-thought). These results demonstrate the benefit of utilizing knowledge from cognitive neuroscience to improve planning in LLMs.","[{'name': 'Taylor Webb, Shanka Subhra Mondal, Chi Wang, Brian Krabach, Ida Momennejad'}]",
1437,Self-Supervised Deconfounding Against Spatio-Temporal Shifts: Theory and Modeling,https://arxiv.org/abs/2311.12472,"arXiv:2311.12472v2 Announce Type: replace 
Abstract: As an important application of spatio-temporal (ST) data, ST traffic forecasting plays a crucial role in improving urban travel efficiency and promoting sustainable development. In practice, the dynamics of traffic data frequently undergo distributional shifts attributed to external factors such as time evolution and spatial differences. This entails forecasting models to handle the out-of-distribution (OOD) issue where test data is distributed differently from training data. In this work, we first formalize the problem by constructing a causal graph of past traffic data, future traffic data, and external ST contexts. We reveal that the failure of prior arts in OOD traffic data is due to ST contexts acting as a confounder, i.e., the common cause for past data and future ones. Then, we propose a theoretical solution named Disentangled Contextual Adjustment (DCA) from a causal lens. It differentiates invariant causal correlations against variant spurious ones and deconfounds the effect of ST contexts. On top of that, we devise a Spatio-Temporal sElf-superVised dEconfounding (STEVE) framework. It first encodes traffic data into two disentangled representations for associating invariant and variant ST contexts. Then, we use representative ST contexts from three conceptually different perspectives (i.e., temporal, spatial, and semantic) as self-supervised signals to inject context information into both representations. In this way, we improve the generalization ability of the learned context-oriented representations to OOD ST traffic forecasting. Comprehensive experiments on four large-scale benchmark datasets demonstrate that our STEVE consistently outperforms the state-of-the-art baselines across various ST OOD scenarios.","[{'name': 'Jiahao Ji, Wentao Zhang, Jingyuan Wang, Yue He, Chao Huang'}]",
1438,Explicitly explainable AI solution to the AI black box problem,https://arxiv.org/abs/2401.03093,"arXiv:2401.03093v2 Announce Type: replace 
Abstract: Artificial intelligence based on neural networks has made significant progress. However, there are concerns about the reliability and security of this approach due to its lack of transparency. This is the black box problem of AI. Here we show how this problem can be solved using symbolic AI, which has a transparent white box nature. The widespread use of symbolic AI is hindered by the opacity of mathematical models and natural language terms, the lack of a unified ontology, and the combinatorial explosion of search options. To solve the AI black box problem and to implement general-purpose symbolic AI, we propose to use deterministic logic cellular automata with rules based on first principles of the general theory of the relevant domain. In this case, the general theory of the relevant domain plays the role of a knowledge base for the cellular automaton inference. A cellular automaton implements automatic parallel logical inference at three levels of organization of a complex system. Our verification of several ecological hypotheses provides a successful precedent for the implementation of white-box AI. Finally, we discuss a program for creating a general-purpose symbolic AI capable of processing knowledge and ensuring the reliability and safety of automated decisions.","[{'name': 'V. L. Kalmykov, L. V. Kalmykov'}]",
1439,Integration of cognitive tasks into artificial general intelligence test for large models,https://arxiv.org/abs/2402.02547,"arXiv:2402.02547v2 Announce Type: replace 
Abstract: During the evolution of large models, performance evaluation is necessarily performed to assess their capabilities and ensure safety before practical application. However, current model evaluations mainly rely on specific tasks and datasets, lacking a united framework for assessing the multidimensional intelligence of large models. In this perspective, we advocate for a comprehensive framework of cognitive science-inspired artificial general intelligence (AGI) tests, aimed at fulfilling the testing needs of large models with enhanced capabilities. The cognitive science-inspired AGI tests encompass the full spectrum of intelligence facets, including crystallized intelligence, fluid intelligence, social intelligence, and embodied intelligence. To assess the multidimensional intelligence of large models, the AGI tests consist of a battery of well-designed cognitive tests adopted from human intelligence tests, and then naturally encapsulates into an immersive virtual community. We propose increasing the complexity of AGI testing tasks commensurate with advancements in large models and emphasizing the necessity for the interpretation of test results to avoid false negatives and false positives. We believe that cognitive science-inspired AGI tests will effectively guide the targeted improvement of large models in specific dimensions of intelligence and accelerate the integration of large models into human society.","[{'name': 'Youzhi Qu, Chen Wei, Penghui Du, Wenxin Che, Chi Zhang, Wanli Ouyang, Yatao Bian, Feiyang Xu, Bin Hu, Kai Du, Haiyan Wu, Jia Liu, Quanying Liu'}]",
1440,Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction,https://arxiv.org/abs/2402.04154,"arXiv:2402.04154v3 Announce Type: replace 
Abstract: Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning. However, these works encounter challenges in extending their capabilities to new tasks. Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction. However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks. This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a \"read-to-play\" capability. Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer. Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities.","[{'name': 'Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu Xiang, Shawn Yue, Stephen W. Huang, Wenhu Chen, Zhaofeng He, Jie Fu'}]",
1441,Prompt Learning on Temporal Interaction Graphs,https://arxiv.org/abs/2402.06326,"arXiv:2402.06326v2 Announce Type: replace 
Abstract: Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict'' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios.
  Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straightforward. The application of prompting in static graph contexts falls short in temporal settings due to a lack of consideration for time-sensitive dynamics and a deficiency in expressive power. To address this issue, we introduce Temporal Interaction Graph Prompting (TIGPrompt), a versatile framework that seamlessly integrates with TIG models, bridging both the temporal and semantic gaps. In detail, we propose a temporal prompt generator to offer temporally-aware prompts for different tasks. These prompts stand out for their minimalistic design, relying solely on the tuning of the prompt generator with very little supervision data. To cater to varying computational resource demands, we propose an extended ``pre-train, prompt-based fine-tune'' paradigm, offering greater flexibility. Through extensive experiments, the TIGPrompt demonstrates the SOTA performance and remarkable efficiency advantages.","[{'name': 'Xi Chen, Siwei Zhang, Yun Xiong, Xixi Wu, Jiawei Zhang, Xiangguo Sun, Yao Zhang, Feng Zhao, Yulin Kang'}]",
1442,Brant-2: Foundation Model for Brain Signals,https://arxiv.org/abs/2402.10251,"arXiv:2402.10251v3 Announce Type: replace 
Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintain performance in scenarios with scarce labels. The source code and pre-trained weights are available at: https://github.com/yzz673/Brant-2.","[{'name': 'Zhizhang Yuan, Daoze Zhang, Junru Chen, Gefei Gu, Yang Yang'}]",
1443,The Who in XAI: How AI Background Shapes Perceptions of AI Explanations,https://arxiv.org/abs/2107.13509,"arXiv:2107.13509v2 Announce Type: replace-cross 
Abstract: Explainability of AI systems is critical for users to take informed actions. Understanding \"who\" opens the black-box of AI is just as important as opening it. We conduct a mixed-methods study of how two different groups--people with and without AI background--perceive different types of AI explanations. Quantitatively, we share user perceptions along five dimensions. Qualitatively, we describe how AI background can influence interpretations, elucidating the differences through lenses of appropriation and cognitive heuristics. We find that (1) both groups showed unwarranted faith in numbers for different reasons and (2) each group found value in different explanations beyond their intended design. Carrying critical implications for the field of XAI, our findings showcase how AI generated explanations can have negative consequences despite best intentions and how that could lead to harmful manipulation of trust. We propose design interventions to mitigate them.","[{'name': 'Upol Ehsan, Samir Passi, Q. Vera Liao, Larry Chan, I-Hsiang Lee, Michael Muller, Mark O. Riedl'}]",
1444,SemSegDepth: A Combined Model for Semantic Segmentation and Depth Completion,https://arxiv.org/abs/2209.00381,"arXiv:2209.00381v2 Announce Type: replace-cross 
Abstract: Holistic scene understanding is pivotal for the performance of autonomous machines. In this paper we propose a new end-to-end model for performing semantic segmentation and depth completion jointly. The vast majority of recent approaches have developed semantic segmentation and depth completion as independent tasks. Our approach relies on RGB and sparse depth as inputs to our model and produces a dense depth map and the corresponding semantic segmentation image. It consists of a feature extractor, a depth completion branch, a semantic segmentation branch and a joint branch which further processes semantic and depth information altogether. The experiments done on Virtual KITTI 2 dataset, demonstrate and provide further evidence, that combining both tasks, semantic segmentation and depth completion, in a multi-task network can effectively improve the performance of each task. Code is available at https://github.com/juanb09111/semantic depth.","[{'name': 'Juan Pablo Lagos, Esa Rahtu'}]",
1445,Optimal Settings for Cryptocurrency Trading Pairs,https://arxiv.org/abs/2210.10971,"arXiv:2210.10971v3 Announce Type: replace-cross 
Abstract: The goal of cryptocurrencies is decentralization. In principle, all currencies have equal status. Unlike traditional stock markets, there is no default currency of denomination (fiat), thus the trading pairs can be set freely. However, it is impractical to set up a trading market between every two currencies. In order to control management costs and ensure sufficient liquidity, we must give priority to covering those large-volume trading pairs and ensure that all coins are reachable. We note that this is an optimization problem. Its particularity lies in: 1) the trading volume between most (>99.5%) possible trading pairs cannot be directly observed. 2) It satisfies the connectivity constraint, that is, all currencies are guaranteed to be tradable.
  To solve this problem, we use a two-stage process: 1) Fill in missing values based on a regularized, truncated eigenvalue decomposition, where the regularization term is used to control what extent missing values should be limited to zero. 2) Search for the optimal trading pairs, based on a branch and bound process, with heuristic search and pruning strategies.
  The experimental results show that: 1) If the number of denominated coins is not limited, we will get a more decentralized trading pair settings, which advocates the establishment of trading pairs directly between large currency pairs. 2) There is a certain room for optimization in all exchanges. The setting of inappropriate trading pairs is mainly caused by subjectively setting small coins to quote, or failing to track emerging big coins in time. 3) Too few trading pairs will lead to low coverage; too many trading pairs will need to be adjusted with markets frequently. Exchanges should consider striking an appropriate balance between them.","[{'name': 'Di Zhang, Youzhou Zhou'}]",
1446,Seamful XAI: Operationalizing Seamful Design in Explainable AI,https://arxiv.org/abs/2211.06753,"arXiv:2211.06753v2 Announce Type: replace-cross 
Abstract: Mistakes in AI systems are inevitable, arising from both technical limitations and sociotechnical gaps. While black-boxing AI systems can make the user experience seamless, hiding the seams risks disempowering users to mitigate fallouts from AI mistakes. Instead of hiding these AI imperfections, can we leverage them to help the user? While Explainable AI (XAI) has predominantly tackled algorithmic opaqueness, we propose that seamful design can foster AI explainability by revealing and leveraging sociotechnical and infrastructural mismatches. We introduce the concept of Seamful XAI by (1) conceptually transferring \"seams\" to the AI context and (2) developing a design process that helps stakeholders anticipate and design with seams. We explore this process with 43 AI practitioners and real end-users, using a scenario-based co-design activity informed by real-world use cases. We found that the Seamful XAI design process helped users foresee AI harms, identify underlying reasons (seams), locate them in the AI's lifecycle, learn how to leverage seamful information to improve XAI and user agency. We share empirical insights, implications, and reflections on how this process can help practitioners anticipate and craft seams in AI, how seamfulness can improve explainability, empower end-users, and facilitate Responsible AI.","[{'name': 'Upol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, Hal Daume III'}]",
1447,Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction,https://arxiv.org/abs/2212.04475,"arXiv:2212.04475v2 Announce Type: replace-cross 
Abstract: Robust prediction of citywide traffic flows at different time periods plays a crucial role in intelligent transportation systems. While previous work has made great efforts to model spatio-temporal correlations, existing methods still suffer from two key limitations: i) Most models collectively predict all regions' flows without accounting for spatial heterogeneity, i.e., different regions may have skewed traffic flow distributions. ii) These models fail to capture the temporal heterogeneity induced by time-varying traffic patterns, as they typically model temporal correlations with a shared parameterized space for all time periods. To tackle these challenges, we propose a novel Spatio-Temporal Self-Supervised Learning (ST-SSL) traffic prediction framework which enhances the traffic pattern representations to be reflective of both spatial and temporal heterogeneity, with auxiliary self-supervised learning paradigms. Specifically, our ST-SSL is built over an integrated module with temporal and spatial convolutions for encoding the information across space and time. To achieve the adaptive spatio-temporal self-supervised learning, our ST-SSL first performs the adaptive augmentation over the traffic flow graph data at both attribute- and structure-levels. On top of the augmented traffic graph, two SSL auxiliary tasks are constructed to supplement the main traffic prediction task with spatial and temporal heterogeneity-aware augmentation. Experiments on four benchmark datasets demonstrate that ST-SSL consistently outperforms various state-of-the-art baselines. Since spatio-temporal heterogeneity widely exists in practical datasets, the proposed framework may also cast light on other spatial-temporal applications. Model implementation is available at https://github.com/Echo-Ji/ST-SSL.","[{'name': 'Jiahao Ji, Jingyuan Wang, Chao Huang, Junjie Wu, Boren Xu, Zhenhe Wu, Junbo Zhang, Yu Zheng'}]",
1448,THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression,https://arxiv.org/abs/2302.08545,"arXiv:2302.08545v2 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) are the de facto standard for essential use cases, such as image classification, computer vision, and natural language processing. As DNNs and datasets get larger, they require distributed training on increasingly larger clusters. A main bottleneck is the resulting communication overhead where workers exchange model updates (i.e., gradients) on a per-round basis. To address this bottleneck and accelerate training, a widely-deployed approach is compression. However, previous deployments often apply bi-directional compression schemes by simply using a uni-directional gradient compression scheme in each direction. This results in significant computational overheads at the parameter server and increased compression error, leading to longer training and lower accuracy. We introduce Tensor Homomorphic Compression (THC), a novel bi-directional compression framework that enables the direct aggregation of compressed values and thus eliminating the aforementioned computational overheads. Moreover, THC is compatible with in-network aggregation (INA), which allows for further acceleration. Our evaluation shows that training representative vision and language models with THC reaches target accuracy by 1.40x to 1.47x faster using INA and 1.28x to 1.33x faster using a software PS compared with state-of-the-art systems.","[{'name': 'Minghao LiHarvard University, Ran Ben BasatUniversity College London, Shay VargaftikVMware Research, ChonLam LaoHarvard University, Kevin XuHarvard University, Michael MitzenmacherHarvard University, Minlan YuHarvard University'}]",
1449,Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction,https://arxiv.org/abs/2303.00286,"arXiv:2303.00286v4 Announce Type: replace-cross 
Abstract: Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that consider batches of true and false triples. However, different kinds of false triples exist and recent works suggest that they should not be valued equally, leading to specific negative sampling procedures. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. signatures of relations (domain and range) are high-quality negatives. Hence, we enrich the three main loss functions for link prediction such that all kinds of negatives are sampled but treated differently based on their semantic validity. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results which demonstrates both the generality and superiority of our proposed approach. In fact, the proposed loss functions (1) lead to better MRR and Hits@10 values, and (2) drive KGEMs towards better semantic correctness as measured by the Sem@K metric. This highlights that relation signatures globally improve KGEMs, and thus should be incorporated into loss functions. Domains and ranges of relations being largely available in schema-defined KGs, this makes our approach both beneficial and widely usable in practice.","[{'name': 'Nicolas Hubert, Pierre Monnin, Armelle Brun, Davy Monticolo'}]",
1450,CWTM: Leveraging Contextualized Word Embeddings from BERT for Neural Topic Modeling,https://arxiv.org/abs/2305.09329,"arXiv:2305.09329v3 Announce Type: replace-cross 
Abstract: Most existing topic models rely on bag-of-words (BOW) representation, which limits their ability to capture word order information and leads to challenges with out-of-vocabulary (OOV) words in new documents. Contextualized word embeddings, however, show superiority in word sense disambiguation and effectively address the OOV issue. In this work, we introduce a novel neural topic model called the Contextlized Word Topic Model (CWTM), which integrates contextualized word embeddings from BERT. The model is capable of learning the topic vector of a document without BOW information. In addition, it can also derive the topic vectors for individual words within a document based on their contextualized word embeddings. Experiments across various datasets show that CWTM generates more coherent and meaningful topics compared to existing topic models, while also accommodating unseen words in newly encountered documents.","[{'name': 'Zheng Fang, Yulan He, Rob Procter'}]",
1451,Interpretable Stereotype Identification through Reasoning,https://arxiv.org/abs/2308.00071,"arXiv:2308.00071v2 Announce Type: replace-cross 
Abstract: Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.","[{'name': 'Jacob-Junqi Tian, Omkar Dige, David Emerson, Faiza Khan Khattak'}]",
1452,VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs,https://arxiv.org/abs/2308.02117,"arXiv:2308.02117v3 Announce Type: replace-cross 
Abstract: GNN-to-MLP distillation aims to utilize knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (student MLP) on graph data by mimicking the output representations of teacher GNN. Existing methods mainly make the MLP to mimic the GNN predictions over a few class labels. However, the class space may not be expressive enough for covering numerous diverse local graph structures, thus limiting the performance of knowledge transfer from GNN to MLP. To address this issue, we propose to learn a new powerful graph representation space by directly labeling nodes' diverse local structures for GNN-to-MLP distillation. Specifically, we propose a variant of VQ-VAE to learn a structure-aware tokenizer on graph data that can encode each node's local substructure as a discrete code. The discrete codes constitute a codebook as a new graph representation space that is able to identify different local graph structures of nodes with the corresponding code indices. Then, based on the learned codebook, we propose a new distillation target, namely soft code assignments, to directly transfer the structural knowledge of each node from GNN to MLP. The resulting framework VQGraph achieves new state-of-the-art performance on GNN-to-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828x, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average, respectively. Code: https://github.com/YangLing0818/VQGraph.","[{'name': 'Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, Bin Cui, Muhan Zhang, Jure Leskovec'}]",
1453,Beyond MD17: the reactive xxMD dataset,https://arxiv.org/abs/2308.11155,"arXiv:2308.11155v3 Announce Type: replace-cross 
Abstract: System specific neural force fields (NFFs) have gained popularity in computational chemistry. One of the most popular datasets as a bencharmk to develop NFFs models is the MD17 dataset and its subsequent extension. These datasets comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampled from direct adiabatic dynamics. However, many chemical reactions involve significant molecular geometrical deformations, for example, bond breaking. Therefore, MD17 is inadequate to represent a chemical reaction. To address this limitation in MD17, we introduce a new dataset, called Extended Excited-state Molecular Dynamics (xxMD) dataset. The xxMD dataset involves geometries sampled from direct non-adiabatic dynamics, and the energies are computed at both multireference wavefunction theory and density functional theory. We show that the xxMD dataset involves diverse geometries which represent chemical reactions. Assessment of NFF models on xxMD dataset reveals significantly higher predictive errors than those reported for MD17 and its variants. This work underscores the challenges faced in crafting a generalizable NFF model with extrapolation capability.","[{'name': 'Zihan Pengmei, Junyu Liu, Yinan Shu'}]",
1454,Projected Task-Specific Layers for Multi-Task Reinforcement Learning,https://arxiv.org/abs/2309.08776,"arXiv:2309.08776v2 Announce Type: replace-cross 
Abstract: Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm.","[{'name': 'Josselin Somerville Roberts, Julia Di'}]",
1455,Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis,https://arxiv.org/abs/2309.09553,"arXiv:2309.09553v4 Announce Type: replace-cross 
Abstract: The excellent text-to-image synthesis capability of diffusion models has driven progress in synthesizing coherent visual stories. The current state-of-the-art method combines the features of historical captions, historical frames, and the current captions as conditions for generating the current frame. However, this method treats each historical frame and caption as the same contribution. It connects them in order with equal weights, ignoring that not all historical conditions are associated with the generation of the current frame. To address this issue, we propose Causal-Story. This model incorporates a local causal attention mechanism that considers the causal relationship between previous captions, frames, and current captions. By assigning weights based on this relationship, Causal-Story generates the current frame, thereby improving the global consistency of story generation. We evaluated our model on the PororoSV and FlintstonesSV datasets and obtained state-of-the-art FID scores, and the generated frames also demonstrate better storytelling in visuals.","[{'name': 'Tianyi Song, Jiuxin Cao, Kun Wang, Bo Liu, Xiaofeng Zhang'}]",
1456,Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification,https://arxiv.org/abs/2309.13734,"arXiv:2309.13734v2 Announce Type: replace-cross 
Abstract: Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology that can reduce or even eliminate the need for manual annotations. We investigate 10 open-source models and 7 prompting schemes, finding that LLMs are competitive with in-domain supervised models but are not necessarily consistent in their performance. We also fine-tuned the LLMs, but discovered that fine-tuning process does not necessarily lead to better performance. In general, we discover that LLMs do not routinely outperform their smaller supervised machine learning models, and thus call for stance detection to be a benchmark for which LLMs also optimize for. The code used in this study is available at \\url{https://github.com/ijcruic/LLM-Stance-Labeling}","[{'name': 'Iain J. Cruickshank, Lynnette Hui Xian Ng'}]",
1457,Continual Driving Policy Optimization with Closed-Loop Individualized Curricula,https://arxiv.org/abs/2309.14209,"arXiv:2309.14209v3 Announce Type: replace-cross 
Abstract: The safety of autonomous vehicles (AV) has been a long-standing top concern, stemming from the absence of rare and safety-critical scenarios in the long-tail naturalistic driving distribution. To tackle this challenge, a surge of research in scenario-based autonomous driving has emerged, with a focus on generating high-risk driving scenarios and applying them to conduct safety-critical testing of AV models. However, limited work has been explored on the reuse of these extensive scenarios to iteratively improve AV models. Moreover, it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement. Therefore, we develop a continual driving policy optimization framework featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into a set of standardized sub-modules for flexible implementation choices: AV Evaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a collision prediction task, where it estimates the chance of AV failures in these scenarios at each iteration. Subsequently, by re-sampling from historical scenarios based on these failure probabilities, CLIC tailors individualized curricula for downstream training, aligning them with the evaluated capability of AV. Accordingly, CLIC not only maximizes the utilization of the vast pre-collected scenario library for closed-loop driving policy optimization but also facilitates AV improvement by individualizing its training with more challenging cases out of those poorly organized scenarios. Experimental results clearly indicate that CLIC surpasses other curriculum-based training strategies, showing substantial improvement in managing risky scenarios, while still maintaining proficiency in handling simpler cases.","[{'name': 'Haoyi Niu, Yizhou Xu, Xingjian Jiang, Jianming Hu'}]",
1458,AI-Dentify: Deep learning for proximal caries detection on bitewing x-ray -- HUNT4 Oral Health Study,https://arxiv.org/abs/2310.00354,"arXiv:2310.00354v2 Announce Type: replace-cross 
Abstract: Background: Dental caries diagnosis requires the manual inspection of diagnostic bitewing images of the patient, followed by a visual inspection and probing of the identified dental pieces with potential lesions. Yet the use of artificial intelligence, and in particular deep-learning, has the potential to aid in the diagnosis by providing a quick and informative analysis of the bitewing images.
  Methods: A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were annotated individually by six different experts, and used to train three different object detection deep-learning architectures: RetinaNet (ResNet50), YOLOv5 (M size), and EfficientDet (D0 and D1 sizes). A consensus dataset of 197 images, annotated jointly by the same six dentist, was used for evaluation. A five-fold cross validation scheme was used to evaluate the performance of the AI models.
  Results: he trained models show an increase in average precision and F1-score, and decrease of false negative rate, with respect to the dental clinicians. When compared against the dental clinicians, the YOLOv5 model shows the largest improvement, reporting 0.647 mean average precision, 0.548 mean F1-score, and 0.149 mean false negative rate. Whereas the best annotators on each of these metrics reported 0.299, 0.495, and 0.164 respectively.
  Conclusion: Deep-learning models have shown the potential to assist dental professionals in the diagnosis of caries. Yet, the task remains challenging due to the artifacts natural to the bitewing images.","[{'name': \"Javier P\\\\'erez de Frutos, Ragnhild Holden Helland, Shreya Desai, Line Cathrine Nymoen, Thomas Lang{\\\\o}, Theodor Remman, Abhijit Sen\"}]",
1459,Improving Adversarial Attacks on Latent Diffusion Model,https://arxiv.org/abs/2310.04687,"arXiv:2310.04687v3 Announce Type: replace-cross 
Abstract: Adversarial attacks on Latent Diffusion Model (LDM), the state-of-the-art image generative model, have been adopted as effective protection against malicious finetuning of LDM on unauthorized images. We show that these attacks add an extra error to the score function of adversarial examples predicted by LDM. LDM finetuned on these adversarial examples learns to lower the error by a bias, from which the model is attacked and predicts the score function with biases.
  Based on the dynamics, we propose to improve the adversarial attack on LDM by Attacking with Consistent score-function Errors (ACE). ACE unifies the pattern of the extra error added to the predicted score function. This induces the finetuned LDM to learn the same pattern as a bias in predicting the score function. We then introduce a well-crafted pattern to improve the attack. Our method outperforms state-of-the-art methods in adversarial attacks on LDM.","[{'name': 'Boyang Zheng, Chumeng Liang, Xiaoyu Wu, Yan Liu'}]",
1460,Scalable Neural Network Kernels,https://arxiv.org/abs/2310.13225,"arXiv:2310.13225v2 Announce Type: replace-cross 
Abstract: We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universal random features (or URFs), applied to instantiate several SNNK variants, and interesting on its own in the context of scalable kernel methods. We provide rigorous theoretical analysis of all these concepts as well as an extensive empirical evaluation, ranging from point-wise kernel estimation to Transformers' fine-tuning with novel adapter layers inspired by SNNKs. Our mechanism provides up to 5x reduction in the number of trainable parameters, while maintaining competitive accuracy.","[{'name': 'Arijit Sehanobish, Krzysztof Choromanski, Yunfan Zhao, Avinava Dubey, Valerii Likhosherstov'}]",
1461,Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias,https://arxiv.org/abs/2310.14814,"arXiv:2310.14814v3 Announce Type: replace-cross 
Abstract: Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, although they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on classification datasets of various data modalities. The code is available at https://github.com/ambroiseodt/tsim.","[{'name': 'Ambroise Odonnat, Vasilii Feofanov, Ievgen Redko'}]",
1462,From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach,https://arxiv.org/abs/2311.03260,"arXiv:2311.03260v2 Announce Type: replace-cross 
Abstract: We propose the Kuramoto Graph Neural Network (KuramotoGNN), a novel class of continuous-depth graph neural networks (GNNs) that employs the Kuramoto model to mitigate the over-smoothing phenomenon, in which node features in GNNs become indistinguishable as the number of layers increases. The Kuramoto model captures the synchronization behavior of non-linear coupled oscillators. Under the view of coupled oscillators, we first show the connection between Kuramoto model and basic GNN and then over-smoothing phenomenon in GNNs can be interpreted as phase synchronization in Kuramoto model. The KuramotoGNN replaces this phase synchronization with frequency synchronization to prevent the node features from converging into each other while allowing the system to reach a stable synchronized state. We experimentally verify the advantages of the KuramotoGNN over the baseline GNNs and existing methods in reducing over-smoothing on various graph deep learning benchmark tasks.","[{'name': 'Tuan Nguyen, Hirotada Honda, Takashi Sano, Vinh Nguyen, Shugo Nakamura, Tan M. Nguyen'}]",
1463,Fair Text-to-Image Diffusion via Fair Mapping,https://arxiv.org/abs/2311.17695,"arXiv:2311.17695v2 Announce Type: replace-cross 
Abstract: In this paper, we address the limitations of existing text-to-image diffusion models in generating demographically fair results when given human-related descriptions. These models often struggle to disentangle the target language context from sociocultural biases, resulting in biased image generation. To overcome this challenge, we propose Fair Mapping, a flexible, model-agnostic, and lightweight approach that modifies a pre-trained text-to-image diffusion model by controlling the prompt to achieve fair image generation. One key advantage of our approach is its high efficiency. It only requires updating an additional linear network with few parameters at a low computational cost. By developing a linear network that maps conditioning embeddings into a debiased space, we enable the generation of relatively balanced demographic results based on the specified text condition. With comprehensive experiments on face image generation, we show that our method significantly improves image generation fairness with almost the same image quality compared to conventional diffusion models when prompted with descriptions related to humans. By effectively addressing the issue of implicit language bias, our method produces more fair and diverse image outputs.","[{'name': 'Jia Li, Lijie Hu, Jingfeng Zhang, Tianhang Zheng, Hua Zhang, Di Wang'}]",
1464,Large-scale Training of Foundation Models for Wearable Biosignals,https://arxiv.org/abs/2312.05409,"arXiv:2312.05409v2 Announce Type: replace-cross 
Abstract: Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch. We curated PPG and ECG datasets from AHMS that include data from ~141K participants spanning ~3 years. Our self-supervised learning framework includes participant level positive pair selection, stochastic augmentation module and a regularized contrastive loss optimized with momentum training, and generalizes well to both PPG and ECG modalities. We show that the pre-trained foundation models readily encode information regarding participants' demographics and health conditions. To the best of our knowledge, this is the first study that builds foundation models using large-scale PPG and ECG data collected via wearable consumer devices $\\unicode{x2013}$ prior works have commonly used smaller-size datasets collected in clinical and experimental settings. We believe PPG and ECG foundation models can enhance future wearable devices by reducing the reliance on labeled data and hold the potential to help the users improve their health.","[{'name': 'Salar Abbaspourazad, Oussama Elachqar, Andrew C. Miller, Saba Emrani, Udhyakumar Nallasamy, Ian Shapiro'}]",
1465,A Novel Image Classification Framework Based on Variational Quantum Algorithms,https://arxiv.org/abs/2312.07932,"arXiv:2312.07932v2 Announce Type: replace-cross 
Abstract: Image classification is a crucial task in machine learning with widespread practical applications. The existing classical framework for image classification typically utilizes a global pooling operation at the end of the network to reduce computational complexity and mitigate overfitting. However, this operation often results in a significant loss of information, which can affect the performance of classification models. To overcome this limitation, we introduce a novel image classification framework that leverages variational quantum algorithms (VQAs)-hybrid approaches combining quantum and classical computing paradigms within quantum machine learning. The major advantage of our framework is the elimination of the need for the global pooling operation at the end of the network. In this way, our approach preserves more discriminative features and fine-grained details in the images, which enhances classification performance. Additionally, employing VQAs enables our framework to have fewer parameters than the classical framework, even in the absence of global pooling, which makes it more advantageous in preventing overfitting. We apply our method to different state-of-the-art image classification models and demonstrate the superiority of the proposed quantum architecture over its classical counterpart through a series of experiments on public datasets. Our experiments show that the proposed quantum framework achieves up to a 9.21% increase in accuracy and up to a 15.79% improvement in F1 score, compared to the classical framework.",[{'name': 'Yixiong Chen'}],
1466,Parameterized Projected Bellman Operator,https://arxiv.org/abs/2312.12869,"arXiv:2312.12869v3 Announce Type: replace-cross 
Abstract: Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projected Bellman operator (PBO). We formulate an optimization problem to learn PBO for generic sequential decision-making problems, and we theoretically analyze its properties in two representative classes of RL problems. Furthermore, we theoretically study our approach under the lens of AVI and devise algorithmic implementations to learn PBO in offline and online settings by leveraging neural network parameterizations. Finally, we empirically showcase the benefits of PBO w.r.t. the regular Bellman operator on several RL problems.","[{'name': \"Th\\\\'eo Vincent, Alberto Maria Metelli, Boris Belousov, Jan Peters, Marcello Restelli, Carlo D'Eramo\"}]",
1467,Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models,https://arxiv.org/abs/2312.14197,"arXiv:2312.14197v2 Announce Type: replace-cross 
Abstract: The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box methods based on prompt learning and a white-box defense method based on fine-tuning with adversarial training accordingly. Experimental results demonstrate that black-box defenses are highly effective in mitigating these attacks, while the white-box defense reduces the attack success rate to near-zero levels. Overall, our work systematically investigates indirect prompt injection attacks by introducing a benchmark, analyzing the underlying reason for the success of the attack, and developing an initial set of defenses.","[{'name': 'Jingwei Yi, Yueqi Xie, Bin Zhu, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu'}]",
1468,Understanding Distributed Representations of Concepts in Deep Neural Networks without Supervision,https://arxiv.org/abs/2312.17285,"arXiv:2312.17285v2 Announce Type: replace-cross 
Abstract: Understanding intermediate representations of the concepts learned by deep learning classifiers is indispensable for interpreting general model behaviors. Existing approaches to reveal learned concepts often rely on human supervision, such as pre-defined concept sets or segmentation processes. In this paper, we propose a novel unsupervised method for discovering distributed representations of concepts by selecting a principal subset of neurons. Our empirical findings demonstrate that instances with similar neuron activation states tend to share coherent concepts. Based on the observations, the proposed method selects principal neurons that construct an interpretable region, namely a Relaxed Decision Region (RDR), encompassing instances with coherent concepts in the feature space. It can be utilized to identify unlabeled subclasses within data and to detect the causes of misclassifications. Furthermore, the applicability of our method across various layers discloses distinct distributed representations over the layers, which provides deeper insights into the internal mechanisms of the deep learning model.","[{'name': 'Wonjoon Chang, Dahee Kwon, Jaesik Choi'}]",
1469,Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding,https://arxiv.org/abs/2401.04575,"arXiv:2401.04575v2 Announce Type: replace-cross 
Abstract: Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgrounds. Our experiments on LGS show that the classifiers trained on existing benchmark datasets do not readily generalize to e-commerce data, while specific self-supervised visual feature extractors can better generalize. Furthermore, LGS's high-quality e-commerce-focused images and bimodal nature make it advantageous for vision-language bi-modal tasks: LGS enables image-captioning models to generate richer captions and helps text-to-image generation models achieve e-commerce style transfer.","[{'name': 'Yatong Bai, Utsav Garg, Apaar Shanker, Haoming Zhang, Samyak Parajuli, Erhan Bas, Isidora Filipovic, Amelia N. Chu, Eugenia D Fomitcheva, Elliot Branson, Aerin Kim, Somayeh Sojoudi, Kyunghyun Cho'}]",
1470,DevEval: Evaluating Code Generation in Practical Software Projects,https://arxiv.org/abs/2401.06401,"arXiv:2401.06401v4 Announce Type: replace-cross 
Abstract: How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experiments. We also discuss the challenges and future directions of code generation in practical projects. We open-source DevEval and hope it can facilitate the development of code generation in practical projects.","[{'name': 'Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Zhi Jin, Hao Zhu, Huanyu Liu, Kaibo Liu, Lecheng Wang, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming Zhang, Yihong Dong, Yuqi Zhu, Bin Gu, Mengfei Yang'}]",
1471,SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding,https://arxiv.org/abs/2401.09340,"arXiv:2401.09340v2 Announce Type: replace-cross 
Abstract: 3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-language pairs derived from both human annotations and our scalable scene-graph-based generation approach. We demonstrate that this scaling allows for a unified pre-training framework, Grounded Pre-training for Scenes (GPS), for 3D vision-language learning. Through extensive experiments, we showcase the effectiveness of GPS by achieving state-of-the-art performance on all existing 3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is unveiled through zero-shot transfer experiments in the challenging 3D vision-language tasks. Project website: https://scene-verse.github.io.","[{'name': 'Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, Siyuan Huang'}]",
1472,Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge,https://arxiv.org/abs/2401.10712,"arXiv:2401.10712v2 Announce Type: replace-cross 
Abstract: With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question generation model. Then, we use an image tagging model to identify various instances and send packaged image-tag pairs into the visual question generation model to generate relevant questions with the extracted image tags as answers. Finally, we encode these generated question-answer pairs as prompts with a visual-aware prompting module and send them into pre-trained multi-modal large language models to reason out the final answers. Experimental results show that, compared with state-of-the-art methods, our Q&amp;A Prompts achieves substantial improvements on the challenging visual question answering datasets requiring reasoning over diverse world knowledge, such as OK-VQA and A-OKVQA.","[{'name': 'Haibi Wang, Weifeng Ge'}]",
1473,MedLM: Exploring Language Models for Medical Question Answering Systems,https://arxiv.org/abs/2401.11389,"arXiv:2401.11389v2 Announce Type: replace-cross 
Abstract: In the face of rapidly expanding online medical literature, automated systems for aggregating and summarizing information are becoming increasingly crucial for healthcare professionals and patients. Large Language Models (LLMs), with their advanced generative capabilities, have shown promise in various NLP tasks, and their potential in the healthcare domain, particularly for Closed-Book Generative QnA, is significant. However, the performance of these models in domain-specific tasks such as medical Q&amp;A remains largely unexplored. This study aims to fill this gap by comparing the performance of general and medical-specific distilled LMs for medical Q&amp;A. We aim to evaluate the effectiveness of fine-tuning domain-specific LMs and compare the performance of different families of Language Models. The study will address critical questions about these models' reliability, comparative performance, and effectiveness in the context of medical Q&amp;A. The findings will provide valuable insights into the suitability of different LMs for specific applications in the medical domain.","[{'name': 'Niraj Yagnik, Jay Jhaveri, Vivek Sharma, Gabriel Pila'}]",
1474,SelectLLM: Can LLMs Select Important Instructions to Annotate?,https://arxiv.org/abs/2401.16553,"arXiv:2401.16553v4 Announce Type: replace-cross 
Abstract: Instruction tuning benefits from large and diverse datasets, however creating such datasets involves a high cost of human labeling. While synthetic datasets generated by large language models (LLMs) have partly solved this issue, they often contain low-quality data. One effective solution is selectively annotating unlabelled instructions, especially given the relative ease of acquiring unlabeled instructions or texts from various sources. However, how to select unlabelled instructions is not well-explored, especially in the context of LLMs. Further, traditional data selection methods, relying on input embedding space density, tend to underestimate instruction sample complexity, whereas those based on model prediction uncertainty often struggle with synthetic label quality. Therefore, we introduce SelectLLM, an alternative framework that leverages the capabilities of LLMs to more effectively select unlabeled instructions. SelectLLM consists of two key steps: Coreset-based clustering of unlabelled instructions for diversity and then prompting a LLM to identify the most beneficial instructions within each cluster. Our experiments demonstrate that SelectLLM matches or outperforms other state-of-the-art methods in instruction tuning benchmarks. It exhibits remarkable consistency across human and synthetic datasets, along with better cross-dataset generalization, as evidenced by a 10% performance improvement on the Cleaned Alpaca test set when trained on Dolly data. All code and data are publicly available (https://github.com/minnesotanlp/select-llm).","[{'name': 'Ritik Sachin Parkar, Jaehyung Kim, Jong Inn Park, Dongyeop Kang'}]",
1475,Error Estimation for Physics-informed Neural Networks Approximating Semilinear Wave Equations,https://arxiv.org/abs/2402.07153,"arXiv:2402.07153v2 Announce Type: replace-cross 
Abstract: This paper provides rigorous error bounds for physics-informed neural networks approximating the semilinear wave equation. We provide bounds for the generalization and training error in terms of the width of the network's layers and the number of training points for a tanh neural network with two hidden layers. Our main result is a bound of the total error in the $H^1([0,T];L^2(\\Omega))$-norm in terms of the training error and the number of training points, which can be made arbitrarily small under some assumptions. We illustrate our theoretical bounds with numerical experiments.","[{'name': 'Beatrice Lorenz, Aras Bacho, Gitta Kutyniok'}]",
1476,Average-Case Analysis of Iterative Voting,https://arxiv.org/abs/2402.08144,"arXiv:2402.08144v2 Announce Type: replace-cross 
Abstract: Iterative voting is a natural model of repeated strategic decision-making in social choice when agents have the opportunity to update their votes prior to finalizing the group decision. Prior work has analyzed the efficacy of iterative plurality on the welfare of the chosen outcome at equilibrium, relative to the truthful vote profile, via an adaptation of the price of anarchy. However, prior analyses have only studied the worst- and average-case performances when agents' preferences are distributed by the impartial culture. This work extends average-case analysis to a wider class of distributions and distinguishes when iterative plurality improves or degrades asymptotic welfare.","[{'name': 'Joshua Kavner, Lirong Xia'}]",
1477,Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities,https://arxiv.org/abs/2402.16073,"arXiv:2402.16073v2 Announce Type: replace-cross 
Abstract: In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.","[{'name': 'Binyam Gebre, Karoliina Ranta, Stef van den Elzen, Ernst Kuiper, Thijs Baars, Tom Heskes'}]",
1478,A priori Estimates for Deep Residual Network in Continuous-time Reinforcement Learning,https://arxiv.org/abs/2402.16899,"arXiv:2402.16899v2 Announce Type: replace-cross 
Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \\emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \\emph{a priori} generalization error without the curse of dimensionality.","[{'name': 'Shuyu Yin, Qixuan Zhou, Fei Wen, Tao Luo'}]",
1479,On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction,https://arxiv.org/abs/2402.18061,"arXiv:2402.18061v2 Announce Type: replace-cross 
Abstract: The superior performance of supervised classification methods in the information extraction (IE) area heavily relies on a large amount of gold standard data. Recent zero-shot classification methods converted the task to other NLP tasks (e.g., textual entailment) and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks. However, there is no further investigation into the use of these data. In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) Finetuning the off-the-shelf model using clean data; (4) Inference on the test data. The experimental results show that Clean-LaVe can outperform the baseline by 5% and 6% on TACRED and Wiki80 dataset in the zero-shot relation classification task, and by 3%-7% on Smile (Korean and Polish) in the zero-shot cross-lingual relation classification task, and by 8% on ACE05-E+ in the zero-shot event argument classification task. The code is share in https://github.com/wjw136/Clean_LaVe.git.","[{'name': 'Jianwei Wang, Tianyin Wang, Ziqian Zeng'}]",
1480,Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards,https://arxiv.org/abs/2402.18571,"arXiv:2402.18571v3 Announce Type: replace-cross 
Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO).","[{'name': 'Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang'}]",
1481,Learning with Language-Guided State Abstractions,https://arxiv.org/abs/2402.18759,"arXiv:2402.18759v2 Announce Type: replace-cross 
Abstract: We describe a framework for using natural language to design state abstractions for imitation learning. Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones. These state representations are typically manually specified, or derived from other labor-intensive labeling procedures. Our method, LGA (language-guided abstraction), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks. In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demonstrations and LGA-generated abstract states. Experiments on simulated robotic tasks show that LGA yields state abstractions similar to those designed by humans, but in a fraction of the time, and that these abstractions improve generalization and robustness in the presence of spurious correlations and ambiguous specifications. We illustrate the utility of the learned abstractions on mobile manipulation tasks with a Spot robot.","[{'name': 'Andi Peng, Ilia Sucholutsky, Belinda Z. Li, Theodore R. Sumers, Thomas L. Griffiths, Jacob Andreas, Julie A. Shah'}]",
1482,Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy,https://arxiv.org/abs/2402.19379,"arXiv:2402.19379v2 Announce Type: replace-cross 
Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our preregistered main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is not statistically different from the human crowd. In exploratory analyses, we find that these two approaches are equivalent with respect to medium-effect-size equivalence bounds. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even split of positive and negative resolutions. Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2) can be improved by drawing on human cognitive output. We find that both models' forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between 17% and 28%: though this leads to less accurate predictions than simply averaging human and machine forecasts. Our results suggest that LLMs can achieve forecasting accuracy rivaling that of human crowd forecasting tournaments: via the simple, practically applicable method of forecast aggregation. This replicates the 'wisdom of the crowd' effect for LLMs, and opens up their use for a variety of applications throughout society.","[{'name': 'Philipp Schoenegger, Indre Tuminauskaite, Peter S. Park, Philip E. Tetlock'}]",
1483,AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation,https://arxiv.org/abs/2403.01818,"arXiv:2403.01818v2 Announce Type: replace-cross 
Abstract: Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate the burden of time-consuming pixel-level manual labeling, which leverages limited labeled data along with larger amounts of unlabeled data. Current state-of-the-art methods train the labeled data with ground truths and unlabeled data with pseudo labels. However, the two training flows are separate, which allows labeled data to dominate the training process, resulting in low-quality pseudo labels and, consequently, sub-optimal results. To alleviate this issue, we present AllSpark, which reborns the labeled features from unlabeled ones with the channel-wise cross-attention mechanism. We further introduce a Semantic Memory along with a Channel Semantic Grouping strategy to ensure that unlabeled features adequately represent labeled features. The AllSpark shed new light on the architecture level designs of SSSS rather than framework level, which avoids increasingly complicated training pipeline designs. It can also be regarded as a flexible bottleneck module that can be seamlessly integrated into a general transformer-based segmentation model. The proposed AllSpark outperforms existing methods across all evaluation protocols on Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and model weights are available at: https://github.com/xmed-lab/AllSpark.","[{'name': 'Haonan Wang, Qixiang Zhang, Yi Li, Xiaomeng Li'}]",
1484,Towards Implicit Prompt For Text-To-Image Models,https://arxiv.org/abs/2403.02118,"arXiv:2403.02118v2 Announce Type: replace-cross 
Abstract: Recent text-to-image (T2I) models have had great success, and many benchmarks have been proposed to evaluate their performance and safety. However, they only consider explicit prompts while neglecting implicit prompts (hint at a target without explicitly mentioning it). These prompts may get rid of safety constraints and pose potential threats to the applications of these models. This position paper highlights the current state of T2I models toward implicit prompts. We present a benchmark named ImplicitBench and conduct an investigation on the performance and impacts of implicit prompts with popular T2I models. Specifically, we design and collect more than 2,000 implicit prompts of three aspects: General Symbols, Celebrity Privacy, and Not-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models' capabilities under these implicit prompts. Experiment results show that (1) T2I models are able to accurately create various target symbols indicated by implicit prompts; (2) Implicit prompts bring potential risks of privacy leakage for T2I models. (3) Constraints of NSFW in most of the evaluated T2I models can be bypassed with implicit prompts. We call for increased attention to the potential and risks of implicit prompts in the T2I community and further investigation into the capabilities and impacts of implicit prompts, advocating for a balanced approach that harnesses their benefits while mitigating their risks.","[{'name': 'Yue Yang, Yuqi lin, Hong Liu, Wenqi Shao, Runjian Chen, Hailong Shang, Yu Wang, Yu Qiao, Kaipeng Zhang, Ping Luo'}]",
1485,Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution,https://arxiv.org/abs/2403.02131,"arXiv:2403.02131v2 Announce Type: replace-cross 
Abstract: Evolutionary algorithms, such as Differential Evolution, excel in solving real-parameter optimization challenges. However, the effectiveness of a single algorithm varies across different problem instances, necessitating considerable efforts in algorithm selection or configuration. This paper aims to address the limitation by leveraging the complementary strengths of a group of algorithms and dynamically scheduling them throughout the optimization progress for specific problems. We propose a deep reinforcement learning-based dynamic algorithm selection framework to accomplish this task. Our approach models the dynamic algorithm selection a Markov Decision Process, training an agent in a policy gradient manner to select the most suitable algorithm according to the features observed during the optimization process. To empower the agent with the necessary information, our framework incorporates a thoughtful design of landscape and algorithmic features. Meanwhile, we employ a sophisticated deep neural network model to infer the optimal action, ensuring informed algorithm selections. Additionally, an algorithm context restoration mechanism is embedded to facilitate smooth switching among different algorithms. These mechanisms together enable our framework to seamlessly select and switch algorithms in a dynamic online fashion. Notably, the proposed framework is simple and generic, offering potential improvements across a broad spectrum of evolutionary algorithms. As a proof-of-principle study, we apply this framework to a group of Differential Evolution algorithms. The experimental results showcase the remarkable effectiveness of the proposed framework, not only enhancing the overall optimization performance but also demonstrating favorable generalization ability across different problem classes.","[{'name': 'Hongshu Guo, Yining Ma, Zeyuan Ma, Jiacheng Chen, Xinglin Zhang, Zhiguang Cao, Jun Zhang, Yue-Jiao Gong'}]",
1486,NeuroVoz: a Castillian Spanish corpus of parkinsonian speech,https://arxiv.org/abs/2403.02371,"arXiv:2403.02371v2 Announce Type: replace-cross 
Abstract: The advancement of Parkinson's Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research.
  In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state. This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues. The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, and the first in Castillian Spanish.
  NeuroVoz is composed by 2,903 audio recordings averaging $26.88 \\pm 3.35$ recordings per participant, offering a substantial resource for the scientific exploration of PD's impact on speech. This dataset has already underpinned several studies, achieving a benchmark accuracy of 89% in PD speech pattern identification, indicating marked speech alterations attributable to PD. Despite these advances, the broader challenge of conducting a language-agnostic, cross-corpora analysis of Parkinsonian speech patterns remains an open area for future research. This contribution not only fills a critical void in PD speech analysis resources but also sets a new standard for the global research community in leveraging speech as a diagnostic tool for neurodegenerative diseases.","[{'name': \"Jana\\\\'ina Mendes-Laureano, Jorge A. G\\\\'omez-Garc\\\\'ia, Alejandro Guerrero-L\\\\'opez, Elisa Luque-Buzo, Juli\\\\'an D. Arias-Londo\\\\~no, Francisco J. Grandas-P\\\\'erez, Juan I. Godino-Llorente\"}]",
1487,ImgTrojan: Jailbreaking Vision-Language Models with ONE Image,https://arxiv.org/abs/2403.02910,"arXiv:2403.02910v2 Announce Type: replace-cross 
Abstract: There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.","[{'name': 'Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong'}]",
1488,Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation,https://arxiv.org/abs/2403.02951,"arXiv:2403.02951v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions. To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer valuable insights for enhancing the development of LLM-based Text-to-SQL systems.","[{'name': 'Bin Zhang, Yuxiao Ye, Guoqing Du, Xiaoru Hu, Zhishuai Li, Sun Yang, Chi Harold Liu, Rui Zhao, Ziyue Li, Hangyu Mao'}]",
1489,Introducing ConTextual: How well can your Multimodal model jointly reason over text and image in text-rich scenes?,https://huggingface.co/blog/leaderboard-contextual,,[],"Tue, 05 Mar 2024 00:00:00 GMT"
1490,Data is better together,https://huggingface.co/blog/community-datasets,,[],"Mon, 04 Mar 2024 00:00:00 GMT"
1491,Text-Generation Pipeline on Intel® Gaudi® 2 AI Accelerator,https://huggingface.co/blog/textgen-pipe-gaudi,,[],"Thu, 29 Feb 2024 00:00:00 GMT"
1492,StarCoder2 and The Stack v2,https://huggingface.co/blog/starcoder2,,[],"Wed, 28 Feb 2024 00:00:00 GMT"
1493,TTS Arena: Benchmarking Text-to-Speech Models in the Wild,https://huggingface.co/blog/arena-tts,,[],"Tue, 27 Feb 2024 00:00:00 GMT"
1494,AI Watermarking 101: Tools and Techniques,https://huggingface.co/blog/watermarking,,[],"Mon, 26 Feb 2024 00:00:00 GMT"
1495,Fine-Tuning Gemma Models in Hugging Face,https://huggingface.co/blog/gemma-peft,,[],"Fri, 23 Feb 2024 00:00:00 GMT"
1496,Introducing the Red-Teaming Resistance Leaderboard,https://huggingface.co/blog/leaderboard-haizelab,,[],"Fri, 23 Feb 2024 00:00:00 GMT"
1497,🪆 Introduction to Matryoshka Embedding Models,https://huggingface.co/blog/matryoshka,,[],"Fri, 23 Feb 2024 00:00:00 GMT"
1498,Welcome Gemma - Google's new open LLM,https://huggingface.co/blog/gemma,,[],"Wed, 21 Feb 2024 00:00:00 GMT"
1499,Introducing the Open Ko-LLM Leaderboard: Leading the Korean LLM Evaluation Ecosystem,https://huggingface.co/blog/leaderboard-upstage,,[],"Tue, 20 Feb 2024 00:00:00 GMT"
1500,🤗 PEFT welcomes new merging methods,https://huggingface.co/blog/peft_merging,,[],"Mon, 19 Feb 2024 00:00:00 GMT"
1501,"Synthetic data: save money, time and carbon with open source",https://huggingface.co/blog/synthetic-data-save-costs,,[],"Fri, 16 Feb 2024 00:00:00 GMT"
1502,AMD Pervasive AI Developer Contest!,https://huggingface.co/blog/amd_pervasive_developer_ai_contest,,[],"Wed, 14 Feb 2024 00:00:00 GMT"
1503,From OpenAI to Open LLMs with Messages API,https://huggingface.co/blog/tgi-messages-api,,[],"Thu, 08 Feb 2024 00:00:00 GMT"
1504,SegMoE: Segmind Mixture of Diffusion Experts,https://huggingface.co/blog/segmoe,,[],"Sat, 03 Feb 2024 00:00:00 GMT"
1505,NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates,https://huggingface.co/blog/leaderboard-nphardeval,,[],"Fri, 02 Feb 2024 00:00:00 GMT"
1506,Constitutional AI with Open LLMs,https://huggingface.co/blog/constitutional_ai,,[],"Thu, 01 Feb 2024 00:00:00 GMT"
1507,Hugging Face Text Generation Inference available for AWS Inferentia2,https://huggingface.co/blog/text-generation-inference-on-inferentia2,,[],"Thu, 01 Feb 2024 00:00:00 GMT"
1508,Patch Time Series Transformer in Hugging Face,https://huggingface.co/blog/patchtst,,[],"Thu, 01 Feb 2024 00:00:00 GMT"
1509,Introducing the Enterprise Scenarios Leaderboard: a Leaderboard for Real World Use Cases,https://huggingface.co/blog/leaderboard-patronus,,[],"Wed, 31 Jan 2024 00:00:00 GMT"
1510,Accelerate StarCoder with 🤗 Optimum Intel on Xeon: Q8/Q4 and Speculative Decoding,https://huggingface.co/blog/intel-starcoder-quantization,,[],"Tue, 30 Jan 2024 00:00:00 GMT"
1511,"The Hallucinations Leaderboard, an Open Effort to Measure Hallucinations in Large Language Models",https://huggingface.co/blog/leaderboard-hallucinations,,[],"Mon, 29 Jan 2024 00:00:00 GMT"
1512,An Introduction to AI Secure LLM Safety Leaderboard,https://huggingface.co/blog/leaderboard-decodingtrust,,[],"Fri, 26 Jan 2024 00:00:00 GMT"
1513,Hugging Face and Google partner for open AI collaboration,https://huggingface.co/blog/gcp-partnership,,[],"Thu, 25 Jan 2024 00:00:00 GMT"
1514,Open-source LLMs as LangChain Agents,https://huggingface.co/blog/open-source-llms-as-agents,,[],"Wed, 24 Jan 2024 00:00:00 GMT"
1515,Fine-Tune W2V2-Bert for low-resource ASR with 🤗 Transformers,https://huggingface.co/blog/fine-tune-w2v2-bert,,[],"Fri, 19 Jan 2024 00:00:00 GMT"
1516,PatchTSMixer in HuggingFace,https://huggingface.co/blog/patchtsmixer,,[],"Fri, 19 Jan 2024 00:00:00 GMT"
1517,Preference Tuning LLMs with Direct Preference Optimization Methods,https://huggingface.co/blog/pref-tuning,,[],"Thu, 18 Jan 2024 00:00:00 GMT"
1518,Accelerating SD Turbo and SDXL Turbo Inference with ONNX Runtime and Olive,https://huggingface.co/blog/sdxl_ort_inference,,[],"Mon, 15 Jan 2024 00:00:00 GMT"
1519,A guide to setting up your own Hugging Face leaderboard: an end-to-end example with Vectara's hallucination leaderboard,https://huggingface.co/blog/leaderboard-vectara,,[],"Fri, 12 Jan 2024 00:00:00 GMT"
1520,Faster fine-tuning using TRL & Unsloth,https://huggingface.co/blog/unsloth-trl,,[],"Wed, 10 Jan 2024 00:00:00 GMT"
1521,Welcome aMUSEd: Efficient Text-to-Image Generation,https://huggingface.co/blog/amused,,[],"Thu, 04 Jan 2024 00:00:00 GMT"
1522,"LoRA training scripts of the world, unite!",https://huggingface.co/blog/sdxl_lora_advanced_script,,[],"Tue, 02 Jan 2024 00:00:00 GMT"
1523,Speculative Decoding for 2x Faster Whisper Inference,https://huggingface.co/blog/whisper-speculative-decoding,,[],"Wed, 20 Dec 2023 00:00:00 GMT"
1524,"2023, year of open LLMs",https://huggingface.co/blog/2023-in-llms,,[],"Mon, 18 Dec 2023 00:00:00 GMT"
1525,Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face,https://huggingface.co/blog/mixtral,,[],"Mon, 11 Dec 2023 00:00:00 GMT"
1526,Mixture of Experts Explained,https://huggingface.co/blog/moe,,[],"Mon, 11 Dec 2023 00:00:00 GMT"
1527,AMD + 🤗: Large Language Models Out-of-the-Box Acceleration with AMD GPU,https://huggingface.co/blog/huggingface-and-optimum-amd,,[],"Tue, 05 Dec 2023 00:00:00 GMT"
1528,SetFitABSA: Few-Shot Aspect Based Sentiment Analysis using SetFit,https://huggingface.co/blog/setfit-absa,,[],"Wed, 06 Dec 2023 00:00:00 GMT"
1529,Optimum-NVIDIA - Unlock blazingly fast LLM inference in just 1 line of code,https://huggingface.co/blog/optimum-nvidia,,[],"Tue, 05 Dec 2023 00:00:00 GMT"
1530,Goodbye cold boot - how we made LoRA inference 300% faster,https://huggingface.co/blog/lora-adapters-dynamic-loading,,[],"Tue, 05 Dec 2023 00:00:00 GMT"
1531,Open LLM Leaderboard: DROP deep dive,https://huggingface.co/blog/open-llm-leaderboard-drop,,[],"Fri, 01 Dec 2023 00:00:00 GMT"
1532,SDXL in 4 steps with Latent Consistency LoRAs,https://huggingface.co/blog/lcm_lora,,[],"Thu, 09 Nov 2023 00:00:00 GMT"
1533,Make your llama generation time fly with AWS Inferentia2,https://huggingface.co/blog/inferentia-llama2,,[],"Tue, 07 Nov 2023 00:00:00 GMT"
1534,Introducing Prodigy-HF: a direct integration with Hugging Face,https://huggingface.co/blog/prodigy-hf,,[],"Tue, 07 Nov 2023 00:00:00 GMT"
1535,"Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora",https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral,,[],"Tue, 07 Nov 2023 00:00:00 GMT"
1536,Introducing Storage Regions on the HF Hub,https://huggingface.co/blog/regions,,[],"Fri, 03 Nov 2023 00:00:00 GMT"
1537,Creating open machine learning datasets? Share them on the Hugging Face Hub!,https://huggingface.co/blog/researcher-dataset-sharing,,[],"Mon, 30 Oct 2023 00:00:00 GMT"
1538,Personal Copilot: Train Your Own Coding Assistant,https://huggingface.co/blog/personal-copilot,,[],"Fri, 27 Oct 2023 00:00:00 GMT"
1539,Interactively explore your Huggingface dataset with one line of code,https://huggingface.co/blog/scalable-data-inspection,,[],"Wed, 25 Oct 2023 00:00:00 GMT"
1540,Deploy Embedding Models with Hugging Face Inference Endpoints,https://huggingface.co/blog/inference-endpoints-embeddings,,[],"Tue, 24 Oct 2023 00:00:00 GMT"
1541,The N Implementation Details of RLHF with PPO,https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo,,[],"Tue, 24 Oct 2023 00:00:00 GMT"
1542,Exploring simple optimizations for SDXL,https://huggingface.co/blog/simple_sdxl_optimizations,,[],"Tue, 24 Oct 2023 00:00:00 GMT"
1543,Gradio-Lite: Serverless Gradio Running Entirely in Your Browser,https://huggingface.co/blog/gradio-lite,,[],"Thu, 19 Oct 2023 00:00:00 GMT"
1544,"Accelerating over 130,000 Hugging Face models with ONNX Runtime",https://huggingface.co/blog/ort-accelerating-hf-models,,[],"Wed, 04 Oct 2023 00:00:00 GMT"
1545,Accelerating Stable Diffusion XL Inference with JAX on Cloud TPU v5e,https://huggingface.co/blog/sdxl_jax,,[],"Tue, 03 Oct 2023 00:00:00 GMT"
1546,Chat Templates: An End to the Silent Performance Killer,https://huggingface.co/blog/chat-templates,,[],"Tue, 03 Oct 2023 00:00:00 GMT"
1547,Deploying the AI Comic Factory using the Inference API,https://huggingface.co/blog/ai-comic-factory,,[],"Mon, 02 Oct 2023 00:00:00 GMT"
1548,Ethics and Society Newsletter #5: Hugging Face Goes To Washington and Other Summer 2023 Musings,https://huggingface.co/blog/ethics-soc-5,,[],"Fri, 29 Sep 2023 00:00:00 GMT"
1549,Finetune Stable Diffusion Models with DDPO via TRL,https://huggingface.co/blog/trl-ddpo,,[],"Fri, 29 Sep 2023 00:00:00 GMT"
1550,Non-engineers guide: Train a LLaMA 2 chatbot,https://huggingface.co/blog/Llama2-for-non-engineers,,[],"Thu, 28 Sep 2023 00:00:00 GMT"
1551,Llama 2 on Amazon SageMaker a Benchmark,https://huggingface.co/blog/llama-sagemaker-benchmark,,[],"Tue, 26 Sep 2023 00:00:00 GMT"
1552,Inference for PROs,https://huggingface.co/blog/inference-pro,,[],"Fri, 22 Sep 2023 00:00:00 GMT"
1553,Rocket Money x Hugging Face: Scaling Volatile ML Models in Production,https://huggingface.co/blog/rocketmoney-case-study,,[],"Tue, 19 Sep 2023 00:00:00 GMT"
1554,Introduction to 3D Gaussian Splatting,https://huggingface.co/blog/gaussian-splatting,,[],"Mon, 18 Sep 2023 00:00:00 GMT"
1555,Object Detection Leaderboard,https://huggingface.co/blog/object-detection-leaderboard,,[],"Mon, 18 Sep 2023 00:00:00 GMT"
1556,Optimizing your LLM in production,https://huggingface.co/blog/optimize-llm,,[],"Fri, 15 Sep 2023 00:00:00 GMT"
1557,Introducing Würstchen: Fast Diffusion for Image Generation,https://huggingface.co/blog/wuerstchen,,[],"Wed, 13 Sep 2023 00:00:00 GMT"
1558,Fine-tuning Llama 2 70B using PyTorch FSDP,https://huggingface.co/blog/ram-efficient-pytorch-fsdp,,[],"Wed, 13 Sep 2023 00:00:00 GMT"
1559,Overview of natively supported quantization schemes in 🤗 Transformers,https://huggingface.co/blog/overview-quantization-transformers,,[],"Tue, 12 Sep 2023 00:00:00 GMT"
1560,SafeCoder vs. Closed-source Code Assistants,https://huggingface.co/blog/safecoder-vs-closed-source-code-assistants,,[],"Mon, 11 Sep 2023 00:00:00 GMT"
1561,Efficient Controllable Generation for SDXL with T2I-Adapters,https://huggingface.co/blog/t2i-sdxl-adapters,,[],"Fri, 08 Sep 2023 00:00:00 GMT"
1562,Spread Your Wings: Falcon 180B is here,https://huggingface.co/blog/falcon-180b,,[],"Wed, 06 Sep 2023 00:00:00 GMT"
1563,Fetch Cuts ML Processing Latency by 50% Using Amazon SageMaker & Hugging Face,https://huggingface.co/blog/fetch-case-study,,[],"Fri, 01 Sep 2023 00:00:00 GMT"
1564,"AudioLDM 2, but faster ⚡️",https://huggingface.co/blog/audioldm2,,[],"Wed, 30 Aug 2023 00:00:00 GMT"
1565,Code Llama: Llama 2 learns to code,https://huggingface.co/blog/codellama,,[],"Fri, 25 Aug 2023 00:00:00 GMT"
1566,Deprecation of Git Authentication using password,https://huggingface.co/blog/password-git-deprecation,,[],"Fri, 25 Aug 2023 00:00:00 GMT"
1567,Making LLMs lighter with AutoGPTQ and transformers,https://huggingface.co/blog/gptq-integration,,[],"Wed, 23 Aug 2023 00:00:00 GMT"
1568,Introducing SafeCoder,https://huggingface.co/blog/safecoder,,[],"Tue, 22 Aug 2023 00:00:00 GMT"
1569,Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Language Model,https://huggingface.co/blog/idefics,,[],"Tue, 22 Aug 2023 00:00:00 GMT"
1570,Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account,https://huggingface.co/blog/aws-marketplace,,[],"Thu, 10 Aug 2023 00:00:00 GMT"
1571,Optimizing Bark using 🤗 Transformers,https://huggingface.co/blog/optimizing-bark,,[],"Wed, 09 Aug 2023 00:00:00 GMT"
1572,Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action,https://huggingface.co/blog/deploy-deepfloydif-using-bentoml,,[],"Wed, 09 Aug 2023 00:00:00 GMT"
1573,Fine-tune Llama 2 with DPO,https://huggingface.co/blog/dpo-trl,,[],"Tue, 08 Aug 2023 00:00:00 GMT"
1574,Releasing Swift Transformers: Run On-Device LLMs in Apple Devices,https://huggingface.co/blog/swift-coreml-llm,,[],"Tue, 08 Aug 2023 00:00:00 GMT"
1575,Deploy MusicGen in no time with Inference Endpoints,https://huggingface.co/blog/run-musicgen-as-an-api,,[],"Fri, 04 Aug 2023 00:00:00 GMT"
1576,Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub,https://huggingface.co/blog/huggy-lingo,,[],"Wed, 02 Aug 2023 00:00:00 GMT"
1577,Towards Encrypted Large Language Models with FHE,https://huggingface.co/blog/encrypted-llm,,[],"Wed, 02 Aug 2023 00:00:00 GMT"
1578,Practical 3D Asset Generation: A Step-by-Step Guide,https://huggingface.co/blog/3d-assets,,[],"Tue, 01 Aug 2023 00:00:00 GMT"
1579,Open-sourcing Knowledge Distillation Code and Weights of SD-Small and SD-Tiny,https://huggingface.co/blog/sd_distillation,,[],"Tue, 01 Aug 2023 00:00:00 GMT"
1580,Stable Diffusion XL on Mac with Advanced Core ML Quantization,https://huggingface.co/blog/stable-diffusion-xl-coreml,,[],"Thu, 27 Jul 2023 00:00:00 GMT"
1581,AI Policy @🤗: Open ML Considerations in the EU AI Act,https://huggingface.co/blog/eu-ai-act-oss,,[],"Mon, 24 Jul 2023 00:00:00 GMT"
1582,Introducing Agents.js: Give tools to your LLMs using JavaScript,https://huggingface.co/blog/agents-js,,[],"Mon, 24 Jul 2023 00:00:00 GMT"
1583,Results of the Open Source AI Game Jam,https://huggingface.co/blog/game-jam-first-edition-results,,[],"Fri, 21 Jul 2023 00:00:00 GMT"
1584,Happy 1st anniversary 🤗 Diffusers!,https://huggingface.co/blog/diffusers-turns-1,,[],"Thu, 20 Jul 2023 00:00:00 GMT"
1585,Llama 2 is here - get it on Hugging Face,https://huggingface.co/blog/llama2,,[],"Tue, 18 Jul 2023 00:00:00 GMT"
1586,Building an AI WebTV,https://huggingface.co/blog/ai-webtv,,[],"Mon, 17 Jul 2023 00:00:00 GMT"
1587,Open-Source Text Generation & LLM Ecosystem at Hugging Face,https://huggingface.co/blog/os-llms,,[],"Mon, 17 Jul 2023 00:00:00 GMT"
1588,Fine-tuning Stable Diffusion models on Intel CPUs,https://huggingface.co/blog/stable-diffusion-finetuning-intel,,[],"Fri, 14 Jul 2023 00:00:00 GMT"
1589,Making ML-powered web games with Transformers.js,https://huggingface.co/blog/ml-web-games,,[],"Wed, 05 Jul 2023 00:00:00 GMT"
1590,Deploy LLMs with Hugging Face Inference Endpoints,https://huggingface.co/blog/inference-endpoints-llm,,[],"Tue, 04 Jul 2023 00:00:00 GMT"
1591,Making a web app generator with open ML models,https://huggingface.co/blog/text-to-webapp,,[],"Mon, 03 Jul 2023 00:00:00 GMT"
1592,Leveraging Hugging Face for complex generative AI use cases,https://huggingface.co/blog/writer-case-study,,[],"Sat, 01 Jul 2023 00:00:00 GMT"
1593,Accelerating Vision-Language Models: BridgeTower on Habana Gaudi2,https://huggingface.co/blog/bridgetower,,[],"Thu, 29 Jun 2023 00:00:00 GMT"
1594,Ethics and Society Newsletter #4: Bias in Text-to-Image Models,https://huggingface.co/blog/ethics-soc-4,,[],"Mon, 26 Jun 2023 00:00:00 GMT"
1595,What's going on with the Open LLM Leaderboard?,https://huggingface.co/blog/open-llm-leaderboard-mmlu,,[],"Fri, 23 Jun 2023 00:00:00 GMT"
1596,Panel on Hugging Face,https://huggingface.co/blog/panel-on-hugging-face,,[],"Thu, 22 Jun 2023 00:00:00 GMT"
1597,Fine-tuning MMS Adapter Models for Multi-Lingual ASR,https://huggingface.co/blog/mms_adapters,,[],"Mon, 19 Jun 2023 00:00:00 GMT"
1598,AI Policy @🤗: Response to the U.S. NTIA's Request for Comment on AI Accountability,https://huggingface.co/blog/policy-ntia-rfc,,[],"Tue, 20 Jun 2023 00:00:00 GMT"
1599,"Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)",https://huggingface.co/blog/autoformer,,[],"Fri, 16 Jun 2023 00:00:00 GMT"
1600,"Faster Stable Diffusion with Core ML on iPhone, iPad, and Mac",https://huggingface.co/blog/fast-diffusers-coreml,,[],"Thu, 15 Jun 2023 00:00:00 GMT"
1601,Deploy Livebook notebooks as apps to Hugging Face Spaces,https://huggingface.co/blog/livebook-app-deployment,,[],"Thu, 15 Jun 2023 00:00:00 GMT"
1602,Announcing our new Content Guidelines and Policy,https://huggingface.co/blog/content-guidelines-update,,[],"Thu, 15 Jun 2023 00:00:00 GMT"
1603,Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU platforms,https://huggingface.co/blog/huggingface-and-amd,,[],"Tue, 13 Jun 2023 00:00:00 GMT"
1604,Can foundation models label data like humans?,https://huggingface.co/blog/open-llm-leaderboard-rlhf,,[],"Mon, 12 Jun 2023 00:00:00 GMT"
1605,"The Hugging Face Hub for Galleries, Libraries, Archives and Museums",https://huggingface.co/blog/hf-hub-glam-guide,,[],"Mon, 12 Jun 2023 00:00:00 GMT"
1606,"DuckDB: run SQL queries on 50,000+ datasets on the Hugging Face Hub",https://huggingface.co/blog/hub-duckdb,,[],"Wed, 07 Jun 2023 00:00:00 GMT"
1607,Welcome fastText to the 🤗 Hub,https://huggingface.co/blog/fasttext,,[],"Tue, 06 Jun 2023 00:00:00 GMT"
1608,The Falcon has landed in the Hugging Face ecosystem,https://huggingface.co/blog/falcon,,[],"Mon, 05 Jun 2023 00:00:00 GMT"
1609,AI Speech Recognition in Unity,https://huggingface.co/blog/unity-asr,,[],"Fri, 02 Jun 2023 00:00:00 GMT"
1610,Announcing the Open Source AI Game Jam 🎮,https://huggingface.co/blog/game-jam,,[],"Thu, 01 Jun 2023 00:00:00 GMT"
1611,Hugging Face Selected for the French Data Protection Agency Enhanced Support Program,https://huggingface.co/blog/cnil,,[],"Mon, 15 May 2023 00:00:00 GMT"
1612,Introducing the Hugging Face LLM Inference Container for Amazon SageMaker,https://huggingface.co/blog/sagemaker-huggingface-llm,,[],"Wed, 31 May 2023 00:00:00 GMT"
1613,Introducing BERTopic Integration with Hugging Face Hub,https://huggingface.co/blog/bertopic,,[],"Wed, 31 May 2023 00:00:00 GMT"
1614,Optimizing Stable Diffusion for Intel CPUs with NNCF and 🤗 Optimum,https://huggingface.co/blog/train-optimize-sd-intel,,[],"Thu, 25 May 2023 00:00:00 GMT"
1615,"Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA",https://huggingface.co/blog/4bit-transformers-bitsandbytes,,[],"Wed, 24 May 2023 00:00:00 GMT"
1616,Hugging Face Collaborates with Microsoft to Launch Hugging Face Model Catalog on Azure,https://huggingface.co/blog/hugging-face-endpoints-on-azure,,[],"Wed, 24 May 2023 00:00:00 GMT"
1617,"Hugging Face and IBM partner on watsonx.ai, the next-generation enterprise studio for AI builders",https://huggingface.co/blog/huggingface-and-ibm,,[],"Tue, 23 May 2023 00:00:00 GMT"
1618,Safetensors audited as really safe and becoming the default,https://huggingface.co/blog/safetensors-security-audit,,[],"Tue, 23 May 2023 00:00:00 GMT"
1619,Instruction-tuning Stable Diffusion with InstructPix2Pix,https://huggingface.co/blog/instruction-tuning-sd,,[],"Tue, 23 May 2023 00:00:00 GMT"
1620,Large-scale Near-deduplication Behind BigCode,https://huggingface.co/blog/dedup,,[],"Tue, 16 May 2023 00:00:00 GMT"
1621,"Smaller is better: Q8-Chat, an efficient generative AI experience on Xeon",https://huggingface.co/blog/generative-ai-models-on-intel-cpu,,[],"Tue, 16 May 2023 00:00:00 GMT"
1622,Run a Chatgpt-like Chatbot on a Single GPU with ROCm,https://huggingface.co/blog/chatbot-amd-gpu,,[],"Mon, 15 May 2023 00:00:00 GMT"
1623,Introducing RWKV — An RNN with the advantages of a transformer,https://huggingface.co/blog/rwkv,,[],"Mon, 15 May 2023 00:00:00 GMT"
1624,Assisted Generation: a new direction toward low-latency text generation,https://huggingface.co/blog/assisted-generation,,[],"Thu, 11 May 2023 00:00:00 GMT"
1625,Creating a Coding Assistant with StarCoder,https://huggingface.co/blog/starchat-alpha,,[],"Tue, 09 May 2023 00:00:00 GMT"
1626,A Dive into Text-to-Video Models,https://huggingface.co/blog/text-to-video,,[],"Mon, 08 May 2023 00:00:00 GMT"
1627,StarCoder: A State-of-the-Art LLM for Code,https://huggingface.co/blog/starcoder,,[],"Thu, 04 May 2023 00:00:00 GMT"
1628,How to Install and Use the Hugging Face Unity API,https://huggingface.co/blog/unity-api,,[],"Mon, 01 May 2023 00:00:00 GMT"
1629,Running IF with 🧨 diffusers on a Free Tier Google Colab,https://huggingface.co/blog/if,,[],"Wed, 26 Apr 2023 00:00:00 GMT"
1630,Training a language model with 🤗 Transformers using TensorFlow and TPUs,https://huggingface.co/blog/tf_tpu,,[],"Thu, 27 Apr 2023 00:00:00 GMT"
1631,Databricks ❤️ Hugging Face: up to 40% faster training and tuning of Large Language Models,https://huggingface.co/blog/databricks-case-study,,[],"Wed, 26 Apr 2023 00:00:00 GMT"
1632,Introducing HuggingFace blog for Chinese speakers: Fostering Collaboration with the Chinese AI community,https://huggingface.co/blog/chinese-language-blog,,[],"Mon, 24 Apr 2023 00:00:00 GMT"
1633,How to host a Unity game in a Space,https://huggingface.co/blog/unity-in-spaces,,[],"Fri, 21 Apr 2023 00:00:00 GMT"
1634,Accelerating Hugging Face Transformers with AWS Inferentia2,https://huggingface.co/blog/accelerate-transformers-with-inferentia2,,[],"Mon, 17 Apr 2023 00:00:00 GMT"
1635,Graph Classification with Transformers,https://huggingface.co/blog/graphml-classification,,[],"Fri, 14 Apr 2023 00:00:00 GMT"
1636,Creating Privacy Preserving AI with Substra,https://huggingface.co/blog/owkin-substra,,[],"Wed, 12 Apr 2023 00:00:00 GMT"
1637,Snorkel AI x Hugging Face: unlock foundation models for enterprises,https://huggingface.co/blog/snorkel-case-study,,[],"Thu, 06 Apr 2023 00:00:00 GMT"
1638,StackLLaMA: A hands-on guide to train LLaMA with RLHF,https://huggingface.co/blog/stackllama,,[],"Wed, 05 Apr 2023 00:00:00 GMT"
1639,Ethics and Society Newsletter #3: Ethical Openness at Hugging Face,https://huggingface.co/blog/ethics-soc-3,,[],"Thu, 30 Mar 2023 00:00:00 GMT"
1640,Fast Inference on Large Language Models: BLOOMZ on Habana Gaudi2 Accelerator,https://huggingface.co/blog/habana-gaudi-2-bloom,,[],"Tue, 28 Mar 2023 00:00:00 GMT"
1641,Accelerating Stable Diffusion Inference on Intel CPUs,https://huggingface.co/blog/stable-diffusion-inference-intel,,[],"Tue, 28 Mar 2023 00:00:00 GMT"
1642,Federated Learning using Hugging Face and Flower,https://huggingface.co/blog/fl-with-flower,,[],"Mon, 27 Mar 2023 00:00:00 GMT"
1643,Train your ControlNet with diffusers,https://huggingface.co/blog/train-your-controlnet,,[],"Fri, 24 Mar 2023 00:00:00 GMT"
1644,Jupyter X Hugging Face,https://huggingface.co/blog/notebooks-hub,,[],"Thu, 23 Mar 2023 00:00:00 GMT"
1645,Multivariate Probabilistic Time Series Forecasting with Informer,https://huggingface.co/blog/informer,,[],"Fri, 10 Mar 2023 00:00:00 GMT"
1646,Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU,https://huggingface.co/blog/trl-peft,,[],"Thu, 09 Mar 2023 00:00:00 GMT"
1647,New ViT and ALIGN Models From Kakao Brain,https://huggingface.co/blog/vit-align,,[],"Mon, 06 Mar 2023 00:00:00 GMT"
1648,Using Machine Learning to Aid Survivors and Race through Time,https://huggingface.co/blog/using-ml-for-disasters,,[],"Fri, 03 Mar 2023 00:00:00 GMT"
1649,ControlNet in Diffusers 🧨,https://huggingface.co/blog/controlnet,,[],"Fri, 03 Mar 2023 00:00:00 GMT"
1650,Ethical guidelines for developing the Diffusers library,https://huggingface.co/blog/ethics-diffusers,,[],"Thu, 02 Mar 2023 00:00:00 GMT"
1651,How Hugging Face Accelerated Development of Witty Works Writing Assistant,https://huggingface.co/blog/classification-use-cases,,[],"Wed, 01 Mar 2023 00:00:00 GMT"
1652,Red-Teaming Large Language Models,https://huggingface.co/blog/red-teaming,,[],"Fri, 24 Feb 2023 00:00:00 GMT"
1653,Swift Diffusers: Fast Stable Diffusion for Mac,https://huggingface.co/blog/fast-mac-diffusers,,[],"Fri, 24 Feb 2023 00:00:00 GMT"
1654,Hugging Face and AWS partner to make AI more accessible,https://huggingface.co/blog/aws-partnership,,[],"Tue, 21 Feb 2023 00:00:00 GMT"
1655,Zero-shot image-to-text generation with BLIP-2,https://huggingface.co/blog/blip-2,,[],"Wed, 15 Feb 2023 00:00:00 GMT"
1656,"Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too",https://huggingface.co/blog/mantis-case-study,,[],"Wed, 15 Feb 2023 00:00:00 GMT"
1657,🤗 PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware,https://huggingface.co/blog/peft,,[],"Fri, 10 Feb 2023 00:00:00 GMT"
1658,"Speech Synthesis, Recognition, and More With SpeechT5",https://huggingface.co/blog/speecht5,,[],"Wed, 08 Feb 2023 00:00:00 GMT"
1659,Generating Stories: AI for Game Development #5,https://huggingface.co/blog/ml-for-games-5,,[],"Tue, 07 Feb 2023 00:00:00 GMT"
1660,Introducing ⚔️ AI vs. AI ⚔️ a deep reinforcement learning multi-agents competition system,https://huggingface.co/blog/aivsai,,[],"Tue, 07 Feb 2023 00:00:00 GMT"
1661,"Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2",https://huggingface.co/blog/intel-sapphire-rapids-inference,,[],"Mon, 06 Feb 2023 00:00:00 GMT"
1662,A Dive into Pretraining Strategies for Vision-Language Models,https://huggingface.co/blog/vision_language_pretraining,,[],"Fri, 03 Feb 2023 00:00:00 GMT"
1663,The State of Computer Vision at Hugging Face 🤗,https://huggingface.co/blog/cv_state,,[],"Mon, 30 Jan 2023 00:00:00 GMT"
1664,2D Asset Generation: AI for Game Development #4,https://huggingface.co/blog/ml-for-games-4,,[],"Thu, 26 Jan 2023 00:00:00 GMT"
1665,Using LoRA for Efficient Stable Diffusion Fine-Tuning,https://huggingface.co/blog/lora,,[],"Thu, 26 Jan 2023 00:00:00 GMT"
1666,What Makes a Dialog Agent Useful?,https://huggingface.co/blog/dialog-agents,,[],"Tue, 24 Jan 2023 00:00:00 GMT"
1667,"Optimum+ONNX Runtime - Easier, Faster training for your Hugging Face models",https://huggingface.co/blog/optimum-onnxruntime-training,,[],"Tue, 24 Jan 2023 00:00:00 GMT"
1668,3D Asset Generation: AI for Game Development #3,https://huggingface.co/blog/ml-for-games-3,,[],"Fri, 20 Jan 2023 00:00:00 GMT"
1669,Universal Image Segmentation with Mask2Former and OneFormer,https://huggingface.co/blog/mask2former,,[],"Thu, 19 Jan 2023 00:00:00 GMT"
1670,Welcome PaddlePaddle to the Hugging Face Hub,https://huggingface.co/blog/paddlepaddle,,[],"Tue, 17 Jan 2023 00:00:00 GMT"
1671,Image Similarity with Hugging Face Datasets and Transformers,https://huggingface.co/blog/image-similarity,,[],"Mon, 16 Jan 2023 00:00:00 GMT"
1672,AI for Game Development: Creating a Farming Game in 5 Days. Part 2,https://huggingface.co/blog/ml-for-games-2,,[],"Mon, 09 Jan 2023 00:00:00 GMT"
1673,Introduction to Graph Machine Learning,https://huggingface.co/blog/intro-graphml,,[],"Tue, 03 Jan 2023 00:00:00 GMT"
1674,AI for Game Development: Creating a Farming Game in 5 Days. Part 1,https://huggingface.co/blog/ml-for-games-1,,[],"Mon, 02 Jan 2023 00:00:00 GMT"
1675,"Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 1",https://huggingface.co/blog/intel-sapphire-rapids,,[],"Mon, 02 Jan 2023 00:00:00 GMT"
1676,Zero-shot image segmentation with CLIPSeg,https://huggingface.co/blog/clipseg-zero-shot,,[],"Wed, 21 Dec 2022 00:00:00 GMT"
1677,Model Cards: Introducing HF Model documentation tools,https://huggingface.co/blog/model-cards,,[],"Tue, 20 Dec 2022 00:00:00 GMT"
1678,Ethics and Society Newsletter #2: Let's talk about bias!,https://huggingface.co/blog/ethics-soc-2,,[],"Thu, 15 Dec 2022 00:00:00 GMT"
1679,A Complete Guide to Audio Datasets,https://huggingface.co/blog/audio-datasets,,[],"Thu, 15 Dec 2022 00:00:00 GMT"
1680,Faster Training and Inference: Habana Gaudi®2 vs Nvidia A100 80GB,https://huggingface.co/blog/habana-gaudi-2-benchmark,,[],"Wed, 14 Dec 2022 00:00:00 GMT"
1681,Illustrating Reinforcement Learning from Human Feedback (RLHF),https://huggingface.co/blog/rlhf,,[],"Fri, 09 Dec 2022 00:00:00 GMT"
1682,From GPT2 to Stable Diffusion: Hugging Face arrives to the Elixir community,https://huggingface.co/blog/elixir-bumblebee,,[],"Fri, 09 Dec 2022 00:00:00 GMT"
1683,Deep Learning with Proteins,https://huggingface.co/blog/deep-learning-with-proteins,,[],"Fri, 02 Dec 2022 00:00:00 GMT"
1684,Using Stable Diffusion with Core ML on Apple Silicon,https://huggingface.co/blog/diffusers-coreml,,[],"Thu, 01 Dec 2022 00:00:00 GMT"
1685,Probabilistic Time Series Forecasting with 🤗 Transformers,https://huggingface.co/blog/time-series-transformers,,[],"Thu, 01 Dec 2022 00:00:00 GMT"
1686,VQ Diffusion with 🧨 Diffusers,https://huggingface.co/blog/vq-diffusion,,[],"Wed, 30 Nov 2022 00:00:00 GMT"
1687,We are hiring interns!,https://huggingface.co/blog/interns-2023,,[],"Tue, 29 Nov 2022 00:00:00 GMT"
1688,Diffusion Models Live Event,https://huggingface.co/blog/diffusion-models-event,,[],"Fri, 25 Nov 2022 00:00:00 GMT"
1689,Accelerating Document AI,https://huggingface.co/blog/document-ai,,[],"Mon, 21 Nov 2022 00:00:00 GMT"
1690,An Overview of Inference Solutions on Hugging Face,https://huggingface.co/blog/inference-update,,[],"Mon, 21 Nov 2022 00:00:00 GMT"
1691,Director of Machine Learning Insights [Part 4],https://huggingface.co/blog/ml-director-insights-4,,[],"Wed, 23 Nov 2022 00:00:00 GMT"
1692,Hugging Face Machine Learning Demos on arXiv,https://huggingface.co/blog/arxiv,,[],"Thu, 17 Nov 2022 00:00:00 GMT"
1693,Sentiment Classification with Fully Homomorphic Encryption using Concrete ML,https://huggingface.co/blog/sentiment-analysis-fhe,,[],"Thu, 17 Nov 2022 00:00:00 GMT"
1694,Generating Human-level Text with Contrastive Search in Transformers 🤗,https://huggingface.co/blog/introducing-csearch,,[],"Tue, 08 Nov 2022 00:00:00 GMT"
1695,Introducing our new pricing,https://huggingface.co/blog/pricing-update,,[],"Tue, 08 Nov 2022 00:00:00 GMT"
1696,Training Stable Diffusion with Dreambooth using 🧨 Diffusers,https://huggingface.co/blog/dreambooth,,[],"Mon, 07 Nov 2022 00:00:00 GMT"
1697,Fine-Tune Whisper with 🤗 Transformers,https://huggingface.co/blog/fine-tune-whisper,,[],"Thu, 03 Nov 2022 00:00:00 GMT"
1698,Accelerate your models with 🤗 Optimum Intel and OpenVINO,https://huggingface.co/blog/openvino,,[],"Wed, 02 Nov 2022 00:00:00 GMT"
1699,Evaluating Language Model Bias with 🤗 Evaluate,https://huggingface.co/blog/evaluating-llm-bias,,[],"Mon, 24 Oct 2022 00:00:00 GMT"
1700,"From PyTorch DDP to 🤗 Accelerate to 🤗 Trainer, mastery of distributed training with ease",https://huggingface.co/blog/pytorch-ddp-accelerate-transformers,,[],"Fri, 21 Oct 2022 00:00:00 GMT"
1701,MTEB: Massive Text Embedding Benchmark,https://huggingface.co/blog/mteb,,[],"Wed, 19 Oct 2022 00:00:00 GMT"
1702,Getting started with Hugging Face Inference Endpoints,https://huggingface.co/blog/inference-endpoints,,[],"Fri, 14 Oct 2022 00:00:00 GMT"
1703,Stable Diffusion in JAX/Flax 🚀,https://huggingface.co/blog/stable_diffusion_jax,,[],"Thu, 13 Oct 2022 00:00:00 GMT"
1704,Optimization story: Bloom inference,https://huggingface.co/blog/bloom-inference-optimization,,[],"Wed, 12 Oct 2022 00:00:00 GMT"
1705,Introducing DOI: the Digital Object Identifier to Datasets and Models,https://huggingface.co/blog/introducing-doi,,[],"Fri, 07 Oct 2022 00:00:00 GMT"
1706,Japanese Stable Diffusion,https://huggingface.co/blog/japanese-stable-diffusion,,[],"Wed, 05 Oct 2022 00:00:00 GMT"
1707,Very Large Language Models and How to Evaluate Them,https://huggingface.co/blog/zero-shot-eval-on-the-hub,,[],"Mon, 03 Oct 2022 00:00:00 GMT"
1708,Image Classification with AutoTrain,https://huggingface.co/blog/autotrain-image-classification,,[],"Wed, 28 Sep 2022 00:00:00 GMT"
1709,How 🤗 Accelerate runs very large models thanks to PyTorch,https://huggingface.co/blog/accelerate-large-models,,[],"Tue, 27 Sep 2022 00:00:00 GMT"
1710,SetFit: Efficient Few-Shot Learning Without Prompts,https://huggingface.co/blog/setfit,,[],"Mon, 26 Sep 2022 00:00:00 GMT"
1711,Ethics and Society Newsletter #1,https://huggingface.co/blog/ethics-soc-1,,[],"Thu, 22 Sep 2022 00:00:00 GMT"
1712,Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate,https://huggingface.co/blog/bloom-inference-pytorch-scripts,,[],"Fri, 16 Sep 2022 00:00:00 GMT"
1713,How to train a Language Model with Megatron-LM,https://huggingface.co/blog/megatron-training,,[],"Wed, 07 Sep 2022 00:00:00 GMT"
1714,What's new in Diffusers? 🎨,https://huggingface.co/blog/diffusers-2nd-month,,[],"Mon, 12 Sep 2022 00:00:00 GMT"
1715,Train your first Decision Transformer,https://huggingface.co/blog/train-decision-transformers,,[],"Thu, 08 Sep 2022 00:00:00 GMT"
1716,OpenRAIL: Towards open and responsible AI licensing frameworks,https://huggingface.co/blog/open_rail,,[],"Wed, 31 Aug 2022 00:00:00 GMT"
1717,Visualize proteins on Hugging Face Spaces,https://huggingface.co/blog/spaces_3dmoljs,,[],"Wed, 24 Aug 2022 00:00:00 GMT"
1718,Stable Diffusion with 🧨 Diffusers,https://huggingface.co/blog/stable_diffusion,,[],"Mon, 22 Aug 2022 00:00:00 GMT"
1719,Pre-Train BERT with Hugging Face Transformers and Habana Gaudi,https://huggingface.co/blog/pretraining-bert,,[],"Mon, 22 Aug 2022 00:00:00 GMT"
1720,Deploying 🤗 ViT on Vertex AI,https://huggingface.co/blog/deploy-vertex-ai,,[],"Fri, 19 Aug 2022 00:00:00 GMT"
1721,Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore,https://huggingface.co/blog/vision-transformers,,[],"Thu, 18 Aug 2022 00:00:00 GMT"
1722,"A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes",https://huggingface.co/blog/hf-bitsandbytes-integration,,[],"Wed, 17 Aug 2022 00:00:00 GMT"
1723,Introducing Skops,https://huggingface.co/blog/skops,,[],"Fri, 12 Aug 2022 00:00:00 GMT"
1724,Hugging Face's TensorFlow Philosophy,https://huggingface.co/blog/tensorflow-philosophy,,[],"Fri, 12 Aug 2022 00:00:00 GMT"
1725,Deploying 🤗 ViT on Kubernetes with TF Serving,https://huggingface.co/blog/deploy-tfserving-kubernetes,,[],"Thu, 11 Aug 2022 00:00:00 GMT"
1726,Train and Fine-Tune Sentence Transformers Models,https://huggingface.co/blog/how-to-train-sentence-transformers,,[],"Wed, 10 Aug 2022 00:00:00 GMT"
1727,Proximal Policy Optimization (PPO),https://huggingface.co/blog/deep-rl-ppo,,[],"Fri, 05 Aug 2022 00:00:00 GMT"
1728,Introducing the Private Hub: A New Way to Build With Machine Learning,https://huggingface.co/blog/introducing-private-hub,,[],"Wed, 03 Aug 2022 00:00:00 GMT"
1729,"Nyströmformer, Approximating self-attention in linear time and memory via the Nyström method",https://huggingface.co/blog/nystromformer,,[],"Tue, 02 Aug 2022 00:00:00 GMT"
1730,AI Policy @🤗: Comments on U.S. National AI Research Resource Interim Report,https://huggingface.co/blog/us-national-ai-research-resource,,[],"Mon, 01 Aug 2022 00:00:00 GMT"
1731,Introducing new audio and vision documentation in 🤗 Datasets,https://huggingface.co/blog/datasets-docs-update,,[],"Thu, 28 Jul 2022 00:00:00 GMT"
1732,Faster Text Generation with TensorFlow and XLA,https://huggingface.co/blog/tf-xla-generate,,[],"Wed, 27 Jul 2022 00:00:00 GMT"
1733,Deploying TensorFlow Vision Models in Hugging Face with TF Serving,https://huggingface.co/blog/tf-serving-vision,,[],"Mon, 25 Jul 2022 00:00:00 GMT"
1734,Advantage Actor Critic (A2C),https://huggingface.co/blog/deep-rl-a2c,,[],"Fri, 22 Jul 2022 00:00:00 GMT"
1735,How to train your model dynamically using adversarial data,https://huggingface.co/blog/mnist-adversarial,,[],"Sat, 16 Jul 2022 00:00:00 GMT"
1736,The Technology Behind BLOOM Training,https://huggingface.co/blog/bloom-megatron-deepspeed,,[],"Thu, 14 Jul 2022 00:00:00 GMT"
1737,Building a Playlist Generator with Sentence Transformers,https://huggingface.co/blog/playlist-generator,,[],"Wed, 13 Jul 2022 00:00:00 GMT"
1738,Introducing The World's Largest Open Multilingual Language Model: BLOOM,https://huggingface.co/blog/bloom,,[],"Tue, 12 Jul 2022 00:00:00 GMT"
1739,Getting Started with Sentiment Analysis on Twitter,https://huggingface.co/blog/sentiment-analysis-twitter,,[],"Thu, 07 Jul 2022 00:00:00 GMT"
1740,Policy Gradient with PyTorch,https://huggingface.co/blog/deep-rl-pg,,[],"Thu, 30 Jun 2022 00:00:00 GMT"
1741,Liftoff! How to get started with your first ML project 🚀,https://huggingface.co/blog/your-first-ml-project,,[],"Wed, 29 Jun 2022 00:00:00 GMT"
1742,Accelerate Large Model Training using DeepSpeed,https://huggingface.co/blog/accelerate-deepspeed,,[],"Tue, 28 Jun 2022 00:00:00 GMT"
1743,Announcing Evaluation on the Hub,https://huggingface.co/blog/eval-on-the-hub,,[],"Tue, 28 Jun 2022 00:00:00 GMT"
1744,Getting Started With Embeddings,https://huggingface.co/blog/getting-started-with-embeddings,,[],"Thu, 23 Jun 2022 00:00:00 GMT"
1745,Convert Transformers to ONNX with Hugging Face Optimum,https://huggingface.co/blog/convert-transformers-to-onnx,,[],"Wed, 22 Jun 2022 00:00:00 GMT"
1746,Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration,https://huggingface.co/blog/intel,,[],"Wed, 15 Jun 2022 00:00:00 GMT"
1747,Director of Machine Learning Insights [Part 3: Finance Edition],https://huggingface.co/blog/ml-director-insights-3,,[],"Tue, 14 Jun 2022 00:00:00 GMT"
1748,Using Sentence Transformers for semantic search,https://huggingface.co/spaces/sentence-transformers/Sentence_Transformers_for_semantic_search,,[],"Fri, 10 Jun 2022 00:00:00 GMT"
1749,Code generation with Hugging Face,https://huggingface.co/spaces/loubnabnl/code-generation-models,,[],"Wed, 08 Jun 2022 00:00:00 GMT"
1750,The Annotated Diffusion Model,https://huggingface.co/blog/annotated-diffusion,,[],"Tue, 07 Jun 2022 00:00:00 GMT"
1751,Deep Q-Learning with Atari,https://huggingface.co/blog/deep-rl-dqn,,[],"Tue, 07 Jun 2022 00:00:00 GMT"
1752,Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers,https://huggingface.co/blog/graphcore-update,,[],"Thu, 26 May 2022 00:00:00 GMT"
1753,Introducing Pull Requests and Discussions 🥳,https://huggingface.co/blog/community-update,,[],"Wed, 25 May 2022 00:00:00 GMT"
1754,Efficient Table Pre-training without Real Data: An Introduction to TAPEX,https://huggingface.co/blog/tapex,,[],"Mon, 23 May 2022 00:00:00 GMT"
1755,An Introduction to Q-Learning Part 2,https://huggingface.co/blog/deep-rl-q-part2,,[],"Fri, 20 May 2022 00:00:00 GMT"
1756,How Sempre Health is leveraging the Expert Acceleration Program to accelerate their ML roadmap,https://huggingface.co/blog/sempre-health-eap-case-study,,[],"Thu, 19 May 2022 00:00:00 GMT"
1757,Putting ethical principles at the core of research lifecycle,https://huggingface.co/blog/ethical-charter-multimodal,,[],"Thu, 19 May 2022 00:00:00 GMT"
1758,An Introduction to Q-Learning Part 1,https://huggingface.co/blog/deep-rl-q-part1,,[],"Wed, 18 May 2022 00:00:00 GMT"
1759,Machine Learning Experts - Sasha Luccioni Interview,https://huggingface.co/blog/sasha-luccioni-interview,,[],"Tue, 17 May 2022 00:00:00 GMT"
1760,Announcing the Hugging Face Fellowship Program,https://huggingface.co/blog/fellowship,,[],"Tue, 17 May 2022 00:00:00 GMT"
1761,Gradio 3.0 is Out!,https://huggingface.co/blog/gradio-blocks,,[],"Mon, 16 May 2022 00:00:00 GMT"
1762,Director of Machine Learning Insights [Part 2: SaaS Edition],https://huggingface.co/blog/ml-director-insights-2,,[],"Fri, 13 May 2022 00:00:00 GMT"
1763,Student Ambassador Program's call for applications is open!,https://huggingface.co/blog/ambassadors,,[],"Fri, 13 May 2022 00:00:00 GMT"
1764,Accelerated Inference with Optimum and Transformers Pipelines,https://huggingface.co/blog/optimum-inference,,[],"Tue, 10 May 2022 00:00:00 GMT"
1765,We Raised $100 Million for Open & Collaborative Machine Learning 🚀,https://huggingface.co/blog/series-c,,[],"Mon, 09 May 2022 00:00:00 GMT"
1766,Welcome fastai to the Hugging Face Hub,https://huggingface.co/blog/fastai,,[],"Fri, 06 May 2022 00:00:00 GMT"
1767,An Introduction to Deep Reinforcement Learning,https://huggingface.co/blog/deep-rl-intro,,[],"Wed, 04 May 2022 00:00:00 GMT"
1768,Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel,https://huggingface.co/blog/pytorch-fsdp,,[],"Mon, 02 May 2022 00:00:00 GMT"
1769,Opinion Classification with Kili and HuggingFace AutoTrain,https://huggingface.co/blog/opinion-classification-with-kili,,[],"Thu, 28 Apr 2022 00:00:00 GMT"
1770,Director of Machine Learning Insights [Series],https://huggingface.co/blog/ml-director-insights,,[],"Wed, 27 Apr 2022 00:00:00 GMT"
1771,Getting Started with Transformers on Habana Gaudi,https://huggingface.co/blog/getting-started-habana,,[],"Tue, 26 Apr 2022 00:00:00 GMT"
1772,Introducing Hugging Face for Education,https://huggingface.co/blog/education,,[],"Mon, 25 Apr 2022 00:00:00 GMT"
1773,Supercharged Customer Service with Machine Learning,https://huggingface.co/blog/supercharge-customer-service-with-machine-learning,,[],"Mon, 25 Apr 2022 00:00:00 GMT"
1774,CO2 Emissions and the 🤗 Hub: Leading the Charge,https://huggingface.co/blog/carbon-emissions-on-the-hub,,[],"Fri, 22 Apr 2022 00:00:00 GMT"
1775,Machine Learning Experts - Lewis Tunstall Interview,https://huggingface.co/blog/lewis-tunstall-interview,,[],"Wed, 13 Apr 2022 00:00:00 GMT"
1776,Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training,https://huggingface.co/blog/habana,,[],"Tue, 12 Apr 2022 00:00:00 GMT"
1777,Don't repeat yourself - 🤗 Transformers Design Philosophy,https://huggingface.co/blog/transformers-design-philosophy,,[],"Tue, 05 Apr 2022 00:00:00 GMT"
1778,Introducing Decision Transformers on Hugging Face 🤗,https://huggingface.co/blog/decision-transformers,,[],"Mon, 28 Mar 2022 00:00:00 GMT"
1779,Machine Learning Experts - Meg Mitchell Interview,https://huggingface.co/blog/meg-mitchell-interview,,[],"Wed, 23 Mar 2022 00:00:00 GMT"
1780,Announcing the 🤗 AI Research Residency Program,https://huggingface.co/blog/ai-residency,,[],"Tue, 22 Mar 2022 00:00:00 GMT"
1781,Fine-Tune a Semantic Segmentation Model with a Custom Dataset,https://huggingface.co/blog/fine-tune-segformer,,[],"Thu, 17 Mar 2022 00:00:00 GMT"
1782,Accelerate BERT inference with Hugging Face Transformers and AWS inferentia,https://huggingface.co/blog/bert-inferentia-sagemaker,,[],"Wed, 16 Mar 2022 00:00:00 GMT"
1783,Image search with 🤗 datasets,https://huggingface.co/blog/image-search-datasets,,[],"Wed, 16 Mar 2022 00:00:00 GMT"
1784,Guiding Text Generation with Constrained Beam Search in 🤗 Transformers,https://huggingface.co/blog/constrained-beam-search,,[],"Fri, 11 Mar 2022 00:00:00 GMT"
1785,BERT 101 🤗 State Of The Art NLP Model Explained,https://huggingface.co/blog/bert-101,,[],"Wed, 02 Mar 2022 00:00:00 GMT"
1786,Fine-Tune ViT for Image Classification with 🤗 Transformers,https://huggingface.co/blog/fine-tune-vit,,[],"Fri, 11 Feb 2022 00:00:00 GMT"
1787,Getting Started with Sentiment Analysis using Python,https://huggingface.co/blog/sentiment-analysis-python,,[],"Wed, 02 Feb 2022 00:00:00 GMT"
1788,Making automatic speech recognition work on large files with Wav2Vec2 in 🤗 Transformers,https://huggingface.co/blog/asr-chunking,,[],"Tue, 01 Feb 2022 00:00:00 GMT"
1789,Supercharged Searching on the Hugging Face Hub,https://huggingface.co/blog/searching-the-hub,,[],"Tue, 25 Jan 2022 00:00:00 GMT"
1790,Welcome Stable-baselines3 to the Hugging Face Hub 🤗,https://huggingface.co/blog/sb3,,[],"Fri, 21 Jan 2022 00:00:00 GMT"
1791,Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs,https://huggingface.co/blog/infinity-cpu-performance,,[],"Thu, 13 Jan 2022 00:00:00 GMT"
1792,Boost Wav2Vec2 with n-gram LM in 🤗 Transformers,https://huggingface.co/blog/wav2vec2-with-ngram,,[],"Wed, 12 Jan 2022 00:00:00 GMT"
1793,Deploy GPT-J 6B for inference using  Hugging Face Transformers and Amazon SageMaker,https://huggingface.co/blog/gptj-sagemaker,,[],"Tue, 11 Jan 2022 00:00:00 GMT"
1794,Active Learning with AutoNLP and Prodigy,https://huggingface.co/blog/autonlp-prodigy,,[],"Thu, 23 Dec 2021 00:00:00 GMT"
1795,Gradio joins Hugging Face!,https://huggingface.co/blog/gradio-joins-hf,,[],"Tue, 21 Dec 2021 00:00:00 GMT"
1796,"Perceiver IO: a scalable, fully-attentional model that works on any modality",https://huggingface.co/blog/perceiver,,[],"Wed, 15 Dec 2021 00:00:00 GMT"
1797,Training CodeParrot 🦜 from Scratch,https://huggingface.co/blog/codeparrot,,[],"Wed, 08 Dec 2021 00:00:00 GMT"
1798,"Introducing Snowball Fight ☃️, our First ML-Agents Environment",https://huggingface.co/blog/snowball-fight,,[],"Thu, 02 Dec 2021 00:00:00 GMT"
1799,Getting Started with Hugging Face Transformers for IPUs with Optimum,https://huggingface.co/blog/graphcore-getting-started,,[],"Tue, 30 Nov 2021 00:00:00 GMT"
1800,Introducing the Data Measurements Tool: an Interactive Tool for Looking at Datasets,https://huggingface.co/blog/data-measurements-tool,,[],"Mon, 29 Nov 2021 00:00:00 GMT"
1801,Accelerating PyTorch distributed fine-tuning with Intel technologies,https://huggingface.co/blog/accelerating-pytorch,,[],"Fri, 19 Nov 2021 00:00:00 GMT"
1802,Fine-tuning XLS-R for Multi-Lingual ASR with 🤗 Transformers,https://huggingface.co/blog/fine-tune-xlsr-wav2vec2,,[],"Mon, 15 Nov 2021 00:00:00 GMT"
1803,Scaling up BERT-like model Inference on modern CPU - Part 2,https://huggingface.co/blog/bert-cpu-scaling-part-2,,[],"Thu, 04 Nov 2021 00:00:00 GMT"
1804,Course Launch Community Event,https://huggingface.co/blog/course-launch-event,,[],"Tue, 26 Oct 2021 00:00:00 GMT"
1805,Large Language Models: A New Moore's Law?,https://huggingface.co/blog/large-language-models,,[],"Tue, 26 Oct 2021 00:00:00 GMT"
1806,Train a Sentence Embedding Model with 1B Training Pairs,https://huggingface.co/blog/1b-sentence-embeddings,,[],"Mon, 25 Oct 2021 00:00:00 GMT"
1807,The Age of Machine Learning As Code Has Arrived,https://huggingface.co/blog/the-age-of-ml-as-code,,[],"Wed, 20 Oct 2021 00:00:00 GMT"
1808,Fine tuning CLIP with Remote Sensing (Satellite) images and captions,https://huggingface.co/blog/fine-tune-clip-rsicd,,[],"Wed, 13 Oct 2021 00:00:00 GMT"
1809,Hosting your Models and Datasets on Hugging Face Spaces using Streamlit,https://huggingface.co/blog/streamlit-spaces,,[],"Tue, 05 Oct 2021 00:00:00 GMT"
1810,Showcase Your Projects in Spaces using Gradio,https://huggingface.co/blog/gradio-spaces,,[],"Tue, 05 Oct 2021 00:00:00 GMT"
1811,Summer at Hugging Face ☀️,https://huggingface.co/blog/summer-at-huggingface,,[],"Fri, 24 Sep 2021 00:00:00 GMT"
1812,Hugging Face and Graphcore partner for IPU-optimized Transformers,https://huggingface.co/blog/graphcore,,[],"Tue, 14 Sep 2021 00:00:00 GMT"
1813,Introducing Optimum: The Optimization Toolkit for Transformers at Scale,https://huggingface.co/blog/hardware-partners-program,,[],"Tue, 14 Sep 2021 00:00:00 GMT"
1814,Deep Learning over the Internet: Training Language Models Collaboratively,https://huggingface.co/blog/collaborative-training,,[],"Thu, 15 Jul 2021 00:00:00 GMT"
1815,Welcome spaCy to the 🤗 Hub,https://huggingface.co/blog/spacy,,[],"Tue, 13 Jul 2021 00:00:00 GMT"
1816,Deploy Hugging Face models easily with Amazon SageMaker,https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker,,[],"Thu, 08 Jul 2021 00:00:00 GMT"
1817,Sentence Transformers in the 🤗 Hub,https://huggingface.co/blog/sentence-transformers-in-the-hub,,[],"Mon, 28 Jun 2021 00:00:00 GMT"
1818,Few-shot learning in practice: GPT-NEO and the 🤗 Accelerated Inference API,https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api,,[],"Thu, 03 Jun 2021 00:00:00 GMT"
1819,Using & Mixing Hugging Face Models with Gradio 2.0,https://huggingface.co/blog/gradio,,[],"Tue, 25 May 2021 00:00:00 GMT"
1820,Scaling-up BERT Inference on CPU (Part 1),https://huggingface.co/blog/bert-cpu-scaling-part-1,,[],"Tue, 20 Apr 2021 00:00:00 GMT"
1821,Introducing 🤗 Accelerate,https://huggingface.co/blog/accelerate-library,,[],"Fri, 16 Apr 2021 00:00:00 GMT"
1822,Distributed Training: Train BART/T5 for Summarization using 🤗 Transformers and Amazon SageMaker,https://huggingface.co/blog/sagemaker-distributed-training-seq2seq,,[],"Thu, 08 Apr 2021 00:00:00 GMT"
1823,How many data points is a prompt worth?,https://huggingface.co/blog/how_many_data_points,,[],"Mon, 05 Apr 2021 00:00:00 GMT"
1824,Understanding BigBird's Block Sparse Attention,https://huggingface.co/blog/big-bird,,[],"Wed, 31 Mar 2021 00:00:00 GMT"
1825,The Partnership: Amazon SageMaker and Hugging Face,https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face,,[],"Tue, 23 Mar 2021 00:00:00 GMT"
1826,My Journey to a serverless transformers pipeline on Google Cloud,https://huggingface.co/blog/how-to-deploy-a-pipeline-to-google-clouds,,[],"Thu, 18 Mar 2021 00:00:00 GMT"
1827,Fine-Tune Wav2Vec2 for English ASR with 🤗 Transformers,https://huggingface.co/blog/fine-tune-wav2vec2-english,,[],"Fri, 12 Mar 2021 00:00:00 GMT"
1828,"Hugging Face Reads, Feb. 2021 - Long-range Transformers",https://huggingface.co/blog/long-range-transformers,,[],"Tue, 09 Mar 2021 00:00:00 GMT"
1829,Simple considerations for simple people building fancy neural networks,https://huggingface.co/blog/simple-considerations,,[],"Thu, 25 Feb 2021 00:00:00 GMT"
1830,Retrieval Augmented Generation with Huggingface Transformers and Ray,https://huggingface.co/blog/ray-rag,,[],"Wed, 10 Feb 2021 00:00:00 GMT"
1831,Hugging Face on PyTorch / XLA TPUs,https://huggingface.co/blog/pytorch-xla,,[],"Tue, 09 Feb 2021 00:00:00 GMT"
1832,Faster TensorFlow models in Hugging Face Transformers,https://huggingface.co/blog/tf-serving,,[],"Tue, 26 Jan 2021 00:00:00 GMT"
1833,Fit More and Train Faster With ZeRO via DeepSpeed and FairScale,https://huggingface.co/blog/zero-deepspeed-fairscale,,[],"Tue, 19 Jan 2021 00:00:00 GMT"
1834,How we sped up transformer inference 100x for 🤗 API customers,https://huggingface.co/blog/accelerated-inference,,[],"Mon, 18 Jan 2021 00:00:00 GMT"
1835,Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models,https://huggingface.co/blog/warm-starting-encoder-decoder,,[],"Mon, 09 Nov 2020 00:00:00 GMT"
1836,Porting fairseq wmt19 translation system to transformers,https://huggingface.co/blog/porting-fsmt,,[],"Tue, 03 Nov 2020 00:00:00 GMT"
1837,Hyperparameter Search with Transformers and Ray Tune,https://huggingface.co/blog/ray-tune,,[],"Mon, 02 Nov 2020 00:00:00 GMT"
1838,Transformer-based Encoder-Decoder Models,https://huggingface.co/blog/encoder-decoder,,[],"Sat, 10 Oct 2020 00:00:00 GMT"
1839,Retrieval Augmented Generation (RAG),https://huggingface.co/rag,,[],"Mon, 28 Sep 2020 00:00:00 GMT"
1840,Block Sparse Matrices for Smaller and Faster Language Models,https://huggingface.co/blog/pytorch_block_sparse,,[],"Thu, 10 Sep 2020 00:00:00 GMT"
1841,The Reformer - Pushing the limits of language modeling,https://huggingface.co/blog/reformer,,[],"Fri, 03 Jul 2020 00:00:00 GMT"
1842,Long Form Question Answering with ELI5,https://yjernite.github.io/lfqa.html,,[],"Wed, 17 Jun 2020 00:00:00 GMT"
1843,How Big Should My Language Model Be?,https://huggingface.co/calculator,,[],"Mon, 08 Jun 2020 00:00:00 GMT"
1844,Zero Shot Topic Classification,https://huggingface.co/zero-shot,,[],"Fri, 29 May 2020 00:00:00 GMT"
1845,How to generate text: using different decoding methods for language generation with Transformers,https://huggingface.co/blog/how-to-generate,,[],"Sun, 01 Mar 2020 00:00:00 GMT"
1846,How to train a new language model from scratch using Transformers and Tokenizers,https://huggingface.co/blog/how-to-train,,[],"Fri, 14 Feb 2020 00:00:00 GMT"
1847,What U.S. Members Think About Regulating AI,https://spectrum.ieee.org/regulating-ai-ieee-survey,"<img src=\"https://spectrum.ieee.org/media-library/a-box-with-the-letters-q-a-on-top-with-small-interconnected-boxes-on-the-side-against-a-dark-background-with-binary-code.jpg?id=51656119&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C0%2C0\" /><br /><br /><p>With the rapid proliferation of AI systems, public policymakers and industry leaders are calling for clearer guidance on governing the technology. The majority of U.S. IEEE members express that the current regulatory approach to managing artificial intelligence (AI) systems is inadequate. They also say that prioritizing AI governance should be a matter of public policy, equal to issues such as health care, education, immigration, and the environment. That’s according to the results of a survey conducted by IEEE for the <a href=\"https://ieeeusa.org/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE-USA</a><a href=\"https://ieeeusa.org/committees/aipc/\" rel=\"noopener noreferrer\" target=\"_blank\"> AI Policy Committee</a>.</p><p>We serve as chairs of<a href=\"https://ieeeusa.org/\" rel=\"noopener noreferrer\" target=\"_blank\"> </a>the AI Policy Committee, and know that IEEE’s members are a crucial, invaluable resource for informed insights into the technology. To guide our public policy advocacy work in Washington, D.C., and to better understand opinions about the governance of AI systems in the U.S., IEEE surveyed a random sampling of 9,000 active IEEE-USA members plus 888 active members working on AI and neural networks.</p><p>The survey intentionally did not define the term <em>AI</em>. Instead, it asked respondents to use their own interpretation of the technology when answering. The results demonstrated that, even among IEEE’s membership, there is no clear consensus on a definition of <em>AI</em>. Significant variances exist in how members think of AI systems, and this lack of convergence has public policy repercussions.</p><p>Overall, members were asked their opinion on how to govern the use of algorithms in consequential decision-making and on data privacy, and whether the U.S. government should increase its workforce capacity and expertise in AI.</p><h2>The state of AI governance</h2><p>For years, IEEE-USA has been advocating for strong governance to control AI’s impact on society. It is apparent that U.S. public policy makers <a href=\"https://www.reuters.com/legal/legalindustry/seeking-synergy-between-ai-privacy-regulations-2023-11-17/\" rel=\"noopener noreferrer\" target=\"_blank\">struggle with regulation of the data that drives AI systems</a>. Existing federal laws protect certain types of <a href=\"https://www.hhs.gov/hipaa/index.html\" rel=\"noopener noreferrer\" target=\"_blank\">health</a> and<a href=\"https://www.ftc.gov/business-guidance/privacy-security/gramm-leach-bliley-act\" rel=\"noopener noreferrer\" target=\"_blank\"> financial</a> data, but Congress has yet to pass legislation that would implement a national data privacy standard, despite <a href=\"https://iapp.org/news/a/us-house-lawmakers-keep-federal-privacy-legislation-top-of-mind/\" rel=\"noopener noreferrer\" target=\"_blank\">numerous attempts</a> to do so. Data protections for Americans are piecemeal, and compliance with the complex federal and state data privacy laws can be costly for industry. </p><p>Numerous <a href=\"https://energycommerce.house.gov/events/safeguarding-data-and-innovation-setting-the-foundation-for-the-use-of-artificial-intelligence\" rel=\"noopener noreferrer\" target=\"_blank\">U.S. policymakers have espoused</a> that governance of AI cannot happen without a national data privacy law that provides standards and technical guardrails around data collection and use, particularly in the commercially available information market. The data is a critical resource for third-party large-language models, which use it to train AI tools and generate content. As the U.S. <a href=\"https://www.dni.gov/files/ODNI/documents/assessments/ODNI-Declassified-Report-on-CAI-January2022.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">government has acknowledged</a>, the commercially available information market allows any buyer to obtain hordes of data about individuals and groups, including details otherwise protected under the law. The issue raises significant privacy and civil liberties concerns. </p><p>Regulating data privacy, it turns out, is an area where IEEE members have strong and clear consensus views.</p><h2>Survey takeaways</h2><p>The majority of respondents—about 70 percent—said the current regulatory approach is inadequate. Individual responses tell us more. To provide context, we have broken down the results into four areas of discussion: governance of AI-related public policies; risk and responsibility; trust; and comparative perspectives.</p><h3>Governance of AI as public policy </h3><p>Although there are divergent opinions around aspects of AI governance, what stands out is the consensus around regulation of AI in specific cases. More than 93 percent of respondents support protecting individual data privacy and favor regulation to address AI-generated misinformation.</p><p>About 84 percent support requiring risk assessments for medium- and high-risk AI products. Eighty percent called for placing transparency or explainability requirements on AI systems, and 78 percent called for restrictions on autonomous weapon systems. More than 72 percent of members support policies that restrict or govern the use of facial recognition in certain contexts, and nearly 68 percent support policies that regulate the use of algorithms in consequential decisions.</p><p>There was strong agreement among respondents around prioritizing AI governance as a matter of public policy. Two-thirds said the technology should be given at least equal priority as other areas within the government’s purview, such as health care, education, immigration, and the environment. </p><p>Eighty percent support the development and use of AI, and more than 85 percent say it needs to be carefully managed, but respondents disagreed as to how and by whom such management should be undertaken. While only a little more than half of the respondents said the government should regulate AI, this data point should be juxtaposed with the majority’s clear support of government regulation in specific areas or use case scenarios.</p><p>Only a very small percentage of non-AI focused computer scientists and software engineers thought private companies should self-regulate AI with minimal government oversight. In contrast, almost half of AI professionals prefer government monitoring. </p><p>More than three quarters of IEEE members support the idea that governing bodies of all types should be doing more to govern AI’s impacts.</p><h3>Risk and responsibility</h3><p>A number of the survey questions asked about the perception of AI risk. Nearly 83 percent of members said the public is inadequately informed about AI. Over half agree that AI’s benefits outweigh its risks.</p><p>In terms of responsibility and liability for AI systems, a little more than half said the developers should bear the primary responsibility for ensuring that the systems are safe and effective. About a third said the government should bear the responsibility.</p><h3>Trusted organizations</h3><p>Respondents ranked academic institutions, nonprofits and small and midsize technology companies as the most trusted entities for responsible design, development, and deployment. The three least trusted factions are large technology companies, international organizations, and governments.</p><p>The entities most trusted to manage or govern AI responsibly are academic institutions and independent third-party institutions. The least trusted are large technology companies and international organizations.</p><h3>Comparative perspectives </h3><p>Members demonstrated a strong preference for regulating AI to mitigate social and ethical risks, with 80 percent of non-AI science and engineering professionals and 72 percent of AI workers supporting the view.</p><p>Almost 30 percent of professionals working in AI express that regulation might stifle innovation, compared with about 19 percent of their non-AI counterparts. A majority across all groups agree that it’s crucial to start regulating AI, rather than waiting, with 70 percent of non-AI professionals and 62 percent of AI workers supporting immediate regulation.</p><p>A significant majority of the respondents acknowledged the social and ethical risks of AI, emphasizing the need for responsible innovation. Over half of AI professionals are inclined toward nonbinding regulatory tools such as standards. About half of non-AI professionals favor specific government rules.</p><h2>A mixed governance approach</h2><p>The survey establishes that a majority of U.S.-based IEEE members support AI development and strongly advocate for its careful management. The results will guide IEEE-USA in working with Congress and the White House.</p><p>Respondents acknowledge the benefits of AI, but they expressed concerns about its societal impacts, such as inequality and misinformation. Trust in entities responsible for AI’s creation and management varies greatly; academic institutions are considered the most trustworthy entities.</p><p>A notable minority oppose government involvement, preferring non regulatory guidelines and standards, but the numbers should not be viewed in isolation. Although conceptually there are mixed attitudes toward government regulation, there is an overwhelming consensus for prompt regulation in specific scenarios such as data privacy, the use of algorithms in consequential decision-making, facial recognition, and autonomous weapons systems.</p><p>Overall, there is a preference for a mixed governance approach, using laws, regulations, and technical and industry standards.</p>",[{'name': 'Carlos Ignacio Gutierrez'}],"Thu, 07 Mar 2024 19:00:03 +0000"
1848,AI Prompt Engineering Is Dead,https://spectrum.ieee.org/prompt-engineering-is-dead,"<img src=\"https://spectrum.ieee.org/media-library/man-in-blue-shirt-and-briefcase-walking-away-from-camera-in-a-environment-with-lines-and-circles-connected-together-to-look-like.jpg?id=51643003&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C0%2C0\" /><br /><br /><p>Since ChatGPT <a href=\"https://openai.com/blog/chatgpt\" target=\"_blank\">dropped</a> in the fall of 2022, everyone and their donkey has tried their hand at <a href=\"https://en.wikipedia.org/wiki/Prompt_engineering\" target=\"_blank\">prompt engineering</a>—finding a clever way to phrase your query to a <a href=\"https://spectrum.ieee.org/large-language-models-math\" target=\"_blank\">large language model</a> (LLM) or <a href=\"https://spectrum.ieee.org/these-ai-tools-generate-breathtaking-art-and-controversy\" target=\"_blank\">AI art or video generator</a> to get the best results or <a href=\"https://spectrum.ieee.org/midjourney-copyright\" target=\"_self\">sidestep protections</a>. The Internet is replete with prompt-engineering <a href=\"https://www.promptingguide.ai/\" target=\"_blank\">guides</a>, <a href=\"https://medium.com/aimonks/chatgpt-cheat-sheet-drafting-the-perfect-prompt-part-1-5149c9b1d8ab\" target=\"_blank\">cheat sheets</a>, and <a href=\"https://www.reddit.com/r/PromptEngineering/?rdt=62865\" target=\"_blank\">advice threads</a> to help you get the most out of an LLM.<br /></p><p>In the commercial sector, companies are now wrangling LLMs to build <a href=\"https://jannikreinhard.com/2023/12/11/deep-dive-into-co-pilots-understanding-architecture-llms-and-advanced-concepts/\" target=\"_blank\">product copilots</a>, automate <a href=\"https://cognitiveclass.ai/courses/course-v1:IBMSkillsNetwork+GPXX0C2NEN+v1\" target=\"_blank\">tedious work</a>, create <a href=\"https://arxiv.org/html/2401.05459v1\" target=\"_blank\">personal assistants</a>, and more, says Austin Henley, a former Microsoft employee who <a href=\"https://arxiv.org/abs/2312.14231\" target=\"_blank\">conducted a series of interviews</a> with people developing LLM-powered copilots. “Every business is trying to use it for virtually every use case that they can imagine,” Henley says.</p><p class=\"pull-quote\">“The only real trend may be no trend. What’s best for any given model, dataset, and prompting strategy is likely to be specific to the particular combination at hand.” <strong>—Rick Battle & Teja Gollapudi, VMware</strong></p><p>To do so, they’ve enlisted the help of prompt engineers professionally.</p><p>However, new research suggests that prompt engineering is best done by the model itself, and not by a human engineer. This has cast doubt on prompt engineering’s future—and increased suspicions that a fair portion of prompt-engineering jobs may be a passing fad, at least as the field is currently imagined.</p><h2>Autotuned prompts are successful and strange</h2><p><a href=\"https://www.linkedin.com/in/battler/\" rel=\"noopener noreferrer\" target=\"_blank\">Rick Battle</a> and <a href=\"https://www.linkedin.com/in/teja-gollapudi/\" rel=\"noopener noreferrer\" target=\"_blank\">Teja Gollapudi</a> at California-based cloud computing company <a href=\"https://www.vmware.com/\" rel=\"noopener noreferrer\" target=\"_blank\">VMware</a> were perplexed by how finicky and unpredictable LLM performance was in response to weird prompting techniques. For example, people have found that asking models to explain its reasoning step-by-step—a technique called <a href=\"https://arxiv.org/abs/2201.11903\" rel=\"noopener noreferrer\" target=\"_blank\">chain-of-thought</a>—improved their performance on a range of math and logic questions. Even weirder, Battle found that giving a model positive prompts, such as “this will be fun” or “you are as smart as chatGPT,” sometimes improved performance. </p><p>Battle and Gollapudi decided to <a href=\"https://arxiv.org/pdf/2402.10949.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">systematically test</a> how different prompt-engineering strategies impact an LLM’s ability to solve grade-school math questions. They tested three different open-source language models with 60 different prompt combinations each. What they found was a surprising lack of consistency. Even chain-of-thought prompting sometimes helped and other times hurt performance. “The only real trend may be no trend,” they write. “What’s best for any given model, dataset, and prompting strategy is likely to be specific to the particular combination at hand.”</p><p class=\"pull-quote\">According to one research team, no human should manually optimize prompts ever again.</p><p>There is an alternative to the trial-and-error-style prompt engineering that yielded such inconsistent results: Ask the language model to devise its own optimal prompt. Recently, <a href=\"https://arxiv.org/abs/2310.03714\" rel=\"noopener noreferrer\" target=\"_blank\">new tools</a> have been <a href=\"https://arxiv.org/abs/2309.03409\" rel=\"noopener noreferrer\" target=\"_blank\">developed</a> to automate this process. Given a few examples and a quantitative success metric, these tools will iteratively find the optimal phrase to feed into the LLM. Battle and his collaborators found that in almost every case, this automatically generated prompt did better than the best prompt found through trial-and-error. And, the process was much faster, a couple of hours rather than several days of searching.</p><p>The optimal prompts the algorithm spit out were so bizarre, no human is likely to have ever come up with them. “I literally could not believe some of the stuff that it generated,” Battle says. In one instance, the prompt was just an extended Star Trek reference: “Command, we need you to plot a course through this turbulence and locate the source of the anomaly. Use all available data and your expertise to guide us through this challenging situation.” Apparently, thinking it was Captain Kirk helped this particular LLM do better on grade-school math questions.</p><p>Battle says that optimizing the prompts algorithmically fundamentally makes sense given what language models really are—models. “A lot of people anthropomorphize these things because they ‘speak English.’ No, they don’t,” Battle says. “It doesn’t speak English. It does a lot of math.” </p><p>In fact, in light of his team’s results, Battle says no human should manually optimize prompts ever again.</p><p>“You’re just sitting there trying to figure out what special magic combination of words will give you the best possible performance for your task,” Battle says, “But that’s where hopefully this research will come in and say ‘don’t bother.’ Just develop a scoring metric so that the system itself can tell whether one prompt is better than another, and then just let the model optimize itself.”</p><h2>Autotuned prompts make pictures prettier, too</h2><p>Image-generation algorithms can benefit from automatically generated prompts as well. Recently, a team at <a href=\"https://www.intel.com/content/www/us/en/research/overview.html\" rel=\"noopener noreferrer\" target=\"_blank\">Intel labs</a>, led by <a href=\"https://www.linkedin.com/in/vasudev-lal-79bb336/\" rel=\"noopener noreferrer\" target=\"_blank\">Vasudev Lal</a>, set out on a similar quest to optimize prompts for the image-generation model <a href=\"https://clipdrop.co/stable-diffusion?utm_campaign=stable_diffusion_promo&amp;utm_medium=cta_button&amp;utm_source=stability_ai\" rel=\"noopener noreferrer\" target=\"_blank\">Stable Diffusion</a>. “It seems more like a bug of LLMs and diffusion models, not a feature, that you have to do this expert prompt engineering,” Lal says. “So, we wanted to see if we can automate this kind of prompt engineering.”</p><p class=\"pull-quote\">“Now we have this full machinery, the full loop that’s completed with this reinforcement learning.… This is why we are able to outperform human prompt engineering.” <strong>—Vasudev Lal, Intel Labs</strong></p><p>Lal’s team created a tool called <a href=\"https://arxiv.org/abs/2311.12229\" target=\"_blank\">NeuroPrompts</a> that takes a simple input prompt, such as “boy on a horse,” and automatically enhances it to produce a better picture. To do this, they started with a range of prompts generated by human prompt-engineering experts. They then trained a language model to transform simple prompts into these expert-level prompts. On top of that, they used reinforcement learning to optimize these prompts to create more aesthetically pleasing images, as rated by yet another machine-learning model, <a href=\"https://arxiv.org/abs/2305.01569\" target=\"_blank\">PickScore</a>, a recently developed image-evaluation tool.<br /></p><p class=\"shortcode-media shortcode-media-rebelmouse-image\">
<img alt=\"two images of a boy on a horse\" class=\"rm-shortcode\" id=\"a4975\" src=\"https://spectrum.ieee.org/media-library/two-images-of-a-boy-on-a-horse.jpg?id=51659496&amp;width=980\" />
<small class=\"image-media media-caption\">NeuroPrompts is a generative AI auto prompt tuner that transforms simple prompts into more detailed and visually stunning StableDiffusion results—as in this case, an image generated by a generic prompt [left] versus its equivalent NeuroPrompt-generated image.</small><small class=\"image-media media-photo-credit\">Intel Labs/Stable Diffusion</small></p><p>Here too, the automatically generated prompts did better than the expert-human prompts they used as a starting point, at least according to the PickScore metric. Lal found this unsurprising. “Humans will only do it with trial and error,” Lal says. “But now we have this full machinery, the full loop that’s completed with this reinforcement learning.… This is why we are able to outperform human prompt engineering.”</p><p>Since aesthetic quality is infamously subjective, Lal and his team wanted to give the user some control over how the prompt was optimized. In their <a href=\"https://www.youtube.com/watch?v=Cmca_RWYn2g\" target=\"_blank\">tool</a>, the user can specify the original prompt (say, “boy on a horse”) as well as an artist to emulate, a style, a format, and other modifiers.</p><p>Lal believes that as generative AI models evolve, be it image generators or large language models, the weird quirks of prompt dependence should go away. “I think it’s important that these kinds of optimizations are investigated and then ultimately, they’re really incorporated into the base model itself so that you don’t really need a complicated prompt-engineering step.”</p><h2>Prompt engineering will live on, by some name</h2><p>Even if autotuning prompts becomes the industry norm, prompt-engineering jobs in some form are not going away, says <a href=\"https://www.linkedin.com/in/ticramer/\" rel=\"noopener noreferrer\" target=\"_blank\">Tim Cramer</a>, senior vice president of software engineering at <a href=\"https://www.redhat.com/en\" rel=\"noopener noreferrer\" target=\"_blank\">Red Hat</a>. Adapting generative AI for industry needs is a complicated, multistage endeavor that will continue requiring humans in the loop for the foreseeable future.</p><p class=\"pull-quote\">“Maybe we’re calling them prompt engineers today. But I think the nature of that interaction will just keep on changing as AI models also keep changing.” <strong>—Vasudev Lal, Intel Labs</strong></p><p>“I think there are going to be prompt engineers for quite some time, and data scientists,” Cramer says. “It’s not just asking questions of the LLM and making sure that the answer looks good. But there’s a raft of things that prompt engineers really need to be able to do.”</p><p>“It’s very easy to make a prototype,” Henley says. “It’s very hard to production-ize it.” Prompt engineering seems like a big piece of the puzzle when you’re building a prototype, Henley says, but many other considerations come into play when you’re making a commercial-grade product.</p><p>Challenges of making a commercial product include ensuring reliability—for example, failing gracefully when the model goes offline; adapting the model’s output to the appropriate format, since many use cases require outputs other than text; testing to make sure the AI assistant won’t do something harmful in even a small number of cases; and ensuring safety, privacy, and compliance. Testing and compliance are particularly difficult, Henley says, as traditional software-development testing strategies are maladapted for nondeterministic LLMs.</p><p>To fulfill these myriad tasks, many <a href=\"https://www.ibm.com/topics/llmops\" rel=\"noopener noreferrer\" target=\"_blank\">large companies</a> are <a href=\"https://www.redhat.com/en/topics/ai/llmops\" rel=\"noopener noreferrer\" target=\"_blank\">heralding</a> a new job title: Large Language Model Operations, or <a href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-llmops/\" rel=\"noopener noreferrer\" target=\"_blank\">LLMOps,</a> which includes prompt engineering in its life cycle but also entails all the other tasks needed to deploy the product. Henley says LLMOps’ predecessors, machine learning operations (MLOps) engineers, are best positioned to take on these jobs. </p><p>Whether the job titles will be “prompt engineer,” “LLMOps engineer,” or something new entirely, the nature of the job will continue evolving quickly. “Maybe we’re calling them prompt engineers today,” Lal says, “But I think the nature of that interaction will just keep on changing as AI models also keep changing.”</p><p>“I don’t know if we’re going to combine it with another sort of job category or job role,” Cramer says, “But I don’t think that these things are going to be going away anytime soon. And the landscape is just too crazy right now. Everything’s changing so much. We’re not going to figure it all out in a few months.” </p><p>Henley says that, to some extent in this early phase of the field, the only overriding rule seems to be the absence of rules. “It’s kind of the Wild, Wild West for this right now.” he says.</p>",[{'name': 'Dina Genkina'}],"Wed, 06 Mar 2024 15:07:23 +0000"
1849,"Meta’s AI Watermarking Plan Is Flimsy, at Best",https://spectrum.ieee.org/meta-ai-watermarks,"<img src=\"https://spectrum.ieee.org/media-library/letters-and-numbers-bunched-together-with-the-colors-blue-in-a-box-and-red-and-white-in-stripes.jpg?id=51634902&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C0%2C0\" /><br /><br /><p>In the past few months, we’ve seen a <a href=\"https://www.axios.com/2024/02/07/new-hampshire-fake-robocall-ai-biden-voice-texas\" target=\"_blank\">deepfake robocall</a> of Joe Biden encouraging New Hampshire voters to “save your vote for the November election” and a <a href=\"https://www.nbcnews.com/tech/internet/taylor-swift-deepfake-x-falsely-depict-supporting-trump-grammys-flag-rcna137620\" target=\"_blank\">fake endorsement</a> of Donald Trump from Taylor Swift. It’s clear that 2024 will mark the first “AI election” in United States history.<br /></p><p>With many advocates <a href=\"https://www.brennancenter.org/our-work/research-reports/regulating-ai-deepfakes-and-synthetic-media-political-arena\" target=\"_blank\">calling for safeguards</a> against AI’s potential harms to our democracy, <a href=\"https://about.meta.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Meta</a> (the parent company of Facebook and Instagram) proudly <a href=\"https://about.fb.com/news/2024/02/labeling-ai-generated-images-on-facebook-instagram-and-threads/\" rel=\"noopener noreferrer\" target=\"_blank\">announced</a> last month that it will label AI-generated content that was created using the most popular generative AI tools. The company said it’s “building industry-leading tools that can identify invisible markers at scale—specifically, the ‘AI generated’ information in the <a href=\"https://c2pa.org/\" rel=\"noopener noreferrer\" target=\"_blank\">C2PA</a> and IPTC technical standards.” </p><p>Unfortunately, social media companies will not solve the problem of deepfakes on social media this year with this approach. Indeed, this new effort will do very little to tackle the problem of AI-generated material polluting the election environment.</p><p>The most obvious weakness is that Meta’s system will work only if the bad actors creating deepfakes use tools that already put watermarks—that is, hidden or visible information about the origin of digital content—into their images. Most unsecured “open-source” generative AI tools don’t produce watermarks at all. (We use the term unsecured and put “open-source” in quotes to denote that many such tools don’t meet traditional definitions of open-source software, but <a href=\"https://spectrum.ieee.org/open-source-ai-2666932122\" target=\"_self\">still pose a threat</a> because their underlying code or model weights have been made publicly available.) If new versions of these unsecured tools are released that do contain watermarks, the old tools will still be available and able to produce watermark-free content, including personalized and highly persuasive disinformation and nonconsensual deepfake pornography.</p><p>We are also concerned that bad actors can easily circumvent Meta’s labeling regimen even if they are using the AI tools that Meta says will be covered, which include products from Google, OpenAI, Microsoft, Adobe, Midjourney, and Shutterstock. Given that it takes about 2 seconds to remove a watermark from an image produced using <a href=\"https://spectrum.ieee.org/deepfakes-election\" target=\"_self\">the current C2PA watermarking standard</a> that these companies have implemented, Meta’s promise to label AI-generated images falls flat.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image\">
<img alt=\"website screengrab of an image with text\" class=\"rm-shortcode\" id=\"910ce\" src=\"https://spectrum.ieee.org/media-library/website-screengrab-of-an-image-with-text.jpg?id=51635174&amp;width=980\" />
<small class=\"image-media media-caption\">When the authors uploaded an image they’d generated to a website that checks for watermarks, the site correctly stated that it was a synthetic image generated by an OpenAI tool. </small><small class=\"image-media media-photo-credit\">IEEE Spectrum</small></p><p>We know this because we were able to easily remove the watermarks Meta claims it will detect—and neither of us is an engineer. Nor did we have to write a single line of code or install any software. </p><p>First, we generated an image with OpenAI’s DALL-E 3. Then, to see if the watermark worked, we uploaded the image to the <a href=\"https://contentcredentials.org/verify\" rel=\"noopener noreferrer\" target=\"_blank\">C2PA content credentials verification</a> website. A simple and elegant interface showed us that this image was indeed made with OpenAI’s DALL-E 3. How did we then remove the watermark? By taking a screenshot. When we uploaded the screenshot to the same verification website, the verification site found no evidence that the image had been generated by AI. The same process worked when we made an image with <a href=\"https://imagine.meta.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Meta’s AI image generator</a> and took a screenshot of it—and uploaded it to a <a href=\"https://compress-or-die.com/analyze\" rel=\"noopener noreferrer\" target=\"_blank\">website</a> that detects the IPTC metadata that contains Meta’s AI “watermark.”</p><p class=\"shortcode-media shortcode-media-rebelmouse-image\">
<img alt=\"website screengrab of an image with text\" class=\"rm-shortcode\" id=\"190c3\" src=\"https://spectrum.ieee.org/media-library/website-screengrab-of-an-image-with-text.png?id=51635179&amp;width=980\" />
<small class=\"image-media media-caption\">However, when the authors took a screenshot of the image and uploaded that screenshot to the same verification site, the site found no watermark and therefore no evidence that the image was AI generated. </small><small class=\"image-media media-photo-credit\">IEEE Spectrum</small></p><h2>Is there a better way to identify AI-generated content?</h2><p>Meta’s announcement states that it’s “working hard to develop classifiers that can help...to automatically detect AI-generated content, even if the content lacks invisible markers.” It’s nice that the company is working on it, but until it succeeds and shares this technology with the entire industry, we will be stuck wondering whether anything we see or hear online is real. </p><p>For a more immediate solution, the industry could adopt <em>maximally indelible</em> watermarks—meaning watermarks that are as difficult to remove as possible.</p><p>Today’s imperfect watermarks typically attach information to a file in the form of metadata. For maximally indelible watermarks to offer an improvement, they need to hide information imperceptibly in the actual pixels of images, the waveforms of audio (Google Deepmind claims to have done this with its proprietary <a href=\"https://deepmind.google/technologies/synthid/\" rel=\"noopener noreferrer\" target=\"_blank\">SynthID watermark</a>) or through slightly modified <a href=\"https://www.nytimes.com/interactive/2023/02/17/business/ai-text-detection.html\" rel=\"noopener noreferrer\" target=\"_blank\">word frequency</a> patterns in AI-generated text. We use the term “maximally” to acknowledge that there may never be a perfectly indelible watermark. This is not a problem just with watermarks though. The celebrated security expert <a href=\"https://www.schneier.com/blog/about/\" rel=\"noopener noreferrer\" target=\"_blank\">Bruce Schneier</a> notes that “<a href=\"https://www.schneier.com/news/archives/2007/05/is_security_a_solvab.html\" target=\"_blank\">computer security is not a solvable problem</a>…. Security has always been an arms race, and always will be.”<strong></strong></p><p>In metaphorical terms, it’s instructive to consider automobile safety. No car manufacturer has ever produced a car that cannot crash. Yet that hasn’t stopped regulators from implementing comprehensive safety standards that require seatbelts, airbags, and backup cameras on cars. If we waited for safety technologies to be perfected before requiring implementation of the best available options, we would be much worse off in many domains.</p><p>There’s increasing political momentum to tackle deepfakes. Fifteen of the biggest AI companies—including almost every one mentioned in this article—signed on to the <a href=\"https://www.whitehouse.gov/briefing-room/statements-releases/2023/09/12/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-eight-additional-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/\" rel=\"noopener noreferrer\" target=\"_blank\">White House Voluntary AI Commitments</a> last year, which included pledges to “develop robust mechanisms, including provenance and/or watermarking systems for audio or visual content” and to “develop tools or APIs to determine if a particular piece of content was created with their system.” Unfortunately, the White House did not set any timeline for the voluntary commitments.</p><p>Then, in October, the White House, in its <a href=\"https://spectrum.ieee.org/biden-ai-executive-order\" target=\"_self\">AI Executive Order</a>, defined AI watermarking as “the act of embedding information, which is typically difficult to remove, into outputs created by AI—including into outputs such as photos, videos, audio clips, or text—for the purposes of verifying the authenticity of the output or the identity or characteristics of its provenance, modifications, or conveyance.”</p><p>Next, at the <a href=\"https://securityconference.org/en/\" rel=\"noopener noreferrer\" target=\"_blank\">Munich Security Conference</a> on 16 February, a group of 20 tech companies (half of which had previously signed the voluntary commitments) signed onto a new “<a href=\"https://www.aielectionsaccord.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Tech Accord to Combat Deceptive Use of AI in 2024 Elections</a>.” Without making any concrete commitments or providing any timelines, the accord offers a vague intention to implement some form of watermarking or content-provenance efforts. Although a standard is not specified, the accord lists both C2PA and SynthID as examples of technologies that could be adopted.<strong></strong></p><h2>Could regulations help?</h2><p>We’ve seen examples of robust pushback against deepfakes. Following the <a rel=\"noopener noreferrer\" target=\"_blank\">AI-generated Biden robocalls, the New Hampshire Department of Justice launched an </a><a href=\"https://www.doj.nh.gov/news/2024/20240206-voter-robocall-update.html\" rel=\"noopener noreferrer\" target=\"_blank\">investigation</a> in coordination with state and federal partners, including a bipartisan task force made up of all 50 state attorneys general and the Federal Communications Commission. Meanwhile, in early February the FCC clarified that calls using voice-generation AI will be considered artificial and <a href=\"https://spectrum.ieee.org/ai-robocalls-2667266649\" target=\"_blank\">subject to restrictions</a> under existing laws regulating robocalls.</p><p>Unfortunately, we don’t have laws to force action by either AI developers or social media companies. Congress and the states should mandate that all generative AI products embed maximally indelible watermarks in their image, audio, video, and text content using state-of-the-art technology. They should also address risks from unsecured “open-source” systems that can either have their watermarking functionality disabled or be used to remove watermarks from other content. Furthermore, any company that makes a generative AI tool should be encouraged to release a detector that can identify, with the highest accuracy possible, any content it produces. This proposal shouldn’t be controversial, as its rough outlines have already been agreed to by the signers of the voluntary commitments and the recent elections accord.</p><p>Standards organizations like C2PA, the National Institute of Standards and Technology, and the International Organization for Standardization should also move faster to build consensus and release standards for maximally indelible watermarks and content labeling in preparation for laws requiring these technologies. Google, as C2PA’s newest steering committee member, should also quickly move to open up its <a href=\"https://www.theverge.com/2023/8/29/23849107/synthid-google-deepmind-ai-image-detector\" rel=\"noopener noreferrer\" target=\"_blank\">seemingly best-in-class</a> SynthID watermarking technology to all members for testing.</p><p>Misinformation and voter deception are nothing new in elections. But AI is accelerating existing threats to our already fragile democracy. Congress must also consider what steps it can take to protect our elections more generally from those who are seeking to undermine them. That should include some basic steps, such as passing the <a href=\"https://www.rules.senate.gov/news/majority-news/on-senate-floor-ranking-member-klobuchar-calls-for-immediate-consideration-of-new-legislation-to-stop-foreign-interference-in-our-elections\" rel=\"noopener noreferrer\" target=\"_blank\">Deceptive Practices and Voter Intimidation Act</a>, which would make it illegal to knowingly lie to voters about the time, place, and manner of elections with the intent of preventing them from voting in the period before a federal election.</p><p>Congress has been woefully slow to take up <a href=\"https://www.brennancenter.org/our-work/research-reports/pass-freedom-vote-act\" rel=\"noopener noreferrer\" target=\"_blank\">comprehensive democracy reform</a> in the face of recent shocks. The potential amplification of these shocks through abuse of AI ought to be enough to finally get lawmakers to act.</p>",[{'name': 'Lawrence Norden'}],"Mon, 04 Mar 2024 17:35:31 +0000"
1850,"Faster, More Secure Photonic Chip Boosts AI Training",https://spectrum.ieee.org/photonic-ai-chip,"<img src=\"https://spectrum.ieee.org/media-library/a-series-of-colored-micrographs-showing-a-blow-out-and-then-a-second-blow-out-of-a-chip-on-a-circuit-board.jpg?id=51579120&amp;width=2000&amp;height=1500&amp;coordinates=645%2C0%2C646%2C0\" /><br /><br /><p>A microchip that uses light instead of electricity can potentially be faster and more energy efficient at the complex computations essential to training AI  than conventional electronics. In addition, researchers say the new chips may be significantly more secure against hacking.</p><p>AI typically relies on <a href=\"https://spectrum.ieee.org/dendrocentric-learning\" target=\"_blank\">neural networks</a> in applications such as analyzing medical scans and supporting autonomous vehicles. In these systems, components known as neurons are fed data and cooperate to solve a problem, such as recognizing faces.</p><p>As neural networks grow in size and power, they are becoming more energy hungry when run on conventional electronics. This has led some researchers to investigate <a href=\"https://spectrum.ieee.org/optical-neural-networks\" target=\"_blank\">optical computing</a> as a promising, next-generation foundation for AI. This photonic approach uses light instead of electricity to perform computations more quickly and with less power than an electronic counterpart. “It might be around 1,000 to 10,000 times faster,” says <a href=\"https://live-sas-physics.pantheon.sas.upenn.edu/people/standing-faculty/nader-engheta\" target=\"_blank\">Nader Engheta</a>, a professor of electrical and systems engineering at the University of Pennsylvania.</p><p>In the new study, researchers created a silicon wafer that varied in height from 150 to 220 nanometers. The height variations were organized so that the chip could scatter light in specific patterns. When input in the form of light flows into the chip, the output light encodes data from complex tasks.</p><p>The scientists designed the microchip to perform <u><a href=\"https://spectrum.ieee.org/matrix-multiplication-deepmind\" target=\"_self\">vector matrix multiplication operations</a></u>. These calculations, which involve multiplying grids of numbers known as matrices, are key to many computational tasks, including operating neural networks.</p><p class=\"pull-quote\">“It might be around 1,000 to 10,000 times faster.” <strong>—Nader Engheta, University of Pennsylvania</strong></p><p>Whereas conventional electronics perform these calculations line by line, the new optical device performs the entirety of these computations at once. This means “one does not need to store the intermediate-stage information in a memory,” Engheta says. “Therefore, the results and processes are less vulnerable to hacking.”</p><p>Previously, when it came to designing photonics to perform these kinds of calculations, one challenge that scientists faced arose from how they had to use complex 3D simulations to model the three-dimensional behavior of the light waves inside these chips. This in turn made it computationally difficult to scale the devices to larger matrix sizes.</p><p>In the new study, by limiting how much the silicon’s thickness could vary across the chip, the researchers developed a way to use much simpler 2D simulations to model the devices. This in turn let them operate with larger matrix sizes.</p><p>In experiments, the scientists tested microchips capable of calculations with matrices as large as 3 by 3 on a side. They also designed a device capable of supporting matrices up to 10 by 10 on a side.</p><p>“To me, one of the exciting features of our work is conducting mathematical operations with near the speed of light using light,” Engheta says.</p><p>In the future, the scientists want to explore “how to add reconfigurability and programmability to such structures,” Engheta says. “Another possible direction to consider will be to have structures with more inputs and more outputs.”</p><p>The scientists detailed <u><a href=\"https://www.nature.com/articles/s41566-024-01394-2\" rel=\"noopener noreferrer\" target=\"_blank\">their findings</a></u> online 16 February in the journal <em>Nature Photonics</em>.</p>",[{'name': 'Charles Q. Choi'}],"Mon, 04 Mar 2024 16:09:20 +0000"
1851,A Few Social Media Influencers Are Shaping AI,https://spectrum.ieee.org/social-media-ai,"<img src=\"https://spectrum.ieee.org/media-library/illustration-of-a-group-of-people-standing-on-gray-lines-that-branch-out-in-different-directions-on-a-navy-background.jpg?id=51614082&amp;width=2000&amp;height=1500&amp;coordinates=279%2C0%2C35%2C0\" /><br /><br /><p>The term “social media influencer” may call to mind Instagram accounts shilling hair-growth gummies and cute outfits—but in reality, influencers influence all types of things. Including artificial-intelligence research trends.</p><p>Mainstream interest in AI and machine learning (ML) is at an all-time high, and the industry is responding—churning out thousands of AI and ML works for conferences and journals. The AI/ML community is also particularly active in posting non-peer-reviewed preprints via online platforms like <a href=\"https://arxiv.org/\" target=\"_blank\">ArXiv</a>. Given this glut of work, what rises to the top and receives attention?</p><p>The answer, at least in part, is: the research that a pair of highly influential users of <a href=\"https://twitter.com/home\" target=\"_blank\">X</a> (formerly Twitter) choose to highlight, according to <a href=\"https://arxiv.org/html/2401.13782v1\" rel=\"noopener noreferrer\" target=\"_blank\">a new preprint</a> from researchers at <a href=\"https://www.ucsb.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">University of California, Santa Barbara</a>.</p><p>The UCSB paper analyzed more than 8,000 AI and ML papers, considering both social-media mentions and the number of citations. Reviewing tweets from December 2018 to October 2023, the researchers concluded AI/ML papers shared by two specific influencers had median citation counts two to three times higher than those of the control group.</p><p>This is crucial because academic citations aren’t just about recognition in one’s field; they also affect decisions like research funding and tenure at academic institutions. And it’s a change from the status quo. As recently as 2018, a <a href=\"https://www.semanticscholar.org/paper/Predictive-power-of-conference-related-factors-on-Lee/0aa6c99d04808c4bc53336694205960eb1f53b99\" target=\"_blank\">study of conference papers</a> showed that a paper’s review score—meaning acceptance to top conferences—was a primary indicator of future citation count.</p><p>Now, “the correlation between influencer tweets and citation count—and not review scores—points to a shift in how the community finds and reads papers,” this new work concludes.</p><h2>Two influencers with an outsized effect on AI</h2><p>The researchers selected two influencers as case studies. Both share AI/ML papers consistently and have a significant following on X (formerly Twitter): <a href=\"https://twitter.com/_akhaliq\" rel=\"noopener noreferrer\" target=\"_blank\">@_akhaliq</a> and <a href=\"https://twitter.com/arankomatsuzaki\" rel=\"noopener noreferrer\" target=\"_blank\">@arankomatsuzaki</a>. “These influencers have emerged as pivotal figures in navigating the flood of information, akin to journalists in civic society, highlighting and contextualizing significant works for the community,” the authors write.</p><p>That curation is a helpful—and, of course, unpaid—service from these influencers given the deluge of research. But “an overreliance on a select group of curators may inadvertently skew the research landscape, emphasizing certain topics or perspectives over others,” the researchers write. Inadvertent bias in sharing certain labs’ or researchers’ work may entrench a lack of geographic, gender, or institutional diversity, the paper adds.<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#_msocom_1\" rel=\"noopener noreferrer\" target=\"_blank\"></a></p><p>Awareness is the first step to busting this social media echo chamber, says lead author <a href=\"https://haewonjeong.com/people/iain-weissburg\" rel=\"noopener noreferrer\" target=\"_blank\">Iain Xie Weissburg,</a> a first-year master’s student in UCSB’s electrical and computer engineering program.</p><p>“We wanted to help the community recognize this and be vigilant in ensuring that research remains an evenly leveled domain,” he tells <em>IEEE</em> <em>Spectrum</em>. “As it stands now, we all tend to get our information from a select few, we conclude that these are the hot topics, and then we often select our research based on that hype.”</p><p>The point is not to shame or place undue responsibility on these influencers or others, Weissburg is careful to note. “It’s that the publication and conference systems need to adapt to the vast increase in the volume of AI/ML research, which we can see continuing for the foreseeable future—especially with the influx of generative AI in the public sphere,” Weissburg says.<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#_msocom_2\" target=\"_blank\"></a></p><p>The analysis highlights not only social media’s expanding influence in AI/ML research, but also the importance of an evolving ecosystem to bring diversity of thought to today’s digital academic landscape.</p><h2>The volume of AI papers is overwhelming</h2><p>The researchers’ selection of just two influencers is “far from perfect” methodology, says <a href=\"https://deliprao.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Delip Rao</a>, an independent researcher affiliated with the <a href=\"https://www.upenn.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">University of Pennsylvania</a> and <a href=\"https://www.ucsc.edu/\" target=\"_blank\">University of California, Santa Cruz</a>. He adds “these two folks tend to tweet out papers from big labs and famous names. So, it is not clear who is influencing whom.”</p><p>Still, he agrees with the overall conclusion that a small group of influencers have an outsize effect, “which is problematic for science.” Citation counts are meant to be driven by experts who deeply understand the work they’re citing and the context, he notes, adding that it’s “unrealistic” to expect even expert influencers to bring this type of rigorous review to the social media activity that’s outside of their day jobs.<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#_msocom_3\" rel=\"noopener noreferrer\" target=\"_blank\"></a></p><p>That’s especially true given the proliferation of AI/ML research as of late, as one of the two featured influencers acknowledged recently. “I hate when arXiv casually releases like 500 papers almost every day,” <a href=\"https://arankomatsuzaki.wordpress.com/about-me/\" rel=\"noopener noreferrer\" target=\"_blank\">Aran Komatsuzaki</a> <a href=\"https://twitter.com/arankomatsuzaki/status/1757242871680287222\" rel=\"noopener noreferrer\" target=\"_blank\">wrote on X</a>.</p><p>The “sheer volume of papers published daily is overwhelming, making it impractical for individuals to sift through arXiv feeds,” Komatsuzaki, the chief technology officer of <a href=\"https://www.teraflop.ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Teraflop.ai</a>, tells <em>Spectrum</em> in an interview. “The community relies on curators like @_akhaliq and myself to highlight a selection of noteworthy papers each day…. [I’m] cautious about not promoting research with unconvincing, weak, or dubious results.”</p><p>The other influencer cited, Ahsen Khaliq or “AK,” is a machine-learning engineer at <a href=\"https://huggingface.co/\" target=\"_blank\">Hugging Face</a>. “I think there is some shift in the community toward finding/discovering new research and citing it through Twitter or other social media rather than conferences/reviewer scores—although each has their own place in the community,” Khaliq says in an interview.</p><h2>Can the AI social media bubble be popped?</h2><p>The paper’s core conclusion, Weissburg says, is that researchers, conference organizers, and academic institutions must be aware of the shifting norms as preprint platforms and social media accounts change the landscape of sharing research—especially in the AI/ML space. The authors also argue that conferences and peer-review processes may have to evolve to ensure that quality research is effectively distributed. </p><p>Weissburg says that engineers working in the field should at times resist the urge to hop on the hype train, lest they ignore other important areas of research. Influencers, too, can take note that “paper sharing isn’t a zero-sum game. You have an effect, and you don’t necessarily always have to share the biggest companies and most famous researchers. It’s important we have a diverse community in terms of different ideas and different backgrounds.” </p><p>In future work, Weissburg hopes to explore possible outsize social-media effects in other areas of science. There’s also an opportunity to explore the underlying mechanisms of social media—like how algorithms surface content to users—as they relate to academic recognition.</p><p>Rao would like to see new ideas for surfacing and disseminating quality papers. “When there is excessive production, it is natural to rely on curators, and the community’s reliance on such folks is a cry for help,” he says. “We need better ways to combat this information deluge, and the answer is, hopefully, not influencers.”</p>",[{'name': 'Julianne Pepitone'}],"Sat, 02 Mar 2024 14:00:02 +0000"
1852,"AI Is Being Built on Dated, Flawed Motion-Capture Data",https://spectrum.ieee.org/motion-capture-standards,"<img src=\"https://spectrum.ieee.org/media-library/colorful-illustration-of-different-people-walking.jpg?id=51546172&amp;width=2000&amp;height=1500&amp;coordinates=555%2C0%2C0%2C0\" /><br /><br /><p>Diversity of thought in industrial design is crucial: If no one thinks to design a technology for multiple body types, people can get hurt. The invention of seat belts is an oft-cited example of this phenomenon, as they were designed based on crash dummies that had traditionally male proportions, reflecting the bodies of the team members working on them.</p><p>The same phenomenon is now at work in the field of motion-capture technology. Throughout history, scientists have endeavored to understand how the human body moves. But how do we define the human body? Decades ago many studies assessed “healthy male” subjects; others used surprising models like dismembered cadavers. Even now, some <a href=\"https://www.frontiersin.org/articles/10.3389/frobt.2020.00071/full\" target=\"_blank\">modern studies</a> used in the design of fall-detection technology rely on methods like hiring stunt actors who pretend to fall.</p><p>Over time, a variety of flawed assumptions have become codified into standards for motion-capture data that’s being used to design some AI-based technologies. These flaws mean that AI-based applications may not be as safe for people who don’t fit a preconceived “typical” body type, according to new work recently <a href=\"https://arxiv.org/html/2401.10877v1\" target=\"_blank\">published as a preprint</a> and set to be presented at the <a href=\"https://chi2024.acm.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Conference on Human Factors in Computing Systems</a> in May.<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#_msocom_1\" rel=\"noopener noreferrer\" target=\"_blank\"></a></p><p>“We dug into these so-called gold standards being used for all kinds of studies and designs, and many of them had errors or were focused on a very particular type of body,” says <a href=\"https://www.si.umich.edu/people/abigail-jacobs\" rel=\"noopener noreferrer\" target=\"_blank\">Abigail Jacobs,</a> coauthor of the study and an assistant professor at the University of Michigan’s <a href=\"https://www.si.umich.edu/\" target=\"_blank\">School of Information</a> and the <a href=\"https://lsa.umich.edu/cscs\" rel=\"noopener noreferrer\" target=\"_blank\">Center for the Study of Complex Systems</a>. “We want engineers to be aware of how these social aspects become coded into the technical—hidden in mathematical models that seem objective or infrastructural.”</p><p>It’s an important moment for AI-based systems, Jacobs says, as we may still have time to catch and avoid potentially dangerous assumptions from being codified into applications informed by AI. </p><p>Motion-capture systems create representations of bodies by collecting data from sensors placed on the subjects, logging how these bodies move through space. These schematics become part of the tools that researchers use, such as open-source libraries of movement data and measurement systems that are meant to provide baseline standards for how human bodies move. Developers are increasingly using these baselines to build all manner of AI-based applications: fall-detection algorithms for smartwatches and other wearables, self-driving vehicles that need to detect pedestrians, computer-generated imagery for movies and video games, manufacturing equipment that interacts safely with human workers, and more.</p><p>“Many researchers don’t have access to advanced motion-capture labs to collect data, so we’re increasingly relying on benchmarks and standards to build new tech,” Jacobs says. “But when these benchmarks don’t include representations of all bodies, especially those people who are likely to be involved in real-world use cases—like elderly people who may fall—these standards can be quite flawed.”</p><p>She hopes we can learn from past mistakes, such as cameras that didn’t accurately capture all skin tones and seat belts and airbags that didn’t protect people of all shapes and sizes in car crashes.<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#_msocom_3\" target=\"_blank\"></a></p><h2>The Cadaver in the Machine</h2><p>Jacobs and her collaborators from Cornell University, Intel, and the University of Virginia performed a systematic literature review of 278 motion-capture-related studies. In most cases, they concluded, motion-capture systems captured the motion of “those who are male, white, ‘able-bodied,’ and of unremarkable weight.”</p><p>And sometimes these white male bodies were dead. In reviewing works dating back to the 1930s and running through three historical eras of motion-capture science, the researchers studied projects that were influential in how scientists of the time understood the movement of body segments. A <a href=\"https://deepblue.lib.umich.edu/bitstream/handle/2027.42/73105/j.1749-6632.1955.tb32112.x.pdf?sequence=1\" target=\"_blank\">seminal 1955 study</a> funded by the Air Force, for example, used overwhelmingly white, male, and slender or athletic bodies to create the optimal cockpit based on pilots’ range of motion. That study also gathered data from eight dismembered cadavers. </p><p>A full 20 years later, <a href=\"https://www.researchgate.net/publication/235057658_Investigation_of_Inertial_Properties_of_the_Human_Body\" target=\"_blank\">a study</a> prepared for the National Highway Traffic Safety Administration used similar methods: Six dismembered male cadavers were used to inform the design of impact-protection systems in vehicles.<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#_msocom_5\" target=\"_blank\"></a></p><p class=\"pull-quote\">In most of the 278 studies reviewed, motion-capture systems captured the motion of “those who are male, white, ‘able-bodied,’ and of unremarkable weight.”</p><p>Although those studies are many decades old, these assumptions became baked in over time. Jacobs and her colleagues found many examples of these outdated inferences being passed down to later studies and ultimately still influencing modern motion-capture studies.</p><p>“If you look at technical documents of a modern system in production, they’ll explain the ‘traditional baseline standards’ they’re using,” Jacobs says. “By digging through that, you quickly start hopping through time: OK, that’s based on this prior study, which is based on this one, which is based on this one, and eventually we’re back to the Air Force study designing cockpits with frozen cadavers.”</p><p>The components that underpin technological best practices are “man-made—intentional emphasis on man, rather than human—often preserving biases and inaccuracies from the past,” says <a href=\"https://www.linkedin.com/in/kchmielinski/\" rel=\"noopener noreferrer\" target=\"_blank\">Kasia Chmielinski</a>, project lead of the <a href=\"https://datanutrition.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Data Nutrition Project</a> and a fellow at Stanford University’s <a href=\"https://pacscenter.stanford.edu/research/digital-civil-society-lab/\" rel=\"noopener noreferrer\" target=\"_blank\">Digital Civil Society Lab</a>. “Thus historical errors often inform the ‘neutral’ basis of our present-day technological systems. This can lead to software and hardware that does not work equally for all populations, experiences, or purposes.”</p><p>These problems may hinder engineers who want to make things right, Chmielinski says. “Since many of these issues are baked into the foundational elements of the system, teams innovating today may not have quick recourse to address bias or error, even if they want to,” they say. “If you’re building an application that uses third-party sensors, and the sensors themselves have a bias in what they detect or do not detect, what is the appropriate recourse?”</p><p>Jacobs says that engineers must interrogate their sources of “ground truth” and confirm that the gold standards they measure against are, in fact, gold. Technicians must consider these social evaluations to be part of their jobs in order to design technologies for all.<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#_msocom_7\" target=\"_blank\"></a></p><p>“If you go in saying, ‘I know that human assumptions get built in and are often hidden or obscured,’ that will inform how you choose what’s in your dataset and how you report it in your work,” Jacobs says. “It’s sociotechnical, and technologists need that lens to be able to say: My system does what I say it does, and it doesn’t create undue harm.”</p>",[{'name': 'Julianne Pepitone'}],"Fri, 01 Mar 2024 15:00:04 +0000"
1853,Figure Raises $675M for Its Humanoid Robot Development,https://spectrum.ieee.org/figure-robot-video,"<img src=\"https://spectrum.ieee.org/media-library/a-close-up-shot-of-a-metal-humanoid-torso-in-a-white-room.png?id=51595601&amp;width=2000&amp;height=1500&amp;coordinates=0%2C6%2C0%2C7\" /><br /><br /><p>Today, <a href=\"https://www.figure.ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Figure</a> is announcing an astonishing US $675 million Series B raise, which values the company at an even more astonishing $2.6 billion. <a href=\"https://spectrum.ieee.org/figure-humanoid-robot-2665982283\" target=\"_blank\">Figure</a> is <a href=\"https://spectrum.ieee.org/humanoid-robots\" target=\"_self\">one of the companies</a> working toward a multipurpose or general-purpose (depending on whom you ask) bipedal or humanoid (depending on whom you ask) robot. The astonishing thing about this valuation is that Figure’s robot is still very much in the development phase—although they’re making rapid progress, which they demonstrate in a new video posted this week.</p><hr /><p>This round of funding comes from Microsoft, OpenAI Startup Fund, Nvidia, Jeff Bezos (through Bezos Expeditions), Parkway Venture Capital, Intel Capital, Align Ventures, and ARK Invest. Figure says that they’re going to use this new capital “for scaling up AI training, robot manufacturing, expanding engineering head count, and advancing commercial deployment efforts.” In addition, Figure and OpenAI will be collaborating on the development of “next-generation AI models for humanoid robots” which will “help accelerate Figure’s commercial timeline by enhancing the capabilities of humanoid robots to process and reason from language.”</p><p>As far as that commercial timeline goes, here’s the most recent update:</p><p class=\"shortcode-media shortcode-media-youtube\">
<span class=\"rm-shortcode\" style=\"display: block; padding-top: 56.25%;\"></span>
<small class=\"image-media media-photo-credit\">Figure</small></p><p>And to understand everything that’s going on here, we sent a whole bunch of questions to <a href=\"https://www.linkedin.com/in/jennareher/\" rel=\"noopener noreferrer\" target=\"_blank\">Jenna Reher</a>, senior robotics/AI engineer at Figure.</p><p><strong>What does “fully autonomous” mean, exactly?</strong></p><p><strong>Jenna Reher: </strong>In this case, we simply put the robot on the ground and hit go on the task with no other user input. What you see is using a learned vision model for bin detection that allows us to localize the robot relative to the target bin and get the bin pose. The robot can then navigate itself to within reach of the bin, determine grasp points based on the bin pose, and detect grasp success through the measured forces on the hands. Once the robot turns and sees the conveyor, the rest of the task rolls out in a similar manner. By doing things in this way we can move the bins and conveyor around in the test space or start the robot from a different position and still complete the task successfully.</p><p><strong>How many takes did it take to get this take?</strong></p><p><strong>Reher: </strong>We’ve been running this use case consistently for some time now as part of our work in the lab, so we didn’t really have to change much for the filming here. We did two or three practice runs in the morning and then three filming takes. All of the takes were successful, so the extras were to make sure we got the cleanest one to show.</p><p><strong>What’s back in the Advanced Actuator Lab?</strong></p><p><strong>Reher:</strong> We have an awesome team of folks working on some exciting custom actuator designs for our future robots, as well as supporting and characterizing the actuators that went into our current robots.</p><p><strong>That’s a very specific number for “speed vs. human.” Which human did you measure the robot’s speed against?</strong></p><p><strong>Reher: </strong>We timed <a href=\"https://www.linkedin.com/in/brettadcock/\" target=\"_blank\">Brett</a> [Adcock, founder of Figure] and a few poor engineers doing the task and took the average to get a rough baseline. If you are observant, that seemingly overspecific number is just saying we’re at 1/6 human speed. The main point that we’re trying to make here is that we are aware we are currently below human speed, and it’s an important metric to track as we improve.</p><p><strong>What’s the tether for?</strong></p><p><strong>Reher:</strong> For this task we currently process the camera data off-robot while all of the behavior planning and control happens on board in the computer that’s in the torso. Our robots should be fully tetherless in the near future as we finish packaging all of that on board. We’ve been developing behaviors quickly in the lab here at Figure in parallel to all of the other systems engineering and integration efforts happening, so hopefully folks notice all of these subtle parallel threads converging as we try to release regular updates.</p><p><strong>How the heck do you keep your robotics lab so clean?</strong></p><p><strong>Reher:</strong> Everything we’ve filmed so far is in our large robot test lab, so it’s a lot easier to keep the area clean when people’s desks aren’t intruding in the space. Definitely no guarantees on that level of cleanliness if the camera were pointed in the other direction!</p><p><strong>Is the robot in the background doing okay?</strong></p><p><strong>Reher: </strong>Yes! The other robot was patiently standing there in the background, waiting for the filming to finish up so that our manipulation team could get back to training it to do more manipulation tasks. We hope we can share some more developments with that robot as the main star in the near future.</p><p><strong>What would happen if I put a single bowling ball into that tote?</strong></p><p><strong>Reher: </strong>A bowling ball is particularly menacing to this task primarily due to the moving mass, in addition to the impact if you are throwing it in. The robot would in all likelihood end up dropping the tote, stay standing, and abort the task. With what you see here, we assume that the mass of the tote is known a priori so that our whole-body controller can compensate for the external forces while tracking the manipulation task. Reacting to and estimating larger unknown disturbances such as this is a challenging problem, but we’re definitely working on it.</p><p><strong>Tell me more about that very Zen arm and hand pose that the robot adopts after putting the tote on the conveyor.</strong></p><p><strong>Reher:</strong> It does look kind of Zen! If you rewatch our coffee video, you’ll notice the same pose after the robot gets things brewing. This is a reset pose that our controller will go into between manipulation tasks while the robot is awaiting commands to execute either an engineered behavior or a learned policy.</p><p><strong>Are the fingers less fragile than they look?</strong></p><p><strong>Reher: </strong>They are more robust than they look, but not impervious to damage by any means. The design is pretty modular, which is great, meaning that if we damage one or two fingers, there is a small number of parts to swap to get everything back up and running. The current fingers won’t necessarily survive a direct impact from a bad fall, but can pick up totes and do manipulation tasks all day without issues.</p><p><strong>Is the Figure logo footsteps?</strong></p><strong>Reher:</strong> One of the reasons I really like the Figure logo is that it has a bunch of different interpretations depending on how you look at it. In some cases it’s just an F that looks like a footstep plan rollout, while some of the logo animations we have look like active stepping. One other possible interpretation could be an occupancy grid.",[{'name': 'Evan Ackerman'}],"Thu, 29 Feb 2024 13:00:03 +0000"
1854,What If the Biggest AI Fear Is AI Fear Itself?,https://spectrum.ieee.org/ai-fears-jobs-technology,"<img src=\"https://spectrum.ieee.org/media-library/illustration-of-a-man-holding-a-large-jigsaw-puzzle-piece-in-front-of-a-robot-on-a-computer-screen-holding-a-jigsaw-puzzle-piece.jpg?id=51567004&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C0%2C1\" /><br /><br /><p>It’s been just about a year now—a nonprofit called the <a href=\"https://spectrum.ieee.org/ai-pause-letter-stokes-fear\" target=\"_self\">Future of Life Institute</a> posted an <a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\" rel=\"noopener noreferrer\" target=\"_blank\">open letter</a> reflecting people’s darkest fears about <a href=\"https://spectrum.ieee.org/topic/artificial-intelligence/\" target=\"_self\">artificial intelligence</a>. </p><p>“Contemporary AI systems are now becoming human-competitive at general tasks,” it said. It called for a pause in training of the most advanced AI, so that technology companies could develop safety protocols. It expressed worry about disinformation and out-of-control machines. And it struck a nerve with its concerns that some AI could make human work irrelevant. </p><p class=\"pull-quote\">“AI is a tool.... And tools generally aren’t substitutes for expertise but rather levers for its application.” <strong>—David Autor, MIT </strong></p><p>“Should we automate away all the jobs, including the fulfilling ones?” the letter asked. “Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization?” At last count, the institute said more than 33,000 people involved in computer science (including about <a href=\"https://spectrum.ieee.org/ai-pause-letter-stokes-fear\" target=\"_blank\">100 IEEE members</a>) had signed it.<br /></p><p>But there are many others who say we are far—perhaps very far—from a world in which smart machines make human talent redundant. On the contrary, they can extend human reach. That argument is laid out most recently by <a href=\"https://economics.mit.edu/people/faculty/david-h-autor\" target=\"_blank\">David Autor</a>, an economist at the Massachusetts Institute of Technology who has written and spoken extensively on the future of work. </p><p>“It’s important to understand that many of our tools are not our competitors,” he says. “They are more like enablers of the use of human expertise.”</p><h3>AI as Calculator—and Chainsaw</h3><p>In an essay, posted on the website of the <a href=\"https://www.nber.org/papers/w32140\" target=\"_blank\">National Bureau of Economic Research</a> and published in the magazine <a href=\"https://www.noemamag.com/how-ai-could-help-rebuild-the-middle-class/\" rel=\"noopener noreferrer\" target=\"_blank\"><em>Noema</em></a>, Autor says that if we do it right, AI can create many more opportunities than it disrupts. Certainly, there are jobs that will go away, and many things that once demanded a human touch will be done more cheaply and quickly by machines. But, he argues, many new lines of work will be created, or made more effective, by AI’s help. Autor says they may outnumber the jobs made obsolete, potentially by a substantial degree. He writes that “AI—used well—can assist with restoring the middle-skill, middle-class heart of the U.S. labor market that has been hollowed out by automation and globalization.”</p><p class=\"pull-quote\">“People are worried about the wrong things. They’re worried primarily about whether we’ll run out of work, when they ought to be worrying about how we will use human expertise, whether we use it well or badly.” <strong>—David Autor, MIT</strong></p><p>“AI is a tool, like a calculator or a chainsaw,” he says. “And tools generally aren’t substitutes for expertise but rather levers for its application.”</p><p>He cites, among other evidence, an <a href=\"https://arxiv.org/abs/2302.06590\" rel=\"noopener noreferrer\" target=\"_blank\">experiment</a> led by economist <a href=\"https://www.sidapeng.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Sida Peng</a> of <a href=\"https://www.microsoft.com/en-us/research/\" target=\"_blank\">Microsoft Research</a>, in which software developers were given access to <a href=\"https://github.com/features/copilot\" rel=\"noopener noreferrer\" target=\"_blank\">Github Copilot</a>, a <a href=\"https://spectrum.ieee.org/what-is-generative-ai\" target=\"_self\">generative AI</a> programming aid. They were asked to implement an HTTP server in JavaScript. If they used Copilot, they did the job 56 percent faster than a control group. </p><p>In another experiment, published in the journal <em><a href=\"https://www.science.org/doi/10.1126/science.adh2586\" rel=\"noopener noreferrer\" target=\"_blank\">Science</a></em>, grant writers, consultants, and managers were invited to use <a href=\"https://chat.openai.com/\" rel=\"noopener noreferrer\" target=\"_blank\">ChatGPT</a> to help them write short documents, such as press releases and analysis plans. The AI didn’t take over the writing—but it sped the writers’ progress by 40 percent, and an outside peer group rated the quality of their work as 18 percent better.</p><p>Autor emphasizes that he is not saying there’s nothing to worry about. There’s plenty—much of it still unknowable. But, he says, “People are worried about the wrong things. They’re worried primarily about whether we’ll run out of work, when they ought to be worrying about how we will use human expertise, whether we use it well or badly.”</p><h3>The “Pause” That Never Happened</h3><p><a href=\"https://homes.cs.washington.edu/~etzioni/site/\" rel=\"noopener noreferrer\" target=\"_blank\">Oren Etzioni</a> is now focused on that question. An entrepreneur and computer scientist, he has started a nonprofit called <a href=\"https://www.truemedia.org/\" rel=\"noopener noreferrer\" target=\"_blank\">TrueMedia.org</a>, dedicated to fighting the rise of political deepfakes. He says that in many ways he agrees with Autor’s theme: “I think it can help to train people and it can help to level their expertise, and obviously those are positive things, but it’s a nuanced topic.”</p><p class=\"pull-quote\">People would never have guessed a few decades ago at the rise of programmers, statisticians, or social-media managers. Or cybersecurity analysts. Or AI ethicists.</p><p>Etzioni says AI technologies can create work opportunities in countless different fields, but they can also make it easier and cheaper for small groups to do what he calls “disinformation terrorism,” planting falsehoods on social channels to take down opponents.</p><p>AI technologies in their current stage have considerable limits, says Autor. What’s not happening, he says, is AI bots going off on their own and doing creative work or exercising judgment the way a person can. An AI system can improve computer code, for instance, but not suggest new applications out of the blue. </p><p>Among other things, Autor points out that generative AI is transforming a lot of cognitive work—in computing, medicine, finance, and the like—but not physical work, where too many things are beyond a machine’s control. </p><p>“The progress in robotics that deals in an uncertain world, as opposed to robotics on assembly lines where everything is bolted down and under control, that progress has been incredibly slow,” he says. “I mean, how many robots do you encounter in the course of a day? Just about none, right? Maybe your Roomba?”</p><p>The “pause” proposed in that open letter didn’t happen. AI in its many forms continues to transform people’s work at dizzying speed, much as software did a generation ago, or, long before it, electricity. Many jobs have disappeared, like those of farriers or typesetters. But, as Autor says, people would never have guessed a few decades ago at the rise of programmers, statisticians, or social-media managers. </p><p>Or, on the other hand, cybersecurity analysts. Or AI ethicists. </p><p>“A favorite line which I repeat over and over to people,” says Etzioni, “is that AI is the tool, but the choice is ours.” </p>",[{'name': 'Ned Potter'}],"Tue, 27 Feb 2024 14:37:51 +0000"
1855,AI Spam Threatens the Internet—AI Can Also Protect It,https://spectrum.ieee.org/ai-detection,"<img src=\"https://spectrum.ieee.org/media-library/boxes-of-greens-blues-and-red-with-large-error-letters-in-middle-with-bright-pink-yellow-purple-and-red.jpg?id=51566558&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C0%2C0\" /><br /><br /><p>2023 wasn’t a great year for AI detectors. Leaders like GPTZero surged in popularity but <a href=\"https://futurism.com/gptzero-accuracy\" rel=\"noopener noreferrer\" target=\"_blank\">faced a backlash</a> as false positives led to incorrect accusations. Then OpenAI quietly tossed ice-cold water on the idea <a href=\"https://help.openai.com/en/articles/8313351-how-can-educators-respond-to-students-presenting-ai-generated-content-as-their-own\" rel=\"noopener noreferrer\" target=\"_blank\">with an FAQ to answer whether AI detectors work</a>. The verdict? “No, not in our experience.”</p><p>OpenAI’s conclusion was correct—at the time. Yet the demise of AI detectors is greatly exaggerated. Researchers are inventing new detectors that perform better than their predecessors and can operate at scale. And these come alongside “data poisoning” attacks that individuals can use to safeguard their work from being scooped up against their wishes to train AI models.</p><p>“Language model detection can be done with a high enough level of accuracy to be useful, and it can also be done in the ‘zero shot’ sense, meaning you can detect all sorts of different language models at the same time,” says <a href=\"https://www.cs.umd.edu/~tomg/\" target=\"_blank\">Tom Goldstein</a>, a professor of computer science at the University of Maryland. “It’s a real counterpoint to the narrative that language model detection is basically impossible.”</p><h2>Using AI to detect AI</h2><p>Goldstein coauthored a <a href=\"https://arxiv.org/abs/2401.12070\" target=\"_blank\">paper recently uploaded to the arXiv preprint server</a> that describes “Binoculars”: A detector that pairs an AI detective with a helpful sidekick.</p><p>Early AI detectors played at detective by asking a simple question: How surprising is this text? The assumption was that statistically less surprising text is more likely to be AI-generated. It’s an LLM’s mission to predict the “correct” word at each point in a string of text, which should lead to patterns a detector can pick up. Most detectors answered by giving users a numerical probability that the text submitted to it was AI-generated.<strong></strong></p><p>But that approach is flawed. AI-generated text can still be surprising if it’s generated in response to a surprising prompt, which the detector has no way to deduce. And the opposite is true, as well. Humans may write unsurprising text if covering a well-worn topic. </p><p class=\"pull-quote\">Detectors will only prove useful to companies, governments, and educational institutions if they create fewer headaches than they solve, and false positives cause many headaches.</p><p>Binoculars asks its AI detective (in this case <a href=\"https://huggingface.co/tiiuae/falcon-7b\" target=\"_blank\">Falcon-7B</a>, an open-source large language model) the same question as previous detectors, but also asks an AI sidekick to do the same work. The results are compared to calculate how much the sidekick surprised the detective, creating a benchmark for comparison. Text written by a human should prove more surprising to the detective than the AI sidekick. </p><p>There are gaps in what Binoculars can see. <a href=\"https://www.cs.umd.edu/people/vinu\" target=\"_blank\">Vinu Sankar Sadasivan</a>, a graduate student in the University of Maryland’s computer science department and a coauthor on another preprint paper <a href=\"https://arxiv.org/abs/2303.11156\" target=\"_blank\">evaluating a variety of LLM detection techniques</a>, says that Binoculars “significantly improves the performance of zero-shot detection, but it’s still not better than watermarking or retrieval-based methods in terms of accuracy.” Binoculars is also still being peer reviewed; <a href=\"https://www.avischwarzschild.com/\" target=\"_blank\">Avi Schwarzschild</a>, a coauthor on the Binoculars paper, says the goal is to present at a leading AI conference.<strong></strong></p><p>However, Goldstein contends that accuracy isn’t Binoculars’ secret sauce. He believes its real advantage lies in the ability to reveal AI text with fewer false positives. </p><p>“People tend to dwell a lot on accuracy, but this is a mistake,” says Goldstein. “If a detector mistakenly says human-written text was written by a language model…that can lead to false accusations. But if you make a mistake and say AI text is human, it’s not so bad.”</p><p>That might feel counterintuitive. AI can generate text at incredible scale, so even a near-perfect detector could let a lot of AI-generated text slip through. </p><p>But detectors will only prove useful to companies, governments, and educational institutions if they create fewer headaches than they solve, and false positives cause many headaches. Goldstein points out that even a false-positive rate in the single digits will, at the scale of a modern social network, make tens of thousands of false accusations each day, eroding faith in the detector itself. </p><h2>Deepfakes fool people, but not detectors</h2><p>Of course, AI-generated text is just one front in this fight. AI-generated images are of equal concern and, in recent studies, have proven they can fool humans. </p><p>A preprint paper from researchers at Ruhr University Bochum and the Helmholtz Center for Information Security, both in Germany, found people <a href=\"https://arxiv.org/pdf/2312.05976.pdf\" target=\"_blank\">can’t reliably separate AI-generated images of human faces from real photos</a>. Another from researchers at Indiana University and Northeastern University in Boston estimated that between 8,500 and 17,500 daily active accounts on social-media platform X (formerly Twitter) <a href=\"https://arxiv.org/pdf/2401.02627.pdf\" target=\"_blank\">use an AI-generated profile picture</a>.</p><p>It gets worse. These findings focus on generative adversarial networks (GANs), an older class of AI image model that is better understood and known to cause identifiable image artifacts. But the state-of-the-art image generators currently sweeping the Internet, such as AI’s Stable Diffusion, instead use diffusion probabilistic models, which learn to convert random noise into an image likely to mimic what a user wanted. Diffusion models <a href=\"https://openreview.net/pdf?id=AAWuCvzaVt\" target=\"_blank\">significantly outperform</a> their GAN predecessors. </p><p>But even so, it turns out that diffusion isn’t as sly as feared. <a href=\"https://arxiv.org/abs/2210.14571\" rel=\"noopener noreferrer\" target=\"_blank\">A preprint paper to be presented</a> at the International Conference on Computer Vision Theory and Applications (<a href=\"https://visapp.scitevents.org/\" target=\"_blank\">VISAPP 2024</a>), being held in Rome from 27 to 29 February, found that detectors trained on GANs can, with some tweaks, also detect diffusion models. </p><p>“We found that the detectors that we already had, that were trained on GANs, failed to detect diffusion,” says paper coauthor <a href=\"https://jonasricker.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Jonas Ricker</a>, a graduate student at Ruhr University Bochum. “But it’s not undetectable. If we update those detectors, we are still able to detect [images generated by diffusion models].” This suggests that earlier AI image detectors don’t need to be reinvented from scratch to detect the latest models.</p><p>Accuracy is reduced when compared to the detection of GANs, which in some cases can reach 100 percent, but accuracy when detecting diffusion models still often remains higher than 90 percent (the results depend on the diffusion model and detector used). The detectors tweaked to detect diffusion models also remain capable of detecting images generated by a GAN, which makes them useful in many situations.</p><p>But what, exactly, do the updated detectors detect? The answer, as is so often true of modern AI models, is a bit mysterious. “It’s not super clear which artifacts are detectable,” says <a href=\"https://scholar.google.com/citations?user=rswJ0gUAAAAJ&amp;hl=en\" rel=\"noopener noreferrer\" target=\"_blank\">Simon Damm</a>, another graduate student at Ruhr University Bochum and one of the VISAPP 2024 paper’s coauthors. “The performance shows they are for sure detectable…but the interpretability isn’t there.” </p><h2>Data poison degrades spam</h2><p>The latest AI detectors are promising, but detection is an inherently defensive approach. Some researchers are investigating proactive tactics like data poisoning, which disrupts an AI model at training. </p><p>Perhaps the strongest example is <a href=\"https://arxiv.org/pdf/2310.13828.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">Nightshade</a>, a technique invented by researchers at the University of Chicago. Nightshade is a prompt-specific data-poisoning attack built to degrade diffusion models. It laces an image with subtle changes to pixels, most of which aren’t visible to the human eye. However, an AI model trained on these images will learn incorrect associations. It might learn that a car looks like a cow or that a hat looks like a toaster.</p><p>“The simplest way to describe it is a small poison pill you can put in your own art,” says <a href=\"https://people.cs.uchicago.edu/~ravenben/\" target=\"_blank\">Ben Y. Zhao</a>, a professor of computer science at the University of Chicago and one of the researchers who developed Nightshade. “If it’s downloaded against your wishes [and used for training an AI model], it can have a negative effect on the model.”</p><p>Critically, Nightshade can degrade a model even when a slim slice of the training data is poisoned. As little as 100 poisoned samples can successfully attack specific concepts (like “dog” or “cat”) in the latest models, including <a href=\"https://clipdrop.co/stable-diffusion?utm_campaign=stable_diffusion_promo&amp;utm_medium=cta_button&amp;utm_source=stability_ai\" target=\"_blank\">Stable Diffusion XL</a>. The effects compound over multiple attacks, bleed into related concepts, and degrade a wide variety of models.</p><p class=\"pull-quote\">“The simplest way to describe it is a small poison pill you can put in your own art.” <strong>—Ben Y. Zhao, University of Chicago</strong></p><p>How that will translate to the real world remains murky. The researchers had just one Nvidia A100 GPU for training—far less than the dozens or hundreds used to train the latest models—so they were limited to training on smaller datasets than those used by OpenAI or Stability AI. Nightshade is still under peer review, though the team behind it intends to present it at a conference in May. “There’s still unknown factors that are hard to control,” says Zhao. “But at a high level, we should see the effect.”</p><p>Zhao hopes that Nightshade, <a href=\"https://nightshade.cs.uchicago.edu/faq.html\" rel=\"noopener noreferrer\" target=\"_blank\">which is available for anyone to download and use for free</a>, will prove a more effective tool than opt-out schemes and no-crawl requests. Compliance with such requests is voluntary, and while many large AI companies and organizations have pledged to respect them, it’s not legally required and difficult to verify. Data poisoning doesn’t require cooperation, instead offering protection that degrades any image model which includes the poisoned data.</p><p>“We’re not trying to break models out of spite,” says Zhao. “We’re trying to provide a tool for content owners to discourage unauthorized scraping for AI training. This is the way to push back, to provide a real disincentive to model trainers.” </p>",[{'name': 'Matthew S. Smith'}],"Mon, 26 Feb 2024 17:33:35 +0000"
1856,What Is Generative AI?,https://spectrum.ieee.org/what-is-generative-ai,"<img src=\"https://spectrum.ieee.org/media-library/an-animation-showing-a-conceptual-representation-of-a-generative-ai-network.gif?id=51461227&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C0%2C1\" /><br /><br /><p>Generative AI is today’s buzziest form of artificial intelligence, and it’s what powers chatbots like <a href=\"https://spectrum.ieee.org/chatbot-chatgpt-interview\" target=\"_self\">ChatGPT</a>, <a href=\"https://www.youtube.com/watch?v=ZLTpvQpY95w\" target=\"_blank\">Ernie</a>, <a href=\"https://spectrum.ieee.org/llama-2-llm\" target=\"_self\">LLaMA</a>, <a href=\"https://www.anthropic.com/news/introducing-claude\" target=\"_blank\">Claude</a>, and <a href=\"https://cohere.com/models/command\" target=\"_blank\">Command</a>—as well as <a href=\"https://spectrum.ieee.org/ai-design\" target=\"_self\">image generators</a> like <a href=\"https://spectrum.ieee.org/openai-dall-e-2\" target=\"_self\">DALL-E 2</a>, <a href=\"https://stability.ai/stable-image\" target=\"_blank\">Stable Diffusion</a>, <a href=\"https://www.adobe.com/products/firefly.html\" target=\"_blank\">Adobe Firefly</a>, and <a href=\"https://spectrum.ieee.org/tag/midjourney\" target=\"_self\">Midjourney</a>. Generative AI is the branch of AI that enables machines to learn patterns from vast datasets and then to autonomously produce new content based on those patterns. Although generative AI is fairly new, there are already many examples of models that can produce text, images, videos, and audio.</p><p>
	Many “<a href=\"https://crfm.stanford.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">foundation models</a>” have been trained on enough data to be competent in a wide variety of tasks. For example, a large language model can generate essays, <a href=\"https://spectrum.ieee.org/ai-programming\" target=\"_blank\">computer code</a>, recipes, <a href=\"https://spectrum.ieee.org/ai-protein-design\" target=\"_blank\">protein structures</a>, jokes, <a href=\"https://spectrum.ieee.org/chatgpt-medical-exam\" target=\"_blank\">medical diagnostic advice</a>, and <a href=\"https://blogs.nvidia.com/blog/what-are-large-language-models-used-for/\" target=\"_blank\">much more</a>. It can also theoretically generate instructions for building a bomb or creating a bioweapon, though safeguards are supposed to prevent such types of misuse.
</p><h2>What’s the difference between AI, machine learning, and generative AI?</h2><p>
	Artificial intelligence (AI) refers to a wide variety of computational approaches to mimicking human intelligence. 
	<a href=\"https://spectrum.ieee.org/can-machine-learning-teach-us-anything\" target=\"_self\">Machine learning</a> (ML) is a subset of AI; it focuses on algorithms that enable systems to learn from data and improve their performance. Before generative AI came along, most ML models learned from datasets to perform tasks such as classification or prediction. Generative AI is a specialized type of ML involving models that perform the task of generating new content, venturing into the realm of creativity.
</p><h2>What architectures do generative AI models use?</h2><p>
	Generative models are built using a variety of neural network architectures—essentially the design and structure that defines how the model is organized and how information flows through it. Some of the most well-known architectures are 
	<a href=\"https://en.wikipedia.org/wiki/Variational_autoencoder\" rel=\"noopener noreferrer\" target=\"_blank\">variational autoencoders</a> (VAEs), <a href=\"https://en.wikipedia.org/wiki/Generative_adversarial_network\" rel=\"noopener noreferrer\" target=\"_blank\">generative adversarial networks</a> (GANs), and <a href=\"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\" rel=\"noopener noreferrer\" target=\"_blank\">transformers</a>. It’s the transformer architecture, first shown in this seminal <a href=\"https://arxiv.org/abs/1706.03762\" rel=\"noopener noreferrer\" target=\"_blank\">2017 paper from Google</a>, that powers today’s large language models. However, the transformer architecture is less suited for other types of generative AI, such as image and audio generation.</p><p>
	Autoencoders learn efficient representations of data through an 
	<a href=\"https://www.cloudskillsboost.google/course_templates/543\" rel=\"noopener noreferrer\" target=\"_blank\">encoder-decoder framework</a>. The encoder compresses input data into a lower-dimensional space, known as the <a href=\"https://en.wikipedia.org/wiki/Latent_space\" rel=\"noopener noreferrer\" target=\"_blank\">latent (or embedding) space</a>, that preserves the most essential aspects of the data. A decoder can then use this compressed representation to reconstruct the original data. Once an autoencoder has been trained in this way, it can use novel inputs to generate what it considers the appropriate outputs. These models are often deployed in <a href=\"https://medium.com/@judyyes10/generate-images-using-variational-autoencoder-vae-4d429d9bdb5\" rel=\"noopener noreferrer\" target=\"_blank\">image-generation tools</a> and have also found use in <a href=\"https://keras.io/examples/generative/molecule_generation/\" rel=\"noopener noreferrer\" target=\"_blank\">drug discovery</a>, where they can be used to generate new molecules with desired properties.
</p><p>
	With generative adversarial networks (GANs), the training involves a 
	<a href=\"https://developers.google.com/machine-learning/gan/generator\" rel=\"noopener noreferrer\" target=\"_blank\">generator and a discriminator</a> that can be considered adversaries. The generator strives to create realistic data, while the discriminator aims to distinguish between those generated outputs and real “ground truth” outputs. Every time the discriminator catches a generated output, the generator uses that feedback to try to improve the quality of its outputs. But the discriminator also receives feedback on its performance. This adversarial interplay results in the refinement of both components, leading to the generation of increasingly authentic-seeming content. GANs are best known for <a href=\"https://www.sciencedirect.com/science/article/pii/S1877050923001916\" rel=\"noopener noreferrer\" target=\"_blank\">creating deepfakes</a> but can also be used for more benign forms of image generation and many other applications.
</p><p>
	The transformer is arguably the reigning champion of generative AI architectures for its ubiquity in today’s powerful large language models (LLMs). Its strength lies in its attention mechanism, which enables the model to focus on different parts of an input sequence while making predictions. In the case of language models, the input consists of strings of words that make up sentences, and the transformer predicts what words will come next (we’ll get into the details below). In addition, transformers can process all the elements of a sequence in parallel rather than marching through it from beginning to end, as earlier types of models did; this 
	<a href=\"https://huggingface.co/docs/transformers/v4.15.0/parallelism\" rel=\"noopener noreferrer\" target=\"_blank\">parallelization</a> makes training faster and more efficient. When developers added vast datasets of text for transformer models to learn from, today’s remarkable chatbots emerged.
</p><h2>How do large language models work?</h2><p>
	A transformer-based LLM is trained by giving it a vast dataset of text to learn from. The attention mechanism comes into play as it processes sentences and looks for patterns. By looking at all the words in a sentence at once, it gradually begins to understand which words are most commonly found together and which words are most important to the meaning of the sentence. It learns these things by trying to predict the next word in a sentence and comparing its guess to the ground truth. Its errors act as feedback signals that cause the model to adjust the weights it assigns to various words before it tries again.
</p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left\" style=\"float: left;\">
<img alt=\"A chart shows the size of five LLMs in parameters and their performance on a benchmark.\" class=\"rm-shortcode rm-resized-image\" id=\"91ba0\" src=\"https://spectrum.ieee.org/media-library/a-chart-shows-the-size-of-five-llms-in-parameters-and-their-performance-on-a-benchmark.png?id=51455578&amp;width=980\" />
<small class=\"image-media media-caption\">These five LLMs vary greatly in size (given in parameters), and the larger models have better performance on a standard LLM benchmark test. </small><small class=\"image-media media-photo-credit\">IEEE Spectrum</small>
</p><p>
	To explain the training process in slightly more technical terms, the text in the training data is broken down into elements called 
	<a href=\"https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/\" rel=\"noopener noreferrer\" target=\"_blank\">tokens</a>, which are words or pieces of words—but for simplicity’s sake, let’s say all tokens are words. As the model goes through the sentences in its training data and learns the relationships between tokens, it creates a list of numbers, called a vector, for each one. All the numbers in the vector represent various aspects of the word: its semantic meanings, its relationship to other words, its frequency of use, and so on. Similar words, like <em>elegant </em>and <em>fancy</em>, will have similar vectors and will also be near each other in the vector space. These vectors are called word embeddings. The parameters of an LLM include the weights associated with all the word embeddings and the attention mechanism. <a href=\"https://spectrum.ieee.org/tag/gpt-4\" target=\"_blank\">GPT-4</a>, the OpenAI model that’s considered the current champion, is rumored to have more than 1 trillion parameters. </p><p>
	Given enough data and training time, the LLM begins to understand the subtleties of language. While much of the training involves looking at text sentence by sentence, the attention mechanism also captures relationships between words throughout a longer text sequence of many paragraphs. Once an LLM is trained and is ready for use, the attention mechanism is still in play. When the model is generating text in response to a prompt, it’s using its predictive powers to decide what the next word should be. When generating longer pieces of text, it predicts the next word in the context of all the words it has written so far; this function increases the coherence and continuity of its writing.
</p><h2>Why do large language models hallucinate?</h2><p>
	You may have heard that LLMs sometimes “<a href=\"https://spectrum.ieee.org/ai-hallucination\" target=\"_self\">hallucinate</a>.” That’s a polite way to say they make stuff up very convincingly. A model sometimes generates text that fits the context and is grammatically correct, yet the material is erroneous or nonsensical. This bad habit stems from LLMs training on vast troves of data drawn from the Internet, plenty of which is not factually accurate. Since the model is simply trying to predict the next word in a sequence based on what it has seen, it may generate plausible-sounding text that has no grounding in reality.
</p><h2>Why is generative AI controversial?</h2><p>
	One source of controversy for generative AI is the provenance of its training data. Most AI companies that train large models to generate text, images, video, and audio have 
	<a href=\"https://spectrum.ieee.org/ai-ethics\" target=\"_self\">not been transparent</a> about the content of their training datasets. Various leaks and experiments have revealed that those datasets <a href=\"https://spectrum.ieee.org/midjourney-copyright\" target=\"_self\">include copyrighted material</a> such as books, newspaper articles, and movies. A <a href=\"https://www.reuters.com/legal/litigation/artists-take-new-shot-stability-midjourney-updated-copyright-lawsuit-2023-11-30\" rel=\"noopener noreferrer\" target=\"_blank\">number</a> of <a href=\"https://www.reuters.com/legal/john-grisham-other-top-us-authors-sue-openai-over-copyrights-2023-09-20/\" rel=\"noopener noreferrer\" target=\"_blank\">lawsuits</a> are underway to determine whether <a href=\"https://spectrum.ieee.org/generative-ai-ip-problem\" target=\"_self\">use of copyrighted material</a> for training AI systems constitutes fair use, or whether the AI companies need to pay the copyright holders for use of their material.
</p><p>
	On a related note, many people are concerned that the widespread use of generative AI <a href=\"https://spectrum.ieee.org/ai-taking-over-jobs\" target=\"_blank\">will take jobs away</a> from creative humans who make art, music, written works, and so forth. People are also concerned that it could take jobs from humans who do a wide range of white-collar jobs, including translators, paralegals, customer-service representatives, and journalists. There have already been a few 
	<a href=\"https://www.washingtonpost.com/technology/2024/01/10/duolingo-ai-layoffs/\" rel=\"noopener noreferrer\" target=\"_blank\">troubling layoffs</a>, but it’s hard to say yet whether generative AI will be reliable enough for large-scale enterprise applications. (See above about hallucinations.)</p><p>
	Finally, there’s the danger that generative AI will be used to make bad stuff. And there are of course many categories of bad stuff it could theoretically be used for. Generative AI can be used for personalized scams and phishing attacks: For example, using “voice cloning,” scammers can 
	<a href=\"https://www.cbsnews.com/news/scammers-ai-mimic-voices-loved-ones-in-distress/\" rel=\"noopener noreferrer\" target=\"_blank\">copy the voice of a specific person</a> and call the person’s family with a plea for help (and money). All formats of generative AI—text, audio, image, and video—can be used to generate misinformation by creating plausible-seeming representations of things that never happened, which is a particularly worrying possibility when it comes to <a href=\"https://spectrum.ieee.org/deepfakes-election\" target=\"_self\">elections</a>. (Meanwhile, as <em>IEEE </em><em>Spectrum</em> reported this week, the U.S. Federal Communications Commission has responded by <a href=\"https://spectrum.ieee.org/ai-robocalls-2667266649\" target=\"_blank\">outlawing AI-generated robocalls</a>.) Image- and video-generating tools can be used to produce nonconsensual pornography, although the tools made by mainstream companies disallow such use. And chatbots can theoretically walk a would-be terrorist through the steps of making a bomb, nerve gas, and a host of other horrors. Although the big LLMs have safeguards to prevent such misuse, some hackers delight in circumventing those safeguards. What’s more, “uncensored” versions of <a href=\"https://spectrum.ieee.org/open-source-ai-2666932122\" target=\"_self\">open-source LLMs</a> are out there.
</p><p>
	Despite such potential problems, many people think that generative AI can also make people more productive and could be used as a tool to enable entirely new forms of creativity. We’ll likely see both disasters and creative flowerings and plenty else that we don’t expect. But knowing the basics of how these models work is increasingly crucial for tech-savvy people today. Because no matter how sophisticated these systems grow, it’s the humans’ job to keep them running, make the next ones better, and with any luck, help people out too.
</p>",[{'name': 'Eliza Strickland'}],"Wed, 14 Feb 2024 16:34:55 +0000"
1857,The FCC’s Ban on AI in Robocalls Won’t Be Enough,https://spectrum.ieee.org/ai-robocalls-2667266649,"<img src=\"https://spectrum.ieee.org/media-library/an-illustration-of-a-red-robot-hand-holding-a-blue-phone-with-a-line-emanating-from-it-in-the-shape-of-a-face.jpg?id=51451321&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C0%2C0\" /><br /><br /><p>
	In the days before the U.S. Democratic Party’s New Hampshire primary election on 23 January, potential voters began receiving a call with <a href=\"https://apnews.com/article/new-hampshire-primary-biden-ai-deepfake-robocall-f3469ceb6dd613079092287994663db5\" rel=\"noopener noreferrer\" target=\"_blank\">AI-generated audio of a fake President Biden</a> urging them not to vote until the general election in November. In Slovakia a <a href=\"https://www.wired.co.uk/article/slovakia-election-deepfakes\" target=\"_blank\">Facebook post</a> contained fake, AI-generated audio of a presidential candidate planning to steal the election—which <a href=\"https://www.cnn.com/2024/02/01/politics/election-deepfake-threats-invs/index.html\" target=\"_blank\">may have tipped</a> the election in another candidate’s favor. Recent elections in <a href=\"https://www.cnn.com/2024/02/12/asia/suharto-deepfake-ai-scam-indonesia-election-hnk-intl/index.html\" target=\"_blank\">Indonesia</a> and <a href=\"https://www.npr.org/2024/01/11/1216340756/taiwan-election-disinformation-social-media-ptt\" target=\"_blank\">Taiwan</a> have been marred by AI-generated misinformation, too.</p><p>
	In response to the faux-Biden robocall in New Hampshire, the <a href=\"https://www.fcc.gov/\" rel=\"noopener noreferrer\" target=\"_blank\">U.S. Federal Communications Commission</a> moved to make <a href=\"https://www.fcc.gov/document/fcc-makes-ai-generated-voices-robocalls-illegal\" rel=\"noopener noreferrer\" target=\"_blank\">AI-generated voices in robocalls illegal</a> on 8 February. But experts <em>IEEE Spectrum</em> spoke to aren’t convinced that the move will be enough, even as generative AI brings new twists to old robocall scams and offers opportunities to turbocharge efforts to defraud individuals.
</p><p>
	The total lost to scams and spam in the United States in 2022 is thought to be US <a href=\"https://www.truecaller.com/blog/insights/truecaller-insights-2022-us-spam-scam-report\" rel=\"noopener noreferrer\" target=\"_blank\">$39.5 billion</a>, according to TrueCaller, which makes a caller ID and spam-blocking app. That same year, the average amount of money lost by people scammed in the United States <a href=\"https://www.hiya.com/state-of-the-call\" rel=\"noopener noreferrer\" target=\"_blank\">was $431.26</a>, according to a survey by Hiya, a company that provides call-protection and identity services. Hiya says that amount stands to go up as the usage of generative AI gains traction.</p><p>“In aggregate, it’s mind-boggling how much is lost to fraud perpetuated through robocalls,” says <a href=\"https://spia.vt.edu/people/Faculty/bios/burger.html\" rel=\"noopener noreferrer\" target=\"_blank\">Eric Burger</a>, the research director of the <a href=\"https://cyberinitiative.org/\" target=\"_blank\">Commonwealth Cyber Initiative</a> at Virginia Tech.
</p><p class=\"pull-quote\">
	“I don’t think we can appreciate just how fast the telephone experience is going to change because of this.” <strong>—Jonathan Nelson, Hiya</strong>
</p><h2>AI Will Make It Easier for Scammers to Target Individuals</h2><p>
	“The big fear with generative AI is it’s going to take custom-tailored scams and take them mainstream,” says Jonathan Nelson, director of product management at Hiya. In particular, he says, generative AI will make it easier to carry out <a href=\"https://usa.kaspersky.com/resource-center/definitions/spear-phishing\" rel=\"noopener noreferrer\" target=\"_blank\">spear-phishing attacks</a>.
</p><h3>The Cost of Phone Fraud</h3><br /><div>The average amount of money lost by a phone-scam victim in 2022, in U.S. dollars:</div><ul>
<li>United States: $431.26</li>
<li>UK: $324.04</li>
<li>Canada: $472.87</li>
<li>France: $360.62</li>
<li>Germany: $325.87</li>
<li>Spain: $282.35</li>
</ul><p class=\"photo-credit\">
	Source: Hiya
</p><p>
	Generally, <a href=\"https://en.wikipedia.org/wiki/Phishing\" rel=\"noopener noreferrer\" target=\"_blank\">phishing attacks</a> aim to trick people into parting with personal information, such as passwords and financial information. <a href=\"https://us.norton.com/blog/online-scams/spear-phishing\" target=\"_blank\">Spear-phishing</a>, however, is more targeted: The scammer knows exactly whom they’re targeting, and they’re hoping for a bigger payout through a more tailored approach. Now, with generative AI, Nelson says, a scammer can scrape social-media sites, draft text, and even <a href=\"https://blog.hiya.com/voice-fraud-and-the-threat-of-generative-ai\" target=\"_blank\">clone a trusted voice</a> to part unsuspecting individuals from their money en masse.
</p><p>
	With the FCC’s unanimous vote to make generative AI in robocalls illegal, the question naturally turns to enforcement. That’s where the experts whom <em>IEEE Spectrum </em>spoke to are generally doubtful, although many also see it as a necessary first step. “It’s a helpful step,” says <a href=\"https://www.brennancenter.org/experts/daniel-i-weiner\" rel=\"noopener noreferrer\" target=\"_blank\">Daniel Weiner</a>, the director of the Brennan Center’s Elections and Government Program, “but it’s not a full solution.” Weiner says that it’s difficult for the FCC to take a broader regulatory approach in the same vein as the <a href=\"https://www.politico.eu/article/eu-big-tech-help-deepfake-proof-election-2024/\" rel=\"noopener noreferrer\" target=\"_blank\">general prohibition on deepfakes</a> being mulled by the European Union, given the FCC’s scope of authority.
</p><p>
	Burger, who was the FCC’s chief technology officer from 2017 to 2019, says that the agency’s vote will ultimately have an impact only if it starts enforcing the ban on robocalls more generally. Most types of robocalls <a href=\"https://www.fcc.gov/general/telemarketing-and-robocalls\" rel=\"noopener noreferrer\" target=\"_blank\">have been prohibited</a> since the agency instituted the Telephone Consumer Protection Act in 1991. (There are some exceptions, such as prerecorded messages from your dentist’s office, for example, reminding you of an upcoming appointment.)</p><p>
	“Enforcement doesn’t seem to be happening,” says Burger. “The politicians like to say, ‘We’re going after the bad guys,’ and they don’t—not with the vigor we’d like to see.”
</p><h2>Robocall Enforcement Tools May Not Be Enough Against AI</h2><p>
	The key method to identify the source of a robocall—and therefore prevent bad actors from continuing to make them—is to trace the call back through the complex network of telecom infrastructure and identify the call’s originating point. Tracebacks used to be complicated affairs, as a call typically traverses infrastructure maintained by multiple network operators like AT&amp;T and T-Mobile. However, in 2020, <a href=\"https://www.fcc.gov/document/mandating-stirshaken-combat-spoofed-robocalls-0\" rel=\"noopener noreferrer\" target=\"_blank\">the FCC approved a mandate</a> for network operators to begin implementing <a href=\"https://spectrum.ieee.org/how-your-phone-company-aims-to-stop-robocalls\" target=\"_self\">a protocol called STIR/SHAKEN</a> that would, among other antirobocall measures, make one-step tracebacks possible.
</p><p>
	“One-step traceback has been borne out,” says Burger. Traceback, for example, <a href=\"https://apnews.com/article/biden-robocalls-artificial-intelligence-new-hampshire-texas-a8665277d43d05380d2c7594edf27617\" rel=\"noopener noreferrer\" target=\"_blank\">identified the source of the fake Biden calls</a> targeting New Hampshire voters as a Texas-based company called Life Corporation. The problem, Burger says, is that the FCC, the U.S. Federal Bureau of Investigation, and state agencies aren’t providing the resources to make it possible to go after the sheer number of illegal robocall operations. Historically, the FCC has gone after only <a href=\"https://www.jacksonville.com/story/news/state/2018/05/10/florida-man-behind-100-million-robocalls-hit-with-huge-fcc-fine/12273922007/\" rel=\"noopener noreferrer\" target=\"_blank\">the very largest perpetrators</a>.
</p><p>
	“There is no stopping these calls,” says Hiya’s Nelson—at least not entirely. “Our job isn’t to stop them, it’s to make them unprofitable.” Hiya, like similar companies, aims to accomplish that goal by lowering the amount of successful fraud through protective services, including exposing where a call was created and by whom, to make it less likely that an individual will answer the call in the first place.
</p><p>
	However, Nelson worries that generative AI will make the barrier to entry so low that those preventative actions will be less effective. For example, today’s scams still almost always require transferring the victim to a live agent in a call center to close out the scam successfully. With AI-generated voices, scam operators can eventually cut out the call center entirely.
</p><p class=\"pull-quote\">
	“In aggregate, it’s mind-boggling how much is lost to fraud perpetuated through robocalls.” <strong>—Eric Burger, Virginia Tech</strong>
</p><p>
	Nelson is also concerned that as generative AI improves, it will be harder for people to even recognize that they weren’t speaking to an actual person in the first place. “That’s where we’re going to start to lose our footing,” says Nelson. “We may have an increase in call recipients not realizing it’s a scam at all.” Scammers positioning themselves as fake charities, for example, could successfully solicit “donations” without donors ever realizing what actually happened.
</p><p>
	“I don’t think we can appreciate just how fast the telephone experience is going to change because of this,” says Nelson.
</p><p>
	One other complicating issue for enforcement is that the majority of illegal robocalls in the United States originate from beyond the country’s borders. The <a href=\"https://tracebacks.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Industry Traceback Group</a> found that in 2021, for example, <a href=\"https://docs.fcc.gov/public/attachments/DOC-383499A1.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">65 percent of all such calls</a> were international in origin.
</p><p>
	Burger points out that the FCC has taken steps to combat international robocalls. The agency made it possible for other carriers to refuse to pass along traffic from gateway providers—a term for network operators connecting domestic infrastructure to international infrastructure—that are originating scam calls. In December 2023, for example, the FCC ordered two companies, <a href=\"https://docs.fcc.gov/public/attachments/DOC-399297A1.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">Solid Double and CallWin</a>, to stop transmitting illegal robocalls or risk other carriers being required to refuse their traffic.
</p><p class=\"pull-quote\">
	“Enforcement doesn’t seem to be happening. . . . not with the vigor we’d like to see.” <strong>—Eric Burger, Virginia Tech</strong>
</p><p>
	The FCC’s recent action against generative AI in robocalls is the first of its kind, and it remains to be seen if regulatory bodies in other countries will follow. “I certainly think the FCC is setting a good example in swift and bold action in the scope of its regulatory authority,” says Weiner. However, he also notes that the FCC’s counterparts in other democracies will likely end up with more comprehensive results.</p><p>It’s hard to say how the FCC’s actions will stack up versus other regulators, according to Burger. As often as the FCC is way ahead of the curve—such as in <a href=\"https://spectrum.ieee.org/tag/spectrum-sharing\" target=\"_self\">spectrum sharing</a>—it’s just as often way behind, such as the use of <a href=\"https://spectrum.ieee.org/5g-tv\" target=\"_self\">mid-band 5G</a>.
</p><p>Nelson says he expects to see revisions to the FCC’s decision within a couple of years, because it currently prevents companies from using generative AI for legitimate business practices.</p><p>It also remains to be seen whether the FCC’s vote will have any real effect. Burger points out that, in the case of calls like the fake Biden one, it was already illegal to place those robocalls and impersonate the president, so making another aspect of the call illegal likely won’t be a game-changer.<br /></p><p>
	“By making it triply illegal, is that really going to deter people?” Burger says.
</p>",[{'name': 'Michael Koziol'}],"Tue, 13 Feb 2024 17:00:03 +0000"
1858,Open-Source AI Is Good for Us,https://spectrum.ieee.org/open-source-ai-good,"<img src=\"https://spectrum.ieee.org/media-library/a-computer-generated-landscape-with-framework-over-the-land-and-mountains-and-a-beautiful-sun-on-the-horizon-in-the-foreground.jpg?id=51405458&amp;width=2000&amp;height=1500&amp;coordinates=960%2C0%2C960%2C0\" /><br /><br /><p><em>This is a guest post. For the other side of the argument about open-source AI, see the recent guest post “<a href=\"https://spectrum.ieee.org/open-source-ai-2666932122\" target=\"_blank\">Open-Source AI Is Uniquely Dangerous.</a>“</em></p><p>A culture war in AI is emerging between those who believe that the development of models should be restricted or unrestricted by default. In 2024, that clash is spilling over into the law, and it has major implications for the future of open innovation in AI.<br /></p><p>Today, the AI technologies under most scrutiny are generative AI models that have learned how to read, write, draw, animate, and speak, and that can be used to power tools like ChatGPT. Intertwined with the wider debate over AI regulation is a heated and ongoing disagreement over the risk of open models—models that can be used, modified, and shared by other developers—and the wisdom of releasing their distinctive settings, or “weights,” to the public.</p><p>Since the launch of powerful open models like the <a href=\"https://ai.meta.com/blog/large-language-model-llama-meta-ai/\" target=\"_blank\">Llama</a>, <a href=\"https://falconllm.tii.ae/falcon-180b.html\" rel=\"noopener noreferrer\" target=\"_blank\">Falcon</a>, <a href=\"https://mistral.ai/news/announcing-mistral-7b/\" rel=\"noopener noreferrer\" target=\"_blank\">Mistral</a>, and <a href=\"https://stability.ai/stable-image\" rel=\"noopener noreferrer\" target=\"_blank\">Stable Diffusion</a> families, critics have pressed to keep other such genies in the bottle. “Open source software and open data can be an extraordinary resource for furthering science,” <a href=\"https://www.hawley.senate.gov/hawley-and-blumenthal-demand-answers-meta-warn-misuse-after-leak-metas-ai-model\" rel=\"noopener noreferrer\" target=\"_blank\">wrote</a> two U.S. senators to Meta (creator of Llama), but “centralized AI models can be more effectively updated and controlled to prevent and respond to abuse.” Think tanks and closed-source firms have <a href=\"https://openai.com/blog/governance-of-superintelligence\" rel=\"noopener noreferrer\" target=\"_blank\">called</a> for AI development to be regulated like nuclear research, with restrictions on who can develop the most powerful AI models. Last month, one commentator <a href=\"https://spectrum.ieee.org/open-source-ai-2666932122\" target=\"_self\">argued</a> in <em>IEEE </em><em>Spectrum</em> that “open-source AI is uniquely dangerous,” echoing calls for the registration and licensing of AI models.</p><p>The debate is surfacing in recent efforts to regulate AI. First, the European Union has just finalized its <a href=\"https://artificialintelligenceact.eu/the-act/\" rel=\"noopener noreferrer\" target=\"_blank\">AI Act</a> to govern the development and deployment of AI systems. Among its most hotly contested provisions was whether to apply these rules to “free and open-source” models. Second, following President Biden’s <a href=\"https://spectrum.ieee.org/biden-ai-executive-order\" target=\"_self\">executive order</a> on AI, the U.S. government has begun to <a href=\"https://www.axios.com/2024/01/30/commerce-ai-model-info-sharing\" rel=\"noopener noreferrer\" target=\"_blank\">compel reports</a> from the developers of certain AI models, and will soon launch a <a href=\"https://www.ntia.gov/press-release/2023/ntia-kicks-public-engagement-executive-order-ai-work\" rel=\"noopener noreferrer\" target=\"_blank\">public inquiry</a> into the regulation of “widely-available” AI models.</p><p>However our governments choose to regulate AI, we need to promote a diverse AI ecosystem: from large companies building proprietary superintelligence to everyday tinkerers experimenting with open technology. Open models are the bedrock for grassroots innovation in AI. </p><p>I serve as <a href=\"https://www.linkedin.com/in/ben-brooks-7527318a/\" target=\"_blank\">head of public policy</a> for <a href=\"https://stability.ai/\" target=\"_blank\">Stability AI</a> (makers of Stable Diffusion), where I work with a small team of passionate researchers who share media and language models that are freely used by millions of everyday developers and creators around the world. My concern is that this grassroots ecosystem is uniquely vulnerable to mounting restrictions on who can develop and share models. Eventually, these regulations may lead to limits on fundamental research and collaboration in ways that erode this culture of open development, which made AI possible in the first place and helps make it safer.</p><h2>Open models promote transparency and competition</h2><p>Open models play a vital role in helping to drive transparency and competition in AI. Over the coming years, generative AI will support creative, analytic, and scientific applications that go far beyond today’s text and image generators; we’ll see such applications as personalized tutors, desktop healthcare assistants, and backyard film studios. These models will revolutionize essential services, reshape how we access information online, and transform our public and private institutions. In short, AI will become critical infrastructure.</p><p>As I have <a href=\"https://www.schumer.senate.gov/imo/media/doc/Ben%20Brooks%20%20-%20Statement.pdf\" target=\"_blank\">argued</a> before the U.S. Congress and U.K. Parliament, the next wave of digital services should not rely solely on a few “black box” systems operated by a cluster of big tech firms. Today, our digital economy runs on opaque systems that feed us content, control our access to information, determine our exposure to advertising, and mediate our online interactions. We’re unable to inspect these systems or build competitive alternatives. If models—our AI building blocks—are owned by a handful of firms, we risk repeating what played out with the Internet. </p><p class=\"pull-quote\">We’ve seen what happens when critical digital infrastructure is controlled by just a few companies.</p><p>In this environment, open models play a vital role. If a model’s weights are released, researchers, developers, and authorities can “look under the hood” of these AI engines to understand their suitability and to mitigate their vulnerabilities before deploying them in real-world tools. Everyday developers and small businesses can adapt these open models to create new AI applications, tune <a href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\" rel=\"noopener noreferrer\" target=\"_blank\">safer AI models</a> for specific tasks, train <a href=\"https://spectrum.ieee.org/inclusive-ai\" target=\"_self\">more representative AI models</a> for diverse communities, or launch new AI ventures without spending tens of millions of dollars to build a model from scratch.</p><p>We know from experience that transparency and competition are the foundation for a thriving digital ecosystem. That’s why open-source software like <a href=\"https://spectrum.ieee.org/tag/android\" target=\"_self\">Android</a> powers most of the world’s smartphones, and why <a href=\"https://spectrum.ieee.org/tag/linux\" target=\"_self\">Linux</a> can be found in data centers, nuclear submarines, and SpaceX rockets. Open-source software has <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4693148&amp;ref=thestack.technology\" rel=\"noopener noreferrer\" target=\"_blank\">contributed</a> as much as US $8.8 trillion in value globally. Indeed, recent breakthroughs in AI were only possible because of open research like the transformer architecture, open code libraries like <a href=\"https://pytorch.org/\" rel=\"noopener noreferrer\" target=\"_blank\">PyTorch</a>, and open collaboration from researchers and developers around the world.</p><h2>Regulations may stifle grassroots innovation </h2><p>Fortunately, no government has ventured to abolish open models altogether. If anything, governments have resisted the most extreme calls to intervene. The White House declined to require premarket licenses for AI models in its executive order. And after a confrontation with its member state governments in December, the E.U. agreed to partially exempt open models from its AI Act. Meanwhile, Singapore is funding a US $52 million open-source development effort for Southeast Asia, and the UAE continues to bankroll some of the largest available open generative AI models. French President Macron has <a href=\"https://www.politico.eu/article/open-source-artificial-intelligence-france-bets-big/\" rel=\"noopener noreferrer\" target=\"_blank\">declared</a> “on croit dans l’open-source”—we believe in open-source.</p><p>However, the E.U. and U.S. regulations could put the brakes on this culture of open development in AI. For the first time, these instruments establish a legal threshold beyond which models will be deemed “dual use” or “systemic risk” technologies. Those thresholds are based on a range of factors, including the computing power used to train the model. Models over the threshold will attract new regulatory controls, such as notifying authorities of test results and maintaining exhaustive research and development records, and they will lose E.U. exemptions for open-source development.</p><p>In one sense, these thresholds are a good faith effort to avoid overregulating AI. They focus regulatory attention on future models with unknown capabilities instead of restricting existing models. Few existing models will meet the current thresholds, and those that do first will be models from well-resourced firms that are equipped to meet the new obligations.</p><p>In another sense, however, this approach to regulation is troubling, and augurs a seismic shift in how we govern novel technology. Grassroots innovation may become collateral damage.</p><h2>Regulations could hurt everyday developers</h2><p>First, regulating “upstream” components like models could have a disproportionate chilling effect on research in “downstream” systems. Many of the restrictions for above-the-threshold models assume that developers are sophisticated firms with formal relationships to those who use their models. For example, the U.S. executive order requires developers to report on individuals who can access the model’s weights, and detail the steps taken to secure those weights. The E.U. legislation requires developers to conduct “state of the art” evaluations and systematically monitor for incidents involving their models.</p><p class=\"pull-quote\">For the first time, these instruments establish a legal threshold beyond which models will be deemed “dual use” or “systemic risk” technologies.</p><p>Yet the AI ecosystem is more than a handful of corporate labs. It also includes countless developers, researchers, and creators who can freely access, refine, and share open models. They can iterate on powerful “base” models to create safer, less biased, or more reliable “fine-tuned” models that they release back to the community.</p><p>If governments treat these everyday developers the same as the companies that first released the model, there will be problems. Developers operating from dorm rooms and dining tables won’t be able to comply with the premarket <a href=\"https://www.blumenthal.senate.gov/newsroom/press/release/blumenthal-and-hawley-announce-bipartisan-framework-on-artificial-intelligence-legislation\" target=\"_blank\">licensing and approval</a> requirements that have been proposed in Congress, or the “one size fits all” <a href=\"https://artificialintelligenceact.eu/wp-content/uploads/2024/01/AIA-Final-Draft-21-January-2024.pdf\" target=\"_blank\">evaluation, mitigation, and documentation requirements</a> initially drafted by the European Parliament. And they would never contribute to model development—or any other kind of software development—if they thought a senator might <a href=\"https://www.judiciary.senate.gov/committee-activity/hearings/oversight-of-ai-rules-for-artificial-intelligence\" rel=\"noopener noreferrer\" target=\"_blank\">hold them liable</a> for how downstream actors use or abuse their research. Individuals releasing new and improved models on <a href=\"https://github.com/\" rel=\"noopener noreferrer\" target=\"_blank\">GitHub</a> shouldn’t face the same compliance burden as OpenAI or Meta.</p><h2>The thresholds for restrictions seem arbitrary</h2><p>Second, the criteria underpinning these thresholds are unclear. Before we put up barriers around the development and distribution of a useful technology, governments should assess the initial risk of the technology, the residual risk after considering all available legal and technical mitigations, and the opportunity cost of getting it wrong.</p><p>Yet there is still no framework for determining whether these models actually pose a serious and unmitigated risk of catastrophic misuse, or for measuring the impact of these rules on AI innovation. The preliminary U.S. threshold—10<sup>26</sup> floating point operations (FLOPs) in training computation—first appeared as a <a href=\"https://arxiv.org/pdf/2307.03718.pdf\" target=\"_blank\">passing footnote</a> in a research paper. The EU threshold of 10<sup>25</sup> FLOPs is an order of magnitude more conservative, and didn’t appear at all until the final month of negotiation. We may <a href=\"https://epochai.org/trends\" rel=\"noopener noreferrer\" target=\"_blank\">cross</a> that threshold in the foreseeable future. What’s more, both governments reserve the right to move these goalposts for any reason, potentially bringing into scope a massive number of smaller but increasingly powerful models, many of which can be run locally on laptops or smartphones.</p><h2>Restrictions are based on speculative risks</h2><p>Third, there is no consensus about precisely which risks justify these exceptional controls. Online safety, election disinformation, smart malware, and fraud are some of the most immediate and tangible risks posed by generative AI. Economic disruption is possible too. However, these risks are rarely invoked to justify premarket controls for other helpful software technologies with dual-use applications. Photoshop, Word, Facebook, Google Search, and WhatsApp have contributed to the proliferation of deepfakes, fake news, and phishing scams, but our first instinct isn’t to regulate their underlying C++ or Java libraries. </p><p>Instead, critics have focused on “existential risk” to make the case for regulating model development and distribution, citing the prospect of runaway agents or homebuilt weapons of mass destruction. However, as a <a href=\"https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf\" target=\"_blank\">recent paper</a> from Stanford’s Institute for Human-Centered Artificial Intelligence (<a href=\"https://hai.stanford.edu/\" target=\"_blank\">HAI</a>) notes of these claims, “the weakness of evidence is striking.” If these arguments are to justify a radical departure from our conventional approach to regulating technology, the standard of proof should be higher than speculation.</p><h2>We should regulate AI while preserving openness</h2><p>There is no debate that AI should be regulated, and all actors—from model developers to application deployers—have a role to play in mitigating emerging risks. However, new rules must account for grassroots innovation in open models. Right now, well-intended efforts to regulate models run the risk of stifling open development. Taken to their extreme, these frameworks may limit access to foundational technology, saddle hobbyists with corporate obligations, or formally restrict the exchange of ideas and resources between everyday developers.</p><p><span></span> In many ways, models are regulated already, thanks to a complex patchwork of legal frameworks that governs the development and deployment of any technology. Where there are gaps in existing law—such as U.S. federal law governing abusive, fraudulent, or political deepfakes—they can and should be closed. </p><p>However, presumptive restrictions on model development should be the option of last resort. We should regulate for emerging risks while preserving the culture of open development that made these breakthroughs possible in the first place, and that drives transparency and competition in AI.</p>",[{'name': 'Ben Brooks'}],"Thu, 08 Feb 2024 20:19:06 +0000"
1859,Tiny Quadrotor Learns to Fly in 18 Seconds,https://spectrum.ieee.org/drone-quadrotor,"<img src=\"https://spectrum.ieee.org/media-library/a-short-gif-of-simulated-x-shaped-devices-scrambling-around-until-they-gradually-settle-down-and-stay-in-place.gif?id=51413016&amp;width=2000&amp;height=1500&amp;coordinates=87%2C0%2C87%2C0\" /><br /><br /><p>
	It’s kind of astonishing how quadrotors have scaled over the past decade. Like, we’re now at the point where they’re verging on disposable, at least from a commercial or research perspective—for a bit over US $200, you can buy a little <a href=\"https://www.bitcraze.io/products/crazyflie-2-1/\" rel=\"noopener noreferrer\" target=\"_blank\">27-gram, completely open-source drone</a>, and all you have to do is teach it to fly. That’s where things do get a bit more challenging, though, because teaching drones to fly is not a straightforward process. Thanks to good simulation and techniques like reinforcement learning, it’s much easier to imbue drones with autonomy than it used to be. But it’s not typically a fast process, and it can be finicky to make a smooth transition from simulation to reality.
</p><p>
	New York University’s <a href=\"https://wp.nyu.edu/arpl/\" target=\"_blank\">Agile Robotics and Perception Lab</a> in collaboration with the <a href=\"https://www.tii.ae/\" target=\"_blank\">Technology Innovation Institute</a> (TII) have managed to streamline the process of getting basic autonomy to work on drones, and streamline it by a lot: The lab’s system is able to train a drone in simulation from nothing up to stable and controllable flying in 18 seconds flat on a MacBook Pro. And it actually takes longer to compile and flash the firmware onto the drone itself than it does for the entire training process.
</p><hr /><p class=\"shortcode-media shortcode-media-youtube\">
<span class=\"rm-shortcode\" style=\"display: block; padding-top: 56.25%;\"></span>
<small class=\"image-media media-photo-credit\">ARPL NYU</small></p><p>
	So not only is the drone able to keep a stable hover while rejecting pokes and nudges and wind, but it’s also able to fly specific trajectories. Not bad for 18 seconds, right?
</p><p>
	One of the things that typically slows down training times is the need to keep refining exactly what you’re training for, without refining it so much that you’re only training your system to fly in your specific simulation rather than the real world. The strategy used here is what the researchers call a curriculum (you can also think of it as a sort of lesson plan) to adjust the reward function used to train the system through reinforcement learning. The curriculum starts things off being more forgiving and gradually increasing the penalties to emphasize robustness and reliability. This is all about efficiency: Doing that training that you need to do in the way that it needs to be done to get the results you want, and no more.
</p><p>
	There are other, more straightforward, tricks that optimize this technique for speed as well. The deep-reinforcement learning algorithms are particularly efficient, and leverage the hardware acceleration that comes along with Apple’s M-series processors. The simulator efficiency multiplies the benefits of the curriculum-driven sample efficiency of the reinforcement-learning pipeline, leading to that wicked-fast training time.
</p><p>
	This approach isn’t limited to simple tiny drones—it’ll work on pretty much any drone, including bigger and more expensive ones, or even a drone that you yourself build from scratch.
</p><p class=\"shortcode-media shortcode-media-youtube\">
<span class=\"rm-shortcode\" style=\"display: block; padding-top: 56.25%;\"></span>
<small class=\"image-media media-photo-credit\">Jonas Eschmann</small></p><p>
We’re told that it took minutes rather than seconds to train a policy for the drone in the video above, although the researchers expect that 18 seconds is achievable even for a more complex drone like this in the near future. And it’s all <a href=\"https://github.com/arplaboratory/learning-to-fly\" rel=\"noopener noreferrer\" target=\"_blank\">open source</a>, so you can, in fact, build a drone and teach it to fly with this system. But if you wait a little bit, it’s only going to get better: The researchers tell us that they’re working on integrating with the PX4 open source drone autopilot. Longer term, the idea is to have a single policy that can adapt to different environmental conditions, as well as different vehicle configurations, meaning that this could work on all kinds of flying robots rather than just quadrotors.
</p><p>
	Everything you need to run this yourself <a href=\"https://github.com/arplaboratory/learning-to-fly\" target=\"_blank\">is available on GitHub</a>, and the paper is on ArXiv <a href=\"https://arxiv.org/abs/2311.13081\" target=\"_blank\">here</a>.</p>",[{'name': 'Evan Ackerman'}],"Thu, 08 Feb 2024 17:00:02 +0000"
1860,"Standards Matter for Cars, Plugs, Wi-Fi—and AI?",https://spectrum.ieee.org/tech-standards-responsible-ai,"<img src=\"https://spectrum.ieee.org/media-library/a-layer-of-cream-and-blue-silhouetted-heads-forming-a-swirl-resulting-in-an-upside-down-head-within-the-head.jpg?id=51406236&amp;width=2000&amp;height=1500&amp;coordinates=166%2C0%2C167%2C0\" /><br /><br /><p><a href=\"https://spectrum.ieee.org/topic/artificial-intelligence/\" target=\"_self\">Artificial intelligence</a> holds much promise for innovation and progress, but it also has the potential to cause harm. To enable the responsible development and use of AI, the <a href=\"https://www.iso.org/home.html\" rel=\"noopener noreferrer\" target=\"_blank\">International Organization for Standardization</a> (ISO) recently released <a href=\"https://www.iso.org/standard/81230.html\" rel=\"noopener noreferrer\" target=\"_blank\">ISO/IEC 42001</a>, a new standard for AI management systems. According to ISO, this standard “offers organizations the comprehensive guidance they need to use AI responsibly and effectively, even as the technology is rapidly evolving.”</p><p>As AI has rapidly matured and broadly been rolled out across the world, there’s been a tangle of conflicting standards from big AI companies like <a href=\"https://ai.meta.com/responsible-ai/\" target=\"_blank\">Meta</a>, <a href=\"https://www.microsoft.com/en-us/ai/responsible-ai\" target=\"_blank\">Microsoft</a>, and <a href=\"https://ai.google/responsibility/responsible-ai-practices/\" target=\"_blank\">Google</a>. (Although in November, Meta <a href=\"https://www.reuters.com/technology/meta-breaks-up-its-responsible-ai-team-information-2023-11-18/\" target=\"_blank\">reportedly</a> disbanded its Responsible AI group.) And the Austin, Tex.-based <a href=\"https://www.linkedin.com/company/responsible-ai-institute\" target=\"_blank\">Responsible AI Institute</a> has its own <a href=\"https://www.responsible.ai/how-we-help\" target=\"_blank\">assessments and certification program</a> for ethical uses and applications for AI. Yet, maintaining consistent standards and practices are also an age-old challenge over the entire history of technology. And standards-keeping organizations like the ISO—and the IEEE—could be natural places to turn for a widely agreed-upon set of parameters for responsible AI development and use. </p><p class=\"pull-quote\">“If there is this kind of buy-in from organizations that are promoting the responsible development and use of AI, others will follow.” <strong>—Virginia Dignum, Umeå University, Umeå, Sweden</strong></p><p>In ISO’s case, their standard concerns AI <em>management</em> systems. These are catalogs or inventories of the different AI systems that a company is using, along with information on how, where, and why these systems are being used, says <a href=\"https://umangsbhatt.github.io/\" target=\"_blank\">Umang Bhatt</a>, an assistant professor and faculty fellow at <a href=\"https://www.nyu.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">New York University</a> and an advisor to the <a href=\"https://www.responsible.ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Responsible AI Institute</a>. And as the standard specifies, an AI management system is “intended to establish policies and objectives, as well as processes to achieve those objectives, in relation to the responsible development, provision, or use of AI systems.”</p><p>So ISO’s new standard provides a set of concrete guidelines—as opposed to just high-level principles—that support responsible AI, says <a href=\"https://www.cs.cmu.edu/~hheidari/\" rel=\"noopener noreferrer\" target=\"_blank\">Hoda Heidari</a>, who coleads the <a href=\"https://www.cmu.edu/block-center/responsible-ai/index.html\" rel=\"noopener noreferrer\" target=\"_blank\">Responsible AI Initiative</a> at <a href=\"https://www.cmu.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">Carnegie Mellon University</a>. Heidari adds that the standard also gives AI developers confidence that “the appropriate processes were followed in the creation and evaluation of the system before it was released, and there are appropriate processes around to monitor it and address any adverse outcomes.”</p><h3>IEEE, ISO, and governments consider</h3><p>Meanwhile, <em>IEEE Spectrum</em>’s parent organization, the <a href=\"http://ieee.org\" target=\"_blank\">IEEE</a>, also maintains and develops a <a href=\"https://standards.ieee.org/\" target=\"_blank\">wide range of standards</a> across many fields of technology. As of press time, <em>Spectrum</em> has learned of at least one effort now afoot within the broad global reach of IEEE standards-making organizations to develop responsible AI standards. It would reportedly be an outgrowth of the <a href=\"https://standards.ieee.org/ieee/2863/10142/\" target=\"_blank\">2020 Recommended Practice standard</a> for AI development and use. In addition, the <a href=\"https://standards.ieee.org/industry-connections/ec/autonomous-systems/\" target=\"_blank\">IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems</a> has made available <a href=\"https://standards.ieee.org/wp-content/uploads/import/documents/other/ead1e-overview.pdf\" target=\"_blank\">this document</a> promoting ethically aligned development of autonomous systems. </p><p>As with some standards in tech, the ISO’s standard is not a mandatory standard. “What would compel companies to adopt this? The standard itself is not enough—you have to have reason and motivation for these developers to adopt it,” says <a href=\"https://chiragshah.org/index.php\" target=\"_blank\">Chirag Shah</a>, a founding codirector of <a href=\"https://www.raise.uw.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">RAISE</a>, a center for responsible AI at the <a href=\"https://www.washington.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">University of Washington</a>. He adds that organizations might also view the standard as an overhead task, especially for small ones without enough resources or even large corporations that already have their own standards.</p><p class=\"pull-quote\">“It’s just like a tracking record that I hope will become part of the culture in the software development community.” <strong>—Umang Bhatt, New York University</strong></p><p><a href=\"https://www.umu.se/en/staff/virginia-dignum/\" rel=\"noopener noreferrer\" target=\"_blank\">Virginia Dignum</a>, a professor in responsible AI and director of the AI Policy Lab at Sweden’s <a href=\"https://www.umu.se/\" rel=\"noopener noreferrer\" target=\"_blank\">Umeå University</a>, echoes the sentiment, noting that the standard “only really works when there is a sufficient number of organizations taking it up, and by doing that, we also identify what will and will not work in the standard.” To address this issue, Dignum suggests turning to <a href=\"https://spectrum.ieee.org/ai-ethics-industry-guidelines\" target=\"_blank\">big tech firms</a> and convincing them to adopt the standard, because “if there is this kind of buy-in from organizations that are promoting the responsible development and use of AI, others will follow.” For instance, <a href=\"https://aws.amazon.com/blogs/machine-learning/iso-42001-a-new-foundational-global-standard-to-advance-responsible-ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon’s AWS participated in creating the standard and is now pursuing its adoption</a>.</p><p>Another motivation to apply the standard is to be prepared and create a framework for looming <a href=\"https://spectrum.ieee.org/ai-regulation-worldwide\" target=\"_self\">regulations</a> from other standards-making bodies, which may align with ISO’s new standard. For example, the U.S. government recently released an <a href=\"https://spectrum.ieee.org/biden-ai-executive-order\" target=\"_self\">executive order on AI</a>, while the <a href=\"https://apnews.com/article/ai-act-europe-regulation-59466a4d8fd3597b04542ef25831322c\" rel=\"noopener noreferrer\" target=\"_blank\">European Union’s AI Act is expected to take full effect by 2025</a>.</p><h3>Trust matters, too</h3><p>An additional incentive for AI companies to take up the standard is to cultivate trust with end users. In the United States, for instance, people express <a href=\"https://www.pewresearch.org/science/2023/02/15/public-awareness-of-artificial-intelligence-in-everyday-activities/\" rel=\"noopener noreferrer\" target=\"_blank\">more concern over excitement about AI’s impact on their daily lives</a>, with these <a href=\"https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2023/10/consumers-are-voicing-concerns-about-ai\" rel=\"noopener noreferrer\" target=\"_blank\">concerns</a> spanning the data used to train AI, its biases and inaccuracies, and its potential for misuse. “When there are standards and best practices around, and we can assure consumers that those are followed, they will trust the system more, and they are more willing to interact with it,” Heidari says.</p><p>Akin to a car’s braking system, which has been built and tested following particular standards and specifications, “even if users don’t understand what the standard is, it will provide them the confidence that things were developed in a certain way, and that there are also some auditing or checks and oversight on what has been developed,” Dignum says.</p><p>For AI firms looking to adopt the standard, Bhatt advises viewing it much like you would the practices you’ve established to keep track of any issues with your AI system. “These standards are going to come in place in a way that is quite similar to the continuous monitoring tools you might build and use,” he says. “It’s just like a tracking record that I hope will become part of the culture in the software development community.”</p><p>Beyond implementation, Heidari hopes ISO’s new standard will spur a mindset shift in AI companies and the people creating them. She points to design choices when training machine-learning models as an example: It may seem like making just another engineering or technical decision that doesn’t have any meaning outside the machinery they’re dealing with, but “all those choices have huge implications when the resulting model will be utilized for decision-making processes or for automating practices on the ground,” she says. “The most important thing for developers of these systems is to keep in mind that whether they know it or not and whether they accept it or not, a lot of the choices they make have real-world consequences.”</p><p><br /></p><p><strong>UPDATE 9 Feb. 2024: </strong>The story was updated to provide a link to the most recent IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems document, from 2019. The original story had linked to the 2017 version. <br /></p>",[{'name': 'Rina Diane Caballar'}],"Wed, 07 Feb 2024 20:32:26 +0000"
1861,Could AI Disrupt Peer Review?,https://spectrum.ieee.org/ai-peer-review,"<img src=\"https://spectrum.ieee.org/media-library/a-3d-illustration-of-a-horizontal-stack-of-papers-all-of-which-are-glowing-green-except-for-one-red-one-that-is-being-pulled-o.jpg?id=51385865&amp;width=2000&amp;height=1500&amp;coordinates=166%2C0%2C167%2C0\" /><br /><br /><p>Spending time poring over manuscripts to offer thoughtful and incisive critique as a peer reviewer is one of academia’s most thankless jobs. Peer review is often the final line of defense between new research and the general public and is aimed at ensuring the accuracy, novelty, and significance of new findings.</p><p>This crucial role is voluntary, unpaid, and often underappreciated by academic publishers and institutions. As with other tedious jobs in today’s world, this raises the question: Can, and more importantly, should,  publishers trust AI to handle peer review instead? A number of researchers say no and are growing concerned about how AI may threaten the integrity of the review process by reinforcing bias and introducing misinformation.</p><p><a href=\"https://www.ihu.gr/ucips/cv/vasiliki-mollaki\" target=\"_blank\">Vasiliki Mollaki</a> is a bioethicist and geneticist at the International Hellenic University in Greece addressed this issue in the journal <em>Research Ethics</em> on 9 January in <a href=\"https://journals.sagepub.com/doi/full/10.1177/17470161231224552\" rel=\"noopener noreferrer\" target=\"_blank\">an article</a> pointedly titled “Death of a Reviewer or Death of Peer Review Integrity?”</p><p>In her paper, Mollaki reviewed the AI policies of top academic publishers—including Elsevier and Wiley—to determine whether they were preparing to address the potential use of AI in peer review. While several journals have developed policies around AI used by authors to write manuscripts, such policies for peer review were almost nonexistent. </p><p>“If [AI] is mentioned, it’s on the basis that there might be confidential data or even personal data that should not be shared with tools [because] they don’t know how this data can be used,” Mollaki says. “The basis is not on ethical grounds.” </p><p>Without concrete policies that lay out guidance on transparency or penalties for using AI in peer review, Mollaki worries that the integrity and good faith trust in the peer review process could collapse. Never mind that the question of whether AI is actually capable yet of providing effective peer review is also up for debate. </p><p class=\"pull-quote\">“Current AI tools are very bad at suggesting specific authors, journals, or papers, and often start hallucinating because their training data is not aimed at forming these connections.”<strong>—Tjibbe Donker, Freiburg University Hospital</strong></p><p><a href=\"https://profiles.stanford.edu/james-zou?tab=bio\" rel=\"noopener noreferrer\" target=\"_blank\">James Zou</a> is an assistant professor of biomedical data science at Stanford University and is the senior author on <a href=\"https://arxiv.org/abs/2310.01783?ref=PDF\" rel=\"noopener noreferrer\" target=\"_blank\">a preprint paper</a> published on arXiv in late 2023 that evaluated how AI’s feedback on research papers compared to that of human reviewers. This work found that AI reviewers’ points overlapped with human reviewers’ points at a rate comparable to two human reviewers and that more than 80 percent of researchers found AI’s feedback more helpful than that of human reviewers.</p><p>“This is especially helpful for authors working on early drafts of manuscripts,” Zou says. “Instead of waiting for weeks to get feedback from mentors or experts, they can get immediate feedback from the LLM.”</p><p>Yet, <a href=\"https://www.thelancet.com/journals/laninf/article/PIIS1473-3099(23)00290-6/fulltext#seccestitle20\" rel=\"noopener noreferrer\" target=\"_blank\">work published</a> that same year in <em>Lancet Infectious</em> <em>Diseases</em> by <a href=\"https://www.linkedin.com/in/tjibbe-donker-294a3a15/\" rel=\"noopener noreferrer\" target=\"_blank\">Tjibbe Donker</a>, an infectious disease epidemiologist at Freiburg University Hospital, in Germany, found that AI struggled to generate personalized feedback responses and even created false citations to support its reviews. </p><p>“Current AI tools are very bad at suggesting specific authors, journals, or papers, and often start hallucinating because their training data is not aimed at forming these connections,” Donker says.</p><p>Despite his reservations, Donker is not necessarily in favor of barring all AI tools from peer review. Instead, he says that using these tools selectively to assist human reviewers in their process could be beneficial, such as helping reviewers assess novelty independent of an author’s writing style by summarizing the paper’s main points. AI could also play a role in consolidating human reviewer’s letters into a single decision letter for authors.</p><p>To ensure that reviewers use AI tools in a minimally invasive way, Mollaki says it will be important for journals to write AI review policies that go beyond issues of privacy and focus on disclosure and transparency.</p><p>“[Journals] should be as clear as possible about what is not permitted,” Mollaki says. “[How] the tools have been used should be disclosed and even the prompts that were used.”<br /></p><p>For authors who break these policies, Mollaki is in favor of a penalty that excludes future participation in peer review. Donker, however, says those repercussions may need to be a little more nuanced. Reacting too strongly to the use of AI in peer review could ironically have the same impact as letting AI run wild.</p><p>“Peer reviewing is done voluntarily, unpaid, without much of a reward for the reviewer,” Donker says. “Most scientists would be quite happy to be excluded from this process, while journals end up with even fewer reviewers to choose from.”</p>",[{'name': 'Sarah Wells'}],"Tue, 06 Feb 2024 14:40:00 +0000"
1862,MIT and IBM Find Clever AI Ways Around Brute-Force Math,https://spectrum.ieee.org/mathematical-model-ai,"<img src=\"https://spectrum.ieee.org/media-library/dots-of-light-starting-from-the-outside-in.jpg?id=51285289&amp;width=2000&amp;height=1500&amp;coordinates=131%2C0%2C131%2C0\" /><br /><br /><p>Since the days of Isaac Newton, the fundamental laws of nature all ultimately reduce to a vital, broad set of equations. Now researchers have found a new way to use brain-inspired neural networks to solve these equations significantly more efficiently than before for numerous potential applications in science and engineering.</p><p>In modern science and engineering, <u><a href=\"https://people.math.harvard.edu/~knill/pedagogy/pde/index.html\" rel=\"noopener noreferrer\" target=\"_blank\">partial differential equations</a></u> help model <u><a href=\"https://uwaterloo.ca/applied-mathematics/future-undergraduates/what-you-can-learn-applied-mathematics/differential-equations/partial-differential-equations-pdes\" rel=\"noopener noreferrer\" target=\"_blank\">complex physical systems involving multiple rates of change, such as ones changing across both space and time</a></u>. They can help <u><a href=\"https://www.ias.edu/ideas/curiosities-partial-differential-equations\" rel=\"noopener noreferrer\" target=\"_blank\">model</a></u> all sorts of things, such as the flow of air past the wings of an airplane, the spreading of a pollutant in the air, or the collapse of a star into a black hole.</p><p>To solve these difficult equations, scientists traditionally used high-precision numerical methods. However, these can be very time-consuming and computationally resource-intensive to run.</p><p>Currently, simpler alternatives exist, known as <a href=\"https://en.wikipedia.org/wiki/Surrogate_model\" target=\"_blank\">data-driven surrogate models</a>. These models, which include <u><a href=\"https://spectrum.ieee.org/deep-neural-network\" target=\"_self\">neural networks</a></u>, are trained on data from numerical solvers to predict what answers they might produce. However, these still require a large amount of data from numerical solvers for training. The amount of data needed increases exponentially as these models grow in size, making this strategy difficult to scale, says study lead author <a href=\"https://cse.gatech.edu/people/raphael-pestourie\" target=\"_blank\">Raphaël Pestourie</a>, a computational scientist at the Georgia Institute of Technology in Atlanta.</p><p>In a new study, researchers developed an approach to developing surrogate models. This strategy uses physics simulators to help train neural networks to match the output of the high-precision numerical systems. The aim is to generate accurate results with the help of expert knowledge in a field—in this case, physics—instead of merely throwing a lot of computational resources at these problems to find solutions using brute force.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image\">
<img alt=\"black and white figures with arrows against a white background\" class=\"rm-shortcode\" id=\"40582\" src=\"https://spectrum.ieee.org/media-library/black-and-white-figures-with-arrows-against-a-white-background.jpg?id=51285294&amp;width=980\" />
<small class=\"image-media media-caption\">Researchers have found that numerical surrogates (symbolized here as a cartoon of James Clerk Maxwell) can arrive at solutions to hard mathematical problems that had previously required high-precision, brute-force math—symbolized by the Maxwell daguerreotype. </small><small class=\"image-media media-photo-credit\">MIT</small></p><p>The scientists tested what they called physics-enhanced deep surrogate (PEDS) models on three kinds of physical systems. These included diffusion, such as a dye spreading in a liquid over time; reaction-diffusion, such as diffusion that might take place following a chemical reaction; and electromagnetic scattering.</p><p>The researchers found these new models can be up to three times as accurate as other neural networks at tackling partial differential equations. At the same time, these models needed only about 1,000 training points. This reduces the training data required by at least a factor of 100 to achieve a target error of 5 percent.</p><p>“The idea is quite intuitive—let the neural networks do the learning and the scientific model do the science,” Pestourie says. “PEDS shows that combining both is far greater than the sum of its parts.”</p><p>Potential applications for PEDS models include accelerating simulations “of complex systems that show up everywhere in engineering—weather forecasts, carbon capture, and nuclear reactors, to name a few,” Pestourie says.</p><p>The scientists detailed <u><a href=\"https://www.nature.com/articles/s42256-023-00761-y\" rel=\"noopener noreferrer\" target=\"_blank\">their findings</a></u> in the journal <em>Nature Machine Intelligence</em>.</p><p><em>Updated 5 Feb 2024 to remove incorrectly stated laws of nature. Spectrum regrets the error.</em></p>",[{'name': 'Charles Q. Choi'}],"Fri, 02 Feb 2024 20:30:43 +0000"
1863,Brain-Connection Maps Help Neuromorphic Chips,https://spectrum.ieee.org/connectome-neuromorphic-chips,"<img src=\"https://spectrum.ieee.org/media-library/black-dots-representing-neurons-and-blue-lines-representing-connections-form-a-web-network.jpg?id=51234408&amp;width=2000&amp;height=1500&amp;coordinates=100%2C0%2C100%2C0\" /><br /><br /><p>In neuroscience—as in geography, genomics, and plenty else—it’s hard to navigate without a good map. Recent advances in <a href=\"https://spectrum.ieee.org/brain-scanning-just-got-very-good-and-very-unsettling\" target=\"_self\"><u>brain mapping technologies</u></a> have enabled scientists to create larger and more detailed models. And as those models for different animals are compared, some surprising similarities have cropped up. So much so that these neurological connection maps (a.k.a. “connectomes”) may inform the designs of advanced <a href=\"https://spectrum.ieee.org/manan-suri-profile\" target=\"_self\"><u>neuromorphic electronics</u></a>: chip and algorithmic models that seek to mimic the computational power and efficiency of brains and neurons.</p><p>Scientists at Yale, Princeton, and the University of Chicago seized the 2021 publication of the <a href=\"https://www.nature.com/articles/s41592-021-01330-0.pdf?proof=t\" rel=\"noopener noreferrer\" target=\"_blank\"><u>fruit-fly connectome</u></a>—a landmark in the emerging field of <a href=\"https://hms.harvard.edu/news/new-field-neuroscience-aims-map-connections-brain#:~:text=Lee%3A%20We%20define%20connectomics%20as,every%20neuron%20and%20every%20connection.\" rel=\"noopener noreferrer\" target=\"_blank\">connectomics</a>—as their opportunity to compare aspects of brain structure across widely different size and complexity scales. </p><p>Mapping out a connectome is no small feat, says Christopher Lynn, <a href=\"https://physics.yale.edu/people/christopher-lynn\" rel=\"noopener noreferrer\" target=\"_blank\">assistant professor of physics</a> at Yale. Even a species as minuscule as the fruit fly represents a substantial neurological challenge. Fruit-fly brains comprise a network of over 120,000 neurons with more than 30 million connections among them. Which, for the purpose of a connectome, means each connection must somehow be isolated, recognized, and graphed. </p><h3>How <em>C. elegans</em> mapped the way</h3><p>“For a sense of scale, <em>C. elegans</em> [the nematode worm] has only 302 neurons,” Lynn says. “They <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4360118/#:~:text=By%201984%2C%20the%20complete%20wiring,the%20synapses%20in%20nervous%20systems.\" target=\"_blank\">mapped out the entire connectome</a> for the first worm in the ’80s, and that was a big breakthrough,” says Lynn. “Each neuron might have between 100 to 10,000 synapses in the case of the fruit fly. When you get to larger systems, you’re tracking hundreds of thousands or millions of synapses.”<br /></p><p class=\"pull-quote\">“There is a simple phrase they say when teaching this: ‘If two neurons fire together, they are likely to wire together.’ ” <strong>—Christopher Lynn, Yale University</strong></p><p>The researchers found persistent connectivity statistics across five different connectomes scaling from the simple <em>C. elegans</em> nematode to the retina of a mouse. They used what’s called <a href=\"https://en.wikipedia.org/wiki/Heavy-tailed_distribution\" target=\"_blank\">heavy-tailed statistics</a>—recognizing that while most connections between neurons are weak, a small number of those connections are much stronger. This result complements <a href=\"https://journals.sagepub.com/doi/full/10.1177/1073858416667720\" target=\"_blank\">prior work</a> showing that brain networks tend to show “<a href=\"https://news.cornell.edu/stories/2023/05/mathematical-model-changed-everything-turns-25\" target=\"_blank\"><u>small-world</u></a>“ characteristics: a small number of neurons are connected to many other neurons, but most neurons aren’t connected to very many at all.</p><p>The presence of heavy-tailed connectivity across species indicates that some of the same principles of neural function may also bridge across organisms of entirely different scales. To help provide a possible framework, the team created a model of brain development that weighed random reorganization with what neuroscientists call <a href=\"https://en.wikipedia.org/wiki/Hebbian_theory\" target=\"_blank\">Hebbian plasticity</a>, or the tendency of nearby, concurrently active neurons to connect to each other. “There is a simple phrase they say when teaching this: ‘If two neurons fire together, they are likely to wire together,’ ” says Lynn. The model, though a simple approximation of how neurons in a brain may come together, consistently recreated the heavy-tailed distributions Lynn and his colleagues saw across connectomes.</p><h3>Brain maps inspire neuromorphic hardware</h3><p>The researchers’ discovery of cross-species connectome similarities could pave the way for new neuromorphic hardware. For instance, the heavy-tailed connectivity patterns they studied suggest a productive line of inspiration for chip design. </p><p class=\"pull-quote\">“There are details that are different, but the heavy-tailed shape is consistent across animals and brain regions.” <strong>—Christopher Lynn, Yale University</strong></p><p>Mike Davies, director of <a href=\"https://spectrum.ieee.org/neuromorphic-computing-with-lohi2\" target=\"_self\"><u>Intel’s Neuromorphic computing laboratory</u></a>, says that small-world connectivity patterns are an attractive feature to model one’s design ideas after. “There’s a tendency to want to simplify the connectivity in a neuromorphic chip to what you can conveniently fabricate,” says Davies. “To represent all possible connections is an <em>n</em>-squared explosion.”</p><p>He adds, however, that Intel has taken a slightly different approach to its neuromorphic designs than strictly following nature’s lead. Instead of trying to explicitly build dense networks in the chips themselves, the company is using adaptive networking systems that route the networking traffic. “It’s the most efficient system we have for modeling these sparse connections,” says Davies. “With that, we actually can replicate some of these heavy-tailed networks.”</p><p>Looking forward, Lynn says the connectomics team will now expand the list of species whose connectomes they map. While the cell-level mouse connectome used in the present study was limited to the animal’s retina, Lynn says the researchers also observed similar heavy-tailed connectivity in much larger parts of the mouse brain. </p><p>While the team’s present results are limited to a handful of connectomes, Lynn says he expects their results will be recapitulated for the connectomes of even larger and more complicated brains. “Based on literally every dataset we’ve seen so far, this distribution looks similar across all of them,” he says. “I would expect these heavy-tailed distributions to be pretty ubiquitous across brains generally.”</p><p>The researchers published <a href=\"https://www.nature.com/articles/s41567-023-02332-9\" target=\"_blank\">their results</a> earlier this month in the journal <em>Nature Physics. </em></p>",[{'name': 'Michael Nolan'}],"Wed, 31 Jan 2024 20:37:59 +0000"
1864,"The Battle for Better, Broader, More Inclusive AI",https://spectrum.ieee.org/inclusive-ai,"<img src=\"https://spectrum.ieee.org/media-library/photo-illustration-of-a-black-man-and-woman-looking-at-the-camera-with-dots-data-and-other-imagery-related-to-ai-around-them.jpg?id=51234058&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C666%2C0\" /><br /><br /><p>AI’s inclusivity problem is no secret. <a href=\"https://www.aclu.org/news/privacy-technology/how-artificial-intelligence-can-deepen-racial-and-economic-inequities\" rel=\"noopener noreferrer\" target=\"_blank\">According to the ACLU</a>, AI systems can perpetuate housing discrimination and bias in the justice system, among other harms. <a href=\"https://spectrum.ieee.org/untold-history-of-ai-the-birth-of-machine-bias\" target=\"_self\">Bias in the data an AI model relies on is reproduced in its results</a>. </p><p>Large Language Models (LLMs) share this problem; <a href=\"https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00225-X/fulltext\">they can reproduce bias in medical settings</a> and perpetuate<a href=\"https://arxiv.org/abs/2304.05335\"> harmful stereotypes</a>, among other problems. To combat that, the New York City–based <a href=\"https://www.futuresum.ai/\" rel=\"noopener noreferrer\" target=\"_blank\">FutureSum AI</a> is building Latimer, the first “racially inclusive large language model.” Latimer—named after a <a href=\"https://spectrum.ieee.org/lewis-latimer-history-lightbulb-moments\" target=\"_blank\">pioneering engineer</a> of the 19th and early 20th centuries—hopes to reduce bias, better represent underrepresented voices, and prevent results that erase or minimize black and brown cultural data.</p><p class=\"pull-quote\">Hugging Face, the hub for the open-source AI community, lists over 2,700 “conversational” AI models. </p><p>“Data is king,” says <a href=\"https://www.linkedin.com/in/malur/\" rel=\"noopener noreferrer\" target=\"_blank\">Malur Narayan, technology advisor for FutureSum AI</a>. “The only way to create a moat is to have the relevant data for the topic you’re trying to address.”</p><h2>Curating a Different Dataset</h2><p>Large Language Models have proliferated with incredible speed. <a href=\"https://huggingface.co/\" target=\"_blank\">Hugging Face</a>, the hub for the open-source AI community, lists over 2,700 “conversational” AI models. Yet most are trained on similar data (<a href=\"https://commoncrawl.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Common Crawl is a popular source</a>), and many user-facing apps that use an LLM lean on one of several large providers, such as OpenAI and Anthropic. In other words, the vast majority of the AI apps and tools popular right now are rooted in a handful of models trained on similar data.</p><p>Latimer also leans on a popular LLM provider (it uses OpenAI’s ChatGPT as its foundation model) but augments that model with additional data to better represent minority voices. The company has an exclusive partnership with <a href=\"https://amsterdamnews.com/\" rel=\"noopener noreferrer\" target=\"_blank\"><em>New York Amsterdam News</em></a>, a black-owned newspaper founded in 1909, and works with <a href=\"https://sites.ed.gov/whhbcu/one-hundred-and-five-historically-black-colleges-and-universities/\" target=\"_blank\">historically black colleges and universities</a> to obtain access to both license-free and licensed data.</p><p>“We’re going for any and all available sources and resources, which we believe are more representative and more accurate sources, based on our own judgement, and a set of criteria we use to determine legitimacy,” says Narayan. Latimer’s data has a particular focus on educational and academic sources, as “they’re more likely to be legitimate sources.” With these sources available, Latimer’s engineers can use weighting techniques to adjust the model’s weights to counteract any known biases. </p><p>Narayan says FutureSum isn’t ready to release benchmark results for Latimer yet. But, he adds, the organization hopes to have some available within weeks. <a href=\"https://www.latimer.ai/#waitlist\" rel=\"noopener noreferrer\" target=\"_blank\">The LLM is currently in beta testing</a> and announced its public wait list on 24 January. Those who sign up for the wait list will join students from Alabama’s <a href=\"https://en.wikipedia.org/wiki/Miles_College\" target=\"_blank\">Miles College</a> in testing the model.</p><h2>A New Kind of Generation </h2><p>At its heart, Latimer relies on a technique known as retrieval-augmented generation (RAG). This technique was first described <a href=\"https://arxiv.org/pdf/2005.11401v4.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">in a 2020 paper from researchers at Meta in collaboration with the University College London and New York University</a>. RAG makes it possible for LLMs to verify and update their knowledge by accessing and cross-referencing a second source of data.</p><p>RAG inspired a major shift in how the world’s best LLMs function, Narayan says. It can improve an LLM’s accuracy, help it find and cite a source for data it provides in its response, or unlock access to new data that wasn’t available when the model was trained. IBM <a href=\"https://research.ibm.com/blog/retrieval-augmented-generation-RAG\" target=\"_blank\">offers it as a feature</a> of its Watsonx.ai platform; Microsoft and OpenAI use <a href=\"https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview\" target=\"_blank\">something like it</a> to present Bing search results in Co-Pilot; OpenAI uses <a href=\"https://cookbook.openai.com/examples/vector_databases/pinecone/gen_qa\" target=\"_blank\">something like it</a> to allow for custom GPTs that reference files provided by users.</p><p class=\"pull-quote\">“We’re developing an API, but the most important thing is the key applications. We want to help pharmaceutical companies have better reach into this community for clinical trials, help recruiters attract a black audience, help banking, and finance, and insurance.” <strong>—Malur Narayan, FutureSumAI</strong></p><p>Latimer specifically uses RAG as a lens to focus its ability to detect bias and promote underrepresented voices. “We’re using that not just for recent information, but also to ensure the data itself is more comprehensive when it comes to the topic we’re addressing, “says Narayan. “When a prompt is sent by a user, it first goes into our RAG model to see if that topic is relevant.” That includes preprompting rules to ensure responses are “more accurate and relevant to black history, black culture, and black heritage, and that there’s minimal bias.”</p><p>The approach is sound in theory, but it’s important to check that it works in practice. Narayan says Latimer’s early testing was mostly conducted through manual human feedback including A/B comparisons between its performance and that of the most popular LLMs, such as ChatGPT and Bard. Manual testing is difficult to scale, however, so the company also relies on automated bias-detection tools and comparisons with fairness metrics. This, in part, is what Latimer’s public beta test should help establish, Narayan says, as more users will provide more responses to examine.</p><p>Once testing is complete, Latimer plans to provide an API that any company or organization can use to tap into the LLM. It’s an obvious move from both a technical and business perspective; many organizations offering a commercial LLM eventually offer an API to let developers access it for a fee. For Latimer, however, it’s ultimately about the fulfillment of Latimer’s purpose.</p><p>“We’re developing an API, but the most important thing is the key applications. We want to help pharmaceutical companies have better reach into this community for clinical trials, help recruiters attract a black audience, help banking, and finance, and insurance,” says Narayan. “That’s our end goal. How do we let businesses better communicate with a black audience, or a brown audience.”</p>",[{'name': 'Matthew S. Smith'}],"Wed, 31 Jan 2024 17:30:45 +0000"
1865,Why the AI Boom is a Windfall for Tiny Anguilla,https://spectrum.ieee.org/ai-domains,"<img src=\"https://spectrum.ieee.org/media-library/a-photo-illustration-of-a-man-a-map-and-some-palm-trees.jpg?id=51098577&amp;width=2000&amp;height=1500&amp;coordinates=37%2C0%2C18%2C0\" /><br /><br /><p>The rising popularity of artificial intelligence has impacted the entire world, including the tiny island of Anguilla. Located in the Caribbean, the country, home to about 15,000 people, has a unique and suddenly in-demand resource.</p><p>In the late 1980s, the Internet Assigned Numbers Authority (IANA) assigned countries and regions of geographic interest<a href=\"https://spectrum.ieee.org/the-most-remote-island-in-the-world-is-home-to-seals-seabirds-and-an-internet-toplevel-domain\" target=\"_self\"> their own two-letter domains</a>. Anguilla received .ai, a luck of the draw that is now paying dividends as the country registers website domains for AI companies. <em>IEEE Spectrum </em>spoke with<a href=\"https://en.wikipedia.org/wiki/Vince_Cate\" rel=\"noopener noreferrer\" target=\"_blank\"> Vince Cate</a>, who manages domain registrations for the Anguillan government, on how AI has had an impact on .ai.</p><h3>Vince Cate</h3><br /><p><a href=\"http://offshore.ai/vince/\" rel=\"noopener noreferrer\" target=\"_blank\">Vince Cate</a> is a software developer and the founder of <a href=\"http://datahaven.net/\" rel=\"noopener noreferrer\" target=\"_blank\">DataHaven.Net</a>, which handles sales of the .ai domain for the Anguillan government.</p><p><strong>How did you end up managing the .ai domain?</strong></p><p><strong>Vince Cate: </strong>I came to Anguilla in 1994. I started out doing an email business, because there wasn’t any email or Internet on this island. And I wanted to have a domain name<a href=\"http://offshore.ai/vince/\" rel=\"noopener noreferrer\" target=\"_blank\"> that was .ai</a>. So I reached out to <a href=\"https://spectrum.ieee.org/computer-networking\" target=\"_self\">Jon Postel</a>—he was the one that was in charge of all these top-level domains. He said, there’s nobody running .ai, do you want to run .ai? And I said, “Okay.” That was really how it went!</p><p>At some point, I said, this shouldn’t be in my name, right? So I changed the [IANA] <a href=\"https://www.iana.org/domains/root/db/ai.html\" rel=\"noopener noreferrer\" target=\"_blank\">admin contact</a> to be the government of Anguilla. Somebody else saw that and convinced the government to give it to them, so it went to this company in Taiwan. After a couple of years, they disappeared. They didn’t answer emails or phone calls or anything. And we got it back. A number of small countries got really messed up by losing their domain names, and I would say we kind of came close.</p><p><strong>How did .ai open up for use outside of Anguilla?</strong></p><p><strong>Cate: </strong>This other company came and convinced the government that they could make a lot of money on it. They had this idea, that in Chinese “ài” means love. They thought they<a href=\"https://en.wikipedia.org/wiki/Vanity_domain\" rel=\"noopener noreferrer\" target=\"_blank\"> could market it</a> to [Chinese websites]. At the time, I thought that artificial intelligence was a much better market.</p><p><strong>Has the surge in AI interest been reflected in the number of .ai domains being registered?</strong></p><p><strong>Cate: </strong>November 30 [2022] is when<a href=\"https://spectrum.ieee.org/chatbot-chatgpt-interview\" target=\"_self\"> ChatGPT came out</a>. In the five months after that, our sales went up by almost a factor of four. Then they sort of leveled off at this new, much higher level. It’s just wild—we’re already like a third of the government’s budget.</p><p><strong>Tuvalu is perhaps the first and most well-known example of a country<a href=\"https://www.washingtonpost.com/video-games/2019/12/23/tuvalu-is-tiny-island-nation-people-its-cashing-thanks-twitch/\" rel=\"noopener noreferrer\" target=\"_blank\"> opening up its top-level domain (</a>.tv). Is Anguilla approaching this opportunity differently than that situation?</strong></p><p><strong>Cate: </strong>Tuvalu gave [domain registrations] to a big foreign company, and locked themselves in for 50 years. And we’re doing it locally. So the government is getting almost all the money. And that’s not what was happening in Tuvalu, right? Most of the money was not going to the country.</p><p><em>[Editor’s note: Tuvalu has</em><a href=\"https://theworld.org/stories/2022-01-24/tuvalu-cashes-its-coveted-internet-domain-name-amid-rise-online-streaming\" rel=\"noopener noreferrer\" target=\"_blank\"> <em>recently renegotiated</em></a><em> and is leasing its domain name for more money, but an outside company still manages domain registrations.]</em></p><p><strong>How much money is being brought in by .ai registrations, and how is that affecting Anguilla?</strong></p><p><strong>Cate:</strong> It’s about US $3 million per month. We do the domains for two years, and so all of our money now is new domains. And if we just stay at this level of $3 million per month for new domains, when the renewals kick in a year from now, we’ll just jump to $6 million per month.</p><p>And it’s just part of the general budget—the government can use it however they want. But I’ve noticed that they’ve paid down some of their debt, which is pretty unusual. They’ve eliminated property taxes on residential buildings. So we’re doing well, I would say.</p><p><em>This article appears in the February 2024 print issue as “5 Questions for Vince Cate.”</em><br /></p>",[{'name': 'Michael Koziol'}],"Tue, 30 Jan 2024 17:00:02 +0000"
1866,"Why Rip Off Creatives, if Generative AI Can Play Fair?",https://spectrum.ieee.org/generative-ai-sony-centering-creators,"<img src=\"https://spectrum.ieee.org/media-library/photo-illustration-of-a-smiling-woman-in-front-of-a-colorful-background-of-connected-lines-and-dots.jpg?id=51177842&amp;width=2000&amp;height=1500&amp;coordinates=255%2C0%2C169%2C0\" /><br /><br /><p>
	In recent years, AI ethicists have had a tough job. The engineers developing generative AI tools have been racing ahead, competing with one another to create models of even more breathtaking abilities, leaving both regulators and ethicists to comment on what’s already been done.
</p><p>
	One of the people working to shift this paradigm is <a href=\"https://ai.sony/people/Alice-Xiang/\" rel=\"noopener noreferrer\" target=\"_blank\">Alice Xiang</a>, global head of AI ethics at <a href=\"https://www.sony.com/en/\" target=\"_blank\">Sony</a>. Xiang has worked to create an ethics-first process in AI development within Sony and in the larger AI community. She spoke to <em>IEEE </em><em>Spectrum</em> about starting with the data and whether Sony, with half its business in content creation, could play a role in building a new kind of generative AI.
</p><p class=\"rm-anchors\" id=\"top\">Alice Xiang on...</p><ol>
<li><a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#data\">Responsible data collection</a></li>
<li><a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#sony\">Her work at Sony</a></li>
<li><a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#regulations\">The impact of new AI regulations</a></li>
<li><a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#creator\">Creator-centric generative AI </a></li>
</ol><h2 class=\"rm-anchors\" id=\"data\">Responsible data collection</h2><p><strong>What’s the origin of your work on <a href=\"https://neurips.cc/virtual/2023/oral/73743\" target=\"_blank\">responsible data collection</a>? And in that work, why have you focused specifically on computer vision?<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#_msocom_1\" rel=\"noopener noreferrer\" target=\"_blank\"></a>
</strong></p><p>
<strong>Alice Xiang: </strong>In recent years, there has been a growing awareness of the importance of looking at AI development in terms of entire life cycle, and not just thinking about AI ethics issues at the endpoint. And that’s something we see in practice as well, when we’re doing AI ethics evaluations within our company: How many AI ethics issues are really hard to address if you’re just looking at things at the end. A lot of issues are rooted in the data-collection process—issues like consent, privacy, fairness, intellectual property. And a lot of AI researchers are not well equipped to think about these issues. It’s not something that was necessarily in their curricula when they were in school.
</p><p>
	In terms of <a href=\"https://spectrum.ieee.org/tag/generative-ai\" rel=\"noopener noreferrer\" target=\"_blank\">generative AI</a>, there is growing awareness of the importance of training data being not just something you can take off the shelf without thinking carefully about where the data came from. And we really wanted to explore what practitioners should be doing and what are best practices for data curation. Human-centric computer vision is an area that is arguably one of the most sensitive for this because you have biometric information.
</p><p><strong>The term “human-centric computer vision”: Does that mean <a href=\"https://spectrum.ieee.org/tag/computer-vision\" rel=\"noopener noreferrer\" target=\"_blank\">computer vision</a> systems that recognize human faces or human bodies?
</strong></p><p>
<strong>Xiang: </strong>Since we’re focusing on the data layer, the way we typically define it is any sort of [computer vision] data that involves humans. So this ends up including a much wider range of AI. If you wanted to create a model that recognizes objects, for example—objects exist in a world that has humans, so you might want to have humans in your data even if that’s not the main focus. This kind of technology is very ubiquitous in both high- and low-risk contexts.
</p><p class=\"pull-quote\">
	“A lot of AI researchers are not well equipped to think about these issues. It’s not something that was necessarily in their curricula when they were in school.” <strong>—Alice Xiang, Sony</strong></p><p><strong>What were some of your findings about best practices in terms of privacy and fairness?
</strong></p><p>
<strong>Xiang:</strong> The current baseline in the human-centric computer vision space is not great. This is definitely a field where researchers have been accustomed to using large Web-scraped datasets that do not have any consideration of these ethical dimensions. So when we talk about, for example, privacy, we’re focused on: Do people have any concept of their data being collected for this sort of use case? Are they informed of how the datasets are collected and used? And this work starts by asking: Are the researchers really thinking about the purpose of this data collection? This sounds very trivial, but it’s something that usually doesn’t happen. People often use datasets as available, rather than really trying to go out and source data in a thoughtful manner.
</p><p>
	This also connects with <a href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4068921\" rel=\"noopener noreferrer\" target=\"_blank\">issues of fairness</a>. How broad is this data collection? When we look at this field, most of the major datasets are extremely U.S.-centric, and a lot of biases we see are a result of that. For example, researchers have found that object-detection models tend to work far worse in lower-income countries versus higher-income countries, because most of the images are sourced from higher-income countries. Then on a human layer, that becomes even more problematic if the datasets are predominantly of Caucasian individuals and predominantly male individuals. A lot of these problems become very hard to fix once you’re already using these [datasets].
</p><p>
	So we start there, and then we go into much more detail as well: If you were to collect a data set from scratch, what are some of the best practices? [Including] these purpose statements, the types of consent and best practices around human-subject research, considerations for vulnerable individuals, and thinking very carefully about the attributes and metadata that are collected.
</p><p><strong>I recently read <a href=\"https://spectrum.ieee.org/joy-buolamwini\" rel=\"noopener noreferrer\" target=\"_blank\">Joy Buolamwini’</a>s book <em>Unmasking AI</em>, in which she documents her painstaking process to put together a dataset that felt ethical. It was really impressive. Did you try to build a dataset that felt ethical in all the dimensions?
</strong></p><p>
<em><strong></strong></em><strong>Xiang</strong>: Ethical data collection is an important area of focus for our research, and we have additional recent work on some of the challenges and opportunities for building more ethical datasets, such as the need for improved <a href=\"https://ai.sony/blog/blog-037/\" rel=\"noopener noreferrer\" target=\"_blank\">skin-tone annotations</a> and <a href=\"https://ai.sony/publications/Flickr-Africa-Examining-Geo-Diversity-in-Large-Scale-Human-Centric-Visual-Data/\" rel=\"noopener noreferrer\" target=\"_blank\">diversity in computer vision</a>. As our own ethical data collection continues, we will have more to say on this subject in the coming months.<em></em>
</p><p>
<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#top\">back to top</a>
</p><h2 class=\"rm-anchors\" id=\"sony\">Her work at Sony</h2><p><strong>How does this work manifest within Sony? Are you working with internal teams who have been using these kinds of datasets? Are you saying they should stop using them?
</strong></p><p>
<strong>Xiang: </strong>An important part of our ethics assessment process is asking folks about the datasets they use. The governance team that I lead spends a lot of time with the business units to talk through specific use cases. For particular datasets, we ask: What are the risks? How do we mitigate those risks? This is especially important for bespoke data collection. In the research and academic space, there’s a primary corpus of datasets that people tend to draw from, but in industry, people are often creating their own bespoke datasets.
</p><p class=\"pull-quote\">“I think with everything AI ethics related, it’s going to be impossible to be purists.” <strong>—Alice Xiang, Sony</strong></p><p><strong>I know you’ve spoken about AI ethics by design. Is that something that’s in place already inside Sony? Are AI ethics talked about from the beginning stages of a product or a use case?
</strong></p><p>
<strong>Xiang:</strong> Definitely. There are a bunch of different processes, but the one that’s probably the most concrete is our process for all our different electronics products. For that one, we have several checkpoints as part of the standard quality-management system. This starts in the design and planning stage, and then goes to the development stage, and then the actual release of the product. As a result, we are talking about AI ethics issues from the very beginning, even before any sort of code has been written, when it’s just about the idea for the product.
</p><p>
<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#top\">back to top</a>
</p><h2 class=\"rm-anchors\" id=\"regulations\">The impact of new AI regulations</h2><p><strong>There’s been a lot of action recently on <a href=\"https://spectrum.ieee.org/ai-regulation-worldwide\" rel=\"noopener noreferrer\" target=\"_blank\">AI regulations</a> and governance initiatives around the world. <a href=\"https://carnegieendowment.org/2023/07/10/china-s-ai-regulations-and-how-they-get-made-pub-90117\" rel=\"noopener noreferrer\" target=\"_blank\">China</a> already has AI regulations, the EU passed its <a href=\"https://artificialintelligenceact.eu/\" rel=\"noopener noreferrer\" target=\"_blank\">AI Act</a>, and here in the United States we had President <a href=\"https://spectrum.ieee.org/biden-ai-executive-order\" rel=\"noopener noreferrer\" target=\"_blank\">Biden’s executive order</a>. Have those changed either your practices or your thinking about product design cycles?
</strong></p><p>
<strong>Xiang:</strong> Overall, it’s been very helpful in terms of increasing the relevance and visibility of AI ethics across the company. Sony’s a unique company in that we are simultaneously a major technology company, but also a major content company. A lot of our business is entertainment, including films, music, video games, and so forth. We’ve always been working very heavily with folks on the technology-development side. Increasingly we’re spending time talking with folks on the content side, because now there’s a huge interest in AI in terms of the artists they represent, the content they’re disseminating, and how to protect rights.
</p><p class=\"pull-quote\">“When people say ‘go get consent,’ we don’t have that debate or negotiation of what is reasonable.” <strong>—Alice Xiang, Sony</strong></p><p>
	Generative AI has also dramatically impacted that landscape. We’ve seen, for example, one of our executives at <a href=\"https://www.sonymusic.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Sony Music</a> making <a href=\"https://musically.com/2023/11/30/sony-musics-kooker-slams-distorted-view-of-copyright-in-ai-debate/\" rel=\"noopener noreferrer\" target=\"_blank\">statements</a> about the importance of <a href=\"https://www.schumer.senate.gov/imo/media/doc/Dennis%20Kooker%20-%20Statement1.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">consent, compensation, and credit</a> for artists whose data is being used to train AI models. So [our work] has expanded beyond just thinking of AI ethics for specific products, but also the broader landscapes of rights, and how do we protect our artists? How do we move AI in a direction that is more creator-centric? That’s something that is quite unique about Sony, because most of the other companies that are very active in this AI space don’t have much of an incentive in terms of protecting data rights.
</p><p>
<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#top\">back to top</a>
</p><h2 class=\"rm-anchors\" id=\"creator\">Creator-centric generative AI </h2><p><strong>I’d love to see what more creator-centric AI would look like. Can you imagine it being something in which the people who make generative AI models get consent or compensate artists if they train on their material?
</strong></p><p>
<strong>Xiang: </strong>It’s a very challenging question. I think this is one area where our work on ethical data curation can hopefully be a starting point, because we see the same problems in generative AI that we see for more classical AI models. Except they’re even more important, because it’s not only a matter of whether my image is being used to train a model, now [the model] might be able to generate new images of people who look like me, or if I’m the copyright holder, it might be able to generate new images in my style. So a lot of these things that we’re trying to push on—consent, fairness, IP, and such—they become a lot more important when we’re thinking about [generative AI]. I hope that both our past research and future research projects will be able to really help.
</p><p><strong>Are you able to say whether Sony is developing generative AI models?
</strong></p><p class=\"pull-quote\">“I don’t think we can just say, ‘Well, it’s way too hard for us to solve today, so we’re just going to try to filter the output at the end.’ ” <strong>—Alice Xiang, Sony</strong></p><p>
<em><strong></strong></em><strong>Xiang</strong>: I can’t speak for all of Sony, but certainly we believe that AI technology, including generative AI, has the potential to augment human creativity. In the context of my work, we think a lot about the need to respect the rights of stakeholders, including creators, through the building of AI systems that creators can use with peace of mind.<em></em>
</p><p><strong>I’ve been thinking a lot lately about generative AI’s <a href=\"https://spectrum.ieee.org/midjourney-copyright\" target=\"_blank\">problems with copyright and IP</a>. Do you think it’s something that can be patched with the Gen AI systems we have now, or do you think we really need to start over with how we train these things? And this can be totally your opinion, not Sony’s opinion.
</strong></p><p>
<strong>Xiang:</strong> In my personal opinion, I think with everything AI ethics related, it’s going to be impossible to be purists. Even though we are pushing very strongly for these best practices, we also acknowledge in all our research papers just how insanely difficult this is. If you were to, for example, uphold the highest practices for obtaining consent, it’s difficult to imagine that you could have datasets of the magnitude that a lot of the models nowadays require. You’d have to maintain relationships with billions of people around the world in terms of informing them of how their data is being used and letting them revoke consent.
</p><p>
	Part of the problem right now is when people say “go get consent,” we don’t have that debate or negotiation of what is reasonable. The tendency becomes either to throw the baby out with the bathwater and ignore this issue, or go to the other extreme, and not have the technology at all. I think the reality will always have to be somewhere in between.
</p><p>
	So when it comes to these issues of reproduction of IP-infringing content, I think it’s great that there’s a lot of research now being done on this specific topic. There are a lot of patches and filters that people are proposing. That said, I think we also will need to think more carefully about the data layer as well. I don’t think we can just say, “Well, it’s way too hard for us to solve today, so we’re just going to try to filter the output at the end.”
</p><p>
	We’ll ultimately see what shakes out in terms of the courts, in terms of whether this is going to be okay from a <a href=\"https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html\" rel=\"noopener noreferrer\" target=\"_blank\">legal perspective</a>. But from an ethics perspective, I think we’re at a point where there needs to be deep conversations on what is reasonable in terms of the relationships between companies that benefit from AI technologies and the people whose works were used to create it. My hope is that Sony can play a role in those conversations.
</p><p>
<a href=\"https://spectrum.ieee.org/feeds/topic/artificial-intelligence.rss#top\">back to top</a>
</p>",[{'name': 'Eliza Strickland'}],"Sun, 28 Jan 2024 14:00:02 +0000"
1867,Google’s New AI Is Learning to Diagnose Patients,https://spectrum.ieee.org/ai-doctor,"<img src=\"https://spectrum.ieee.org/media-library/photo-illustration-of-a-network-and-a-stethoscope.jpg?id=51177447&amp;width=2000&amp;height=1500&amp;coordinates=374%2C0%2C374%2C0\" /><br /><br /><p>Navigating health care systems as a patient can be daunting at the best of times, whether you’re interpreting jargon-filled diagnoses or determining which specialists to see next. Similarly, doctors often have grueling schedules that make it difficult to offer personalized attention to all their patients. These issues are only exacerbated in areas with limited physicians and medical infrastructure.<strong><del></del></strong></p><p>Bringing AI into the doctor’s office to alleviate these problems is a dream that researchers have been working toward since IBM’s Watson made its debut over a decade ago, but <a href=\"https://spectrum.ieee.org/how-ibm-watson-overpromised-and-underdelivered-on-ai-health-care\" target=\"_self\">progress toward these goals</a> has been slow-moving. Now, large language models (LLMs),<a href=\"https://spectrum.ieee.org/chatgpt-medical-exam\" target=\"_self\"> including ChatGPT</a>, could have the potential to reinvigorate those ambitions.</p><p>The team behind <a href=\"https://spectrum.ieee.org/googles-deep-mind-boosts-memory-to-navigate-london-underground\" target=\"_blank\">Google DeepMind</a> have proposed a new AI model called AMIE (Articulate Medical Intelligence Explorer), in a recent <a href=\"https://arxiv.org/abs/2401.05654\" target=\"_blank\">preprint paper</a> published 11 January on arXiv. The model could take in information from patients and provide clear explanations of medical conditions in a wellness visit consultation.</p><p><a href=\"https://www.linkedin.com/in/vivek-natarajan-a3670118/\" rel=\"noopener noreferrer\" target=\"_blank\">Vivek Natarajan</a> is an AI researcher at Google and lead author on the recent paper. He says that while AMIE isn’t designed to replace human physicians, he does believe a similar AI could play a role in assisting both physicians and patients.</p><p>“There may be scenarios when people might benefit from interacting with systems like AMIE as part or in addition to their clinical journeys,” Natarajan says. “These include understanding symptoms and conditions better, including simplifying explanations in local vernaculars…and acting as a valuable second opinion.”</p><p class=\"pull-quote\">“This, in turn, potentially provides a pathway for medical AI towards superhuman diagnostic performance.” <strong>—Vivek Natarajan, Google</strong></p><p><a href=\"https://geiselmed.dartmouth.edu/faculty/facultydb/view.php/?uid=8274\" rel=\"noopener noreferrer\" target=\"_blank\">Thomas Thesen</a> is an associate professor of medical education at Dartmouth’s Geisel School of Medicine who created the <a href=\"https://home.dartmouth.edu/news/2024/01/geisel-professor-harnesses-ai-act-patient\" target=\"_blank\">AI Patient Actor</a> app to help train medical students on diverse patient scenarios. While he thinks AI will play an increasingly larger role in health care, he doesn’t believe it will replace the expertise of human physicians. </p><p>“What I see coming in the next decade is AI increasingly supporting doctors by streamlining their work and contributing to certain limited diagnostic processes,” Thesen says. “However, the expert judgment of a trained doctor will remain crucial for final diagnosis and treatment plans.”</p><p>To bring AMIE up to speed without sending it through medical school, Natarajan and colleagues started by feeding the AI on real-world medical texts, including the transcripts of nearly 100,000 real physician-patient dialogues, 65 clinician-written summaries of intensive care unit medical notes, and thousands of medical reasoning questions taken from the <a href=\"https://en.wikipedia.org/wiki/United_States_Medical_Licensing_Examination\" target=\"_blank\">United States Medical Licensing Examination</a>.</p><p>Yet, these data alone were not enough to set AMIE up for success, Natarajan says, particularly because the data tend to be noisy and capture only a small subset of potential medical scenarios. To fill in these gaps, the team also used a simulated diagnostic environment that allowed AMIE to learn from its own mistakes through two different “self-play” loops.</p><p>“The environment included two self-play loops—an ‘inner’ self-play loop, where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient simulator, and an ‘outer’ self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations,” Natarajan says. “The resulting new version of AMIE could then participate in the inner loop again, creating a virtuous continuous learning cycle.”</p><p>While Natarajan stresses that there is no substitute for real human experience in medicine, this training model does give AMIE a leg up in some ways over human physicians. For example, a human physician may see only 10,000 patients in their career, but AMIE could “see” that many patients in just a couple of training cycles.</p><p>“This, in turn, potentially provides a pathway for medical AI towards superhuman diagnostic performance,” Natarajan says.</p><p>To see how well AMIE stacked up against human physicians, Natarajan and colleagues pitted it against 20 human primary-care providers in a blind and randomly controlled trial to consult with patient actors located in Canada, India, and the United Kingdom. There were 149 different consults conducted via live texting and evaluated by both the patient actors and human specialists. </p><p class=\"pull-quote\">“The expert judgment of a trained doctor will remain crucial for final diagnosis and treatment plans.” <strong>—Thomas Thesen, Dartmouth</strong></p><p>The consults were measured using several factors, including perceived empathy, openness and honesty, diagnostic accuracy, and management planning. Both the patient actors and specialists determined that AMIE provided “greater diagnostic accuracy and superior performance” compared to their human counterparts. However, these results are not necessarily as black and white as they sound.</p><p>For one thing, these consults were completed using the type of live, text-based chats that are typically used to communicate with LLMs. However, this format is very different from the type of face-to-face interaction that human physicians are used to, potentially offering AMIE an advantage. The team found that AMIE also tended to write significantly longer responses than human physicians, which they believe could be interpreted as more time-intensive—and thus thoughtful and empathetic—by patients.</p><p>Going forward, Natarajan says he and colleagues are interested in expanding AMIE’s capabilities to include multimodal sources, such as video chats. The team will also look at problems of equity, fairness, and adversarial testing to better prepare AMIE for the real world.</p><p>As for the human physicians anticipating AMIE’s arrival, Thesen says it’s important that they proactively prepare for how this technology could change medicine.</p><p>“Medical schools have a responsibility to incorporate AI literacy into their curriculum,” Thesen says. “This includes understanding the ethical implications to ensure that as AI becomes more integrated into clinical practice, future doctors can use it responsibly and protect their patients’ well-being.”</p>",[{'name': 'Sarah Wells'}],"Thu, 25 Jan 2024 20:49:41 +0000"
1868,AI-Powered Proof Generator Helps Debug Software,https://spectrum.ieee.org/ai-debug-software,"<img src=\"https://spectrum.ieee.org/media-library/blue-pixelated-1-s-and-0-s-with-error-text-in-middle.jpg?id=51167174&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C0%2C0\" /><br /><br /><p>Not all <a href=\"https://spectrum.ieee.org/tag/Software\" target=\"_self\">software</a> is perfect—many apps, programs, and websites are released despite bugs. But the software behind critical systems like <a href=\"https://spectrum.ieee.org/tag/encryption\" target=\"_self\">cryptographic protocols</a>, <a href=\"https://spectrum.ieee.org/topic/biomedical/\" target=\"_self\">medical devices</a>, and <a href=\"https://spectrum.ieee.org/topic/aerospace/\" target=\"_self\">space shuttles</a> must be error-free, and ensuring the <a href=\"https://spectrum.ieee.org/tla\" target=\"_self\">absence of bugs</a> requires going beyond code reviews and testing. It requires formal verification.</p><p><a href=\"https://www.sciencedirect.com/topics/computer-science/formal-verification\" rel=\"noopener noreferrer\" target=\"_blank\">Formal verification</a> involves writing a mathematical proof of your code and is “one of the hardest but also most powerful ways of making sure your code is correct,” says <a href=\"https://people.cs.umass.edu/~brun/\" rel=\"noopener noreferrer\" target=\"_blank\">Yuriy Brun</a>, a professor<strong> </strong>at the <a href=\"https://www.umass.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">University of Massachusetts Amherst</a>.</p><p>To make formal verification easier, Brun and his colleagues devised a new AI-powered method called Baldur to automatically generate proofs. The accompanying <a href=\"https://dl.acm.org/doi/10.1145/3611643.3616243\" rel=\"noopener noreferrer\" target=\"_blank\">paper</a>, presented in December 2023 at the <a href=\"https://2023.esec-fse.org/\" rel=\"noopener noreferrer\" target=\"_blank\">ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</a> in San Francisco, won a <a href=\"https://2023.esec-fse.org/info/awards\" rel=\"noopener noreferrer\" target=\"_blank\">Distinguished Paper award</a>. The team includes <a href=\"https://people.cs.umass.edu/~efirst/\" rel=\"noopener noreferrer\" target=\"_blank\">Emily First</a>, who completed the study as part of her doctoral dissertation at UMass Amherst; Markus Rabe, a former researcher at <a href=\"https://spectrum.ieee.org/tag/Google\" target=\"_self\">Google</a>, where the study was conducted; and <a href=\"https://dependenttyp.es/\" rel=\"noopener noreferrer\" target=\"_blank\">Talia Ringer</a>, an assistant professor at the <a href=\"https://illinois.edu/\" rel=\"noopener noreferrer\" target=\"_blank\">University of Illinois Urbana-Champaign</a>.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left\" style=\"float: left;\">
<img alt=\"text with arrows and boxes\" class=\"rm-shortcode rm-resized-image\" id=\"3bb58\" src=\"https://spectrum.ieee.org/media-library/text-with-arrows-and-boxes.jpg?id=51167083&amp;width=980\" />
<small class=\"image-media media-caption\">   This flowchart shows at a high level how Baldur generates a proof. </small><small class=\"image-media media-photo-credit\">    University of Massachusetts/    University of Illinois Urbana-Champaign </small></p><p>Baldur is powered by Google’s <a href=\"https://blog.research.google/2022/06/minerva-solving-quantitative-reasoning.html\" rel=\"noopener noreferrer\" target=\"_blank\">Minerva</a> large language model (<a href=\"https://spectrum.ieee.org/tag/llms\" target=\"_self\">LLM</a>)—which was trained on scientific papers and Web pages containing mathematical expressions—and fine-tuned on data about proofs and theorems. Baldur works with <a href=\"https://isabelle.in.tum.de/\" rel=\"noopener noreferrer\" target=\"_blank\">Isabelle</a>, a proof assistant or automated theorem prover,<strong> </strong>to check its proofs. When given a theorem statement, Baldur is able to generate an entire proof almost 41 percent of the time.</p><p>To boost Baldur’s success, the team fed the model additional contextual information—such as other definitions or the lines preceding the theorem statement in a theory file—and found that the proof rate increased to 47.5 percent. This means that Baldur is able to take the context and use it to predict a new correct proof, says First. Similar to a programmer who may be better equipped to fix a bug in a method when they know how that method relates to its surrounding code and the other methods in the same class, Baldur can perform better with extra context.</p><p><a href=\"https://arxiv.org/abs/2205.10893\" rel=\"noopener noreferrer\" target=\"_blank\">Thor</a>, the current state-of-the-art tool for automatic proof generation, has a higher proof rate at 57 percent. Baldur’s advantage lies in its ability to generate whole proofs; Thor predicts the next step in a proof using a smaller language model combined with a method that searches the space of possible proofs. But when Thor and Baldur (<a href=\"https://en.wikipedia.org/wiki/Baldr\" target=\"_blank\">Thor’s brother</a> in Norse mythology) work together, the pair can generate correct proofs nearly 66 percent of the time.</p><p>The team also discovered that Baldur can “repair” its own proofs, further improving its proof rate. When provided with its previous failed attempt plus the error message returned by Isabelle, Baldur can turn its wrong proof into a right one.</p><p>“The fact that the error message helped so much was surprising,” Brun says. “[It] suggests that there’s more useful information that could potentially be fed into the large language model to give better answers. We’re just scratching the surface.”</p><p>The team has yet to find the right amount of information that would be deemed valuable for the model. “One limitation is that we’re giving it some information around the proof that it’s trying to generate, but we don’t know what’s the limit and where it stops being useful,” says Brun. And there’s still a considerable degree of error, which he hopes can be improved by fine-tuning the model using more datasets that better explain what a proof looks like and what a theorem and proof pair look like.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left\" style=\"float: left;\">
<img alt=\"text with different color highlights, arrows and boxes\" class=\"rm-shortcode rm-resized-image\" id=\"86f54\" src=\"https://spectrum.ieee.org/media-library/text-with-different-color-highlights-arrows-and-boxes.jpg?id=51167151&amp;width=980\" />
<small class=\"image-media media-caption\">This flowchart shows how the proof repair model creates new training data for Baldur.</small><small class=\"image-media media-photo-credit\">University of Massachusetts/ University of Illinois Urbana-Champaign</small></p><p>In terms of next steps, First is looking at implementing the approach used for Baldur in other proof assistants, as well as uncovering ways to more smartly gather contextual information that can help enhance the model’s accuracy. The team envisions Baldur helping simplify the jobs of <a href=\"https://proofengineering.org/\" rel=\"noopener noreferrer\" target=\"_blank\">proof engineers</a>, who are tasked with, for example, <a href=\"https://www.darpa.mil/program/proof-engineering-adaptation-repair-and-learning-for-software\" rel=\"noopener noreferrer\" target=\"_blank\">formal verification of national security systems</a> at organizations like the U.S. Department of Defense and its Defense Advanced Research Projects Agency (DARPA).</p><p>On a broader scale, the team is planning on getting feedback from software developers to see how tools like Baldur can help them—be it through debugging an error in their code, refining specifications, or creating a higher-quality system.</p><p>“There’s a lot of power in building interactive tools where a developer is trying to prove some property of their code,” Brun says. “Understanding how developers can interact with these tools and supporting them by building automated approaches can take them even farther.”</p>",[{'name': 'Rina Diane Caballar'}],"Thu, 25 Jan 2024 16:30:03 +0000"
1869,Weighing the Prophecies of AI Doom,https://spectrum.ieee.org/ai-existential-risk-survey,"<img src=\"https://spectrum.ieee.org/media-library/an-illustration-of-a-person-running-away-from-a-tornado-shape-full-of-code.jpg?id=51164800&amp;width=2000&amp;height=1500&amp;coordinates=66%2C0%2C67%2C0\" /><br /><br /><p>As the steady hum of artificial intelligence activity carries on from 2023 into the new year, the research group <a href=\"https://aiimpacts.org/#gsc.tab=0\" target=\"_blank\">AI Impacts</a> has released the results of their <a href=\"https://blog.aiimpacts.org/p/2023-ai-survey-of-2778-six-things\" target=\"_blank\">most recent outlook survey</a> among AI researchers and engineers. Their <a href=\"https://arxiv.org/abs/2401.02843\" target=\"_blank\">accompanying preprint</a> (not peer reviewed as of press time) details the results. The group’s analysis of their 2023 Expert Survey on Progress in AI summarizes responses of 2,788 AI researchers to a series of questions regarding the present and far future of AI research.</p><p class=\"pull-quote\">“When will particular things happen in the future? Is there a risk of bad things happening? In practice, we’re interested in human extinction.”<br /><strong>—Katja Grace, AI Impacts</strong></p><p>The 2023 survey follows AI Impact’s <a href=\"https://wiki.aiimpacts.org/doku.php?id=ai_timelines:predictions_of_human-level_ai_timelines:ai_timeline_surveys:2022_expert_survey_on_progress_in_ai\" target=\"_blank\">2022 survey</a>, whose median participant reasoned the likelihood of AI-caused “human extinction or similarly permanent and severe disempowerment of the human species” at 5 percent. The likelihood of that same extinction or subjugation resulting from “human inability to control future advanced AI systems” was at a median 10 percent. The median responses to those same questions in the 2023 survey were unchanged, while the median 2023 prediction for AI-driven extinction by the year 2100 was 5 percent.<br /></p><p>AI Impacts has been conducting this survey since 2016. <a href=\"https://www.fhi.ox.ac.uk/team/katja-grace/\" target=\"_blank\">Katja Grace</a>, one of the organization’s cofounders and current lead researcher, started the group to map out potential consequences of advancing AI technology. “We’re trying to answer decision-relevant questions about the future of AI,” says Grace. “When will particular things happen in the future? Is there a risk of bad things happening? In practice, we’re interested in human extinction.” </p><p class=\"shortcode-media shortcode-media-rebelmouse-image\">
<img alt=\"A chart showing a curving red line above a curving blue line representing an increase of probability over time.\" class=\"rm-shortcode\" id=\"e6326\" src=\"https://spectrum.ieee.org/media-library/a-chart-showing-a-curving-red-line-above-a-curving-blue-line-representing-an-increase-of-probability-over-time.jpg?id=51164862&amp;width=980\" />
<small class=\"image-media media-caption\">Aggregate and representative responses are graphed here to AI researchers’ projections of the “full automation of labor” (FAOL).  </small><small class=\"image-media media-photo-credit\">AI Impacts</small></p><p>The group runs their now-annual surveys to sample the opinion of the AI research community regarding those consequences. “We are most interested in timelines. We ask about narrow things that will likely happen sooner, which I trust more as straightforward information about what’s plausible,” says Grace. “For longer goals, the numbers are so all over the place that I don’t expect to learn what date they’ll happen, but more expect to learn what the feeling is in the AI community.” Grace goes on to state that mood has some dark corners. “From the survey it’s somewhat hard to judge what the overall mood is, but it seems like there is a fairly widespread belief that there is non-negligible risk of AI destroying humanity in the long run. I have less of a sense of with what mood that belief is held.”</p><p class=\"pull-quote\">“They marketed it, framed it, as ‘the leading AI researchers believe…something,’ when in fact the demographic includes a variety of students.”<br /><strong>—Nirit Weiss-Blatt, author, The Techlash</strong></p><p>Since its founding, AI Impacts has attracted substantial attention for the more alarming results produced from its surveys. The group—currently listing seven contributors on its website—has also received <a href=\"https://openbook.fyi/org/AI%20Impacts\" target=\"_blank\">at least US $2 million in funding</a> as of December 2022. This funding came from a number of individuals and philanthropic associations connected to the <a href=\"https://en.wikipedia.org/wiki/Effective_altruism\" target=\"_blank\">effective altruism movement</a> and concerned with the potential existential risk of artificial intelligence. This includes <a href=\"https://www.openphilanthropy.org/\" target=\"_blank\">Open Philanthropy</a>, technologist <a href=\"https://www.cser.ac.uk/team/jaan-tallinn/\" target=\"_blank\">Jaan Tallinn</a>, and the now-defunct <a href=\"https://www.reuters.com/technology/collapse-ftx-deprives-academics-grants-stokes-fears-forced-repayment-2023-04-06/\" target=\"_blank\">FTX Future Fund</a>. AI Impacts operates within the <a href=\"https://intelligence.org/\" target=\"_blank\">Machine Intelligence Research Institute</a>, an AI existential risk research group codirected by <a href=\"https://en.wikipedia.org/wiki/Eliezer_Yudkowsky\" target=\"_blank\">Eliezer Yudkowsky</a>. Yudkowsky has previously called for bringing a <a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\" target=\"_blank\">decisive and potentially violent end</a> to advanced AI research.</p><p>The 2023 survey also asked participants to estimate the likelihoods and development timelines of a number of AI research milestones. Participants predicted that some AI tasks—generating functional software in <a href=\"https://spectrum.ieee.org/python-programming\" target=\"_blank\">Python</a>, creating a “random video game,” writing a top-40 pop song—were likely achievable by the end of the decade. Arguably, <a href=\"https://spectrum.ieee.org/large-language-models-size\" target=\"_blank\">LLM</a> code generation has already achieved the first of those. Other goals, like <a href=\"https://www.youtube.com/watch?v=a_YGPbWJO5g\" target=\"_blank\">winning a 5K road race as a bipedal robot</a> or <a href=\"https://spectrum.ieee.org/this-year-autonomous-trucks-will-take-to-the-road-with-no-one-on-board\" target=\"_blank\">fully automating the work of long-haul truck drivers</a> were given longer median timeframes by the researchers.</p><p class=\"pull-quote\">“What they are doing is running a well-funded panic campaign. So, that’s not good journalism.”<br /><strong>—Nirit Weiss-Blatt</strong></p><p>The 2022 survey’s participant-selection methods were criticized for being skewed and narrow. AI Impacts sent the survey to 4,271 people—738 responded—whose research was published at the <a href=\"https://nips.cc/Conferences/2021\" target=\"_blank\">2021 machine-learning conferences Neural Information Processing Society</a> (NeurIPS) and the <a href=\"https://icml.cc/\" target=\"_blank\">International Conference on Machine Learning</a> (ICML). Communication researcher and prominent IT <a href=\"https://www.niritweissblatt.com/\" target=\"_blank\">author</a> Nirit Weiss-Blatt, who was <a href=\"https://www.aipanic.news/p/the-ai-panic-campaign-part-1\" target=\"_blank\">publicly critical</a> of the survey, points out that this approach created a skewed survey population. “People that send papers to those conferences, they can be undergrads and grad students,” says Weiss-Blatt. “They marketed it, framed it, as ‘the leading AI researchers believe…something,’ when in fact the demographic includes a variety of students.” AI Impacts changed its selection criteria in the 2023 survey, sending surveys to the participants of a larger number of machine-learning conferences and only to participants with a Ph.D.</p><p>Beyond methodological issues, Weiss-Blatt points out how the survey plays into the current media coverage and public opinion of AI. “The coverage of AI becomes crazy because of <a href=\"https://www.newscientist.com/article/2410839-theres-a-5-chance-of-ai-causing-humans-to-go-extinct-say-scientists/\" target=\"_blank\">headlines</a> like ‘There’s a 5 percent chance of AI causing humans to go extinct, say scientists,’ ” says Weiss-Blatt. “When a regular person sees that, then they think they should panic as well. What they are doing is running a well-funded panic campaign. So, that’s not good journalism. A better representation of this survey would indicate that it was funded, phrased, and analyzed by ‘x-risk’ effective altruists. Behind ‘AI Impacts’ and other ‘AI Safety’ organizations, there’s a well-oiled ‘x-risk’ machine. When the media is covering them, it has to mention it.”</p><p>AI Impacts plans to continue adapting their surveys in response to future criticisms. For the coming 2024 survey, the group has reached out to external sources for advice. “A different thing we’ve done this time was try to run the methodology by more people,” says Grace. “We sent it to Nate Silver because he’s more of an expert in this field than us, probably. He said it was great.” Silver confirmed his involvement when contacted by <em>IEEE Spectrum</em>. </p>",[{'name': 'Michael Nolan'}],"Thu, 25 Jan 2024 13:00:03 +0000"
1870,A How-To Guide on Acquiring AI Systems,https://spectrum.ieee.org/guide-on-acquiring-ai-systems,"<img src=\"https://spectrum.ieee.org/media-library/a-group-of-people-in-front-of-screens-related-to-ai-and-purchasing.jpg?id=51172897&amp;width=2000&amp;height=1500&amp;coordinates=334%2C0%2C0%2C0\" /><br /><br /><p><a href=\"https://www.idc.com/getdoc.jsp?containerId=prUS49670322\" rel=\"noopener noreferrer\" target=\"_blank\">International Data Corp.</a> estimated that US $118 billion was spent globally in 2022 to purchase artificial intelligence hardware, software, and data services. IDC has predicted the figure will nearly triple, to $300 billion, by 2026. But <a href=\"https://www.oecd.org/gov/public-procurement/\" rel=\"noopener noreferrer\" target=\"_blank\">public procurement</a> systems are not ready for the challenges of procuring AI systems, which bring with them new risks to citizens.</p><p>To help address this challenge <a href=\"https://standards.ieee.org/\" rel=\"noopener noreferrer\" target=\"_blank\">IEEE Standards Association</a> has introduced a pioneering standard for AI procurement. The standard, which is in development, can help government agencies be more responsible about how they acquire AI that serves the public interest.</p><p>Governments today are using AI and <a href=\"https://automatingsociety.algorithmwatch.org/wp-content/uploads/2020/12/Automating-Society-Report-2020.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">automated decision-making</a> systems to <a href=\"https://epic.org/wp-content/uploads/2023/09/FINAL-EPIC-Outsourced-Automated-Report-w-Appendix-Updated-9.26.23.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">aid or replace human-made decisions</a>. The ADM systems’ judgments can impact citizens’ access to education, employment, health care, social services, and more.</p><p>The multilayered <a href=\"https://academic.oup.com/edited-volume/41989/chapter-abstract/355438981?redirectedFrom=fulltext&amp;login=false\" rel=\"noopener noreferrer\" target=\"_blank\">complexity</a> of AI systems, and the datasets they’re built on, challenge people responsible for procurement—who rarely <a href=\"https://doi.org/10.1007/s00146-022-01572-2\" rel=\"noopener noreferrer\" target=\"_blank\">understand</a> the systems they’re purchasing and deploying. The vast majority of government procurement models worldwide have yet to adapt their acquisition processes and laws to the systems’ complexity.</p><p>To assist government agencies in being better stewards of public-use technology, in 2021 the IEEE Standards Association approved the development of a new type of socio-technical standard, the IEEE <a href=\"https://standards.ieee.org/ieee/3119/10729/\" rel=\"noopener noreferrer\" target=\"_blank\">P3119 Standard for the Procurement of AI and Automated Decision Systems</a>. The standard was inspired by the findings of the <a href=\"https://archive.nyu.edu/handle/2451/62255\" rel=\"noopener noreferrer\" target=\"_blank\">AI and Procurement: A Primer</a> report from the New York University <a href=\"https://engineering.nyu.edu/research-innovation/centers/center-responsible-ai\" rel=\"noopener noreferrer\" target=\"_blank\">Center for Responsible AI</a>.</p><p>The new, voluntary standard is designed to help strengthen AI procurement approaches with due-diligence processes to ensure that agencies are critically evaluating the kinds of AI services and tools they acquire. The standard can provide agencies with a method to require transparency from AI vendors about associated risks.</p><p>IEEE P3119 also can help governments use their procuring power to <a href=\"https://uploads.dayoneproject.org/2022/02/14125252/Market-Shaping-Primer.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">shape the market</a>—which could increase demand for more responsible AI solutions.</p><h2>A how-to guide</h2><p>The standard aims to help government agencies strengthen their requirements for AI procurement. Added to existing regulations, it offers complementary how-to guidance that can be applied to a variety of processes including pre-solicitation and contract monitoring.</p><p>Existing AI procurement guidelines such as the ones from the <a href=\"https://www.gao.gov/products/gao-21-519sp\" rel=\"noopener noreferrer\" target=\"_blank\">U.S. Government Accountability Office</a>, the <a href=\"https://www.weforum.org/publications/ai-procurement-in-a-box/ai-government-procurement-guidelines/#report-nav\" rel=\"noopener noreferrer\" target=\"_blank\">World Economic Forum</a>, and the <a href=\"https://www.fordfoundation.org/wp-content/uploads/2023/03/final_ford-foundation-guiding-framework-r3-full-document-final2.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">Ford Foundation</a> cover AI literacy, best practices, and red flags for vetting technology vendors. The IEEE P3119 standard goes further by providing guidance, for example, on determining whether a problem requires an AI solution. It also can help identify an agency’s risk tolerance, assess a vendor’s answers to questions about AI, recommend <a href=\"https://www.inclusivechange.org/ai-governance-solutions/ai-contract-clauses\" rel=\"noopener noreferrer\" target=\"_blank\">curated AI-specific contract language</a>, and evaluate an AI solution across multiple criteria.</p><p>IEEE is currently developing such an AI procurement guidance, one that moves beyond principles and best practices to detailed process recommendations. IEEE P3119 explicitly addresses the technical complexity of most AI models and the potential risks to society while also considering the systems’ capacity to scale for deployment in much larger populations.</p><p>Discussions in the standards working group centered around ways to identify and evaluate AI risks, how to mitigate risks within procurement needs, and how to provoke transparency about AI governance from vendors, with AI-specific best practices for solicitations and contracts.</p><p>The IEEE P3119 processes are meant to complement and optimize existing procurement requirements. The primary goal for the standard is to offer government agencies and AI vendors ways to adapt their procurement practices and solicited proposals to maximize the benefits of AI while minimizing the risks.</p><p>The standard is meant to become part of the “request for proposals” stage, integrated with solicitations in order to raise the bar for AI procurement so that the public interest and citizens’ civil rights are proactively protected.</p><p>Putting the standard into practice, however, could be challenging for some governments that are dealing with historical regulatory regimes and limited institutional capacity. </p><p>A future article will describe the need to test the standard against existing regulations, known as <a href=\"https://pubmed.ncbi.nlm.nih.gov/32025244/\" rel=\"noopener noreferrer\" target=\"_blank\">regulatory sandboxes</a>. </p>",[{'name': 'Cari Miller'}],"Tue, 23 Jan 2024 22:00:04 +0000"
1871,Deep Learning Picks Apart DNA Data-Copying Puzzles,https://spectrum.ieee.org/ai-genetics-rna-transcription,"<img src=\"https://spectrum.ieee.org/media-library/colorful-structure-of-green-pieces-with-a-group-of-pink-blobs-in-the-center.jpg?id=51058085&amp;width=2000&amp;height=1500&amp;coordinates=105%2C0%2C105%2C0\" /><br /><br /><p>DNA, as a data-storage medium, is useful only when read, copied, and sent out elsewhere. The medium for conveying genetic information out of a cell’s nuclei is <a href=\"https://www.genome.gov/genetics-glossary/RNA-Ribonucleic-Acid\" target=\"_blank\">RNA</a>—transcribed from DNA, which itself never leaves the cell’s nuclei. Now, using deep learning, researchers at Northwestern University, in Evanston, Ill., have untangled a complex  part of the RNA transcription process: how cells  know when to stop copying. </p><p>In RNA transcription, knowing when to stop is crucial. The information coded into RNA is used throughout a cell to synthesize proteins and regulate a wide range of metabolic processes. Getting the right message to its intended target requires those RNA strands to say just as much as they need to—and nothing more. If they say more or less than they need, as can be the case in a number of diseases like <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9549512/\" target=\"_blank\">epilepsy</a> or <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC137534/#:~:text=The%20primary%20cause%20of%20Duchenne,changes%20in%20skeletal%20muscle%20pathology.\" target=\"_blank\">muscular dystrophy</a>, then any number of those metabolic processes can break down or malfunction to debilitating effect.     </p><p class=\"pull-quote\">“This is a very useful prescreening tool for investigating genetic variants in a high-throughput manner.”<br /><strong>—Emily Kunce Stroup, Northwestern University</strong> </p><p>Halting the RNA copying process—called <a href=\"https://en.wikipedia.org/wiki/Polyadenylation\" target=\"_blank\">polyadenylation</a> (polyA) for the string of <a href=\"https://en.wikipedia.org/wiki/Adenine\" target=\"_blank\">adenine</a> molecules it ties onto the end of a cut-off RNA strand—involves a range of proteins whose interactions have never been fully understood. </p><p>So to help unravel polyA, researchers <a href=\"https://www.feinberg.northwestern.edu/faculty-profiles/az/profile.html?xid=39642\" target=\"_blank\">Zhe Ji</a> and <a href=\"https://www.researchgate.net/scientific-contributions/Emily-Kunce-Stroup-2155960728\" target=\"_blank\">Emily Kunce Stroup </a>at Northwestern University   developed a machine-learning model that can locate and identify  polyA sites. It works by pairing  <a href=\"https://spectrum.ieee.org/tag/convolutional-neural-networks\" target=\"_self\">convolutional neural networks</a> (CNNs) trained to  match important  sequences in the genetic code with   <a href=\"https://spectrum.ieee.org/the-neural-network-that-remembers\" target=\"_self\">r</a><a href=\"https://spectrum.ieee.org/the-neural-network-that-remembers\" target=\"_self\">ecurrent neural networks</a> (RNNs) trained to study the CNN outputs. <br /></p><p>While previous models had taken a similar approach, using both CNNs and RNNs, these researchers then fed the CNN/RNN model’s outputs into two other  deep-learning models trained to locate and identify polyA sites in the genome. </p><p>The two additional models seem to have helped. “Having those tandem outputs is the really unique thing from our work,” says Stroup. “Having the model go outwards to two separate output branches that we then combine to identify sites at high resolution is what distinguishes us from existing work.”<br /></p><p>From their model, the researchers learned a few important aspects of what   can cause polyA to go well or poorly. The CNN part of the model learned genetic  patterns in DNA known to attract the proteins controlling polyA, while the RNN part of the model  revealed that reliably cutting off transcription requires careful spacing between those patterns. And these researchers could make such precise conclusions because of the model’s per-nucleotide resolution. <a target=\"_blank\"></a>“It’s striking that our model can precisely capture this,” says Ji.</p><p>Moving forward, the team says they plan to apply their model and similar techniques to research identifying key genetic mutations that potentially cause diseases and then to develop from that a possible pipeline of more targeted therapeutic drugs. <a target=\"_blank\"></a>“This is a very useful prescreening tool for investigating genetic variants in a high-throughput manner,” says Stroup. “This will hopefully help whittle down the number of candidate mutations to make the process more efficient.” </p><p>Stroup says the team also plans to re-create their research in other organisms to see how RNA transcription changes between different animals. They hope, she says, to use that knowledge to help control or prevent polyA when its processes are out of control—as in the cases of epilepsy and muscular dystrophy—and causing real harm.<br /></p><p>The researchers published their <a href=\"https://www.nature.com/articles/s41467-023-43266-3\" target=\"_blank\"><u>paper</u></a> in the journal <em>Nature.</em></p>",[{'name': 'Michael Nolan'}],"Mon, 22 Jan 2024 22:05:03 +0000"
1872,"Crop Health Sensor Runs on Solar, Microbe Power",https://spectrum.ieee.org/smart-agriculture-crop-moisture-sensor,"<img src=\"https://spectrum.ieee.org/media-library/apple-trees-in-a-line-with-the-sun-shining.jpg?id=51101330&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C0%2C0\" /><br /><br /><p><em>This article is part of our exclusive </em><a href=\"https://spectrum.ieee.org/collections/journal-watch/\" target=\"_self\"><em>IEEE Journal Watch series</em></a><em> in partnership with IEEE Xplore.</em></p><p>As climate change causes many regions of the world to dry up, smart agriculture is one means to adapt to the crisis, and make every last drop of water count. To support this effort, a group of researchers in Italy have created a wearable, low-cost sensor for plants that monitors their water levels, and which is powered via solar energy and electrical energy from microbes in the soil. The sensor is described in a <a href=\"https://ieeexplore.ieee.org/document/10168678\" rel=\"noopener noreferrer\" target=\"_blank\">study</a> published in the December issue of the journal <a href=\"https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9787768\" rel=\"noopener noreferrer\" target=\"_blank\"><em>IEEE Transactions on AgriFood Electronics</em></a>. </p><p>Umberto Garlando, an assistant professor at the <a href=\"https://en.wikipedia.org/wiki/Polytechnic_University_of_Turin\" target=\"_blank\">Polytechnic University of Turin</a> in Italy, was involved in the study. He notes that agriculture consumes a considerable amount of water. “Looking directly at the plants to estimate the water needs in agriculture could lead to water savings and a better use of this resource,” he explains, noting this could increase yield and facilitate better food security for everyone. </p><p>Therefore, his team set about creating their small, low-cost sensor that is connected directly to the stem of a crop with stainless steel needles just 0.4 millimeters long, acting as electrodes. The <a href=\"https://en.wikipedia.org/wiki/Electrical_impedance\" target=\"_blank\">sensor measures the electrical impedance</a> of the plant stem, which indirectly measures moisture in the plant based on ions and conductivity. More conductivity along the stem indicates that the plant is better watered. </p><p>The sensor has a miniature  solar panel for energy harvesting and a <a href=\"https://spectrum.ieee.org/micro-supercapacitor\" target=\"_blank\">supercapacitor</a> for energy storage, which Garlando says are enough to power the device and support continuous operation. The data is transmitted down the stem of the plant as an electrical signal to a receiver placed in the soil. The receiver, which is powered by a <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1364032119303223#:~:text=Plant%20microbial%20fuel%20cell%20(PMFC,bioelectricity%20%5B4%2C5%5D.\" target=\"_blank\">Plant Microbial Fuel Cell</a> (PMFC) that extracts energy from the electrical signals of microbes in the soil, reads the signal frequency and then transmits the data to a remote site for processing and analysis.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left\" style=\"float: left;\">
<img alt=\"ilustration of a green plant with two computer chips on either side with arrows pointing to pot and plant\" class=\"rm-shortcode rm-resized-image\" id=\"bdf73\" src=\"https://spectrum.ieee.org/media-library/ilustration-of-a-green-plant-with-two-computer-chips-on-either-side-with-arrows-pointing-to-pot-and-plant.jpg?id=51101336&amp;width=980\" />
<small class=\"image-media media-caption\">Measuring electrical impedance at the plant stem, a device transmits data to a receiver placed at the base of the plant—which infers water levels and needs of the plant.</small><small class=\"image-media media-photo-credit\">Umberto Garlando</small></p><p>The researchers first tested their sensor in tobacco plants in a controlled environment, where the sensor’s impedance measurements were confirmed using standard laboratory equipment. Garlando says results suggested the sensor can use electrical impendence to infer the water potential of the plant with 85 percent accuracy. After initial validation of the method, Garlando’s team moved the sensor to apple trees for a nearly year-long test in the field, where he says the sensor’s readings clearly correlated with times of water scarcity.</p><p>He notes a lot of advantages of this sensor, including its small dimensions, low power consumption, low cost, wireless capability, and ability to directly monitor plants in the field. “Furthermore, thanks to the flexibility of the designed sensor, it is possible to adapt it to different species of plants. The same device was used both on tobacco plants and apple trees,” he says. </p><p>Garlando notes that more research is still needed before he will consider commercializing this technology. His team is partnering with experts at other research institutes to improve the sensor’s resolution and understand what additional information they can extract from this sensor. For instance, he says, they might also be able to infer the concentration of various nutrients in the plant’s stem. He adds that his team would like to reduce the sensor system’s overall cost to enable sufficient numbers of sensors to be affordably placed across large fields. </p><p>“Another next step will be the introduction of machine learning and artificial intelligence in the data analysis,” says Garlando. “In the long term, microelectronics will be adopted to integrate the sensor into a single chip. In this way, the cost will be reduced, and it will be possible to miniaturize the sensor.”</p>",[{'name': 'Michelle Hampson'}],"Wed, 17 Jan 2024 20:58:00 +0000"
1873,Open-Source AI Is Uniquely Dangerous,https://spectrum.ieee.org/open-source-ai-2666932122,"<img src=\"https://spectrum.ieee.org/media-library/a-glowing-skull-with-colorful-connective-networking-lines-running-through-it-in-an-open-cube-of-cubes.jpg?id=51069635&amp;width=2000&amp;height=1500&amp;coordinates=222%2C0%2C223%2C0\" /><br /><br /><p><em>This is a guest post. <em>For the other side of the argument about open-source AI, see the recent guest post “<a href=\"https://spectrum.ieee.org/open-source-ai-good\" target=\"_blank\">Open Source AI Is Good for Us</a>.”</em></em><em></em></p><p>When people think of AI applications these days, they likely think of “closed-source” AI applications like OpenAI’s <a href=\"https://chat.openai.com/\" rel=\"noopener noreferrer\" target=\"_blank\">ChatGPT</a>—where the system’s software is securely held by its maker and a limited set of vetted partners. Everyday users interact with these systems through a Web interface like a chatbot, and business users can access an application programming interface (API) which allows them to embed the AI system in their own applications or workflows. Crucially, these uses allow the company that owns the model to provide access to it as a service, while keeping the underlying software secure. Less well understood by the public is the rapid and uncontrolled release of powerful unsecured (sometimes called open-source) AI systems.</p><p class=\"pull-quote\">A good first step in understanding the threats posed by unsecured AI is to ask secured AI systems like ChatGPT, Bard, or Claude to misbehave. </p><p><a href=\"https://openai.com/\" rel=\"noopener noreferrer\" target=\"_blank\">OpenAI</a>’s brand name adds to the confusion. While the company was originally founded to produce open-source AI systems, its leaders determined in 2019 that it was <a href=\"https://www.wired.com/story/ai-text-generator-too-dangerous-to-make-public/\" rel=\"noopener noreferrer\" target=\"_blank\">too dangerous</a> to continue releasing its GPT systems’ source code and model weights (the numerical representations of relationships between the nodes in its artificial neural network) to the public. OpenAI worried because these text-generating AI systems can be used to generate massive amounts of well-written but misleading or <a href=\"https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business\" rel=\"noopener noreferrer\" target=\"_blank\">toxic</a> content.</p><p>Companies including <a href=\"https://www.meta.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Meta</a> (my former employer) have moved in the opposite direction, choosing to release powerful unsecured AI systems in the name of <a href=\"https://about.fb.com/news/2023/07/llama-2/\" rel=\"noopener noreferrer\" target=\"_blank\">democratizing</a> access to AI. Other examples of companies releasing unsecured AI systems include <a href=\"https://stability.ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Stability AI</a>, <a href=\"https://huggingface.co/\" rel=\"noopener noreferrer\" target=\"_blank\">Hugging Face</a>, <a href=\"https://mistral.ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Mistral</a>, <a href=\"https://www.eleuther.ai/\" rel=\"noopener noreferrer\" target=\"_blank\">EleutherAI</a>, and the <a href=\"https://www.tii.ae/\" rel=\"noopener noreferrer\" target=\"_blank\">Technology Innovation Institute</a>. These companies and like-minded advocacy groups have made limited progress in <a href=\"https://ec.europa.eu/commission/presscorner/detail/en/QANDA_21_1683\" rel=\"noopener noreferrer\" target=\"_blank\">obtaining exemptions</a> for some unsecured models in the European Union’s <a href=\"https://artificialintelligenceact.eu/\" rel=\"noopener noreferrer\" target=\"_blank\">AI Act</a>, which is designed to reduce the risks of powerful AI systems. They may push for similar exemptions in the United States via the public comment period recently <a href=\"https://docs.google.com/document/d/1u-MUpA7TLO4rnrhE2rceMSjqZK2vN9ltJJ38Uh5uka4/edit#heading=h.t02rblknwe5\" rel=\"noopener noreferrer\" target=\"_blank\">set forth in</a> the White House’s <a href=\"https://spectrum.ieee.org/biden-ai-executive-order\" target=\"_self\">AI Executive Order</a>.</p><p>I think the <a href=\"https://spectrum.ieee.org/tag/open-source\" rel=\"noopener noreferrer\" target=\"_blank\">open-source</a> movement has an important role in AI. With a technology that brings so many new capabilities, it’s important that no single entity acts as a gatekeeper to the technology’s use. However, as things stand today, unsecured AI poses an enormous risk that we are not yet able to contain.</p><h2>Understanding the Threat of Unsecured AI</h2><p>A good first step in understanding the threats posed by unsecured AI is to ask secured AI systems like ChatGPT, <a href=\"https://bard.google.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Bard</a><u>,</u> or <a href=\"https://claude.ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Claude</a> to misbehave. You could ask them to design a more deadly coronavirus, provide instructions for making a bomb, make naked pictures of your favorite actor, or write a series of inflammatory text messages designed to make voters in swing states more angry about immigration. You will likely receive polite refusals to all such requests because <a href=\"https://openai.com/policies/usage-policies\" target=\"_blank\">they violate</a> the <a href=\"https://policies.google.com/terms/generative-ai/use-policy\" rel=\"noopener noreferrer\" target=\"_blank\">usage policies</a> of <a href=\"https://console.anthropic.com/legal/aup\" target=\"_blank\">these AI systems</a>. Yes, it is possible to “jailbreak” these <a href=\"https://arxiv.org/abs/2305.13860\" rel=\"noopener noreferrer\" target=\"_blank\">AI systems</a> and get them to misbehave, but as these vulnerabilities are discovered, they can be fixed.</p><p>Enter the unsecured models. Most famous is Meta’s <a href=\"https://ai.meta.com/llama/\" rel=\"noopener noreferrer\" target=\"_blank\">Llama 2</a>. It was released by Meta with a 27-page “<a href=\"https://ai.meta.com/static-resource/responsible-use-guide/\" rel=\"noopener noreferrer\" target=\"_blank\">Responsible Use Guide</a>,” which was promptly ignored by the creators of “<a href=\"https://huggingface.co/jarradh/llama2_70b_chat_uncensored\" rel=\"noopener noreferrer\" target=\"_blank\">Llama 2 Uncensored</a>,” a derivative model with safety features stripped away, and hosted for free download on the Hugging Face AI repository. Once someone releases an “uncensored” version of an unsecured AI system, the original maker of the system is largely powerless to do anything about it.</p><p class=\"pull-quote\">As things stand today, unsecured AI poses an enormous risk that we are not yet able to contain.</p><p>The threat posed by unsecured AI systems lies in the ease of misuse. They are particularly dangerous in the hands of sophisticated threat actors, who could easily download the original versions of these AI systems and disable their safety features, then make their own custom versions and abuse them for a wide variety of tasks. Some of the abuses of unsecured AI systems also involve taking advantage of vulnerable distribution channels, such as social media and messaging platforms. These platforms cannot yet accurately detect AI-generated content at scale and can be used to distribute massive amounts of personalized misinformation and, of course, <a href=\"https://theconversation.com/ai-scam-calls-imitating-familiar-voices-are-a-growing-problem-heres-how-they-work-208221\" rel=\"noopener noreferrer\" target=\"_blank\">scams</a>. This could have catastrophic effects on the information ecosystem, and on <a href=\"https://spectrum.ieee.org/deepfakes-election\" target=\"_self\">elections</a> in particular. Highly damaging <a href=\"https://www.wired.com/story/deepfake-porn-is-out-of-control/\" rel=\"noopener noreferrer\" target=\"_blank\">nonconsensual deepfake pornography</a> is yet another domain where unsecured AI can have deep negative consequences.</p><p>Unsecured AI also has the potential to facilitate production of dangerous materials, such as <a href=\"https://www.axios.com/2023/06/16/pandemic-bioterror-ai-chatgpt-bioattacks\" rel=\"noopener noreferrer\" target=\"_blank\">biological and chemical weapons</a>. The White House Executive Order references chemical, biological, radiological, and nuclear (<a href=\"https://docs.google.com/document/d/1u-MUpA7TLO4rnrhE2rceMSjqZK2vN9ltJJ38Uh5uka4/edit#heading=h.6fkzizejib9o\" rel=\"noopener noreferrer\" target=\"_blank\">CBRN</a>) risks, and <a href=\"https://www.markey.senate.gov/news/press-releases/sens-markey-budd-announce-legislation-to-assess-health-security-risks-of-ai\" rel=\"noopener noreferrer\" target=\"_blank\">multiple bills</a> are now under consideration by the U.S. Congress to address these threats.</p><h2>Recommendations for AI Regulations</h2><p>We don’t need to specifically regulate unsecured AI—nearly all of the regulations that have been publicly discussed apply to secured AI systems as well. The only difference is that it’s much easier for developers of secured AI systems to comply with these regulations because of the inherent properties of secured and unsecured AI. The entities that operate secured AI systems can actively monitor for abuses or failures of their systems (including bias and the production of dangerous or offensive content) and release regular updates that make their systems more fair and safe.</p><p class=\"pull-quote\">“I think how we regulate open-source AI is THE most important unresolved issue in the immediate term.”<br /><strong>—Gary Marcus, New York University</strong></p><p>Almost all the regulations recommended below generalize to all AI systems. Implementing these regulations would make companies think twice before releasing unsecured AI systems that are ripe for abuse.</p><p><u><strong>Regulatory Action for AI Systems</strong></u></p><ol><li><strong>Pause all new releases</strong> of unsecured AI systems until developers have met the requirements below, and in ways that ensure that safety features cannot be easily removed by bad actors.</li><li><strong>Establish registration and licensing</strong> (both retroactive and ongoing) of all AI systems above a certain capability threshold.</li><li><strong>Create liability </strong>for “reasonably foreseeable misuse” and negligence: Developers of AI systems should be legally liable for harms caused to both individuals and to society.</li><li><strong>Establish risk assessment, mitigation, and independent audit</strong> procedures for AI systems crossing the threshold mentioned above.</li><li><strong>Require watermarking and provenance</strong> best practices so that AI-generated content is clearly labeled and authentic content has metadata that lets users understand its provenance.</li><li><strong>Require transparency of training data</strong> and prohibit training systems on personally identifiable information, content designed to generate hateful content, and content related to biological and chemical weapons.</li><li><strong>Require and fund independent researcher access</strong>, giving vetted researchers and civil society organizations predeployment access to generative AI systems for research and testing.</li><li><strong>Require “know your customer” procedures</strong>, similar to those <a href=\"https://www.swift.com/your-needs/financial-crime-cyber-security/know-your-customer-kyc/meaning-kyc\" rel=\"noopener noreferrer\" target=\"_blank\">used by financial institutions</a><u>,</u> for sales of powerful hardware and cloud services designed for AI use; restrict sales in the same way that weapons sales would be restricted.</li><li><strong>Mandatory incident disclosure</strong>: When developers learn of vulnerabilities or failures in their AI systems, they must be <a href=\"https://csrc.nist.gov/pubs/sp/800/61/r2/final\" rel=\"noopener noreferrer\" target=\"_blank\">legally required to report</a> this to a designated government authority.</li></ol><p><u><strong>Regulatory Action for Distribution Channels and Attack Surfaces</strong></u></p><ol><li><strong>Require content credential implementation</strong> for social media, giving companies a deadline to implement the <a href=\"https://contentcredentials.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Content Credentials labeling standard</a> from C2PA.</li><li><strong>Automate digital signatures</strong> so people can rapidly verify their human-generated content.</li><li><strong>Limit the reach of AI-generated content</strong>: Accounts that haven’t been verified as distributors of human-generated content could have certain features disabled, including viral distribution of their content.</li><li><strong>Reduce chemical, biological, radiological, and nuclear risks</strong> by educating all suppliers of custom nucleic acids or other potentially dangerous substances about best practices.</li></ol><p><u><strong>Government Action</strong></u></p><ol><li><strong>Establish a nimble regulatory body</strong> that can act and enforce quickly and update certain enforcement criteria. This entity would have the power to approve or reject risk assessments, mitigations, and audit results and have the authority to block model deployment.</li><li><strong>Support fact-checking organizations</strong> and civil-society groups (including the “<a href=\"https://ec.europa.eu/commission/presscorner/detail/en/QANDA_20_2348\" rel=\"noopener noreferrer\" target=\"_blank\">trusted flaggers</a>” defined by the <a href=\"https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package\" rel=\"noopener noreferrer\" target=\"_blank\">EU Digital Services Act</a>) and require generative AI companies to work directly with these groups.</li><li><strong>Cooperate internationally</strong> with the goal of eventually creating an international <a href=\"https://www.cigionline.org/articles/voluntary-curbs-arent-enough-ai-risk-requires-a-binding-international-treaty/\" rel=\"noopener noreferrer\" target=\"_blank\">treaty</a> or new international agency to prevent companies from circumventing these regulations. The recent <a href=\"https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023\" rel=\"noopener noreferrer\" target=\"_blank\">Bletchley Declaration</a> was signed by 28 countries, including the home countries of all of the world’s leading AI companies (United States, China, United Kingdom, United Arab Emirates, France, and Germany); this declaration stated shared values and carved out a path for additional meetings.</li><li><strong>Democratize AI access</strong> with public infrastructure: A common concern about regulating AI is that it will limit the number of companies that can produce complicated AI systems to a small handful and tend toward monopolistic business practices. There are many opportunities to democratize access to AI, however, without relying on unsecured AI systems. One is through the creation of <a href=\"https://cdn.vanderbilt.edu/vu-URL/wp-content/uploads/sites/412/2023/10/09151836/VPA-AI-Capacity.10.9.23.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">public AI infrastructure</a> with powerful secured AI models.</li></ol><p>“I think how we regulate open-source AI is THE most important unresolved issue in the immediate term,” <a href=\"http://garymarcus.com/index.html\" target=\"_blank\">Gary Marcus</a>, the cognitive scientist, entrepreneur, and professor emeritus at New York University told me in a recent email exchange.<br /></p><p>I agree, and these recommendations are only a start. They would initially be costly to implement and would require that regulators make certain powerful lobbyists and developers unhappy.</p><p>Unfortunately, given the misaligned incentives in the current AI and information ecosystems, it’s unlikely that industry will take these actions unless forced to do so. If actions like these are not taken, companies producing unsecured AI may bring in billions of dollars in profits while pushing the risks posed by their products onto all of us.</p>",[{'name': 'David Evan Harris'}],"Fri, 12 Jan 2024 17:00:02 +0000"
1874,"At CES 2024, AI Is Here to Help",https://spectrum.ieee.org/ai-assistant,"<img src=\"https://spectrum.ieee.org/media-library/a-crowded-tradeshow-floor-with-a-samsung-booth-and-large-glowing-words-saying-ai-for-all.jpg?id=51056355&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C600%2C0\" /><br /><br /><p>If you believe the hype, AI will someday generate Hollywood films, cure cancer, and make driving a historical oddity. But at CES 2024, it’s tackling some more immediate and achievable goals.</p><p>LG and Samsung touted AI features that tune your television’s picture quality. Nvidia can use its GPUs to find what you want from hundreds of documents. And dozens of startups pitched AI assistants, avatars, coaches, and tutors.</p><h2>LG and Samsung become AI hub hopefuls</h2><p class=\"shortcode-media shortcode-media-rebelmouse-image\">
<img alt=\"A man gestures in front of a screen showing a round white robot on two wheels and a chatbot conversation.\" class=\"rm-shortcode\" id=\"6e813\" src=\"https://spectrum.ieee.org/media-library/a-man-gestures-in-front-of-a-screen-showing-a-round-white-robot-on-two-wheels-and-a-chatbot-conversation.jpg?id=51056395&amp;width=980\" />
<small class=\"image-media media-caption\">Henry Kim, team leader for the ThinQ Platform business at LG Electronics U.S., speaks on stage during the 2024 International CES, in Las Vegas, on 8 January 2024.</small><small class=\"image-media media-photo-credit\">James Atoa/UPI/Alamy</small></p><p>LG and Samsung positioned their new televisions as AI powerhouses. That’s not entirely novel: Both have used AI for years to upscale low-resolution content to 4K resolution. For 2024, though, LG hopes its televisions can become an assistant.</p><p>“We want to help people find the settings they need,” says <a href=\"https://www.linkedin.com/in/david-park-63669351\" rel=\"noopener noreferrer\" target=\"_blank\">David Park,</a> head of customer enablement at LG. “This isn’t just a television throwing up a bunch of words at random. It’s much more conversational.”</p><p>LG demonstrated a chatbot interface users can converse with to find settings, optimize image quality, or troubleshoot problems. It’s not unlike the search functions on many devices today. But the chatbot, unlike search, can help you find a setting with a name you can’t remember or recommend settings based on what you’re trying to achieve.</p><p>Samsung went a step further, branding its new televisions as “AI screens.” Many features that fall under this umbrella are meant to improve image quality or motion clarity, but others offer ease-of-use and accessibility. The company demonstrated an on-device, AI-powered optical character recognition (OCR) technology that can serve as a “voice guide” for subtitles. It also showed a mode called <a href=\"https://news.samsung.com/us/samsung-advancing-equity-entertainment-creating-inclusive-tv-experience-relumino-mode-interview/\" rel=\"noopener noreferrer\" target=\"_blank\">Relumino Together</a>, which uses AI to enhance the image for those with low vision.</p><p>Both also came to the show with AI robots for the home. <a href=\"https://news.samsung.com/global/ces-2024-a-glance-at-samsungs-booth-themed-ai-for-all\" rel=\"noopener noreferrer\" target=\"_blank\">Samsung’s is a new iteration of Ballie</a>, first shown at CES 2020, <a href=\"https://www.lgnewsroom.com/2023/12/lg-ushers-in-zero-labor-home-with-its-smart-home-ai-agent-at-ces-2024/\" rel=\"noopener noreferrer\" target=\"_blank\">while LG touted its “two-wheeled AI agent.” </a>The dream is to embody the AI services people might access on a TV, computer, or phone. Ballie even has a built-in projector that lets you bring whatever you’re viewing on a television or laptop with you.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image\">
<img alt=\"Animated gif of a man moving around a kitchen while a small circular wheeled robot rolls into the frame.\" class=\"rm-shortcode\" id=\"6e9d6\" src=\"https://spectrum.ieee.org/media-library/animated-gif-of-a-man-moving-around-a-kitchen-while-a-small-circular-wheeled-robot-rolls-into-the-frame.gif?id=51056373&amp;width=980\" />
<small class=\"image-media media-caption\">Samsung’s Ballie robot just wants to be helpful.</small><small class=\"image-media media-photo-credit\">Samsung</small></p><p>But these robots also outline the limits of AI at CES 2024. They’re just prototypes for now, and it’s unclear when (or if) they’ll see release.</p><h2>Startup makes AI your instruction manual</h2><p>Of course, it’s not just tech giants looking to get in on the AI buzz. Eureka Park, a hub for startups on the CES show floor, was bursting with AI-powered assistants.</p><p>Among these was <a href=\"https://www.blinkin.io/\" rel=\"noopener noreferrer\" target=\"_blank\">BlinkIn</a>, a German startup building a multimodal “companion experience” called Houston, which is accessed through a smartphone app. Its purpose is similar to the chatbot on LG’s TV, but BlinkIn wants to offer advice for <em>everything </em>in your home, from your coffee maker to your washing machine. You can snap a photo of your device and ask a question about it.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image\">
<img alt=\"A screenshot showing a multistep process of using the phone to get AI assistance on something in the camera frame.\" class=\"rm-shortcode\" id=\"109d7\" src=\"https://spectrum.ieee.org/media-library/a-screenshot-showing-a-multistep-process-of-using-the-phone-to-get-ai-assistance-on-something-in-the-camera-frame.jpg?id=51056450&amp;width=980\" />
<small class=\"image-media media-caption\">BlinkIn’s AI can help with how-to.</small><small class=\"image-media media-photo-credit\">BlinkIn</small></p><p>This is a task that OpenAI’s ChatGPT, <a href=\"https://spectrum.ieee.org/chatgpt-multimodal\" target=\"_self\">which has multimodal capabilities including image and audio prompts</a>, can already tackle. But ChatGPT can prove an unreliable assistant. If I ask it how to clean my coffee maker, for example, it usually provides a meandering, long-winded answer with only some details relevant to my particular appliance.</p><p>“We want to help you not get a wall of text,” says <a href=\"https://www.linkedin.com/in/akkshayjoshii/?originalSubdomain=de\" rel=\"noopener noreferrer\" target=\"_blank\">Akshay Joshi</a>, senior applied AI scientist at BlinkIn. Joshi told me the company is using open-source multimodal models, such as <a href=\"https://minigpt-4.github.io/\" rel=\"noopener noreferrer\" target=\"_blank\">MiniGPT-4 </a>(which is not affiliated with OpenAI), to tune the app’s replies, although it will use ChatGPT as a fallback.</p><p>BlinkIn also hopes to fill the trust gap with a community-driven approach. Users who feel helpful can provide answers that its AI model will use as training data, and Houston will surface and cite these answers (complete with user profiles). <a href=\"https://www.crunchbase.com/person/josef-suess\" rel=\"noopener noreferrer\" target=\"_blank\">Josef Suess</a>, CEO of BlinkIn, says this will add a communal feel similar to Reddit or YouTube. “Who do you trust more as a user? A content creator, or a brand?” he asks. “We want to build a platform where it’s about helping each other.”</p><h2>Nvidia shows off encyclopedia GPU</h2><p>Everyone knows Nvidia’s hardware is great for AI (and, in case you’d forgotten,<a href=\"https://www.nvidia.com/en-us/geforce/news/geforce-rtx-ces-2024-announcements/#:~:text=It%20arrives%20January%2024th%2C%20starting,with%20performance%2Dmultiplying%20frame%20generation.\" rel=\"noopener noreferrer\" target=\"_blank\"> it came to CES 2024 touting the AI power of three new RTX Super desktop graphics cards</a>). Yet, when it comes to using AI for yourself on your home PC, putting an Nvidia GPU on a task often requires a tour through <a href=\"https://github.com/\" rel=\"noopener noreferrer\" target=\"_blank\">GitHub</a> or <a href=\"https://huggingface.co/\" rel=\"noopener noreferrer\" target=\"_blank\">HuggingFace</a>, and several hours of work.</p><p class=\"shortcode-media shortcode-media-rebelmouse-image\">
<img alt=\"A screen says Chat with RTX and shows a chatbot with question and answer\" class=\"rm-shortcode\" id=\"6090e\" src=\"https://spectrum.ieee.org/media-library/a-screen-says-chat-with-rtx-and-shows-a-chatbot-with-question-and-answer.jpg?id=51056488&amp;width=980\" />
<small class=\"image-media media-caption\">Nvidia’s desktop GPU can help you find and understand information.</small><small class=\"image-media media-photo-credit\">Nvidia</small></p><p>Nvidia brought a solution to CES 2024: Chat with RTX. It’s a Windows application that lets you prompt a <a href=\"https://spectrum.ieee.org/tag/llms\" target=\"_self\">large language model</a> with questions about a set of documents provided to the app. Chat with RTX currently supports two large language models: Meta’s <a href=\"https://ai.meta.com/llama/\" rel=\"noopener noreferrer\" target=\"_blank\">Llama 2</a> and <a href=\"https://mistral.ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Mistral AI’s Mistral</a>. The models load on your PC, and all AI inference occurs on your local Nvidia GPU, which means conversations in the app are kept private.</p><p>I had a chance to use Chat with RTX at the show, and I was immediately struck by its speed. The app responded to my questions with zero delay. I was also able to quickly switch between different sets of documents (the app can read Word documents, PDFs, text files, and XML). It’s a solution I’m eager to try at home, as I have several piles of PDF documents that are a bear to search.</p><p>Chat with RTX, like all the AI demos I saw at CES, still needs work. The interface is barebones and its capabilities limited to specific situations. Still, CES made clear there’s no shortage of interest in custom-tailored AI assistants built to achieve particular goals—expect to see more refined iterations of these ideas appear through 2024.</p>",[{'name': 'Matthew S. Smith'}],"Thu, 11 Jan 2024 15:37:47 +0000"
1875,E-Nose Sniffs Out Coffee Varieties Nearly Perfectly,https://spectrum.ieee.org/e-nose-coffee-classifier,"<img src=\"https://spectrum.ieee.org/media-library/an-overhead-photograph-of-a-steaming-cup-of-coffee-on-a-background-of-coffee-beans.jpg?id=51033984&amp;width=2000&amp;height=1500&amp;coordinates=166%2C0%2C167%2C0\" /><br /><br /><p><em>This article is part of our exclusive </em><a href=\"https://spectrum.ieee.org/collections/journal-watch/\" target=\"_self\"><em>IEEE Journal Watch series</em></a><em> in partnership with IEEE Xplore.</em></p><p>Some truly passionate java junkies may be adept at distinguishing between various coffee flavors, but a new e-nose may give these folks a run for their money. The new sensing system, which can distinguish coffee aromas consistently with 98 percent accuracy, is described in a <a href=\"https://ieeexplore.ieee.org/document/10363440\" rel=\"noopener noreferrer\" target=\"_blank\">study</a> published last month in <a href=\"https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9787768\" rel=\"noopener noreferrer\" target=\"_blank\"><em>IEEE Transactions on AgriFood Electronics</em></a>.</p><p>E-noses are devices that assess gases to classify the nature of the substance at hand. They are growing in popularity, thanks to their ability to facilitate quality assurance of products (such as <a href=\"https://spectrum.ieee.org/electronic-nose-whiskey\" target=\"_self\">whiskey</a>), provide real-time health assessments (such as <a href=\"https://spectrum.ieee.org/glucose-test-e-nose\" target=\"_self\">an individual’s glucose levels</a>), and to support sustainable agriculture (such as <a href=\"https://spectrum.ieee.org/new-electronic-nose-sniffs-out-perfectly-ripe-peaches-for-harvest\" target=\"_blank\">identifying when a crop is ready for harvesting</a>). </p><p class=\"pull-quote\">“The broader goal is to contribute to the preservation and understanding of aromas in the face of environmental changes.”<br /><strong>—Chung-Hong Lee, National Kaohsiung University of Science and Technology, Taiwan</strong></p><p><a href=\"https://www.researchgate.net/profile/Chung-Hong-Lee\" target=\"_blank\">Chung-Hong Lee,</a> a professor at the Department of Electrical Engineering at the <a href=\"https://eng.nkust.edu.tw/\" target=\"_blank\">National Kaohsiung University of Science and Technology</a>, in Taiwan, is a passionate coffee drinker and was interested in applying e-nose technology in the context of one of his favorite beverages. </p><p>He notes that coffee varieties rely on an array of factors, including growing area, climate, post-harvest processing (such as dry and wet methods), roasting treatment, and the genetic makeup of beans themselves. But, importantly, climate change is altering the local environments that influence bean flavor and aroma.</p><p>“Beyond [my] personal enjoyment of coffee, the broader goal is to contribute to the preservation and understanding of aromas in the face of environmental changes, ensuring consistency in quality and flavor profiles across different crop years,” explains Lee. </p><p>To create their e-nose, Lee’s team used an array of eight commercial metal semiconductor oxide sensors that are sensitive to specific gases. These sensors detect which molecules are present, data that is then sent to an AI algorithm for processing. The researchers then trained and tested several different AI systems that could distinguish among 16 different coffee-bean varieties from around the world. </p><p>All of the AI algorithms performed very well, achieving an accuracy ranging from 81 to 98 percent, depending on the type of coffee. The best performing algorithm—one called a <a href=\"https://spectrum.ieee.org/breakthrough-listen-trained-an-ai-to-spot-illusive-mysterious-radio-bursts\" target=\"_blank\">convolutional neural network</a> (CNN) algorithm—nearly always had an accuracy exceeding 90 percent and reached an accuracy as high as 98 percent for several coffee varieties. </p><p>“Thus, the journey from drinking coffee to unraveling its aromatic secrets becomes a natural fusion of sensory delight and cutting-edge technology,” says Lee.</p><p>However, the value in this work goes beyond simply identifying the cup of coffee at hand. Lee’s team is using these data from their e-nose to build a comprehensive digital library of coffee aromas, capturing the diverse and complex profiles from various coffee-producing regions globally. Notably, this digital library of coffee flavors could create a benchmark for original flavors, and how they are affected over time as climate change affects coffee-bean agriculture. Or, for example, if some deceitful seller tries to market one bean variety as another, this tech could help ensure that bean fiends get the right cup of coffee they ordered.</p><p>Lee says his team is interested in commercializing their e-nose, and they are open to collaboration with enterprises or startups interested in leveraging this technology. As well, he sees the possibility of creating not just e-noses for coffee, but also an electronic tongue for detecting coffee taste, an electronic eye for filtering the best ripe coffee—even a tactile system for assessing the moisture content of coffee beans. “This next step is crucial for establishing and progressing toward addressing issues in coffee-growing areas and benefiting farmers,” Lee emphasizes. </p>",[{'name': 'Michelle Hampson'}],"Wed, 10 Jan 2024 22:40:06 +0000"
1876,CES 2024: Neuchips Demos Low-Power AI Upgrade for PCs,https://spectrum.ieee.org/neuchips-low-power-ai,"<img src=\"https://spectrum.ieee.org/media-library/silver-square-against-a-blue-background-with-llm-above.jpg?id=51029508&amp;width=2000&amp;height=1500&amp;coordinates=0%2C0%2C0%2C1\" /><br /><br /><p>What if any desktop PC could become an AI inference beast with a single upgrade? And what if that transformed beast still sipped power like it was enjoying a martini? <br /></p><p>That’s the idea pitched by <a href=\"https://neuchips.ai/\" target=\"_blank\">Neuchips</a>, a Taiwanese startup founded in 2019 and known for delivering<a href=\"https://neuchips.ai/page/mlperf\" target=\"_blank\"> top-class AI efficiency</a>. It came to <a href=\"https://www.ces.tech/events-programs/ces-events/las-vegas.aspx\" target=\"_blank\">CES Unveiled 2024</a>—the media pregame show before the <a href=\"https://www.ces.tech/\" target=\"_blank\">main event</a>—with a PCIe add-on card that can upgrade the AI capabilities of a typical desktop computer while adding just 55 watts to the PC’s power budget. </p><p>It’s not just a concept. The card was plugged into a desktop computer on the show floor and offered real-time, offline conversation with a chatbot powered by <a href=\"https://spectrum.ieee.org/llama-2-llm\" target=\"_self\">Meta’s popular Llama 2 7B large language model</a> (Neuchips says the card will also run Llama 2 13B).</p><p>Neuchips’ card, the Evo PCIe accelerator, is built around the company’s Raptor Gen AI accelerator chip. The Raptor chip delivers up to 200 teraoperations per second measured with <a href=\"https://ai.meta.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/\" target=\"_blank\">Meta’s DLRM benchmark</a>, and the company says it’s optimized for transformer-based models.. </p><p>The card that Neuchips demonstrated had the Raptor chip, but a single chip isn’t the card’s final form. <a href=\"https://www.linkedin.com/in/ken-lau-6948662/\" rel=\"noopener noreferrer\" target=\"_blank\">Neuchips’ CEO Ken Lau</a>, an Intel veteran of 26 years, says Raptor can be used to design cards with varying numbers of chips onboard. </p><p>“The chip is actually scalable,” says Lau. “So we start with one chip. And then we have four chips. And then eight chips.” Each chip provides up to 200 trillion operations per second (TOPS), according to <a href=\"https://www.neuchips.ai/news/article/711ZY14mZahw7StF/press-releases\" rel=\"noopener noreferrer\" target=\"_blank\">Neuchip’s press release</a>. The card also carries 32 GB of LPDDR5 memory and reaches 1.6 terabits of memory bandwidth. Memory bandwidth is important, because it’s often a factor when handling AI inference on a single PC.</p><p>Neuchips wants to give owners the tools needed to use the card effectively as well, although with many months until release the details here remain a bit sparse. A Neuchips representative said the company has compiler software and will provide a driver. The demonstration I saw had a custom interface for interacting with the Llama 2 7B model. Neuchips’ card was running, but it appeared bare-bones.</p><h2>A focus on efficiency </h2><p>There’s already hardware that anyone can plug into a desktop’s PCIe slot to greatly improve AI performance. It’s called a GPU, and Nvidia has a stranglehold on the market. Going toe-to-toe with Nvidia on performance would be difficult. In fact, <a href=\"https://nvidianews.nvidia.com/news/geforce-rtx-40-super-series\" rel=\"noopener noreferrer\" target=\"_blank\">Nvidia announced new cards with a focus on AI at CES 2024</a>; the RTX 4080 Super, which will retail for US $999 starting on 31 January, quotes AI performance of up to 836 TOPS.</p><p>Neuchips, however, sees an opening. “We are focused on power efficiency,” says Lau, “and on handling the many different models that are out there.”</p><p>Modern graphics cards are powerful, but also power hungry. The RTX 4080 Super can draw up to 320 W of power and will typically require a computer with a power supply that can deliver at least 750 W. Neuchips’ Evo PCIe accelerator, by contrast, consumes just 55 W of power. It consumes so little power, in fact, that the card Neuchips demonstrated at CES didn’t have an external PCIe power connection. Such connectors are a must for most GPU cards. </p><p>I was also told that the final card, which should ship in the latter half of 2024, will be roughly half the size of the card shown at CES. That’s an important detail, as the card I saw was as large as most current Nvidia GPU cards, and too large to fit most small form-factor desktop computers. A smaller card would make the Evo PCIe accelerator usable in a wide range of modern PC hardware.<br /></p><p>Neuchips’ accelerator, though perhaps the most high-profile AI accelerator card at CES 2024, was far from alone at the show. Several startups came with their own AI accelerators packing unique features. <a href=\"https://panmnesia.com/news_en/#news-en-2023-11-22-ces24-innovation\" rel=\"noopener noreferrer\" target=\"_blank\">Panmnesia won a CES Innovation Award for an AI accelerator</a> that includes a Compute eXpress Link interface for access to huge pools of memory. Other companies with AI accelerators include <a href=\"https://deepx.ai/\" rel=\"noopener noreferrer\" target=\"_blank\">DeepX</a> and <a href=\"https://memryx.com/\" rel=\"noopener noreferrer\" target=\"_blank\">MemryX</a>. <a href=\"https://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/ai-pc.html\" rel=\"noopener noreferrer\" target=\"_blank\">Intel </a>and <a href=\"https://www.amd.com/en/products/processors/consumer/ryzen-ai.html\" rel=\"noopener noreferrer\" target=\"_blank\">AMD </a>are in on it, too; each offers an AI accelerator in its latest CPU architecture. </p><p>Make no mistake: Nvidia remains the 800-pound gorilla in this arena, and that’s not going to change overnight. Still, new AI accelerators like Neuchips’ Raptor and the Evo PCIe card look ready to deliver new options for developers who don’t care about graphics or have a need for improved power efficiency while running AI inference. </p><p>Neuchips’ Evo PCI accelerator is due for full release in the second half of 2024. Pricing remains to be announced. </p><p><em>This post was update on 12 January to clarify benchmark operation speeds and correct the system’s memory bandwidth.</em></p>",[{'name': 'Matthew S. Smith'}],"Tue, 09 Jan 2024 14:00:04 +0000"
1877,Use Case Accelerant: Extraction Service,https://blog.langchain.dev/use-case-accelerant-extraction-service/,"<p>Today we&#x2019;re excited to announce our newest OSS use-case accelerant: an <a href=\"https://github.com/langchain-ai/langchain-extract?ref=blog.langchain.dev\"><u>extraction</u></a><u> </u><a href=\"https://github.com/langchain-ai/langchain-extract?ref=blog.langchain.dev\"><u>service</u></a>. LLMs are a powerful tool for extracting structured data from unstructured sources. We&apos;ve improved our <a href=\"https://python.langchain.com/docs/use_cases/extraction/?ref=blog.langchain.dev\"><u>support for data extraction</u></a> in the open source LangChain library over the past few releases, and now we&</p>",[{'name': 'LangChain'}],"Wed, 06 Mar 2024 18:00:35 GMT"
1878,LangGraph for Code Generation,https://blog.langchain.dev/code-execution-with-langgraph/,"<p></p><h3 id=\"key-links\">Key Links</h3><ul><li><a href=\"https://github.com/langchain-ai/langgraph/blob/main/examples/code_assistant/langgraph_code_assistant.ipynb?ref=blog.langchain.dev\" rel=\"noreferrer\">LangGraph cookbook</a></li><li><a href=\"https://www.youtube.com/watch?v=MvNdgmM7uyc&amp;ref=blog.langchain.dev\">Video</a></li></ul><h3 id=\"motivation\">Motivation</h3><p>Code generation and analysis are two of most important applications of LLMs, as shown by the ubiquity of products like <a href=\"https://github.com/features/copilot?ref=blog.langchain.dev\">GitHub co-pilot</a> and popularity of projects like <a href=\"https://github.com/gpt-engineer-org/gpt-engineer?ref=blog.langchain.dev\">GPT-engineer</a>. The recent <a href=\"https://arxiv.org/pdf/2401.08500.pdf?ref=blog.langchain.dev\">AlphaCodium</a> work showed that code generation can be <a href=\"https://x.com/karpathy/status/1748043513156272416?s=20&amp;ref=blog.langchain.dev\" rel=\"noopener\">improved by using a <code>flow</code> paradigm</a></p>",[{'name': 'Ankush Gola'}],"Tue, 27 Feb 2024 16:10:00 GMT"
1879,[Week of 2/19] LangChain Release Notes,https://blog.langchain.dev/week-of-2-19-langchain-release-notes/,<h3 id=\"%F0%9F%9A%80-langsmith-launch\"><strong>&#x1f680; </strong>LangSmith Launch!</h3><p>LangSmith is now generally available - no more waitlist! Sign up for free <a href=\"https://smith.langchain.com/?ref=blog.langchain.dev\">here</a>.</p><ul><li>Video resources: <a href=\"https://www.youtube.com/watch?v=jx7xuHlfsEQ&amp;ref=blog.langchain.dev\">LangSmith in 10 minutes</a> &amp; <a href=\"https://www.youtube.com/watch?v=3wAON0Lqviw&amp;ref=blog.langchain.dev\">In Depth Platform Overview</a></li><li>Our <a href=\"https://blog.langchain.dev/langsmith-ga/\">release blog</a> covers all of the latest features in LangSmith</li><li>We&#x2019;ve shipped TypeScript improvements. It&#x2019;s now easier</li></ul>,[{'name': 'LangChain'}],"Fri, 23 Feb 2024 16:20:43 GMT"
1880,Reflection Agents,https://blog.langchain.dev/reflection-agents/,"Reflection is a prompting strategy used to improve the quality and success rate of agents and similar AI systems. This post outlines how to build 3 reflection techniques using LangGraph, including implementations of Reflexion and Language Agent Tree Search.",[{'name': 'Ankush Gola'}],"Wed, 21 Feb 2024 09:29:10 GMT"
1881,JSON agents with Ollama & LangChain,https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/,"<h2 id=\"learn-to-implement-an-open-source-mixtral-agent-that-interacts-with-a-graph-database-neo4j-through-a-semantic-layer\">Learn to implement an open-source Mixtral agent that interacts with a graph database Neo4j through a semantic layer</h2><p><strong>Editor&apos;s note: This post is written by </strong><a href=\"https://twitter.com/tb_tomaz?ref=blog.langchain.dev\"><strong>Tomaz Bratanic</strong></a><strong> from Neo4j</strong></p><p>By now, we all have probably recognized that we can significantly enhance the capabilities of LLMs by providing them</p>",[{'name': 'LangChain'}],"Tue, 20 Feb 2024 17:24:29 GMT"
1882,Supercharging If-Statements With Prompt Classification Using Ollama and LangChain,https://blog.langchain.dev/supercharging-if-statements-with-prompt-classification-using-ollama-and-langchain/,"<p><strong>Editor&apos;s Note: </strong><a href=\"https://medium.com/@andrewnguonly?ref=blog.langchain.dev\"><strong>Andrew Nguonly</strong></a><strong> has been building one of the more impressive projects we&apos;ve seen recently - an LLM co-pilot for browsing the web, powered by local LLMs. There are a lot of small architectural decisions made that contain a ton of nuance, and so we&</strong></p>",[{'name': 'LangChain'}],"Mon, 19 Feb 2024 17:16:07 GMT"
1883,Winning in AI means mastering the new stack,https://blog.langchain.dev/winning-in-ai-means-mastering-the-new-stack/,"<p>Authors: Edo Liberty, Guillermo Rauch, Ori Goshen, Robert Nishihara, Harrison Chase</p><h2 id=\"ai-in-2030\">AI in 2030</h2><p>AI is rapidly changing. Too rapidly for most. Ten years ago it was all about big data and ML. Deep Learning was a &#x201c;buzzy&#x201d; term that was just picking up steam. The term &#x201c;</p>",[{'name': 'Harrison Chase'}],"Fri, 16 Feb 2024 20:02:46 GMT"
1884,Announcing the General Availability of LangSmith and Our Series A Led By Sequoia Capital,https://blog.langchain.dev/langsmith-ga/,"<p>Today, we&#x2019;re thrilled to announce the <strong>general availability of LangSmith</strong> &#x2014; our solution for LLM application development, monitoring, and testing. We initially launched LangSmith in closed beta in July 2023. Since then, we&#x2019;ve received invaluable feedback from our early users and customers, and made significant improvements</p>",[{'name': 'Ankush Gola'}],"Thu, 15 Feb 2024 16:00:38 GMT"
1885,Rakuten Group builds with LangChain and LangSmith to deliver premium products for its business clients and employees,https://blog.langchain.dev/rakuten-group-builds-with-langchain-and-langsmith-to-deliver-premium-products-for-its-business-clients-and-employees/,"<p>Rakuten Group is well known for operating one of the largest online shopping malls in Japan. The company has 70+ businesses in fields such as e-commerce, travel, digital content, fintech, communications and more.</p><p>Adopting new technologies to push the frontiers of what&#x2019;s possible is in the DNA of</p>",[{'name': 'LangChain'}],"Wed, 14 Feb 2024 20:20:31 GMT"
1886,How Dataherald Makes Natural Language to SQL Easy,https://blog.langchain.dev/dataherald/,"<p><strong>Editor&apos;s Note: we&apos;re excited to feature this guest post from the </strong><a href=\"https://www.dataherald.com/?ref=blog.langchain.dev\"><strong>Dataherald</strong></a><strong> team. Text-to-SQL is a HUGE use case, and Dataherald is the open-source leader in the space. This is a great look behind the curtains to see what makes it tick.</strong></p><p>When ChatGPT came out</p>",[{'name': 'LangChain'}],"Wed, 14 Feb 2024 17:09:09 GMT"
1887,Plan-and-Execute Agents,https://blog.langchain.dev/planning-agents/,"Plan and execute agents promise faster, cheaper, and more performant task execution over previous agent designs. Learn how to build 3 types of planning agents in LangGraph in this post.",[{'name': 'LangChain'}],"Tue, 13 Feb 2024 16:35:24 GMT"
1888,"BCG X Releases AgentKit, a Full-Stack Starter Kit for Building Constrained Agents",https://blog.langchain.dev/bcg-x-releases-agentkit-a-full-stack-starter-kit-for-building-constrained-agents/,"<p><strong>Editor&apos;s Note: We&apos;re very excited to share this work by BCG. We&apos;ve worked closely with the BCG over the past year to help support companies bring GenAI initiatives into production. We were intrigued to hear about their AgentKit platform, and once we got a</strong></p>",[{'name': 'LangChain'}],"Mon, 12 Feb 2024 16:38:19 GMT"
1889,[Week of 2/5] LangChain Release Notes,https://blog.langchain.dev/week-of-2-5-24-langchain-release-notes/,"<p>We&#x2019;ve been busy shipping &#x1f6a2;! Next Thursday Feb 15th at noon PT, LangChain Co-founder Ankush Gola will host a webinar to share some exciting updates on LangSmith, and we promise everyone will have access to the platform by then! Sign up <a href=\"https://www.crowdcast.io/c/langsmith?ref=blog.langchain.dev\" rel=\"noreferrer\">here</a> to join and ask questions about</p>",[{'name': 'LangChain'}],"Fri, 09 Feb 2024 18:54:56 GMT"
1890,LangChain Partners with CommandBar on their Copilot User Assistant,https://blog.langchain.dev/langchain-partners-with-commandbar-on-their-copilot-user-assistant/,"<p><a href=\"https://commandbar.com/?ref=blog.langchain.dev\"><u>CommandBar</u></a> is a user assistance platform that helps software companies make their products easy to use by capturing and predicting user intent, and then delivering personalized in-product help. CommandBar&#x2019;s Copilot widget, which companies embed into their applications, goes beyond a typical chatbot. It can answer user questions, trigger</p>",[{'name': 'LangChain'}],"Thu, 08 Feb 2024 21:10:49 GMT"
1891,Human-in-the-loop with OpenGPTs and LangGraph,https://blog.langchain.dev/human-in-the-loop-with-opengpts-and-langgraph/,"<p><strong>TLDR; Today we&#x2019;re launching two &#x201c;human in the loop&#x201d; features in OpenGPTs, Interrupt and Authorize, both powered by LangGraph.</strong></p><p>We&apos;ve recently launched LangGraph, a library to help developers build multi-actor, multi-step, stateful LLM applications. That&apos;s a lot words packed into a short</p>",[{'name': 'Nuno Campos'}],"Thu, 08 Feb 2024 16:56:41 GMT"
1892,How open source voting machines could boost trust in US elections,https://www.technologyreview.com/2024/03/07/1089524/open-source-voting-machines-us-elections/,"While the vendors pitched their latest voting machines in Concord, New Hampshire, this past August, the election officials in the room g­­asped. They whispered, “No way.” They nodded their heads and filled out the scorecards in their laps. Interrupting if they had to, they asked every kind of question: How much does the new scanner&#8230;",[{'name': 'Spenser Mestel'}],"Thu, 07 Mar 2024 16:36:03 +0000"
1893,"The Download: hydropower’s rocky path ahead, and how to reverse falling birth rates",https://www.technologyreview.com/2024/03/07/1089593/the-download-hydropowers-rocky-path-ahead-and-how-to-reverse-falling-birth-rates/,"This is today&#8217;s edition of The Download, our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology. Emissions hit a record high in 2023. Blame hydropower. Hydropower is one of the world’s largest sources of renewable electricity. But last year, weather conditions caused hydropower to fall short in a major&#8230;",[{'name': 'Rhiannon Williams'}],"Thu, 07 Mar 2024 13:10:00 +0000"
1894,Emissions hit a record high in 2023. Blame hydropower.,https://www.technologyreview.com/2024/03/07/1089585/hydropower-trouble-droughts/,"This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here. Hydropower is a staple of clean energy—the modern version has been around for over a century, and it’s one of the world’s largest sources of renewable electricity. But last year, weather conditions&#8230;",[{'name': 'Casey Crownhart'}],"Thu, 07 Mar 2024 11:00:00 +0000"
1895,"The Download: AI comics, and US tensions with China over EVs",https://www.technologyreview.com/2024/03/06/1089507/the-download-ai-comics-and-us-tensions-with-china-over-evs/,"This is today&#8217;s edition of The Download, our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology. I used generative AI to turn my story into a comic—and you can too —Will Douglas Heaven Thirteen years ago, as an assignment for a journalism class, I wrote a stupid short story&#8230;",[{'name': 'Rhiannon Williams'}],"Wed, 06 Mar 2024 13:10:00 +0000"
1896,Chinese EVs have entered center stage in US-China tensions,https://www.technologyreview.com/2024/03/06/1089487/chinese-evs-security-threat-politics/,"This story first appeared in China Report, MIT Technology Review’s newsletter about technology in China. Sign up to receive it in your inbox every Tuesday. So far, electric vehicles have mostly been discussed in the US through a scientific, economic, or environmental lens. But all of a sudden, they have become highly political.  Last Thursday, the Biden&#8230;",[{'name': 'Zeyi Yang'}],"Wed, 06 Mar 2024 11:00:00 +0000"
1897,I used generative AI to turn my story into a comic—and you can too,https://www.technologyreview.com/2024/03/05/1089458/generative-ai-turn-my-story-into-comic-images-lore-machine/,"Thirteen years ago, as an assignment for a journalism class, I wrote a stupid short story about a man who eats luxury cat food. This morning, I sat and watched as a generative AI platform called Lore Machine brought my weird words to life. I fed my story into a text box and got this&#8230;",[{'name': 'Will Douglas Heaven'}],"Tue, 05 Mar 2024 14:01:11 +0000"
1898,"The Download: rise of the robots, and what organoids can teach us",https://www.technologyreview.com/2024/03/05/1089451/the-download-rise-of-the-robots-and-what-organoids-can-teach-us/,"This is today&#8217;s edition of The Download, our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology. The robots are coming. And that’s a good thing. —This is an excerpt from a new book, The Heart and the Chip: Our Bright Future with Robots, by MIT CSAIL director Daniela Rus&#160;&#8230;",[{'name': 'Rhiannon Williams'}],"Tue, 05 Mar 2024 13:10:00 +0000"
1899,Nobody knows how AI works,https://www.technologyreview.com/2024/03/05/1089449/nobody-knows-how-ai-works/,"This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. I’ve been experimenting with using AI assistants in my day-to-day work. The biggest obstacle to their being useful is they often get things blatantly wrong. In one case, I used an&#8230;",[{'name': 'Melissa Heikkilä'}],"Tue, 05 Mar 2024 10:31:28 +0000"
1900,The robots are coming. And that’s a good thing.,https://www.technologyreview.com/2024/03/05/1087646/the-robots-are-coming-and-thats-a-good-thing-2/,"In this excerpt from the new book, The Heart and the Chip: Our Bright Future with Robots, CSAIL Director Daniela Rus explores how robots can extend the reach of human capabilities. Years ago, I befriended the biologist Roger Payne at a meeting of MacArthur Foundation fellows. Roger, who died in 2023, was best known for&#8230;","[{'name': 'Daniela Rus, Gregory Mone'}]","Tue, 05 Mar 2024 05:01:00 +0000"
1901,Roundtables: The AI Economy,https://www.technologyreview.com/2024/03/04/1088983/roundtables-the-ai-economy/,"Recorded on August 10, 2023 The AI Economy Speakers: Mat Honan, Editor in chief and David Rotman, Editor at large There’s no doubt that generative AI will impact the economy—but how, exactly, remains an open question. Despite fears that these AI tools will upend workers and exacerbate wealth inequality, early evidence suggests the technology could&#8230;",[{'name': 'MIT Technology Review'}],"Mon, 04 Mar 2024 18:12:30 +0000"
1902,Robot trained to read braille at twice the speed of humans,https://www.sciencedaily.com/releases/2024/01/240129122413.htm,Researchers have developed a robotic sensor that incorporates artificial intelligence techniques to read braille at speeds roughly double that of most human readers.,[],"Mon, 29 Jan 2024 12:24:13 EST"
1903,A long-lasting neural probe,https://www.sciencedaily.com/releases/2024/01/240126171626.htm,An interdisciplinary team of researchers has developed a soft implantable device with dozens of sensors that can record single-neuron activity in the brain stably for months.,[],"Fri, 26 Jan 2024 17:16:26 EST"
1904,Transparent brain implant can read deep neural activity from the surface,https://www.sciencedaily.com/releases/2024/01/240111113149.htm,"Researchers have developed a neural implant that provides information about activity deep inside the brain while sitting on its surface. The implant is made up of a thin, transparent and flexible polymer strip that is packed with a dense array of graphene electrodes. The technology, tested in transgenic mice, brings the researchers a step closer to building a minimally invasive brain-computer interface (BCI) that provides high-resolution data about deep neural activity by using recordings from the brain surface.",[],"Thu, 11 Jan 2024 11:31:49 EST"
1905,"Cognitive strategies for augmenting the body with a wearable, robotic arm",https://www.sciencedaily.com/releases/2023/12/231213143711.htm,"Scientists show that breathing may be used to control a wearable extra robotic arm in healthy individuals, without hindering control of other parts of the body.",[],"Wed, 13 Dec 2023 14:37:11 EST"
1906,Deep neural networks show promise as models of human hearing,https://www.sciencedaily.com/releases/2023/12/231213143706.htm,"In the largest study yet of deep neural networks trained to perform auditory tasks, researchers found most of these models generate internal representations that share properties of representations seen in the human brain when people are listening to the same sounds.",[],"Wed, 13 Dec 2023 14:37:06 EST"
1907,Artificial intelligence makes gripping more intuitive,https://www.sciencedaily.com/releases/2023/12/231205144408.htm,Artificial hands can be operated via app or with sensors placed in the muscles of the forearm. New research shows: a better understanding of muscle activity patterns in the forearm supports a more intuitive and natural control of artificial limbs. This requires a network of 128 sensors and artificial intelligence based techniques.,[],"Tue, 05 Dec 2023 14:44:08 EST"
1908,Scientists use A.I.-generated images to map visual functions in the brain,https://www.sciencedaily.com/releases/2023/11/231130145431.htm,Researchers have demonstrated the use of AI-selected natural images and AI-generated synthetic images as neuroscientific tools for probing the visual processing areas of the brain. The goal is to apply a data-driven approach to understand how vision is organized while potentially removing biases that may arise when looking at responses to a more limited set of researcher-selected images.,[],"Thu, 30 Nov 2023 14:54:31 EST"
1909,Brain implant may enable communication from thoughts alone,https://www.sciencedaily.com/releases/2023/11/231106134844.htm,"A speech prosthetic developed by a collaborative team of neuroscientists, neurosurgeons and engineers can translate a person's brain signals into what they're trying to say. The new technology might one day help people unable to talk due to neurological disorders regain the ability to communicate through a brain-computer interface.",[],"Mon, 06 Nov 2023 13:48:44 EST"
1910,Nanowire 'brain' network learns and remembers 'on the fly',https://www.sciencedaily.com/releases/2023/11/231101134804.htm,"Like a collection of 'Pick Up Sticks', this neural network has passed a critical step for developing machine intelligence. For the first time, a physical neural network has successfully been shown to learn and remember 'on the fly', in a way inspired by and similar to how the brain's neurons work. The result opens a pathway for developing efficient and low-energy machine intelligence for more complex, real-world learning and memory tasks.",[],"Wed, 01 Nov 2023 13:48:04 EDT"
1911,Adaptive optical neural network connects thousands of artificial neurons,https://www.sciencedaily.com/releases/2023/10/231023124404.htm,"Physicists working with computer specialists have developed a so-called event-based architecture, using photonic processors. In a similar way to the brain, this makes possible the continuous adaptation of the connections within the neural network.",[],"Mon, 23 Oct 2023 12:44:04 EDT"
1912,"Robotic prosthetic ankles improve 'natural' movement, stability",https://www.sciencedaily.com/releases/2023/10/231018161856.htm,"Robotic prosthetic ankles that are controlled by nerve impulses allow amputees to move more 'naturally,' improving their stability, according to a new study.",[],"Wed, 18 Oct 2023 16:18:56 EDT"
1913,Efficient training for artificial intelligence,https://www.sciencedaily.com/releases/2023/09/230922110755.htm,New physics-based self-learning machines could replace the current artificial neural networks and save energy.,[],"Fri, 22 Sep 2023 11:07:55 EDT"
1914,Evolution wired human brains to act like supercomputers,https://www.sciencedaily.com/releases/2023/09/230914103332.htm,"Scientists have confirmed that human brains are naturally wired to perform advanced calculations, much like a high-powered computer, to make sense of the world through a process known as Bayesian inference.",[],"Thu, 14 Sep 2023 10:33:32 EDT"
1915,Brain signals transformed into speech through implants and AI,https://www.sciencedaily.com/releases/2023/08/230828130347.htm,"Researchers have succeeded in transforming brain signals into audible speech. By decoding signals from the brain through a combination of implants and AI, they were able to predict the words people wanted to say with an accuracy of 92 to 100%.",[],"Mon, 28 Aug 2023 13:03:47 EDT"
1916,How artificial intelligence gave a paralyzed woman her voice back,https://www.sciencedaily.com/releases/2023/08/230823122530.htm,Researchers have developed a brain-computer interface (BCI) that has enabled a woman with severe paralysis from a brainstem stroke to speak through a digital avatar.,[],"Wed, 23 Aug 2023 12:25:30 EDT"
1917,Robotic exoskeletons and neurorehabilitation for acquired brain injury: Determining the potential for recovery of overground walking,https://www.sciencedaily.com/releases/2023/08/230815131855.htm,"Developing a framework for future research requires a comprehensive approach based on diagnosis, stage of recovery, and domain.",[],"Tue, 15 Aug 2023 13:18:55 EDT"
1918,Modified virtual reality tech can measure brain activity,https://www.sciencedaily.com/releases/2023/08/230804140510.htm,The research team at The University of Texas at Austin created a noninvasive electroencephalogram (EEG) sensor that they installed in a Meta VR headset that can be worn comfortably for long periods. The EEG measures the brain's electrical activity during the immersive VR interactions.,[],"Fri, 04 Aug 2023 14:05:10 EDT"
1919,Future AI algorithms have potential to learn like humans,https://www.sciencedaily.com/releases/2023/07/230720124956.htm,"Memories can be as tricky to hold onto for machines as they can be for humans. To help understand why artificial agents develop holes in their own cognitive processes, electrical engineers have analyzed how much a process called 'continual learning' impacts their overall performance.",[],"Thu, 20 Jul 2023 12:49:56 EDT"
1920,AI-guided brain stimulation aids memory in traumatic brain injury,https://www.sciencedaily.com/releases/2023/07/230718164310.htm,"AI-guided electrical stimulation in the brains of patients with traumatic brain injury improved memory, a collaborative new study shows. This builds on previous research involving epilepsy patients without traumatic brain injury. Brain injuries can result in profound memory loss, and the current study provides a proof-of-concept for future AI-guided brain stimulation therapies.",[],"Tue, 18 Jul 2023 16:43:10 EDT"
1921,Surgical and engineering innovations enable unprecedented control over every finger of a bionic hand,https://www.sciencedaily.com/releases/2023/07/230712165138.htm,"For the first time, a person with an arm amputation can manipulate each finger of a bionic hand as if it was his own. Thanks to revolutionary surgical and engineering advancements that seamlessly merge humans with machines, this breakthrough offers new hope and possibilities for people with amputations worldwide. A study presents the first documented case of an individual whose body was surgically modified to incorporate implanted sensors and a skeletal implant. A.I. algorithms then translated the user's intentions into movement of the prosthesis.",[],"Wed, 12 Jul 2023 16:51:38 EDT"
1922,A varied life boosts the brain's functional networks,https://www.sciencedaily.com/releases/2023/07/230711133136.htm,"That experiences leave their trace in the connectivity of the brain has been known for a while, but a pioneering study now shows how massive these effects really are. The findings in mice provide unprecedented insights into the complexity of large-scale neural networks and brain plasticity. Moreover, they could pave the way for new brain-inspired artificial intelligence methods.",[],"Tue, 11 Jul 2023 13:31:36 EDT"
1923,Growing bio-inspired polymer brains for artificial neural networks,https://www.sciencedaily.com/releases/2023/07/230705105850.htm,"A new method for connecting neurons in neuromorphic wetware has been developed. The wetware comprises conductive polymer wires grown in a three-dimensional configuration, done by applying square-wave voltage to electrodes submerged in a precursor solution. The voltage can modify wire conductance, allowing the network to be trained. This fabricated network is able to perform unsupervised Hebbian learning and spike-based learning.",[],"Wed, 05 Jul 2023 10:58:50 EDT"
1924,Robotic glove that 'feels' lends a 'hand'  to relearn playing piano after a stroke,https://www.sciencedaily.com/releases/2023/06/230630130152.htm,"A new soft robotic glove is lending a 'hand' and providing hope to piano players who have suffered a disabling stroke or other neurotrauma. Combining flexible tactile sensors, soft actuators and AI, this robotic glove is the first to 'feel' the difference between correct and incorrect versions of the same song and to combine these features into a single hand exoskeleton. Unlike prior exoskeletons, this new technology provides precise force and guidance in recovering the fine finger movements required for piano playing and other complex tasks.",[],"Fri, 30 Jun 2023 13:01:52 EDT"
1925,How the brain processes numbers -- New procedure improves measurement of human brain activity,https://www.sciencedaily.com/releases/2023/06/230627225142.htm,"Measuring human brain activity down to the cellular level: until now, this has been possible only to a limited extent. With a new approach it will now be much easier. The method relies on microelectrodes along with the support of brain tumor patients, who participate in studies while undergoing 'awake' brain surgery. This enabled the team to identify how our brain processes numbers.",[],"Tue, 27 Jun 2023 22:51:42 EDT"
1926,Brain activity decoder can reveal stories in people's minds,https://www.sciencedaily.com/releases/2023/05/230501114006.htm,"A new AI-based system called a semantic decoder can translate a person's brain activity -- while listening to a story or silently imagining telling a story -- into a continuous stream of text. Unlike other thought decoding systems in development, this system does not require subjects to have surgical implants, making the process noninvasive.",[],"Mon, 01 May 2023 11:40:06 EDT"
1927,Nanowire networks learn and remember like a human brain,https://www.sciencedaily.com/releases/2023/04/230421195040.htm,Scientists have demonstrated nanowire networks can exhibit both short- and long-term memory like the human brain.,[],"Fri, 21 Apr 2023 19:50:40 EDT"
1928,Table tennis brain teaser: Playing against robots makes our brains work harder,https://www.sciencedaily.com/releases/2023/04/230410132157.htm,Brain scans taken during table tennis reveal differences in how we respond to human versus machine opponents.,[],"Mon, 10 Apr 2023 13:21:57 EDT"
1929,Mind-control robots a reality?,https://www.sciencedaily.com/releases/2023/03/230320102104.htm,"Researchers have developed biosensor technology that will allow you to operate devices, such as robots and machines, solely through thought control.",[],"Mon, 20 Mar 2023 10:21:04 EDT"
1930,Will future computers run on human brain cells?,https://www.sciencedaily.com/releases/2023/02/230228075739.htm,"A 'biocomputer' powered by human brain cells could be developed within our lifetime, according to researchers who expect such technology to exponentially expand the capabilities of modern computing and create novel fields of study.",[],"Tue, 28 Feb 2023 07:57:39 EST"
1931,Computational modeling sheds light on human cognition and the origins of brain disorders,https://www.sciencedaily.com/releases/2023/02/230215143606.htm,Researchers used computational modeling to uncover mutations in the human genome that likely influenced the evolution of human cognition. This groundbreaking research in human genomics could lead to a better understanding of human health and the discovery of novel treatments for complex brain disorders. The study is to be published in Science Advances.,[],"Wed, 15 Feb 2023 14:36:06 EST"
1932,A neuro-chip to manage brain disorders,https://www.sciencedaily.com/releases/2023/01/230130103022.htm,"Researchers have combined low-power chip design, machine learning algorithms, and soft implantable electrodes to produce a neural interface that can identify and suppress symptoms of various neurological disorders.",[],"Mon, 30 Jan 2023 10:30:22 EST"
1933,Clinical trial results indicate low rate of adverse events associated with implanted brain computer interface,https://www.sciencedaily.com/releases/2023/01/230114142750.htm,"There were no safety events that required removal of the device, no infections of the brain or nervous system, and no adverse events resulting in permanently increased disability related to the investigational device.",[],"Sat, 14 Jan 2023 14:27:50 EST"
1934,"A soft, stimulating scaffold supports brain cell development ex vivo",https://www.sciencedaily.com/releases/2023/01/230105151247.htm,"Brain-computer interface companies like Neuralink are in the news a lot these days for their potential to revolutionize how humans interact with machines, but electrodes are not the most brain-friendly materials -- they're hard and stiff, while brains are soft and squishy, which limits their efficacy and increases the risk of damaging brain tissue.  A new hydrogel-based electrode developed at the Wyss Institute solves that problem by providing a tunable, conductive scaffold that human neurons and other cell types feel right at home in. Not only does the scaffold mimic the soft, porous conditions of brain tissue, it supported the growth and differentiation of human neural progenitor cells (NPCs) into multiple different brain cell types for up to 12 weeks. The achievement is reported in Advanced Healthcare Materials. Not only can the new electrode be used to study the formation of human neural networks in vitro, it could enable the creation of implantable devices that more seamlessly integrate with a patient's brain tissue, improving performance and decreasing risk of injury.",[],"Thu, 05 Jan 2023 15:12:47 EST"
1935,Human brain organoids implanted into mouse cortex respond to visual stimuli for first time,https://www.sciencedaily.com/releases/2022/12/221229115657.htm,"A team of engineers and neuroscientists has demonstrated for the first time that human brain organoids implanted in mice have established functional connectivity to the animals' cortex and responded to external sensory stimuli. The implanted organoids reacted to visual stimuli in the same way as surrounding tissues, an observation that researchers were able to make in real time over several months thanks to an innovative experimental setup that combines transparent graphene microelectrode arrays and two-photon imaging.",[],"Thu, 29 Dec 2022 11:56:57 EST"
1936,Artificial neural networks learn better when they spend time not learning at all,https://www.sciencedaily.com/releases/2022/11/221118160305.htm,"Researchers discuss how mimicking sleep patterns of the human brain in artificial neural networks may help mitigate the threat of catastrophic forgetting in the latter, boosting their utility across a spectrum of research interests.",[],"Fri, 18 Nov 2022 16:03:05 EST"
1937,"With training, people in mind-controlled wheelchairs can navigate normal, cluttered spaces",https://www.sciencedaily.com/releases/2022/11/221118114824.htm,"A mind-controlled wheelchair can help a paralyzed person gain new mobility by translating users' thoughts into mechanical commands. Researchers now demonstrate that tetraplegic users can operate mind-controlled wheelchairs in a natural, cluttered environment after training for an extended period.",[],"Fri, 18 Nov 2022 11:48:24 EST"
1938,Engineers light the way to nerve-operated prosthetics of the future,https://www.sciencedaily.com/releases/2022/10/221026103143.htm,"A multi-disciplinary team has found a way to convert nerve impulses into light, opening the way for more scalable neural implants.",[],"Wed, 26 Oct 2022 10:31:43 EDT"
1939,How the brain develops: A new way to shed light on cognition,https://www.sciencedaily.com/releases/2022/09/220920115627.htm,Researchers introduce a new neurocomputational model of the human brain that could bridge the gap in understanding AI and the biological mechanisms underlying mental disorders.,[],"Tue, 20 Sep 2022 11:56:27 EDT"
1940,How the sounds we hear help us predict how things feel,https://www.sciencedaily.com/releases/2022/08/220824102948.htm,"Researchers have made an important discovery about the way our brains process the sensations of sound and touch. The new study reveals how the brain's different sensory systems are all closely interconnected -- with regions that respond to touch also involved when we listen to sounds associated with touching objects (for example the sound of typing on a keyboard or crushing paper). It is hoped that understanding this key area of brain function may in future help people who are neurodiverse, or with conditions such as schizophrenia or anxiety. And it could lead to developments in brain-inspired computing and AI.",[],"Wed, 24 Aug 2022 10:29:48 EDT"
1941,Realistic computer models of brain cells,https://www.sciencedaily.com/releases/2022/08/220809141159.htm,Investigators have created the most bio-realistic and complex computer models of individual brain cells -- in unparalleled quantity. Their research details how these models could one day answer questions about neurological disorders -- and even human intellect -- that aren't possible to explore through biological experiments.,[],"Tue, 09 Aug 2022 14:11:59 EDT"
1942,Advancing dynamic brain imaging with AI,https://www.sciencedaily.com/releases/2022/08/220801133143.htm,"New research introduces a novel, AI-based dynamic brain imaging technology alternative which could map out rapidly changing electrical activity in the brain with high speed, high resolution, and low cost.",[],"Mon, 01 Aug 2022 13:31:43 EDT"
1943,A new model sheds light on how we learn motor skills,https://www.sciencedaily.com/releases/2022/06/220624092344.htm,"Researchers have developed a mathematical model of motor learning that reflects the motor learning process in the human brain. Their findings suggest that motor exploration -- that is, increased variability in movements -- is important when learning a new task. These results may lead to improved motor rehabilitation in patients after injury or disease.",[],"Fri, 24 Jun 2022 09:23:44 EDT"
1944,How the brain interprets motion while in motion,https://www.sciencedaily.com/releases/2022/06/220621155127.htm,New research provides insights into a neural mechanism involved in how the brain interprets sensory information. The research may have applications for treating brain disorders and designing artificial intelligence.,[],"Tue, 21 Jun 2022 15:51:27 EDT"
1945,Learning and remembering movement,https://www.sciencedaily.com/releases/2022/06/220609173627.htm,"Researchers examining the brain at a single-neuron level found that computation happens not just in the interaction between neurons, but within each individual neuron. Each of these cells, it turns out, is not a simple switch, but a complicated calculating machine. This discovery promises changes not only to our understanding of how the brain works, but better understanding of conditions ranging from Parkinson's disease to autism. The findings are also expected to advance machine learning, offering inspiration for new architectures.",[],"Thu, 09 Jun 2022 17:36:27 EDT"
1946,Researchers investigate the links between facial recognition and Alzheimer's disease,https://www.sciencedaily.com/releases/2022/05/220531102702.htm,"In recent years Alzheimer's disease has been on the rise throughout the world and is rarely diagnosed at an early stage when it can still be effectively controlled. Using artificial intelligence, researchers conducted a study to identify whether human-computer interfaces could be adapted for people with memory impairments to recognize a visible object in front of them.",[],"Tue, 31 May 2022 10:27:02 EDT"
1947,Significant energy savings using neuromorphic hardware,https://www.sciencedaily.com/releases/2022/05/220524100612.htm,New research illustrates neuromorphic technology is up to sixteen times more energy-efficient for large deep learning networks than other AI systems.,[],"Tue, 24 May 2022 10:06:12 EDT"
1948,Neuromorphic memory device simulates neurons and synapses,https://www.sciencedaily.com/releases/2022/05/220520132904.htm,"Researchers have reported a nano-sized neuromorphic memory device that emulates neurons and synapses simultaneously in a unit cell, another step toward completing the goal of neuromorphic computing designed to rigorously mimic the human brain with semiconductor devices.",[],"Fri, 20 May 2022 13:29:04 EDT"
1949,Component for brain-inspired computing,https://www.sciencedaily.com/releases/2022/05/220518130721.htm,Researchers have developed a new material for an electronic component that can be used in a wider range of applications than its predecessors. Such components will help create electronic circuits that emulate the human brain and that are more efficient at performing machine-learning tasks.,[],"Wed, 18 May 2022 13:07:21 EDT"
1950,Energy-efficient AI hardware technology via a brain-inspired stashing system?,https://www.sciencedaily.com/releases/2022/05/220517210435.htm,"Researchers have proposed a novel system inspired by the neuromodulation of the brain, referred to as a 'stashing system,' that requires less energy consumption. Computer scientists have now developed a technology that can efficiently handle mathematical operations for artificial intelligence by imitating the continuous changes in the topology of the neural network according to the situation.",[],"Tue, 17 May 2022 21:04:35 EDT"
1951,'Nanomagnetic' computing can provide low-energy AI,https://www.sciencedaily.com/releases/2022/05/220505114646.htm,Researchers have shown it is possible to perform artificial intelligence using tiny nanomagnets that interact like neurons in the brain.,[],"Thu, 05 May 2022 11:46:46 EDT"
1952,Engineers get under the skin of ionic skin,https://www.sciencedaily.com/releases/2022/04/220428142837.htm,"In the quest to build smart skin that mimics the sensing capabilities of natural skin, ionic skins have shown significant advantages. They're made of flexible, biocompatible hydrogels that use ions to carry an electrical charge. These hydrogels can generate voltages when touched, but scientists did not clearly understand how -- until a team of researchers devised a unique experiment.",[],"Thu, 28 Apr 2022 14:28:37 EDT"
1953,The ethics of research on 'conscious' artificial brains,https://www.sciencedaily.com/releases/2022/04/220412095354.htm,Authors lay out an ethical framework that assumes brain organoids already have consciousness. The paper argues that this framework is not exclusive to brain organoids and can be applied to anything that is perceived to hold consciousness.,[],"Tue, 12 Apr 2022 09:53:54 EDT"
1954,Rational neural network advances machine-human discovery,https://www.sciencedaily.com/releases/2022/04/220405171749.htm,"Math is the language of the physical world, and some see mathematical patterns everywhere: in weather, in the way soundwaves move, and even in the spots or stripes zebra fish develop in embryos.",[],"Tue, 05 Apr 2022 17:17:49 EDT"
1955,Honey holds potential for making brain-like computer chips,https://www.sciencedaily.com/releases/2022/04/220405084610.htm,"Honey might be a sweet solution for developing environmentally friendly components for neuromorphic computers, systems designed to mimic the neurons and synapses found in the human brain.",[],"Tue, 05 Apr 2022 08:46:10 EDT"
1956,A new brain-computer interface with a flexible backing,https://www.sciencedaily.com/releases/2022/03/220315165029.htm,"Engineering researchers have invented an advanced brain-computer interface with a flexible and moldable backing and penetrating microneedles. Adding a flexible backing to this kind of brain-computer interface allows the device to more evenly conform to the brain's complex curved surface and to more uniformly distribute the microneedles that pierce the cortex. The microneedles, which are 10 times thinner than the human hair, protrude from the flexible backing, penetrate the surface of the brain tissue without piercing surface venules, and record signals from nearby nerve cells evenly across a wide area of the cortex. This novel brain-computer interface has thus far been tested in rodents.",[],"Tue, 15 Mar 2022 16:50:29 EDT"
1957,Entanglement unlocks scaling for quantum machine learning,https://www.sciencedaily.com/releases/2022/02/220224125214.htm,The field of machine learning on quantum computers got a boost from new research removing a potential roadblock to the practical implementation of quantum neural networks.,[],"Thu, 24 Feb 2022 12:52:14 EST"
1958,Can machine-learning models overcome biased datasets?,https://www.sciencedaily.com/releases/2022/02/220221115403.htm,"Researchers have applied the tools of neuroscience to study when and how an artificial neural network can overcome bias in a dataset. They found that data diversity, not dataset size, is key and that the emergence of certain types of neurons during training plays a major role in how well a neural network is able to overcome dataset bias.",[],"Mon, 21 Feb 2022 11:54:03 EST"
1959,Neuroscientists use deep learning model to simulate brain topography,https://www.sciencedaily.com/releases/2022/02/220203161125.htm,"A more accurate model of the visual system may help neuroscientists and clinicians develop better treatments for alexia, prosopagnosia and agnosia.",[],"Thu, 03 Feb 2022 16:11:25 EST"
1960,The brain’s secret to life-long learning can now come as hardware for artificial intelligence,https://www.sciencedaily.com/releases/2022/02/220203160544.htm,"As companies use more and more data to improve how AI recognizes images, learns languages and carries out other complex tasks, a recent article shows a way that computer chips could dynamically rewire themselves to take in new data like the brain does, helping AI to keep learning over time.",[],"Thu, 03 Feb 2022 16:05:44 EST"
1961,2D materials could be used to simulate brain synapses in computers,https://www.sciencedaily.com/releases/2022/01/220128100738.htm,"Computers could mimic neural networks in the brain -- and be much more energy efficient -- with a new computer component that mimics how the brain works by acting like a synaptic cell. It's called an electrochemical random access memory (ECRAM), and researchers have developed materials that offer a commercially-viable way to build these components.",[],"Fri, 28 Jan 2022 10:07:38 EST"
1962,Generate Stunning Images with Stable Diffusion XL on the NVIDIA AI Inference Platform,https://developer.nvidia.com/blog/generate-stunning-images-with-stable-diffusion-xl-on-the-nvidia-ai-inference-platform/,<img alt=\"Four images of products against enhanced backgrounds.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/sdxl-featured-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"sdxl-featured\" width=\"768\" />Diffusion models are transforming creative workflows across industries. These models generate stunning images based on simple text or image inputs by...,[{'name': 'Amr Elmeleegy'}],2024-03-07T19:05:46Z
1963,"Make the Most of NVIDIA GTC 2024 with In-Person, Hands-On Learning",https://developer.nvidia.com/blog/make-the-most-of-nvidia-gtc-2024-with-in-person-hands-on-learning/,"<img alt=\"Picture of people at Connect with the Experts hall at NVIDIA GTC.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/nvidia-gtc-developer-days-featured-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"nvidia-gtc-developer-days-featured\" width=\"768\" />We are so excited to be back in person at GTC this year at the San Jose Convention Center. With thousands of developers, industry leaders, researchers, and...",[{'name': 'Richard Kerris'}],2024-03-07T18:08:30Z
1964,Simplifying Cumulus Linux Migrations,https://developer.nvidia.com/blog/simplifying-cumulus-linux-migrations/,<img alt=\"Decorative image of a web of green light on a dark background.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/03/simplifying-cl-migration-featured-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"simplifying-cl-migration-featured\" width=\"768\" />Migrating between major versions of software can present several challenges to the infrastructure management teams: Data format changes Feature deprecations...,[{'name': 'Krishna Vasudevan'}],2024-03-07T17:55:29Z
1965,Turning Machine Learning to Federated Learning in Minutes with NVIDIA FLARE 2.4,https://developer.nvidia.com/blog/turning-machine-learning-to-federated-learning-in-minutes-with-nvidia-flare-2-4/,"<img alt=\"Decorative collage of a sun with flaring light and federated learning models.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/03/nvflare-2-4-featured-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"nvflare-2-4-featured\" width=\"768\" />Federated learning (FL) is experiencing accelerated adoption due to its decentralized, privacy-preserving nature. In sectors such as healthcare and financial...",[{'name': 'Chester Chen'}],2024-03-07T00:39:33Z
1966,Featured Smart Spaces Sessions at NVIDIA GTC 2024,https://nvda.ws/3URP0b1#new_tab,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/03/smart-traffic-1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"smart-traffic (1)\" width=\"768\" />From cities and airports to Olympic Stadiums, AI is transforming public spaces into safer, smarter, and more sustainable environments.",[{'name': 'Tanya Lenz'}],2024-03-07T00:19:10Z
1967,CUDA Toolkit 12.4 Enhances Support for NVIDIA Grace Hopper and Confidential Computing,https://developer.nvidia.com/blog/cuda-toolkit-12-4-enhances-support-for-nvidia-grace-hopper-and-confidential-computing/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/03/cuda-abstract-graphic-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"cuda-abstract-graphic\" width=\"768\" />The latest release of CUDA Toolkit, version 12.4, continues to push accelerated computing performance using the latest NVIDIA GPUs. This post explains the new...",[{'name': 'Rob Armstrong'}],2024-03-06T19:55:00Z
1968,How to Accelerate Quantitative Finance with ISO C++ Standard Parallelism,https://developer.nvidia.com/blog/how-to-accelerate-quantitative-finance-with-iso-c-standard-parallelism/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/graph-grid-background-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"graph-grid-background\" width=\"768\" />Quantitative finance libraries are software packages that consist of mathematical, statistical, and, more recently, machine learning models designed for use in...",[{'name': 'Ioana Boier'}],2024-03-06T19:00:00Z
1969,New Video: Pioneering Climate Tech and Mitigating the Impact of Natural Disasters,https://developer.nvidia.com/blog/new-video-pioneering-climate-tech-and-mitigating-the-impact-of-natural-disasters/,"<img alt=\"Photo of a flooded town.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/climate-tech-featured-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"climate-tech-featured\" width=\"768\" />In 2022, the city of Lismore, Australia bore the brunt of devastating floods, leaving over 3K homes damaged and communities shattered. With $6B in losses, this...",[{'name': 'Kristen Perez'}],2024-03-05T19:17:25Z
1970,Spotlight: Honeywell Accelerates Industrial Process Simulation with NVIDIA cuDSS,https://developer.nvidia.com/blog/spotlight-honeywell-accelerates-industrial-process-simulation-with-nvidia-cudss/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/03/honeywell-process-digital-twin-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"honeywell-process-digital-twin\" width=\"768\" />For over a decade, traditional industrial process modeling and simulation approaches have struggled to fully leverage multicore CPUs or acceleration devices to...",[{'name': 'Jeffrey Renfro'}],2024-03-05T19:00:00Z
1971,Solve Complex AI Tasks with Leaderboard-Topping Smaug 72B from NVIDIA AI Foundation Models,https://developer.nvidia.com/blog/solve-complex-ai-tasks-with-leaderboard-topping-smaug-72b-from-nvidia-ai-foundation-models/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/03/llm-chatbot-graphic-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"llm-chatbot-graphic\" width=\"768\" />This week’s model release features the NVIDIA-optimized language model Smaug 72B, which you can experience directly from your browser. NVIDIA AI Foundation...",[{'name': 'Chintan Patel'}],2024-03-04T21:22:47Z
1972,Featured Energy Sessions at NVIDIA GTC 2024,https://nvda.ws/49Sl1UF#new_tab,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/top-energy-sessions-gtc24-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"top-energy-sessions-gtc24\" width=\"768\" />Hear from ExxonMobil, Honeywell, Siemens Energy, and more as they explore AI and HPC innovation in oil, gas, power, and utilities.",[{'name': 'Tanya Lenz'}],2024-03-01T21:30:00Z
1973,Explainer: What Is Stream Processing?,https://nvda.ws/48AGiBw#new_tab,"<img alt=\"Decorative image of colorful pixels in a grid pattern.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"243\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Stream-Processing-in-Python.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"Stream-Processing-in-Python\" width=\"432\" />Stream processing is the continuous processing of new data events as they’re received. A lot of data is produced as a stream of events, for example financial...",[{'name': 'Michelle Horton'}],2024-03-01T20:00:00Z
1974,Top Telecom Sessions at NVIDIA GTC 2024,https://nvda.ws/3TeSV0q#new_tab,"<img alt=\"Image of a car with light streams connected to different towers along the street.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/top-telecom-sessions-gtc24-hero-image-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"top-telecom-sessions-gtc24-hero-image\" width=\"768\" />Hear from Amdocs, Indosat, KT, NTT, ServiceNow, Singtel, SoftBank, and Verizon, plus a special address from NVIDIA at GTC. Explore AI transforming customer...",[{'name': 'Ronnie Vasishta'}],2024-03-01T18:00:00Z
1975,Top Synthetic Data Generation Sessions at NVIDIA GTC 2024,https://nvda.ws/49QuDze#new_tab,"<img alt=\"Collage of four computer vision images.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/synthetic-data-webinar-thumbnail-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"synthetic-data-webinar-thumbnail\" width=\"768\" />Learn how synthetic data is supercharging 3D simulation and computer vision workflows, from visual inspection to autonomous machines.",[{'name': 'Nate Bradford'}],2024-02-29T23:31:18Z
1976,Video Series: Getting Started with Universal Scene Description (OpenUSD),https://nvda.ws/3P0Kxis#new_tab,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/cube-blocks-circle-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"cube-blocks-circle\" width=\"768\" />Gain a foundational understanding of USD, the open and extensible framework for creating, editing, querying, rendering, collaborating, and simulating within 3D...",[{'name': 'Nate Bradford'}],2024-02-29T22:00:00Z
1977,Scalable Federated Learning with NVIDIA FLARE for Enhanced LLM Performance,https://developer.nvidia.com/blog/scalable-federated-learning-with-nvidia-flare-for-enhanced-llm-performance/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"431\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/abstract-image-circle-with-rays-squares-768x431.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"abstract-image-circle-with-rays-squares\" width=\"768\" />In the ever-evolving landscape of large language models (LLMs), effective data management is a key challenge. Data is at the heart of model performance. While...",[{'name': 'Ziyue Xu'}],2024-02-29T21:00:00Z
1978,Event: Speech and Generative AI Developer Day at NVIDIA GTC 2024,https://nvda.ws/49yqIar#new_tab,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/generative-ai-graphic-gtc-logo-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"generative-ai-graphic-gtc-logo\" width=\"768\" />Learn how to build a RAG-powered application with a human voice interface at NVIDIA GTC 2024 Speech and Generative AI Developer Day.,[{'name': 'Tanya Lenz'}],2024-02-29T21:00:00Z
1979,Optimizing OpenFold Training for Drug Discovery,https://developer.nvidia.com/blog/optimizing-openfold-training-for-drug-discovery/,"<img alt=\"Decorative image of colorful protein structures.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/openfold-training-drug-discovery-featured-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"openfold-training-drug-discovery-featured\" width=\"768\" />Predicting 3D protein structures from amino acid sequences has been an important long-standing question in bioinformatics. In recent years, deep...",[{'name': 'Feiwen Zhu'}],2024-02-28T19:29:02Z
1980,Event: Cybersecurity Developer Day at NVIDIA GTC 2024,https://nvda.ws/3waljI7#new_tab,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/gtc24-spring-security-cyber-dev-day-nm-bleed-1920x1080-1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"gtc24-spring-security-cyber-dev-day-nm-bleed-1920x1080\" width=\"768\" />Join us on March 20 for Cybersecurity Developer Day at GTC to gain insights on leveraging generative AI for cyber defense.,[{'name': 'Michelle Horton'}],2024-02-28T17:00:00Z
1981,Unlock Your LLM Coding Potential with StarCoder2,https://developer.nvidia.com/blog/unlock-your-llm-coding-potential-with-starcoder2/,"<img alt=\"Illustration representing LLMs.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/llm-translation-e1709081778625-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"llm-translation\" width=\"768\" />Coding is essential in the digital age, but it can also be tedious and time-consuming. That's why many developers are looking for ways to automate and...",[{'name': 'Chia-Chih Chen'}],2024-02-28T14:00:00Z
1982,Top Public Sector Developer Sessions at NVIDIA GTC 2024,https://nvda.ws/48BsW80#new_tab,"<img alt=\"Split image of two landscapes with a mapping overlay.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/gtc24-public-sector-blog-1920x1080-1-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"gtc24-public-sector-blog-1920x1080\" width=\"768\" />Join experts from NVIDIA and the public sector industry to learn how cybersecurity, generative AI, digital twins, and more are impacting the way that government...",[{'name': 'Michelle Horton'}],2024-02-27T22:26:13Z
1983,Video: Build a RAG-Powered Chatbot in Five Minutes,https://developer.nvidia.com/blog/video-build-a-rag-powered-chatbot-in-five-minutes/,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/rag-chatbot-video-thumbnail-v2-1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"rag-chatbot-video-thumbnail-v2 (1)\" width=\"768\" />Retrieval-augmented generation (RAG) is exploding in popularity as a technique for boosting large language model (LLM) application performance. From highly...,[{'name': 'Jess Nguyen'}],2024-02-27T21:30:00Z
1984,"Unlock the Power of Small Language Model Phi-2 for Chat, Research, Coding, and More",https://developer.nvidia.com/blog/unlock-the-power-of-small-language-model-phi-2-for-chat-research-coding-and-more/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/five-icons-blue-background-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"five-icons-blue-background\" width=\"768\" />This week’s model release features the NVIDIA-optimized language model Phi-2, which can be used for a wide range of natural language processing (NLP) tasks....",[{'name': 'Chintan Patel'}],2024-02-27T18:00:39Z
1985,Detecting Real-Time Waste Contamination Using Edge Computing and Video Analytics,https://developer.nvidia.com/blog/detecting-real-time-waste-contamination-using-edge-computing-and-video-analytics/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/scene-with-waste-and-labels-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"scene-with-waste-and-labels\" width=\"768\" />The past few decades have witnessed a surge in rates of waste generation, closely linked to economic development and urbanization. This escalation in waste...",[{'name': 'Umair Iqbal'}],2024-02-26T21:00:00Z
1986,Developer Days at NVIDIA GTC 2024,https://nvda.ws/3UHD5fY#new_tab,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/fantastical-world-content-creation-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"fantastical-world-content-creation\" width=\"768\" />Connect with industry leaders, learn from technical experts, and collaborate with peers at NVIDIA GTC 2024 Developer Days.",[{'name': 'Tanya Lenz'}],2024-02-26T20:30:00Z
1987,Ray-Tracing Validation at the Driver Level,https://developer.nvidia.com/blog/ray-tracing-validation-at-the-driver-level/,"<img alt=\"Decorative image of code block with green lightbeams shining on a figure walking on a computer chip between monitors.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/DALL-E-RT-Validation-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"DALL-E RT Validation\" width=\"768\" />For developers working on Microsoft DirectX ray-tracing applications, ray-tracing validation is here to help you improve performance, find hard-to-debug issues,...",[{'name': 'Andrew Allan'}],2024-02-26T18:00:00Z
1988,Top Student Sessions at NVIDIA GTC 2024,https://nvda.ws/3IbDAre#new_tab,<img alt=\"Photo of students sitting together with a laptop.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/HER_GTC24_Students-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"HER_GTC24_Students\" width=\"768\" />Discover a wide variety of AI tools and resources designed to equip students with practical solutions for real-world problem-solving.&nbsp;Join experts from...,[{'name': 'Michelle Horton'}],2024-02-26T17:24:28Z
1989,Explainer: What Is Energy Efficiency?,https://developer.nvidia.com/blog/explainer-what-is-energy-efficiency/,"<img alt=\"Idealized photo of solar panels and wind turbines in the sunshine, with a city in the background.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/05/sustainable-computing-featured-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"sustainable-computing-featured\" width=\"768\" />Energy efficiency refers to a system or device’s ability to use as little energy as possible to perform a particular task or function within acceptable...",[{'name': 'Michelle Horton'}],2024-02-23T20:00:00Z
1990,Evaluating Retriever for Enterprise-Grade RAG,https://developer.nvidia.com/blog/evaluating-retriever-for-enterprise-grade-rag/,"<img alt=\"Illustration demonstrating RAG.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/RAG-Retriever-LLM-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"RAG-Retriever-LLM\" width=\"768\" />The conversation about designing and evaluating Retrieval-Augmented Generation (RAG) systems is a long, multi-faceted discussion. Even when we look at retrieval...",[{'name': 'Benedikt Schifferer'}],2024-02-23T19:02:26Z
1991,Featured Researcher and Educator Sessions at NVIDIA GTC 2024,https://nvda.ws/3uxLFTX#new_tab,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/students-teachers-computers-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"students-teachers-computers\" width=\"768\" />Join experts from Stanford, Cornell, Meta, and more to learn about the latest in AI for academia and what’s next in cutting-edge research.",[{'name': 'Tanya Lenz'}],2024-02-22T22:30:00Z
1992,"Benchmarking NVIDIA Spectrum-X for AI Network Performance, Now Available from Supermicro",https://developer.nvidia.com/blog/benchmarking-nvidia-spectrum-x-for-ai-network-performance-now-available-from-supermicro/,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/image-panels-with-multicolor-cat-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"image-panels-with-multicolor-cat\" width=\"768\" />NVIDIA Spectrum-X is swiftly gaining traction as the leading networking platform tailored for AI in hyperscale cloud infrastructures. Spectrum-X networking...,[{'name': 'Brian Sparks'}],2024-02-22T17:54:05Z
1993,"Enhance Immersive Experiences with the New Varjo XR-4 Series Headsets, Powered by NVIDIA",https://developer.nvidia.com/blog/enhance-immersive-experiences-with-the-new-varjo-xr-4-series-headsets-powered-by-nvidia/,"<img alt=\"Car model shown recorded inside of the Varjo XR-4\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"338\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/varjo-xr-car-model.gif\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"varjo-xr-car-model\" width=\"600\" />Developers and enterprises can now deploy lifelike virtual and mixed reality experiences with Varjo's latest XR-4 series headsets, which are integrated with...",[{'name': 'David Weinstein'}],2024-02-22T17:43:03Z
1994,Top Computer Vision/Video Analytics Sessions at NVIDIA GTC 2024,https://nvda.ws/48qKHH1#new_tab,"<img alt=\"Computer-generated image of a canal with bounding boxes for houses, trees, boats, and people.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"338\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/GTC-600x338-1.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"GTC 600x338\" width=\"600\" />Discover the transformative power of computer vision and video analytics at GTC. Dive into cutting-edge techniques such as vision transformers, AI agents,...",[{'name': 'Michelle Horton'}],2024-02-21T22:00:00Z
1995,Build an LLM-Powered API Agent for Task Execution,https://developer.nvidia.com/blog/build-an-llm-powered-api-agent-for-task-execution/,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/magnifying-glass-with-icons-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"magnifying-glass-with-icons\" width=\"768\" />Developers have long been building interfaces like web apps to enable users to leverage the core products being built. To learn how to work with data in your...,[{'name': 'Tanay Varshney'}],2024-02-21T21:30:00Z
1996,Spotlight: HOMEE AI Delivers AI-Powered Spatial Planning to Your Living Room,https://developer.nvidia.com/blog/spotlight-homee-ai-delivers-ai-powered-spatial-planning-to-your-living-room/,"<img alt=\"Image of a livingroom with a couch that is partially dissolved into pixels.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/homee-ai-featured-b-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"homee-ai-featured-b\" width=\"768\" />HOMEE AI, an NVIDIA Inception member based in Taiwan, has developed an “AI-as-a-service” spatial planning solution to disrupt the $650B global home decor...",[{'name': 'Don Johnson'}],2024-02-21T21:00:14Z
1997,Podcast: NVIDIA and the Future of 3D Development with Aaron Luk,https://nvda.ws/3UIRZCJ#new_tab,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"315\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/openusd-podcast.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"openusd-podcast\" width=\"560\" />Discover why OpenUSD is central to the future of 3D development with Aaron Luk, a founding developer of Universal Scene Description.",[{'name': 'Nate Bradford'}],2024-02-21T18:30:00Z
1998,Limiting CPU Threads for Better Game Performance,https://developer.nvidia.com/blog/limiting-cpu-threads-for-better-game-performance/,"<img alt=\"Decorative image of scissors near a CPU with green light streaming out.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/limit-thread-count-featured-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"limit-thread-count-featured\" width=\"768\" />Many PC games are designed around an eight-core console with an assumption that their software threading system ‘just works’ on all PCs, especially...",[{'name': 'Jon Kennedy'}],2024-02-21T17:38:17Z
1999,Webinar: Accelerate Edge AI Development With NVIDIA Metropolis Microservices For Jetson,https://nvda.ws/3uJG90r#new_tab,"<img alt=\"Decorative image of avatars working in different office locations.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/metropolis-iva-microservices-kv-devblog-1920x1080-1-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"metropolis-iva-microservices-kv-devblog-1920x1080\" width=\"768\" />On March 5, 8am PT, learn how NVIDIA Metropolis microservices for Jetson Orin helps you modernize your app stack, streamline development and deployment, and...",[{'name': 'Michelle Horton'}],2024-02-21T17:30:00Z
2000,NVIDIA TensorRT-LLM Revs Up Inference for Google Gemma,https://developer.nvidia.com/blog/nvidia-tensorrt-llm-revs-up-inference-for-google-gemma/,"<img alt=\"An illustration representing LLM optimization.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/llm-summerization-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"llm-summerization\" width=\"768\" />NVIDIA is collaborating as a launch partner with Google in delivering Gemma, a newly optimized family of open models built from the same research and technology...",[{'name': 'Anjali Shah'}],2024-02-21T13:00:00Z
2001,Event: NVIDIA at GDC 2024,https://nvda.ws/3xVpGnG#new_tab,"<img alt=\"Rich image of a carnival at night, with the NVIDIA logo as a pond sculpture in the foreground.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/RTPT_AmusementPark-featured-c-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"RTPT_AmusementPark-featured-c\" width=\"768\" />Join us at the Game Developers Conference March 18-22 to discover how the latest generative AI and NVIDIA RTX technologies are accelerating game development.",[{'name': 'Diego Farinha'}],2024-02-20T22:42:55Z
2002,Build an LLM-Powered Data Agent for Data Analysis,https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/gear-icons-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"gear-icons\" width=\"768\" />An AI agent is a system consisting of planning capabilities, memory, and tools to perform tasks requested by a user. For complex tasks such as data analytics or...",[{'name': 'Tanay Varshney'}],2024-02-20T19:30:00Z
2003,Experience NVIDIA cuOpt Accelerated Optimization to Boost Operational Efficiency,https://developer.nvidia.com/blog/experience-nvidia-cuopt-accelerated-optimization-to-boost-operational-efficiency/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/city-block-graphic-1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"city-block-graphic\" width=\"768\" />This week’s model release features NVIDIA cuOpt, a world-record-breaking accelerated optimization engine that helps teams solve complex routing problems and...",[{'name': 'Moon Chung'}],2024-02-19T19:30:00Z
2004,Explainer: What Is a Virtual Assistant?,https://nvda.ws/48vNa3e#new_tab,<img alt=\"Person sitting at a desk having a conversation with a speech ai chatbot.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/speech-ai-riva-fsi-abm-devnews-1920x10801-1-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"SpeechAI Riva speech recognition\" width=\"768\" />A virtual digital assistant is a program that understands natural language and can answer questions or complete tasks based on voice commands.,[{'name': 'Michelle Horton'}],2024-02-16T20:00:00Z
2005,Featured Developer Sessions at NVIDIA GTC 2024,https://nvda.ws/42DkWSk#new_tab,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/developer-gtc-2024-sessions-1-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"developer-gtc-2024-sessions\" width=\"768\" />Advances in AI are rapidly transforming every industry. Join us in person or virtually to learn about the latest technologies, from retrieval-augmented...",[{'name': 'Tanya Lenz'}],2024-02-15T21:00:00Z
2006,Accelerating Drug Discovery at Receptor.AI with NVIDIA BioNeMo Cloud APIs,https://developer.nvidia.com/blog/accelerating-drug-discovery-at-receptor-ai-with-nvidia-bionemo-cloud-apis/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/stylized-3d-structure-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"stylized-3d-structure\" width=\"768\" />The quest for new, effective treatments for diseases that remain stubbornly resistant to current therapies is at the heart of drug discovery. This traditionally...",[{'name': 'Alan Nafiiev'}],2024-02-14T21:00:00Z
2007,Featured Cybersecurity Sessions at NVIDIA GTC 2024,https://nvda.ws/4bL7WhV#new_tab,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/gtc24-spring-cybersecurity-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"gtc24-spring-cybersecurity\" width=\"768\" />Discover how generative AI is powering cybersecurity solutions with enhanced speed, accuracy, and scalability.",[{'name': 'Tanya Lenz'}],2024-02-14T20:26:51Z
2008,Release: NVIDIA DOCA 2.6,https://nvda.ws/3uyp2P3#new_tab,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/nvidia-doca-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"nvidia-doca\" width=\"768\" />The NVIDIA DOCA 2.6 release includes support for NVIDIA Spectrum-X reference architecture with the NVIDIA BlueField-3 SuperNIC and enhances DOCA host-based...,[{'name': 'David Wills'}],2024-02-13T20:30:00Z
2009,Upcoming Event: OpenUSD Day at NVIDIA GTC 2024,https://nvda.ws/3HUMgSw%20#new_tab,"<img alt=\"Picture of a kitchen modeled in OpenUSD.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/Pixar-Kitchen-featured-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"Pixar Kitchen featured\" width=\"768\" />On March 19, learn how to build generative AI-enabled 3D pipelines and tools using Universal Scene Description for industrial digitalization.",[{'name': 'Nate Bradford'}],2024-02-13T18:00:00Z
2010,Top Inference for Large Language Models Sessions at NVIDIA GTC 2024,https://nvda.ws/3SzYNzV#new_tab,"<img alt=\"Decorative image of inference steps: LLM, optimize, deploy. The GTC logo is in one corner.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/press-gtc24-llm-inference-1920x1080-full-bleed-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"press-gtc24-llm-inference-1920x1080-full-bleed\" width=\"768\" />Learn how inference for LLMs is driving breakthrough performance for AI-enabled applications and services.",[{'name': 'Michelle Horton'}],2024-02-13T17:00:00Z
2011,Performance-Efficient Mamba-Chat from NVIDIA AI Foundation Models,https://developer.nvidia.com/blog/performance-efficient-mamba-chat-from-nvidia-ai-foundation-models/,"<img alt=\"Decorative image of groups of people using speech AI in different ways standing around a globe.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/11/exploring-speech-ai-possibilities-video-featured-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"exploring-speech-ai-possibilities-video-featured\" width=\"768\" />This week’s release features the NVIDIA-optimized Mamba-Chat model, which you can experience directly from your browser. This post is part of Model Mondays, a...",[{'name': 'Chintan Patel'}],2024-02-12T21:24:04Z
2012,New Workshops and Certification at NVIDIA GTC 2024,https://developer.nvidia.com/blog/new-workshops-and-certification-at-nvidia-gtc-2024/,"<img alt=\"People sitting in a classroom.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/DLI-DEV-Lifestyle-0868-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"DLI-DEV-Lifestyle-0868\" width=\"768\" />With the GTC session catalog now live, it’s time to start building your personalized agenda for the conference. For those of you who will be joining us in San...",[{'name': 'Ann Sheridan'}],2024-02-12T20:28:17Z
2013,Explainer: What Is Clustering?,https://nvda.ws/4aFMzxF#new_tab,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2022/12/ai-for-dev-devnews-faster-hdbscan-soft-clustering-blog-1920x1080-1-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"ai-for-dev-devnews-faster-hdbscan-soft-clustering-blog-1920x1080\" width=\"768\" />Cluster analysis is the grouping of objects such that objects in the same cluster are more similar to each other than they are to objects in another cluster.,[{'name': 'Michelle Horton'}],2024-02-09T20:00:00Z
2014,Featured Large Language Models Sessions at NVIDIA GTC 2024,https://nvda.ws/487B5R9#new_tab,"<img alt=\"Image generation, video generation and code generation talks at NVIDIA GTC 2024\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/press-gtc24-llm-playlist-1920x1080-1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"NVIDIA GTC LLM sessions\" width=\"768\" />Speakers from NVIDIA, Meta, Microsoft, OpenAI, and ServiceNow will be talking about the latest tools, optimizations, trends and best practices for large...",[{'name': 'Brad Nemire'}],2024-02-08T02:09:25Z
2015,CUDA Quantum Introduces More Capabilities for Quantum Accelerated Supercomputing,https://developer.nvidia.com/blog/cuda-quantum-introduces-more-capabilities-for-quantum-accelerated-supercomputing/,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/abstract-image-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"abstract-image\" width=\"768\" />CUDA Quantum is an open-source programming model for building quantum-classical applications. Useful quantum computing workloads will run on heterogeneous...,[{'name': 'Efrat Shabtai'}],2024-02-07T19:00:00Z
2016,Generative AI Research Spotlight: Personalizing Text-to-Image Models,https://developer.nvidia.com/blog/generative-ai-research-spotlight-personalizing-text-to-image-models/,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/woman-sitting-at-desktop-computer-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"woman-sitting-at-desktop-computer\" width=\"768\" />Visual generative AI is the process of creating images from text prompts. The technology is based on vision-language foundation models that are pretrained on...,[{'name': 'Gal Chechik'}],2024-02-06T23:41:01Z
2017,Top Retrieval-Augmented Generation (RAG) Sessions at NVIDIA GTC 2024 Sessions,https://nvda.ws/49khUoR#new_tab,"<img alt=\"Retrieval-Augmented Generation Conference Sessions at GTC\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/NVIDIA-GTC-RAG-sessions-recommendations-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"NVIDIA GTC RAG sessions recommendations\" width=\"768\" />Join us in-person or virtually and learn about the power of RAG with insights and best practices from experts at NVIDIA, visionary CEOs, data scientists, and...",[{'name': 'Brad Nemire'}],2024-02-06T19:38:44Z
2018,"Generate Code, Answer Queries, and Translate Text with New NVIDIA AI Foundation Models",https://developer.nvidia.com/blog/generate-code-answer-queries-and-translate-text-with-leading-generative-ai-models/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/02/ai-image-generation-graphic-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"ai-image-generation-graphic\" width=\"768\" />This week’s Model Monday release features the NVIDIA-optimized code Llama, Kosmos-2, and SeamlessM4T, which you can experience directly from your browser....",[{'name': 'Chintan Patel'}],2024-02-05T18:48:17Z
2019,Deploy an AI Coding Assistant with NVIDIA TensorRT-LLM and NVIDIA Triton,https://developer.nvidia.com/blog/deploy-an-ai-coding-assistant-with-nvidia-tensorrt-llm-and-nvidia-triton/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/llm-optimize-deploy-graphic-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"llm-optimize-deploy-graphic\" width=\"768\" />Large language models (LLMs) have revolutionized the field of AI, creating entirely new ways of interacting with the digital world. While they provide a good...",[{'name': 'Amit Bleiweiss'}],2024-02-01T21:00:00Z
2020,Just Released: NVIDIA HPC SDK v24.1,https://nvda.ws/3vVy3Cc#new_tab,"<img alt=\"Illustration representing HPC.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/HPC_SW_KV-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"HPC_SW_KV\" width=\"768\" />This NVIDIA HPC SDK update includes the cuBLASMp preview library, along with minor bug fixes and enhancements.",[{'name': 'Jay Gould'}],2024-02-01T16:36:12Z
2021,Just Released: NVIDIA Modulus 24.01,https://nvda.ws/3OppBSb,<img alt=\"Image of windvanes over water at night.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"338\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/modulus-release-windvanes-over-water-night-featured.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"modulus-release-windvanes-over-water-night-featured\" width=\"600\" />NVIDIA Modulus 24.01 updates distributed utilities and samples for physics informing DeepONet and GNNs.,[{'name': 'Bhoomi Gadhia'}],2024-01-31T22:31:12Z
2022,New Self-Paced Course: Synthetic Tabular Data Generation Using Transformers,https://nvda.ws/3vUDGRd#new_tab,<img alt=\"Three examples of synthetic tabular data generation visuals.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/dli-tech-blog-synthetic-data-generation-1920x1080-1-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"dli-tech-blog-synthetic-data-generation-1920x1080\" width=\"768\" />Synthetic data generation is a data augmentation technique necessary for increasing the robustness of models by supplying training data. Explore the use of...,[{'name': 'Michelle Horton'}],2024-01-31T17:00:00Z
2023,"Create, Share, and Scale Enterprise AI Workflows with NVIDIA AI Workbench, Now in Beta",https://developer.nvidia.com/blog/create-share-and-scale-enterprise-ai-workflows-with-nvidia-ai-workbench-now-in-beta/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-workbench-graphic-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"nvidia-ai-workbench-graphic\" width=\"768\" />NVIDIA AI Workbench is now in beta, bringing a wealth of new features to streamline how enterprise developers create, use, and share AI and machine learning...",[{'name': 'Shruthii Sathyanarayanan'}],2024-01-30T20:02:55Z
2024,Modernizing the Data Center with Accelerated Networking,https://developer.nvidia.com/blog/modernizing-the-data-center-with-accelerated-networking/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/highway-lights-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"highway-lights\" width=\"768\" />Accelerated networking combines CPUs, GPUs, DPUs (data processing units), or SuperNICs into an accelerated computing fabric specifically designed to optimize...",[{'name': 'Tim Lustig'}],2024-01-30T20:00:00Z
2025,Emulating the Attention Mechanism in Transformer Models with a Fully Convolutional Network,https://developer.nvidia.com/blog/emulating-the-attention-mechanism-in-transformer-models-with-a-fully-convolutional-network/,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/NVIDIA-Convolution-Self-Attention-Blocks-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"NVIDIA Convolution Self-Attention Blocks\" width=\"768\" />The past decade has seen a remarkable surge in the adoption of deep learning techniques for computer vision (CV) tasks. Convolutional neural networks (CNNs)...,[{'name': 'John Yang'}],2024-01-29T17:00:00Z
2026,New Self-Paced Course: Augment Your LLM Using Retrieval-Augmented Generation,https://nvda.ws/3u4liEM#new_tab,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/retrieval-augmented-generation-graphic-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"retrieval-augmented-generation-graphic\" width=\"768\" />Learn the basics of retrieval-augmented generation (RAG), an end-to-end architecture used to optimize the output of an LLM.",[{'name': 'Tanya Lenz'}],2024-01-26T18:00:00Z
2027,Announcing NVIDIA Metropolis Microservices for Jetson for Rapid Edge AI Development,https://developer.nvidia.com/blog/announcing-metropolis-microservices-on-nvidia-jetson-orin-for-rapid-edge-ai-development/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-iva-microservices-graphic-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"metropolis-iva-microservices-graphic\" width=\"768\" />Building vision AI applications for the edge often comes with notoriously long and costly development cycles. At the same time, quickly developing edge AI...",[{'name': 'Chintan Shah'}],2024-01-25T18:30:00Z
2028,Advancing Production AI with NVIDIA AI Enterprise,https://developer.nvidia.com/blog/advancing-production-ai-with-nvidia-ai-enterprise/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-enterprise-production-branch-graphic-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"nvidia-ai-enterprise-production-branch-graphic\" width=\"768\" />While harnessing the potential of AI is a priority for many of today’s enterprises, developing and deploying an AI model involves time and effort. Often,...",[{'name': 'Phoebe Lee'}],2024-01-25T18:00:00Z
2029,Build Enterprise-Grade AI with NVIDIA AI Software,https://developer.nvidia.com/blog/build-enterprise-grade-ai-with-nvidia-ai-software/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-production-ai-graphic-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"nvidia-production-ai-graphic\" width=\"768\" />Following the introduction of ChatGPT, enterprises around the globe are realizing the benefits and capabilities of AI, and are racing to adopt it into their...",[{'name': 'Nirmal Kumar Juluru'}],2024-01-24T20:30:00Z
2030,"Delivering Efficient, High-Performance AI Clouds with NVIDIA DOCA 2.5",https://developer.nvidia.com/blog/delivering-efficient-high-performance-ai-clouds-with-nvidia-doca-2-5/,"<img alt=\"Image shows the range of applications available for delivery on an NVIDIA BlueField networking platform with the NVIDIA DOCA SDK and acceleration framework.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/tech-blog-doca-2.5-1920x1080-1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"tech-blog-doca-2.5-1920x1080\" width=\"768\" />As a comprehensive software framework for data center infrastructure developers, NVIDIA DOCA has been adopted by leading AI, cloud, enterprise, and ISV...",[{'name': 'David Wills'}],2024-01-24T19:31:39Z
2031,Webinar: Improve Spear Phishing Detection with AI,https://nvda.ws/48FJ4pR#new_tab,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/security-spear-phish-graphic-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"security-spear-phish-graphic\" width=\"768\" />Learn how generative AI can help defend against spear phishing in this January 30 webinar.,[{'name': 'Nicola Sessions'}],2024-01-24T18:00:00Z
2032,Using the Power of AI to Make Factories Safer,https://developer.nvidia.com/blog/using-the-power-of-ai-to-make-factories-safer/,"<img alt=\"Decorative image of a robotic assembly line with bounding boxes.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/power-ai-factory-safety-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"power-ai-factory-safety\" width=\"768\" />As industrial automation increases, safety becomes a greater challenge and top priority for enterprises.&nbsp; Safety encompasses multiple aspects:&nbsp; System...",[{'name': 'Riccardo Mariani'}],2024-01-24T17:00:00Z
2033,Simplifying Network Operations for AI with NVIDIA Quantum InfiniBand,https://developer.nvidia.com/blog/simplifying-network-operations-for-ai-with-nvidia-quantum-infiniband/,"<img alt=\"Photo of a person standing at a computer terminal in a data center.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/quantum-infiniband-guide-featured-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"quantum-infiniband-guide-featured\" width=\"768\" />A common technological misconception is that performance and complexity are directly linked. That is, the highest-performance implementation is also the most...",[{'name': 'Taylor Allison'}],2024-01-23T18:00:00Z
2034,Bringing Generative AI to the Edge with NVIDIA Metropolis Microservices for Jetson,https://developer.nvidia.com/blog/bringing-generative-ai-to-the-edge-with-nvidia-metropolis-microservices-for-jetson/,"<img alt=\"GIF of factory floor with people, pallets, and equipment in bounding boxes.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"338\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-iva-gen-ai-featured.gif\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"metropolis-iva-gen-ai-featured\" width=\"600\" />NVIDIA Metropolis Microservices for Jetson provides a suite of easy-to-deploy services that enable you to quickly build production-quality vision AI...",[{'name': 'Samuel Ochoa'}],2024-01-23T17:00:00Z
2035,Build Vision AI Applications at the Edge with NVIDIA Metropolis Microservices and APIs,https://developer.nvidia.com/blog/how-to-build-vision-ai-applications-at-the-edge-with-nvidia-metropolis-microservices-and-apis/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"338\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/metropolis-icon-shuffle-gif.gif\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"metropolis-icon-shuffle-gif\" width=\"600\" />NVIDIA Metropolis microservices provide powerful, customizable, cloud-native APIs and microservices to develop vision AI applications and solutions. The...",[{'name': 'Bhanu Pisupati'}],2024-01-23T17:00:00Z
2036,Query Graphs with Optimized DePlot Model,https://developer.nvidia.com/blog/query-graphs-with-optimized-deplot-model/,"<img alt=\"Decorative image.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ngc-tech-blog-deplot-blog-1920x1080-1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"ngc-tech-blog-deplot-blog-1920x1080\" width=\"768\" />NVIDIA AI Foundation Models and Endpoints provides access to a curated set of community and NVIDIA-built generative AI models to experience, customize, and...",[{'name': 'Shashank Verma'}],2024-01-23T00:34:34Z
2037,Benchmarking Camera Performance on Your Workstation with NVIDIA Isaac Sim,https://developer.nvidia.com/blog/benchmarking-camera-performance-on-your-workstation-with-nvidia-isaac-sim/,"<img alt=\"A fisheye camera view of a manufacturing plant.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/Isaac-Sim-Workstation-Benchmark-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"Isaac-Sim-Workstation-Benchmark\" width=\"768\" />Robots are typically equipped with cameras. When designing a digital twin simulation, it’s important to replicate its performance in a simulated environment...",[{'name': 'Raffaello Bonghi'}],2024-01-22T15:00:00Z
2038,Generate Synthetic Data for Deep Object Pose Estimation Training with NVIDIA Isaac ROS,https://developer.nvidia.com/blog/generate-synthetic-data-for-deep-object-pose-estimation-training-with-nvidia-isaac-ros/,"<img alt=\"An image of the inside of an industrial manufacturing building.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/Isaac-ROS-DOPE-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"Isaac-ROS-DOPE\" width=\"768\" />For robotic agents to interact with objects in their environment, they must know the position and orientation of objects around them. This information describes...",[{'name': 'Asawaree Bhide'}],2024-01-18T21:45:18Z
2039,Webinar: Quantum ESPRESSO on GPUs: Porting Strategy and Results,https://bit.ly/openacc_QEnvn#new_tab,<img alt=\"Decorative image of two block matrices with connections against a shadowed background.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/quantum-espresso-webinar-featured-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"quantum-espresso-webinar-featured\" width=\"768\" />Explore the status of Quantum ESPRESSO porting strategies that enable state-of-the-art performance on HPC systems.,[{'name': 'Michelle Horton'}],2024-01-18T18:00:00Z
2040,Release: PyTorch Geometric Container for GNNs on NGC,https://nvda.ws/3SgrAdP#new_tab,"<img alt=\"PyG and Accelerated with NVIDIA logos on a white background.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"414\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/pyg-container-ngc-release-featured-1.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"pyg-container-ngc-release-featured\" width=\"736\" />The NVIDIA PyG container, now generally available, packages PyTorch Geometric with accelerations for GNN models, dataloading, and pre-processing using...",[{'name': 'Rishi Puri'}],2024-01-17T23:05:40Z
2041,Simulating Railroads with OpenUSD,https://developer.nvidia.com/blog/simulating-railroads-with-openusd/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/trains-on-tracks-1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"trains-on-tracks\" width=\"768\" />Railroad simulation is important in modern transportation and logistics, providing a virtual testing ground for the intricate interplay of tracks, switches, and...",[{'name': 'Marc-Michael Horstmann'}],2024-01-17T21:00:00Z
2042,New Support for Dutch and Persian Released by NVIDIA NeMo ASR,https://developer.nvidia.com/blog/new-support-for-dutch-and-persian-released-by-nemo-asr/,"<img alt=\"Person sitting at a desk having a conversation with a speech ai chatbot.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/speech-ai-riva-fsi-abm-devnews-1920x10801-1-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"SpeechAI Riva speech recognition\" width=\"768\" />Breaking barriers in speech recognition, NVIDIA NeMo proudly presents pretrained models tailored for Dutch and Persian—languages often overlooked in the AI...",[{'name': 'Piotr Żelasko'}],2024-01-16T18:29:16Z
2043,Robust Scene Text Detection and Recognition: Inference Optimization,https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-inference-optimization/,"<img alt=\"Decorative image of a workflow and the text &quot;Part 3&quot;.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/wipro-series-featured-part3-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"wipro-series-featured-part3\" width=\"768\" />In this post, we delve deeper into the inference optimization process to improve the performance and efficiency of our machine learning models during the...",[{'name': 'Vishal Chavan'}],2024-01-16T17:02:00Z
2044,Robust Scene Text Detection and Recognition: Implementation,https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-implementation/,"<img alt=\"Decorative image of a workflow and the text &quot;Part 2&quot;.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/wipro-series-featured-part2-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"wipro-series-featured-part2\" width=\"768\" />To make scene text detection and recognition work on irregular text or for specific use cases, you must have full control of your model so that you can do...",[{'name': 'Vishal Chavan'}],2024-01-16T17:01:00Z
2045,Robust Scene Text Detection and Recognition: Introduction,https://developer.nvidia.com/blog/robust-scene-text-detection-and-recognition-introduction/,"<img alt=\"Decorative image of a workflow and the text &quot;Part 1&quot;.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/wipro-series-featured-part1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"wipro-series-featured-part1\" width=\"768\" />Identification and recognition of text from natural scenes and images become important for use cases like video caption text recognition, detecting signboards...",[{'name': 'Vishal Chavan'}],2024-01-16T17:00:00Z
2046,Webinar: State of Ray Tracing and NvRTX 5.3,https://gateway.on24.com/wcc/experience/elitenvidiabrill/1407606/3849291/level-up-with-nvidia#new_tab,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/game-dev-ray-tracing-nvrtx-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"game-dev-ray-tracing-nvrtx\" width=\"768\" />Get up to speed on the current state of ray tracing in the NVIDIA RTX Branch of Unreal Engine and what’s coming next.,[{'name': 'Diego Farinha'}],2024-01-12T21:00:00Z
2047,Just Released: cuBLASDx,https://nvda.ws/3SdQyKO#new_tab,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"335\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/cublasdx-featured.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"cublasdx-featured\" width=\"596\" />cuBLASDx allows you to perform BLAS calculations inside your CUDA kernel, improving the performance of your application. Available to download in Preview...",[{'name': 'Robert Jensen'}],2024-01-12T18:58:48Z
2048,Free Digital Webinar Series: How to Get Started with AI Inference,https://nvda.ws/48NqBXI#new_tab,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/inference-graphic-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"inference-graphic\" width=\"768\" />Learn how to improve your AI model performance with this series of expert-led talks on the NVIDIA AI inference platform.,[{'name': 'Tanya Lenz'}],2024-01-11T19:00:00Z
2049,Experience Real-Time Audio and Video Communication with NVIDIA Maxine,https://developer.nvidia.com/blog/experience-real-time-audio-and-video-communication-with-nvidia-maxine/,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-maxine-live-portrait-input-video-animating-2d-image-1-1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"nvidia-maxine-live-portrait-input-video-animating-2d-image (1)\" width=\"768\" />The NVIDIA Maxine developer platform redefines video conferencing and editing by providing developers and businesses with a variety of low-code implementation...,[{'name': 'Greg Jones'}],2024-01-10T19:00:00Z
2050,Enhancing Phone Customer Service with ASR Customization,https://developer.nvidia.com/blog/enhancing-phone-customer-service-with-asr-customization/,<img alt=\"Decorative image.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/polyai-featured-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"polyai-featured\" width=\"768\" />At the core of understanding people correctly and having natural conversations is automatic speech recognition (ASR). To make customer-led voice assistants and...,[{'name': 'Paweł Budzianowski'}],2024-01-09T17:00:00Z
2051,New Models MolMIM and DiffDock Power Molecule Generation and Molecular Docking in NVIDIA BioNeMo,https://developer.nvidia.com/blog/new-models-molmim-and-diffdock-power-molecule-generation-and-molecular-docking-in-bionemo/,"<img alt=\"Decorative image of molecular displays.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hc-tech-blog-jpm24-bionemo-1920x1080-1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"hc-tech-blog-jpm24-bionemo-1920x1080\" width=\"768\" />The search for viable drugs is one of the most formidable challenges at the intersection of science, technology, and medicine.&nbsp; Mathematically, the odds of...",[{'name': 'Abraham Stern'}],2024-01-08T19:00:00Z
2052,New Stable Diffusion Models Accelerated with NVIDIA TensorRT,https://developer.nvidia.com/blog/new-stable-diffusion-models-accelerated-with-nvidia-tensorrt/,"<img alt=\"Photo of a dog racing through a snowy forest.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ces-stable-diffusion-featured-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"ces-stable-diffusion-featured\" width=\"768\" />At CES, NVIDIA shared that SDXL Turbo, LCM-LoRA, and Stable Video Diffusion are all being accelerated by NVIDIA TensorRT. These enhancements allow GeForce RTX...",[{'name': 'Ayesha Asif'}],2024-01-08T16:31:00Z
2053,Spotlight: Convai Reinvents Non-Playable Character Interactions,https://developer.nvidia.com/blog/spotlight-convai-reinvents-non-playable-character-interactions/,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/two-characters-in-front-of-buildings-1-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"two-characters-in-front-of-buildings\" width=\"768\" />Convai is a versatile developer platform for designing characters with advanced multimodal perception abilities. These characters are designed to integrate...,[{'name': 'Yasmina Benkhoui'}],2024-01-08T16:30:00Z
2054,Get Started with Generative AI Development for Windows PCs with NVIDIA RTX,https://developer.nvidia.com/blog/get-started-with-generative-ai-development-for-windows-pcs-with-rtx-systems/,<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/graphic-generative-ai-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"graphic-generative-ai\" width=\"768\" />Generative AI and large language models (LLMs) are changing human-computer interaction as we know it. Many use cases would benefit from running LLMs locally on...,[{'name': 'Jesse Clayton'}],2024-01-08T16:30:00Z
2055,Contest: Build Generative AI on NVIDIA RTX PCs,https://nvda.ws/3ROt4KC#new_tab,<img alt=\"Decorative image of RTX GPU.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/nvidia-ai-on-rtx-owned-web-module-bb580_440-l-copy-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"nvidia-ai-on-rtx-owned-web-module-bb580_440-l copy\" width=\"768\" />NVIDIA is announcing the Generative AI on RTX PCs Developer Contest - designed to inspire innovation within the developer community. Build and submit your next...,[{'name': 'Annamalai Chockalingam'}],2024-01-08T16:30:00Z
2056,Building Lifelike Digital Avatars with NVIDIA ACE Microservices,https://developer.nvidia.com/blog/building-lifelike-digital-avatars-with-nvidia-ace-microservices/,"<img alt=\"Still image from Kairos demo, of an NPC at a bar.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/ces-ace-microservices-featured-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"ces-ace-microservices-featured\" width=\"768\" />Generative AI technologies are revolutionizing how games are produced and played. Game developers are exploring how these technologies can accelerate their...",[{'name': 'Seth Schneider'}],2024-01-08T16:30:00Z
2057,Supercharging LLM Applications on Windows PCs with NVIDIA RTX Systems,https://developer.nvidia.com/blog/supercharging-llm-applications-on-windows-pcs-with-nvidia-rtx-systems/,"<img alt=\"Decorative image of an open laptop with a lightbulb leaning on it, on a purple background.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2024/01/genai-blog-2936009-1920x1080-1-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"genai-blog-2936009-1920x1080\" width=\"768\" />Large language models (LLMs) are fundamentally changing the way we interact with computers. These models are being incorporated into a wide range of...",[{'name': 'Annamalai Chockalingam'}],2024-01-08T16:30:00Z
2058,Improving CUDA Initialization Times Using cgroups in Certain Scenarios,https://developer.nvidia.com/blog/improving-cuda-initialization-times-using-cgroups-in-certain-scenarios/,"<img alt=\"Decorative image of light fields in green, purple, and blue.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/hpc-featured-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"hpc-featured\" width=\"768\" />Many CUDA applications running on multi-GPU platforms usually use a single GPU for their compute needs. In such scenarios, a performance penalty is paid by...",[{'name': 'Rahul Ramasubramanian'}],2024-01-05T22:14:41Z
2059,Develop ML and AI with Metaflow and Deploy with NVIDIA Triton Inference Server,https://developer.nvidia.com/blog/develop-ml-ai-with-metaflow-deploy-with-triton-inference-server/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/metaflow-featured-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"metaflow-featured\" width=\"768\" />There are many ways to deploy ML models to production. Sometimes, a model is run once per day to refresh forecasts in a database. Sometimes, it powers a...",[{'name': 'Eddie Mattia'}],2024-01-05T19:23:39Z
2060,Video Encoding at 8K60 with Split-Frame Encoding and NVIDIA Ada Lovelace Architecture,https://developer.nvidia.com/blog/video-encoding-at-8k60-with-split-frame-encoding-and-nvidia-ada-lovelace-architecture/,"<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/nvenc-comparison-768x432.png\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"nvenc-comparison\" width=\"768\" />Capturing video footage and playing games at 8K resolution with 60 frames per second (FPS) is now possible, thanks to advances in camera and display...",[{'name': 'Ricardo Monteiro'}],2024-01-05T19:00:00Z
2061,Accelerating Inference on End-to-End Workflows with H2O.ai and NVIDIA,https://developer.nvidia.com/blog/accelerating-inference-on-end-to-end-workflows-with-h2o-ai-and-nvidia/,"<img alt=\"Decorative image.\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"432\" src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/12/h20ai-featured-768x432.jpg\" style=\"display: block; margin-bottom: 5px; clear: both;\" title=\"h20ai-featured\" width=\"768\" />Data scientists are combining generative AI and predictive analytics to build the next generation of AI applications. In financial services, AI modeling and...",[{'name': 'Prabhu Ramamoorthy'}],2024-01-04T14:00:00Z
2062,OpenAI and Elon Musk,https://openai.com/blog/openai-elon-musk,We are dedicated to the OpenAI mission and have pursued it every step of the way.,[],"Tue, 05 Mar 2024 08:00:00 GMT"
2063,Video generation models as world simulators,https://openai.com/research/video-generation-models-as-world-simulators,"We explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.",[],"Thu, 15 Feb 2024 08:00:00 GMT"
2064,Disrupting malicious uses of AI by state-affiliated threat actors,https://openai.com/blog/disrupting-malicious-uses-of-ai-by-state-affiliated-threat-actors,"We terminated accounts associated with state-affiliated threat actors. Our findings show our models offer only limited, incremental capabilities for malicious cybersecurity tasks.",[],"Wed, 14 Feb 2024 08:00:00 GMT"
2065,Memory and new controls for ChatGPT,https://openai.com/blog/memory-and-new-controls-for-chatgpt,We’re testing the ability for ChatGPT to remember things you discuss to make future chats more helpful. You’re in control of ChatGPT’s memory.,[],"Tue, 13 Feb 2024 08:00:00 GMT"
2066,Building an early warning system for LLM-aided biological threat creation,https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation,"We’re developing a blueprint for evaluating the risk that a large language model (LLM) could aid someone in creating a biological threat. In an evaluation involving both biology experts and students, we found that GPT-4 provides at most a mild uplift in biological threat creation accuracy. While this uplift is not large enough to be conclusive, our finding is a starting point for continued research and community deliberation.",[],"Wed, 31 Jan 2024 08:00:00 GMT"
2067,New embedding models and API updates,https://openai.com/blog/new-embedding-models-and-api-updates,"We are launching a new generation of embedding models, new GPT-4 Turbo and moderation models, new API usage management tools, and soon, lower pricing on GPT-3.5 Turbo.",[],"Thu, 25 Jan 2024 08:00:00 GMT"
2068,Democratic inputs to AI grant program: lessons learned and implementation plans,https://openai.com/blog/democratic-inputs-to-ai-grant-program-update,"We funded 10 teams from around the world to design ideas and tools to collectively govern AI. We summarize the innovations, outline our learnings, and call for researchers and engineers to join us as we continue this work.",[],"Tue, 16 Jan 2024 08:00:00 GMT"
2069,How OpenAI is approaching 2024 worldwide elections,https://openai.com/blog/how-openai-is-approaching-2024-worldwide-elections,"We’re working to prevent abuse, provide transparency on AI-generated content, and improve access to accurate voting information.",[],"Mon, 15 Jan 2024 08:00:00 GMT"
2070,Introducing the GPT Store,https://openai.com/blog/introducing-the-gpt-store,We’re launching the GPT Store to help you find useful and popular custom versions of ChatGPT.,[],"Wed, 10 Jan 2024 08:00:00 GMT"
2071,Introducing ChatGPT Team,https://openai.com/blog/introducing-chatgpt-team,"We’re launching a new ChatGPT plan for teams of all sizes, which provides a secure, collaborative workspace to get the most out of ChatGPT at work.",[],"Wed, 10 Jan 2024 08:00:00 GMT"
2072,OpenAI and journalism,https://openai.com/blog/openai-and-journalism,"We support journalism, partner with news organizations, and believe The New York Times lawsuit is without merit.",[],"Mon, 08 Jan 2024 08:00:00 GMT"
2073,Superalignment Fast Grants,https://openai.com/blog/superalignment-fast-grants,"We’re launching $10M in grants to support technical research towards the alignment and safety of superhuman AI systems, including weak-to-strong generalization, interpretability, scalable oversight, and more.",[],"Thu, 14 Dec 2023 08:00:00 GMT"
2074,Practices for Governing Agentic AI Systems,https://openai.com/research/practices-for-governing-agentic-ai-systems,,[],"Thu, 14 Dec 2023 08:00:00 GMT"
2075,Weak-to-strong generalization,https://openai.com/research/weak-to-strong-generalization,"We present a new research direction for superalignment, together with promising initial results: can we leverage the generalization properties of deep learning to control strong models with weak supervisors?",[],"Thu, 14 Dec 2023 08:00:00 GMT"
2076,Partnership with Axel Springer to deepen beneficial use of AI in journalism,https://openai.com/blog/axel-springer-partnership,Axel Springer is the first publishing house globally to partner with us on a deeper integration of journalism in AI technologies.,[],"Wed, 13 Dec 2023 08:00:00 GMT"
2077,"Sam Altman returns as CEO, OpenAI has a new initial board",https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board,"Mira Murati as CTO, Greg Brockman returns as President. Read messages from CEO Sam Altman and board chair Bret Taylor.",[],"Wed, 29 Nov 2023 08:00:00 GMT"
2078,OpenAI announces leadership transition,https://openai.com/blog/openai-announces-leadership-transition,,[],"Fri, 17 Nov 2023 08:00:00 GMT"
2079,OpenAI Data Partnerships,https://openai.com/blog/data-partnerships,Working together to create open-source and private datasets for AI training.,[],"Thu, 09 Nov 2023 08:00:00 GMT"
2080,Introducing GPTs,https://openai.com/blog/introducing-gpts,"You can now create custom versions of ChatGPT that combine instructions, extra knowledge, and any combination of skills.",[],"Mon, 06 Nov 2023 08:00:00 GMT"
2081,New models and developer products announced at DevDay,https://openai.com/blog/new-models-and-developer-products-announced-at-devday,"GPT-4 Turbo with 128K context and lower prices, the new Assistants API, GPT-4 Turbo with Vision, DALL·E 3 API, and more.",[],"Mon, 06 Nov 2023 08:00:00 GMT"
2082,Frontier risk and preparedness,https://openai.com/blog/frontier-risk-and-preparedness,"To support the safety of highly-capable AI systems, we are developing our approach to catastrophic risk preparedness, including building a Preparedness team and launching a challenge.",[],"Thu, 26 Oct 2023 07:00:00 GMT"
2083,Frontier Model Forum updates,https://openai.com/blog/frontier-model-forum-updates,"Together with Anthropic, Google, and Microsoft, we’re announcing the new Executive Director of the Frontier Model Forum and a new $10 million AI Safety Fund.",[],"Wed, 25 Oct 2023 07:00:00 GMT"
2084,DALL·E 3 is now available in ChatGPT Plus and Enterprise,https://openai.com/blog/dall-e-3-is-now-available-in-chatgpt-plus-and-enterprise,We developed a safety mitigation stack to ready DALL·E 3 for wider release and are sharing updates on our provenance research.,[],"Thu, 19 Oct 2023 07:00:00 GMT"
2085,DALL·E 3 system card,https://openai.com/research/dall-e-3-system-card,,[],"Tue, 03 Oct 2023 07:00:00 GMT"
2086,"ChatGPT can now see, hear, and speak",https://openai.com/blog/chatgpt-can-now-see-hear-and-speak,"We are beginning to roll out new voice and image capabilities in ChatGPT. They offer a new, more intuitive type of interface by allowing you to have a voice conversation or show ChatGPT what you’re talking about.",[],"Mon, 25 Sep 2023 07:00:00 GMT"
2087,GPT-4V(ision) system card,https://openai.com/research/gpt-4v-system-card,,[],"Mon, 25 Sep 2023 07:00:00 GMT"
2088,Confidence-Building Measures for Artificial Intelligence: Workshop proceedings,https://openai.com/research/confidence-building-measures-for-artificial-intelligence,,[],"Tue, 01 Aug 2023 07:00:00 GMT"
2089,Frontier AI regulation: Managing emerging risks to public safety,https://openai.com/research/frontier-ai-regulation,,[],"Thu, 06 Jul 2023 07:00:00 GMT"
2090,Improving mathematical reasoning with process supervision,https://openai.com/research/improving-mathematical-reasoning-with-process-supervision,"We've trained a model to achieve a new state-of-the-art in mathematical problem solving by rewarding each correct step of reasoning (“process supervision”) instead of simply rewarding the correct final answer (“outcome supervision”). In addition to boosting performance relative to outcome supervision, process supervision also has an important alignment benefit: it directly trains the model to produce a chain-of-thought that is endorsed by humans.",[],"Wed, 31 May 2023 07:00:00 GMT"
2091,Language models can explain neurons in language models,https://openai.com/research/language-models-can-explain-neurons-in-language-models,We use GPT-4 to automatically write explanations for the behavior of neurons in large language models and to score those explanations. We release a dataset of these (imperfect) explanations and scores for every neuron in GPT-2.,[],"Tue, 09 May 2023 07:00:00 GMT"
2092,GPTs are GPTs: An early look at the labor market impact potential of large language models,https://openai.com/research/gpts-are-gpts,,[],"Fri, 17 Mar 2023 07:00:00 GMT"
2093,GPT-4,https://openai.com/research/gpt-4,"We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks.",[],"Tue, 14 Mar 2023 07:00:00 GMT"
2094,Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk,https://openai.com/research/forecasting-misuse,"OpenAI researchers collaborated with Georgetown University’s Center for Security and Emerging Technology and the Stanford Internet Observatory to investigate how large language models might be misused for disinformation purposes. The collaboration included an October 2021 workshop bringing together 30 disinformation researchers, machine learning experts, and policy analysts, and culminated in a co-authored report building on more than a year of research. This report outlines the threats that language models pose to the information environment if used to augment disinformation campaigns and introduces a framework for analyzing potential mitigations. Read the full report here.",[],"Wed, 11 Jan 2023 08:00:00 GMT"
2095,Point-E: A system for generating 3D point clouds from complex prompts,https://openai.com/research/point-e,,[],"Fri, 16 Dec 2022 08:00:00 GMT"
2096,Scaling laws for reward model overoptimization,https://openai.com/research/scaling-laws-for-reward-model-overoptimization,,[],"Wed, 19 Oct 2022 07:00:00 GMT"
2097,Introducing Whisper,https://openai.com/research/whisper,We’ve trained and are open-sourcing a neural net called Whisper that approaches human level robustness and accuracy on English speech recognition.,[],"Wed, 21 Sep 2022 07:00:00 GMT"
2098,Efficient training of language models to fill in the middle,https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle,,[],"Thu, 28 Jul 2022 07:00:00 GMT"
2099,A hazard analysis framework for code synthesis large language models,https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models,,[],"Mon, 25 Jul 2022 07:00:00 GMT"
2100,DALL·E 2 pre-training mitigations,https://openai.com/research/dall-e-2-pre-training-mitigations,"In order to share the magic of DALL·E 2 with a broad audience, we needed to reduce the risks associated with powerful image generation models. To this end, we put various guardrails in place to prevent generated images from violating our content policy.",[],"Tue, 28 Jun 2022 07:00:00 GMT"
2101,Learning to play Minecraft with Video PreTraining,https://openai.com/research/vpt,"We trained a neural network to play Minecraft by Video PreTraining (VPT) on a massive unlabeled video dataset of human Minecraft play, while using only a small amount of labeled contractor data. With fine-tuning, our model can learn to craft diamond tools, a task that usually takes proficient humans over 20 minutes (24,000 actions). Our model uses the native human interface of keypresses and mouse movements, making it quite general, and represents a step towards general computer-using agents.",[],"Thu, 23 Jun 2022 07:00:00 GMT"
2102,Graph neural networks in TensorFlow,https://blog.tensorflow.org/2024/02/graph-neural-networks-in-tensorflow.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmB2uY1xF7sEeT_0hkfCj1oQypkcE9ksjrPXfoOS6hWe6MjNa6OdIZLdu8m8Z2IAx0gk4EhD6fQH5EpOobdT4z0E4w1iSw5YCI7IaRU6jUIL9RpHaU-BEufRlz5Cw2bF6ww4mF6_0N43tSFSkKXVTuy2hvmcx6xYd_hPKzJ_1QvYnKdt3kLNH2iffSmbs/s1600/TFgraph-netural-networks-social.png\" style=\"display: none;\" />

<em>Posted by Dustin Zelle – Software Engineer, Research and Arno Eigenwillig – Software Engineer, CoreML</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFzx_P7ogIv5kpZcabEMbL5hkABibribiGZtH-aiI6UPSBeXFD392VF7p30Aq57Jj4LxIxVcpeOQdpm-5pGDzk1kLG5Dj85pktgsNX1f7FC8sbTWnR6iS8H2WkW0ESZAn30OcDvWRcaUA1e7FgTKD0PzHRk-8Yn73eiePFnoN78uB2tmIF06ySty1_N3I/s1600/TFgraph-netural-networks-header.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjFzx_P7ogIv5kpZcabEMbL5hkABibribiGZtH-aiI6UPSBeXFD392VF7p30Aq57Jj4LxIxVcpeOQdpm-5pGDzk1kLG5Dj85pktgsNX1f7FC8sbTWnR6iS8H2WkW0ESZAn30OcDvWRcaUA1e7FgTKD0PzHRk-8Yn73eiePFnoN78uB2tmIF06ySty1_N3I/s1600/TFgraph-netural-networks-header.png\" /></a>

<a name=\"more\"></a><p></p>

<p><em>This article is also shared on the&nbsp;<a href=\"https://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html\" target=\"_blank\">Google Research Blog</a></em></p><p><em><br /></em></p>



<p>Objects and their relationships are ubiquitous in the world around us, and relationships can be as important to understanding an object as its own attributes viewed in isolation — for example: transportation networks, production networks, knowledge graphs, or social networks.  Discrete mathematics and computer science have a long history of formalizing such networks them as <a href=\"https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29\" target=\"_blank\"><i>graphs</i></a>, consisting of <i>nodes</i> arbitrarily connected by <i>edges</i> in various irregular ways. Yet most machine learning (ML) algorithms allow only for regular and uniform relations between input objects, such as a grid of pixels, a sequence of words, or no relation at all.</p> 
  
<p><a href=\"https://distill.pub/2021/gnn-intro/\" target=\"_blank\">Graph neural networks</a>, or GNNs for short, have emerged as a powerful technique to leverage both the graph’s connectivity (as in the older algorithms <a href=\"http://perozzi.net/projects/deepwalk/\" target=\"_blank\">DeepWalk</a> and <a href=\"https://snap.stanford.edu/node2vec/\" target=\"_blank\">Node2Vec</a>) and the input features on the various nodes and edges. GNNs can make predictions for graphs as a whole (Does this molecule react in a certain way?), for individual nodes (What’s the topic of this document, given its citations?) or for potential edges (Is this product likely to be purchased together with that product?). Apart from making predictions about graphs, GNNs are a powerful tool used to bridge the chasm to more typical neural network use cases. They encode a graph's <i>discrete</i>, <i>relational</i> information in a <i>continuous</i> way so that it can be included naturally in another deep learning system.</p>


<p>We are excited to announce the release of <a href=\"https://github.com/tensorflow/gnn\" target=\"_blank\">TensorFlow GNN 1.0</a>&nbsp;(TF-GNN), a production-tested library for building GNNs at large scale. It supports both modeling and training in TensorFlow as well as the extraction of input graphs from huge data stores. TF-GNN is built from the ground up for heterogeneous graphs where types and relations are represented by distinct sets of nodes and edges. Real-world objects and their relations occur in distinct types and TF-GNN's heterogeneous focus makes it natural to represent them.</p>

<p>Inside TensorFlow, such graphs are represented by objects of type <code>tfgnn.GraphTensor</code>. This is a composite tensor type (a collection of tensors in one Python class) accepted as a <a href=\"https://en.wikipedia.org/wiki/First-class_citizen\" target=\"_blank\">first-class citizen</a> in <code>tf.data.Dataset</code>,&nbsp;<code>tf.function</code>, etc. It stores both the graph structure and its features attached to nodes, edges and the graph as a whole. Trainable transformations of GraphTensors can be defined as Layers objects in the high-level <a href=\"https://www.tensorflow.org/guide/keras\" target=\"_blank\">Keras API</a>, or directly using the <code>tfgnn.GraphTensor</code> primitive.</p>

<h2>GNNs: Making predictions for an object in context</h2>


<p>For illustration, let’s look at one typical application of TF-GNN: predicting a property of a certain type of node in a graph defined by cross-referencing tables of a huge database. For example, a citation database of Computer Science (CS) arXiv papers with one-to-many cites and many-to-one cited relationships where we would like to predict the subject area of each paper.</p>

<p>Like most neural networks, a GNN is trained on a dataset of many labeled examples (~millions), but each training step consists only of a much smaller batch of training examples (say, hundreds). To scale to millions, the GNN gets trained on a stream of reasonably small subgraphs from the underlying graph. Each subgraph contains enough of the original data to compute the GNN result for the labeled node at its center and train the model. This process — typically referred to as subgraph sampling — is extremely consequential for GNN training. Most existing tooling accomplishes sampling in a batch way, producing static subgraphs for training. TF-GNN provides tooling to improve on this by sampling dynamically and interactively. </p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"moving image illustrating the process of subgraph sampling where small, tractable subgraphs are sampled from a larger graph to create input examples for GNN training.\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgdcxMo1kRE6OY7Xi_6sz1iah067JWj5Ic4-myeZwcwpqOVH9raRXhuxxp3xkra5arDS_IZHB6H_Aiwyjd-4daydjwFxtD9YohzAd7axB3260lTjwLo7PuQ1BxIBGZ83IK8WGKcwDyXJlGIPLOivTqG8FR3kzRUi8vOhlwxF-URAts2Vbc8ZXoEAwBxq50/s1600/image2.gif\" style=\"width: auto;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Pictured, the process of subgraph sampling where small, tractable subgraphs are sampled from a larger graph to create input examples for GNN training.</i></td></tr></tbody></table></div>


<p>TF-GNN 1.0 debuts a flexible Python API to configure dynamic or batch subgraph sampling at all relevant scales: interactively in a Colab notebook (like <a href=\"https://colab.research.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb\" target=\"_blank\">this one</a>), for efficient sampling of a small dataset stored in the main memory of a single training host, or distributed by <a href=\"https://beam.apache.org/\" target=\"_blank\">Apache Beam</a> for huge datasets stored on a network filesystem (up to hundreds of millions of nodes and billions of edges). For details, please refer to our user guides for <a href=\"https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/inmemory_sampler.md\" target=\"_blank\">in-memory</a> and <a href=\"https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/beam_sampler.md\" target=\"_blank\">beam-based</a> sampling, respectively.</p>

<p>On those same sampled subgraphs, the GNN’s task is to compute a hidden (or latent) state at the root node; the hidden state aggregates and encodes the relevant information of the root node's neighborhood. One classical approach is <a href=\"https://research.google/pubs/neural-message-passing-for-quantum-chemistry/\" target=\"_blank\">message-passing neural networks</a>. In each round of message passing, nodes receive messages from their neighbors along incoming edges and update their own hidden state from them. After <i>n</i> rounds, the hidden state of the root node reflects the aggregate information from all nodes within <i>n</i> edges (pictured below for <i>n</i> = 2). The messages and the new hidden states are computed by hidden layers of the neural network. In a heterogeneous graph, it often makes sense to use separately trained hidden layers for the different types of nodes and edges.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"moving image illustrating the process of subgraph sampling where small, tractable subgraphs are sampled from a larger graph to create input examples for GNN training.\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtpQadw1OPvgGJC11NfogydDRpg6-XBK_Cm3E9qq-9J5MYSe7_P8IR-0Yn8puV0Xmq4PifMOZtuKWxfran3yMuYK5YwBRV8Ut_3MNFlxdB-9-L7OTePqy3fUQi3X5PptSyiUjhgihynm5r3NBH0mrjbmW4B3j7H08xXruud7tLFhR-1FE_wQifcLt9ku8/s1600/image1.gif\" style=\"width: auto;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Pictured, a simple message-passing neural network where, at each step, the node state is propagated from outer to inner nodes where it is pooled to compute new node states. Once the root node is reached, a final prediction can be made.</i></td></tr></tbody></table></div>

<p>The training setup is completed by placing an output layer on top of the GNN’s hidden state for the labeled nodes, computing the loss (to measure the prediction error), and updating model weights by backpropagation, as usual in any neural network training.</p> 

<p>Beyond supervised training (i.e., minimizing a loss defined by labels), GNNs can also be trained in an unsupervised way (i.e., without labels). This lets us compute a <i>continuous</i> representation (or <i>embedding</i>) of the <i>discrete</i> graph structure of nodes and their features. These representations are then typically utilized in other ML systems. In this way, the discrete, relational information encoded by a graph can be included in more typical neural network use cases. TF-GNN supports a fine-grained specification of unsupervised objectives for heterogeneous graphs.</p>

<h2>Building GNN architectures</h2>

<p>The TF-GNN library supports building and training GNNs at various levels of abstraction.</p>

<p>At the highest level, users can take any of the predefined models bundled with the library that are expressed in Keras layers. Besides a small collection of models from the research literature, TF-GNN comes with a highly configurable model template that provides a curated selection of modeling choices that we have found to provide strong baselines on many of our in-house problems. The templates implement GNN layers; users need only to initialize the Keras layers.</p>


<div style=\"background: rgb(248, 248, 248); border: 0px; overflow: auto; width: auto;\"><pre style=\"line-height: 125%; margin: 0px;\"><span style=\"color: #444444; font-family: courier;\">import tensorflow_gnn as tfgnn
from tensorflow_gnn<span>.</span>models import mt_albis

def model_fn(graph_tensor_spec<span>:</span> tfgnn<span>.</span>GraphTensorSpec)<span>:</span>
  \"\"\"Builds a GNN as a Keras model<span>.\"\"\"</span>
  graph <span>=</span> inputs <span>=</span> tf<span>.</span>keras<span>.</span>Input(type_spec<span>=</span>graph_tensor_spec)

  # Encode input features (callback omitted <span>for</span> brevity)<span>.</span>
  graph <span>=</span> tfgnn<span>.</span>keras<span>.</span>layers<span>.</span>MapFeatures(
      node_sets_fn<span>=</span>set_initial_node_states)(graph)

  # <span>For</span> each <span>round</span> of message passing<span>...</span>
  <span>for</span> _ in range(<span>2</span>)<span>:</span>
    # <span>...</span> create <span>and</span> apply a Keras layer<span>.</span>
    graph <span>=</span> mt_albis<span>.</span>MtAlbisGraphUpdate(
        units<span>=128</span>, message_dim<span>=64</span>,
        attention_type<span>=\"</span>none\", simple_conv_reduce_type<span>=</span><span>\"mean\"</span>,
        normalization_type<span>=</span><span>\"layer\"</span>, next_state_type<span>=</span><span>\"residual\"</span>,
        state_dropout_rate<span>=0.2</span>, l2_regularization<span>=1e-5</span>,
    )(graph)

  <span>return</span> tf<span>.</span>keras<span>.</span>Model(inputs, graph)</span>
</pre></div>

<p>At the lowest level, users can write a GNN model from scratch in terms of primitives for passing data around the graph, such as broadcasting data from a node to all its outgoing edges or pooling data into a node from all its incoming edges (e.g., computing the sum of incoming messages). TF-GNN’s graph data model treats nodes, edges and whole input graphs equally when it comes to features or hidden states, making it straightforward to express not only node-centric models like the MPNN discussed above but also more general forms of <a href=\"https://arxiv.org/abs/1806.01261\" target=\"_blank\">GraphNets</a>. This can, but need not, be done with Keras as a modeling framework on the top of core TensorFlow. For more details, and intermediate levels of modeling, see the TF-GNN <a href=\"https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/gnn_modeling.md\" target=\"_blank\">user guide</a> and <a href=\"https://github.com/tensorflow/gnn/tree/main/tensorflow_gnn/models\" target=\"_blank\">model collection</a>.</p>

<h2>Training orchestration</h2>

<p>While advanced users are free to do custom model training, the <a href=\"https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/runner.md\" target=\"_blank\">TF-GNN Runner</a> also provides a succinct way to orchestrate the training of Keras models in the common cases. A simple invocation may look like this:</p>

<div style=\"background: rgb(248, 248, 248); border: 0px; overflow: auto; width: auto;\"><pre style=\"line-height: 125%; margin: 0px;\"><span style=\"color: #444444; font-family: courier;\">from tensorflow_gnn import runner

runner<span>.</span><span>run</span>(
   task<span>=</span>runner<span>.</span>RootNodeBinaryClassification(\"papers\", <span>...</span>),
   model_fn<span>=</span>model_fn,
   trainer<span>=</span>runner<span>.</span>KerasTrainer(tf<span>.</span>distribute<span>.</span>MirroredStrategy(), model_dir<span>=</span><span>\"</span><span>/</span>tmp<span>/</span>model\"),
   optimizer_fn<span>=</span>tf<span>.</span>keras<span>.</span>optimizers<span>.</span>Adam,
   epochs<span>=10</span>,
   global_batch_size<span>=128</span>,
   train_ds_provider<span>=</span>runner<span>.</span>TFRecordDatasetProvider(\"<span>/</span>tmp<span>/</span>train<span>*\"</span>),
   valid_ds_provider<span>=</span>runner<span>.</span>TFRecordDatasetProvider(\"<span>/</span>tmp<span>/</span>validation<span>*\"</span>),
   gtspec<span>=...</span>,
)</span>
</pre></div>

<p>The Runner provides ready-to-use solutions for ML pains like distributed training and <code>tfgnn.GraphTensor</code> padding for fixed shapes on Cloud TPUs. Beyond training on a single task (as shown above), it supports joint training on multiple (two or more) tasks in concert. For example, unsupervised tasks can be mixed with supervised ones to inform a final continuous representation (or embedding) with application specific inductive biases. Callers only need substitute the task argument with a mapping of tasks:</p>

<div style=\"background: rgb(248, 248, 248); border: 0px; overflow: auto; width: auto;\"><pre style=\"line-height: 125%; margin: 0px;\"><span style=\"color: #444444; font-family: courier;\">from tensorflow_gnn import runner
from tensorflow_gnn<span>.</span>models import contrastive_losses

runner<span>.</span><span>run</span>(
     task<span>=</span>{
        <span>\"classification\"</span><span>:</span> runner<span>.</span>RootNodeBinaryClassification(\"papers\", <span>...</span>),
        <span>\"dgi\"</span><span>:</span> contrastive_losses<span>.</span>DeepGraphInfomaxTask(<span>\"papers\"</span>),
      },
    <span>...</span>
)</span>
</pre></div>

<p>Additionally, the TF-GNN Runner also includes an implementation of <a href=\"https://www.tensorflow.org/tutorials/interpretability/integrated_gradients\" target=\"_blank\">integrated gradients</a> for use in model attribution. Integrated gradients&nbsp;output is a GraphTensor with the same connectivity as the observed GraphTensor but its features replaced with gradient values where larger values contribute more than smaller values in the GNN prediction. Users can inspect gradient values to see which features their GNN uses the most.</p> 

<h2>Conclusion</h2>

<p>In short, we hope TF-GNN will be useful to advance the application of GNNs in TensorFlow at scale and fuel further innovation in the field. If you’re curious to find out more, please try our <a href=\"https://colab.sandbox.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/ogbn_mag_e2e.ipynb\" target=\"_blank\">Colab demo</a> with the popular OGBN-MAG benchmark (in your browser, no installation required), browse the rest of our <a href=\"https://github.com/tensorflow/gnn/blob/main/tensorflow_gnn/docs/guide/overview.md\" target=\"_blank\">user guides and Colabs</a>, or take a look at our <a href=\"https://arxiv.org/abs/2207.03522\" target=\"_blank\">paper</a>.</p>


<h3>Acknowledgements</h3>
<p><em>The TF-GNN release 1.0 was developed by a collaboration between <b>Google Research</b> (Sami Abu-El-Haija, Neslihan Bulut, Bahar Fatemi, Johannes Gasteiger, Pedro Gonnet, Jonathan Halcrow, Liangze Jiang, Silvio Lattanzi, Brandon Mayer, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin, Dustin Zelle), <b>Google Core ML</b> (Arno Eigenwillig, Oleksandr Ferludin, Parth Kothari, Mihir Paradkar, Jan Pfeifer, Rachael Tamakloe), and <b>Google DeepMind</b> (Alvaro Sanchez-Gonzalez and Lisa Wang).</em></p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Tue, 06 Feb 2024 19:00:00 +0000"
2103,TensorFlow 2.15 update: hot-fix for Linux installation issue,https://blog.tensorflow.org/2023/12/tensorflow-215-update-hot-fix-linux-installation-issue.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVyX9h9jqUfN0LpVCwPg0iWNd4XDkKWd5hL7Wn9CwGLBZ5NyrEFTHHXu6yCxCaLL2szdaQMCkDuRdV4-JapbV329DYTjaH6ThpbzJlbvgunZK9dUgii1kAqcs-4zhyphenhyphenccaLsfffj7K3-8HFLybyOeaHjVl_n7jdPmD33o0TERamUgTdH7H3qhBLS2GuwtY/s1600/Tensorflow-septmber-update-social%20%282%29%20%281%29.png\" style=\"display: none;\" />

<em>Posted by the TensorFlow team</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEja7X29KGgTQBVvr3Xl2pyRZ-KQZJOM1-rq5XUr7AKu1vc_umj40H8y3mMoYq3wccQlA9XZ8OPtSx8SFJOOy8uSeX_MpoIAz7x44Cov-P95v9h85TLGgWCW2gqL6x3fbFfL1Xg6gZYkhylQKMQG7_8ilCTsm81bG87vT-3ttwn8IGdCPU1KfVVDjatuofs/s1600/Tensorflow-septmber-update-header%20%284%29.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEja7X29KGgTQBVvr3Xl2pyRZ-KQZJOM1-rq5XUr7AKu1vc_umj40H8y3mMoYq3wccQlA9XZ8OPtSx8SFJOOy8uSeX_MpoIAz7x44Cov-P95v9h85TLGgWCW2gqL6x3fbFfL1Xg6gZYkhylQKMQG7_8ilCTsm81bG87vT-3ttwn8IGdCPU1KfVVDjatuofs/s1600/Tensorflow-septmber-update-header%20%284%29.png\" /></a>

<a name=\"more\"></a><p></p>

<p>We are releasing a hot-fix for an installation issue affecting the TensorFlow installation process. The TensorFlow 2.15.0 Python package was released such that it requested <code>tensorrt</code>-related packages that cannot be found unless the user installs them beforehand or provides additional installation flags. This dependency affected anyone installing TensorFlow 2.15 alongside NVIDIA CUDA dependencies via <code>pip install tensorflow[and-cuda]</code>. Depending on the installation method, TensorFlow 2.14 would be installed instead of 2.15, or users could receive an installation error due to those missing dependencies.</p>
  
<p>To solve this issue as quickly as possible, we have released TensorFlow 2.15.0.post1 for the Linux x86_64 platform. This version removes the <code>tensorrt</code> Python package dependencies from the <code>tensorflow[and-cuda]</code> installation method. Support for TensorRT is otherwise unaffected as long as TensorRT is already installed on the system. Now, <code>pip install tensorflow[and-cuda]</code> works as originally intended for TensorFlow 2.15.</p>
  
<p>Using .post1 instead of a full minor release allowed us to push this release out quickly. However, please be aware of the following caveat: for users wishing to pin their Python dependency in a requirements file or other situation, under Python's version specification rules, <code>tensorflow[and-cuda]==2.15.0</code> will not install this fixed version. Please use <code>==2.15.0.post1</code> to specify this exact version on Linux platforms, or a fuzzy version specification, such as <code>==2.15.<sup>*</sup></code>, to specify the most recent compatible version of TensorFlow 2.15 on all platforms.</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Tue, 05 Dec 2023 22:00:00 +0000"
2104,Half-precision Inference Doubles On-Device Inference Performance,https://blog.tensorflow.org/2023/11/half-precision-inference-doubles-on-device-inference-performance.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjLe00EnYz8eKv0IaUn3KfPZnEa4ubDV7Ay2qqIFnOYEMLOh6ybHdH9RhUUrwhYgaccnNkTLe8pID8hvyyKd88JqJL2jK6-ePMxmsddBPcGPktJ_i_EUKmJI1x_YMv6gK3DHZzMqtWIw7Zc5Rx5eHDJH0zNSc-Cnp92ue4WYVWX9P5ATGCnOVeFx-jsI2c/s1600/TensorFlow_HalfPrecisionInference_1024x512.png\" style=\"display: none;\" />

<p><em>Posted by Marat Dukhan and Frank Barchard, Software Engineers</em></p><p>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoSV-IQalCzvy3lhcb61AXM4IyP_t2Gwj8nBK1IhvH4fbi0nRB7f3ljwOBCxbRnu_xZwAtZUREerXmSclm8Q09hpTdha_i1-Bvoyw3grtqhW0RrdtDGNMebc7BVZPb6wsBBF_pIYKqnse03TFW07K7OBSPuB_TuuDyn06cmDRKkB2c0lN0-fv3wJO0xKk/s1600/TensorFlow_HalfPrecisionInference_4209x1253.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoSV-IQalCzvy3lhcb61AXM4IyP_t2Gwj8nBK1IhvH4fbi0nRB7f3ljwOBCxbRnu_xZwAtZUREerXmSclm8Q09hpTdha_i1-Bvoyw3grtqhW0RrdtDGNMebc7BVZPb6wsBBF_pIYKqnse03TFW07K7OBSPuB_TuuDyn06cmDRKkB2c0lN0-fv3wJO0xKk/s1600/TensorFlow_HalfPrecisionInference_4209x1253.png\" /></a>

</p><a name=\"more\"></a><p></p>

<p>CPUs deliver the widest reach for ML inference and remain the default target for TensorFlow Lite. Consequently, improving CPU inference performance is a top priority, and we are excited to announce that we <b>doubled</b> floating-point inference performance in TensorFlow Lite’s <a href=\"https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html\" target=\"_blank\">XNNPack backend</a> by enabling half-precision inference on ARM CPUs. This means that more AI powered features may be deployed to older and lower tier devices.</p>

<p>Traditionally, TensorFlow Lite supported two kinds of numerical computations in machine learning models: a) floating-point using IEEE 754 single-precision (32-bit) format and b) quantized using low-precision integers. While single-precision floating-point numbers provide maximum flexibility and ease of use, they come at the cost of 4X overhead in storage and memory and exhibit a performance overhead compared to 8-bit integer computations. In contrast, half-precision (FP16) floating-point numbers pose an interesting alternative balancing ease-of-use and performance: the processor needs to transfer twice fewer bytes and each vector operation produces twice more elements. By virtue of this property, FP16 inference paves the way for 2X speedup for floating-point models compared to the traditional FP32 way.</p>

<p>For a long time FP16 inference on CPUs primarily remained a research topic, as the lack of hardware support for FP16 computations limited production use-cases. However, around 2017 new mobile chipsets started to include support for native FP16 computations, and by now most mobile phones, both on the high-end and the low-end. Building upon this broad availability, we are pleased to announce the general availability for half-precision inference in TensorFlow Lite and XNNPack.</p>

<h2>Performance Improvements</h2>

<p>Half-precision inference has already been battle-tested in production across Google Assistant, Google Meet, YouTube, and ML Kit, and <b>demonstrated close to 2X speedups</b> across a wide range of neural network architectures and mobile devices. Below, we present benchmarks on nine public models covering common computer vision tasks:</p>
<ol>
<li><a href=\"https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html\" target=\"_blank\">MobileNet v2</a> image classification [<a href=\"https://tfhub.dev/tensorflow/lite-model/mobilenet_v2_1.0_224/1/default/1\" target=\"_blank\">download</a>]</li>
<li><a href=\"https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html\" target=\"_blank\">MobileNet v3-Small</a> image classification [<a href=\"https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-small_224_1.0_float.tgz\" target=\"_blank\">download</a>]</li>
<li><a href=\"https://ai.googleblog.com/2018/03/semantic-image-segmentation-with.html\" target=\"_blank\">DeepLab v3</a> segmentation [<a href=\"https://tfhub.dev/tensorflow/lite-model/deeplabv3/1/metadata/1\" target=\"_blank\">download</a>]</li>
<li><a href=\"https://google.github.io/mediapipe/solutions/face_detection.html\" target=\"_blank\">BlazeFace</a> face detection [<a href=\"https://github.com/google/mediapipe/blob/master/mediapipe/models/face_detection_front.tflite\" target=\"_blank\">download</a>]</li>
<li><a href=\"https://arxiv.org/abs/1801.04381\" target=\"_blank\">SSDLite</a> 2D object detection [<a href=\"https://github.com/google/mediapipe/blob/master/mediapipe/models/ssdlite_object_detection.tflite\" target=\"_blank\">download</a>]</li>
<li><a href=\"https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html\" target=\"_blank\">Objectron</a> 3D object detection [<a href=\"https://github.com/google/mediapipe/blob/master/mediapipe/models/object_detection_3d_chair.tflite\" target=\"_blank\">download</a>]</li>
<li><a href=\"https://google.github.io/mediapipe/solutions/face_mesh\" target=\"_blank\">Face Mesh</a> landmarks  [<a href=\"https://github.com/google/mediapipe/blob/master/mediapipe/models/face_landmark.tflite\" target=\"_blank\">download</a>]</li>
<li><a href=\"https://google.github.io/mediapipe/solutions/hands.html\" target=\"_blank\">MediaPipe Hands</a> landmarks [<a href=\"https://github.com/google/mediapipe/blob/master/mediapipe/models/hand_landmark.tflite\" target=\"_blank\">download</a>]</li>
<li><a href=\"https://developers.googleblog.com/2020/04/mediapipe-knift-template-based-feature-matching.html\" target=\"_blank\">KNIFT</a> local feature descriptor [<a href=\"https://github.com/google/mediapipe/blob/master/mediapipe/models/knift_float.tflite\" target=\"_blank\">download</a>]</li>
</ol>

<p>These models were benchmarked on 5 popular mobile devices, including recent and older devices (Pixel 3a, Pixel 5a, Pixel 7, Galaxy M12 and Galaxy S22). The average speedup is shown below.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Graph of Average speedup for fp16 vs fp32\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9Jr2e_1Pg03j5uR3HDCiwIL7XPkVnVm4rfvGcZWzfZbE3yubuCmbmf2mxnOaj2lvbCUXRQLjkhkdAo7gPYQlmsud_hzIMoQz9-I-PLwnRngauGZf3dNZRAofs-EXyXNnK-nFa-rPAIOBz_AeB-H2k8PdLhqe2vv6yoO-FqfF3iMWhvbYskNSfWFK8fOg/s1600/image2.png\" style=\"width: auto;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Single-threaded inference speedup with half-precision (FP16) inference compared to single-precision (FP32) across 5 mobile devices. Higher numbers are better.</i></td></tr></tbody></table></div>



<p>The same models were also benchmarked on three laptop computers (MacBook Air M1, Surface Pro X and Surface Pro 9)</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"ALT TEXT\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOymbNjV4dNtIyoBJOEc0akMN8crCQ-Yb5LdEu1SRfzE0038Prb8zRtwI7J9tUF8fY_d5aSllTXa_ff0IeTc7UNDW-ezY4PI4-AC7SrpNa7x_VK7pKleZUb7IUz8Q6_wiV_NiLhOKq3sUnkuEsJXszdrI70wqpEgXYjwMr4sr31sJO-L48ZBg9hTpwZTI/s1600/image1.png\" style=\"width: auto;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Single-threaded inference speedup with half-precision (FP16) inference compared to single-precision (FP32) across 3 laptop computers. Higher numbers are better.</i></td></tr></tbody></table></div>

<p>Currently, the FP16-capable hardware supported in XNNPack is limited to ARM &amp; ARM64 devices with ARMv8.2 FP16 arithmetics extension, which includes Android phones starting with Pixel 3, Galaxy S9 (Snapdragon SoC), Galaxy S10 (Exynos SoC), iOS devices with A11 or newer SoCs, all Apple Silicon Macs, and Windows ARM64 laptops based with Snapdragon 850 SoC or newer.</p>

<h3>How Can I Use It?</h3>

<p>To benefit from the half-precision inference in XNNPack, the user must provide a floating-point (FP32) model with FP16 weights and special \"reduced_precision_support\" metadata to indicate model compatibility with FP16 inference. The metadata can be added during model conversion using the <code>_experimental_supported_accumulation_type</code> attribute of the <code><a href=\"https://www.tensorflow.org/api_docs/python/tf/lite/TargetSpec\" target=\"_blank\">tf.lite.TargetSpec</a></code> object:</p>

<div style=\"background: rgb(255, 255, 255); border: 0px; overflow: auto; width: auto;\"><pre style=\"line-height: 125%; margin: 0px;\"><span style=\"font-family: courier;\"><span style=\"color: #666666;\">...</span>
converter<span style=\"color: #666666;\">.</span>target_spec<span style=\"color: #666666;\">.</span>supported_types <span style=\"color: #666666;\">=</span> [tf<span style=\"color: #666666;\">.</span>float16]
converter<span style=\"color: #666666;\">.</span>target_spec<span style=\"color: #666666;\">.</span>_experimental_supported_accumulation_type <span style=\"color: #666666;\">=</span> tf<span style=\"color: #666666;\">.</span>dtypes<span style=\"color: #666666;\">.</span>float16</span>
</pre></div>

<p>When the compatible model is delegated to XNNPack on a hardware with native support for FP16 computations, XNNPack will transparently replace FP32 operators with their FP16 equivalents, and insert additional operators to convert model inputs from FP32 to FP16 and convert model outputs back from FP16 to FP32. If the hardware is not capable of FP16 arithmetics, XNNPack will perform model inference with FP32 calculations. Therefore, a single model can be transparently deployed on both recent and legacy devices.</p>

<p>Additionally, the XNNPack delegate provides an option to force FP16 inference regardless of the model metadata. This option is intended for development workflows, and in particular for testing end-to-end accuracy of the model when FP16 inference is used. In addition to devices with native FP16 arithmetics support, forced FP16 inference is supported on x86/x86-64 devices with AVX2 extension in emulation mode: all elementary floating-point operations are computed in FP32, then converted to FP16 and back to FP32. Note that such simulation is slow and not a bit-exact equivalent to native FP16 inference, but simulates the effects of restricted mantissa precision and exponent range in the native FP16 arithmetics. To force FP16 inference, either build TensorFlow Lite with <code>--define xnnpack_force_float_precision=fp16</code> Bazel option, or apply XNNPack delegate explicitly and add <code>TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16</code> flag to the <code>TfLiteXNNPackDelegateOptions.flags</code> bitmask passed into the <code>TfLiteXNNPackDelegateCreate</code> call:</p>

<div style=\"background: rgb(248, 248, 248); border: 0px; overflow: auto; width: auto;\"><pre style=\"line-height: 125%; margin: 0px;\"><span style=\"font-family: courier;\"><span style=\"color: #4285f4;\">TfLiteXNNPackDelegateOptions</span> xnnpack_options <span style=\"color: #666666;\">=</span>
    <span style=\"color: #4285f4;\">TfLiteXNNPackDelegateOptionsDefault</span><span style=\"color: #666666;\">()<span style=\"font-style: italic;\">;</span></span>
<span style=\"color: #666666;\">...</span>
xnnpack_options<span style=\"color: #666666;\">.</span>flags <span style=\"color: #666666;\">|=</span> TFLITE_XNNPACK_DELEGATE_FLAG_FORCE_FP16<span style=\"color: #666666; font-style: italic;\">;</span>
<span style=\"color: #4285f4;\">TfLiteDelegate</span><span style=\"color: #666666;\">*</span> xnnpack_delegate <span style=\"color: #666666;\">=</span>
    <span style=\"color: #4285f4;\">TfLiteXNNPackDelegateCreate</span><span style=\"color: #666666;\">(<span>&amp;</span></span>xnnpack_options<span style=\"color: #666666;\">)<span style=\"font-style: italic;\">;</span></span></span>
</pre></div>

<p>XNNPack provides <b>full feature parity between FP32 and FP16 operators</b>: all operators that are supported for FP32 inference are also supported for FP16 inference, and vice versa. In particular, <a href=\"https://blog.research.google/2021/03/accelerating-neural-networks-on-mobile.html\" target=\"_blank\">sparse inference</a> operators are supported for FP16 inference on ARM processors. Therefore, users can combine the performance benefits of sparse and FP16 inference in the same model.</p>

<h3>Future Work</h3>

<p>In addition to most ARM and ARM64 processors, the most recent Intel processors, code-named Sapphire Rapids, support native FP16 arithmetics via the AVX512-FP16 instruction set, and the recently announced AVX10 instruction set promises to make this capability widely available on x86 platform. We plan to optimize XNNPack for these instruction sets in a future release.</p><br />

<h3>Acknowledgements</h3>

<p><em>We would like to thank Alan Kelly, Zhi An Ng, Artsiom Ablavatski, Sachin Joglekar, T.J. Alumbaugh, Andrei Kulik, Jared Duke, Matthias Grundmann for contributions towards half-precision inference in TensorFlow Lite and XNNPack.</em></p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Wed, 29 Nov 2023 18:00:00 +0000"
2105,What's new in TensorFlow 2.15,https://blog.tensorflow.org/2023/11/whats-new-in-tensorflow-2-15.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVyX9h9jqUfN0LpVCwPg0iWNd4XDkKWd5hL7Wn9CwGLBZ5NyrEFTHHXu6yCxCaLL2szdaQMCkDuRdV4-JapbV329DYTjaH6ThpbzJlbvgunZK9dUgii1kAqcs-4zhyphenhyphenccaLsfffj7K3-8HFLybyOeaHjVl_n7jdPmD33o0TERamUgTdH7H3qhBLS2GuwtY/s1600/Tensorflow-septmber-update-social%20%282%29%20%281%29.png\" style=\"display: none;\" />

<em>Posted by the TensorFlow team</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEja7X29KGgTQBVvr3Xl2pyRZ-KQZJOM1-rq5XUr7AKu1vc_umj40H8y3mMoYq3wccQlA9XZ8OPtSx8SFJOOy8uSeX_MpoIAz7x44Cov-P95v9h85TLGgWCW2gqL6x3fbFfL1Xg6gZYkhylQKMQG7_8ilCTsm81bG87vT-3ttwn8IGdCPU1KfVVDjatuofs/s1600/Tensorflow-septmber-update-header%20%284%29.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEja7X29KGgTQBVvr3Xl2pyRZ-KQZJOM1-rq5XUr7AKu1vc_umj40H8y3mMoYq3wccQlA9XZ8OPtSx8SFJOOy8uSeX_MpoIAz7x44Cov-P95v9h85TLGgWCW2gqL6x3fbFfL1Xg6gZYkhylQKMQG7_8ilCTsm81bG87vT-3ttwn8IGdCPU1KfVVDjatuofs/s1600/Tensorflow-septmber-update-header%20%284%29.png\" /></a>

<a name=\"more\"></a><p></p>

<p>TensorFlow 2.15 has been released! Highlights of this release (and 2.14) include a much simpler installation method for NVIDIA CUDA libraries for Linux, oneDNN CPU performance optimizations for Windows x64 and x86, full availability of <code>tf.function</code> types, an upgrade to Clang 17.0.1, and much more!&nbsp;For the full release note, please check&nbsp;<a href=\"https://github.com/tensorflow/tensorflow/blob/r2.15/RELEASE.md\">here</a>.</p><div><b>Note:</b> Release updates on the new multi-backend Keras will be published on <a href=\"http://keras.io\" target=\"_blank\">keras.io</a> starting with Keras 3.0. For more information, please check <a href=\"https://keras.io/keras_core/announcement/.\" target=\"_blank\">here</a>.</div> 

<h2>TensorFlow Core</h2>

<h4>NVIDIA CUDA libraries for Linux</h4>

<p>The <code>tensorflow</code> pip package has a new, optional installation method for Linux that installs necessary NVIDIA CUDA libraries through pip. As long as the NVIDIA driver is already installed on the system, you may now run <code>pip install tensorflow[and-cuda]</code> to install TensorFlow's NVIDIA CUDA library dependencies in the Python environment. Aside from the NVIDIA driver, no other pre-existing NVIDIA CUDA packages are necessary. In TensorFlow 2.15, CUDA has been upgraded to version 12.2.</p>

<h4><a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20210930-enable-onednn-ops.md\" target=\"_blank\">oneDNN CPU performance optimizations</a></h4>

<p>For Windows x64 &amp; x86 packages, oneDNN optimizations are now enabled by default on X86 CPUs. These optimizations can be enabled or disabled by setting the environment variable <code>TF_ENABLE_ONEDNN_OPTS</code> to <code>1</code> (enable) or <code>0</code> (disable) before running TensorFlow. To fall back to default settings, simply unset the environment variable.</p>
  
<h4>tf.function</h4>
  
<p><code>tf.function</code> types are now fully available.</p>
<ul><blockquote> 
<li><code>tf.types.experimental.TraceType</code> now allows custom tf.function inputs to declare Tensor decomposition and type casting support.&nbsp;</li></blockquote><blockquote>
<li>Introducing <code>tf.types.experimental.FunctionType</code> as the comprehensive representation of the signature of tf.function callables. It can be accessed through the <code>function_type</code> property of <code>tf.function’s</code> and <code>ConcreteFunctions</code>. See the <code>tf.types.experimental.FunctionType</code> documentation for more details.&nbsp;</li></blockquote><blockquote>
<li>Introducing <code>tf.types.experimental.AtomicFunction</code> as the fastest way to perform TF computations in Python. This capability can be accessed through the <code>inference_fn</code> property of <code>ConcreteFunctions</code>. (Does not support gradients.) See the <code>tf.types.experimental.AtomicFunction</code> documentation for how to call and use it.</li>
</blockquote></ul>

<h4>Upgrade to Clang 17.0.1 and CUDA 12.2</h4>

<p>TensorFlow PIP packages are now being built with Clang 17 and CUDA 12.2 to improve performance for NVIDIA Hopper-based GPUs. Moving forward, Clang 17 will be the default C++ compiler for TensorFlow. We recommend upgrading your compiler to Clang 17 when building TensorFlow from source.</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Fri, 17 Nov 2023 18:55:00 +0000"
2106,Join us at the third Women in ML Symposium!,https://blog.tensorflow.org/2023/11/join-us-at-third-women-in-ml-symposium.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtd2GtibClrQS_fEHuU5Y8j25qKKsKNxCet5OOhia8zI09w7WmspXpsXQIV7I7HTNPtEqkP7wnlmE-gSj05yrIWaFKxYiocXH-pOsVen-Aq2dPb7AgtMz4lmGzHnXiLerzl8IHcajiwrkJkP-wIV5IPppznk_3BCi2AWQzcv4cW9B0AaXGIgzDCRFmiQE/s1600/WiML-2023-Social.png\" style=\"display: none;\" />

<em>Posted by Sharbani Roy – Senior Director, Product Management, Google </em>

<a name=\"more\"></a><p></p>

<div style=\"text-align: left;\"><br /></div><div>We're back with the third annual&nbsp;<a href=\"https://aidevelopers.withgoogle.com/events/wiml-symposium-2023?utm_source=tf&amp;utm_medium=embedded&amp;utm_campaign=reg\" target=\"_blank\">Women in Machine Learning Symposium</a>&nbsp;on&nbsp;<b>December 7, 2023</b>! Join us virtually<b> </b>from 9:30 am to 1:00 pm PT for an immersive and insightful&nbsp;set of deep dives for every level of Machine Learning experience.</div><div style=\"text-align: left;\"><br /></div>



<p>The <a href=\"https://aidevelopers.withgoogle.com/events/women-in-machine-learning-2022\" target=\"_blank\">Women in ML Symposium</a> is an inclusive event for anyone passionate about the transformative fields of Machine Learning (ML) and Artificial Intelligence (AI). Dive into the latest advancements in generative AI, explore the intricacies of privacy-preserving AI, dig into the underlying accelerators and ML frameworks that power models, and uncover practical applications of ML across multiple industries.</p>

<p>Our event offers sessions for all expertise levels, from beginners to advanced practitioners. Hear about what’s new in ML and building with Google AI from  our keynote speakers, gain insights from seasoned industry leaders across Google Health, Nvidia,  Adobe, and more – and discover a wealth of knowledge on topics ranging from foundational AI concepts to open source tools, techniques, and beyond.</p>

<p><a href=\"https://aidevelopers.withgoogle.com/events/wiml-symposium-2023?utm_source=tf&amp;utm_medium=embedded&amp;utm_campaign=reg\" target=\"_blank\">RSVP today</a> to secure your spot and explore our exciting agenda. We can't wait to see you there!</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Fri, 17 Nov 2023 17:00:00 +0000"
2107,Simulated Spotify Listening Experiences for Reinforcement Learning with TensorFlow and TF-Agents,https://blog.tensorflow.org/2023/10/simulated-spotify-listening-experiences-reinforcement-learning-tensorflow-tf-agents.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhP2qHWHXHUPcXak3sVnIQK59EBep2I4SxODdtyTmFFgkOujyn5Si1AHZTh9-xru72ZPZ_mFziQQX11NGawnhMh1EikNP1hTPgSKCX1EbPOeNU0Y1_UV7ZCQ8XFNROKmlffOcv5IXDzM6b3ckRYy1PaDKTUNgMttlHiU4CWOQwaHlh-Y3EbX8nrtxOZXJg/s1600/TensorFlowSimulated-Spotify-Listening-Experiences-social-V2.png\" style=\"display: none;\" />

<em>Posted by Surya Kanoria, Joseph Cauteruccio, Federico Tomasi, Kamil Ciosek, Matteo Rinaldi, and Zhenwen Dai – Spotify</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8rdkHFZg7bozT_90hTvkhtGf5w03xXaSnmIPq-v1phY0UK3QXvxyfSjvtepuHA3-32CUbdowZsTSx3QqMy37kh1t2iEj_uusIQgSd2z_7UuuoSH2A5qxNrwIFWub3oItpU12ZKj65PWnfbGfsxUp9DPTJ0jZaKPMYEO_CuhV2EQpIXsHQHwJzQwyPCdk/s1600/TensorFlowSimulated-Spotify-Listening-Experiences-header-V2.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8rdkHFZg7bozT_90hTvkhtGf5w03xXaSnmIPq-v1phY0UK3QXvxyfSjvtepuHA3-32CUbdowZsTSx3QqMy37kh1t2iEj_uusIQgSd2z_7UuuoSH2A5qxNrwIFWub3oItpU12ZKj65PWnfbGfsxUp9DPTJ0jZaKPMYEO_CuhV2EQpIXsHQHwJzQwyPCdk/s1600/TensorFlowSimulated-Spotify-Listening-Experiences-header-V2.png\" /></a>

<a name=\"more\"></a><p></p>

<h2>Introduction</h2>

<p>Many of our music recommendation problems involve providing users with ordered sets of items that satisfy users’ listening preferences and intent at that point in time. We base current recommendations on previous interactions with our application and, in the abstract, are faced with a sequential decision making process as we continually recommend content to users.</p>

<p>Reinforcement Learning (RL) is an established tool for sequential decision making that can be leveraged to solve sequential recommendation problems. We decided to explore how RL could be used to craft listening experiences for users. Before we could start training Agents, we needed to pick a RL library that allowed us to easily prototype, test, and potentially deploy our solutions.</p>

<p>At Spotify we leverage <a href=\"https://www.tensorflow.org/\" target=\"_blank\">TensorFlow</a> and the extended TensorFlow Ecosystem (<a href=\"https://www.tensorflow.org/tfx\" target=\"_blank\">TFX</a>, <a href=\"https://www.tensorflow.org/tfx/guide/serving?hl=en\" target=\"_blank\">TensorFlow Serving</a>, and so on) as part of our production Machine Learning Stack. We made the decision early on to leverage <a href=\"https://www.tensorflow.org/agents/overview\" target=\"_blank\">TensorFlow Agents</a> as our RL Library of choice, knowing that integrating our experiments with our production systems would be vastly more efficient down the line.</p>

<p>One missing bit of technology we required was an offline Spotify environment we could use to prototype, analyze, explore, and train Agents offline prior to online testing. The flexibility of the TF-Agents library, coupled with the broader advantages of TensorFlow and its ecosystem, allowed us to cleanly design a robust and extendable offline Spotify simulator. </p>

<p>We based our simulator design on TF-Agents <a href=\"https://www.tensorflow.org/agents/tutorials/2_environments_tutorial\" target=\"_blank\">Environment</a> primitives and using this simulator we developed, trained and evaluated sequential models for item recommendations, vanilla RL Agents (PPG, DQN) and a modified deep Q-Network, which we call the Action-Head DQN (AH-DQN), that addressed the specific challenges imposed by the large state and action space of our RL formulation. </p>

<p>Through live experiments we were able to show that our offline performance estimates were strongly correlated with online results. This then opened the door for large scale experimentation and application of Reinforcement Learning across Spotify, enabled by the technological foundations unlocked by TensorFlow and TF-Agents. </p>

<p>In this post we’ll provide more details about our RL problem and how we used TF-Agents to enable this work end to end.</p>

<h3>The RL Loop and Simulated Users</h3>

<div style=\"text-align: center;\"><img alt=\"Reinforcement Learning loop\" border=\"0\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbXOZI7T4iiAGjYNVId68JaJLtGqjk4g7a1058T3zuagJlm-cEBd2gNGUFYwnDu369omim5LVQL-1p0-SmbTRP6MsmziihreYJtH4trTsvkF50h2HyQMFZ-fkhkx_shg6JPZp2SMMCsC7fMreQkNxIxMBnnhl3pe-UaqAHU69IaWQsvBSePq_EDs1DzaA/s1600/TensorFlowSimulated%20Spotify%20Listening%20Experiences-charts-02.png\" width=\"100%\" /></div><div style=\"text-align: left;\">In RL, Agents interact with the environment continuously. At a given time step the Agent consumes an observation from the environment and, using this observation, produces an action given its policy at time t. The environment then processes the action and emits both a reward and the next observation (note that although typically used interchangeably, State is the complete information required to summarize the environment post action, Observation is the portion of this information actually exposed to the Agent). </div>

<p>In our case the reward emitted from the environment is the response of a user to music recommendations driven by the Agent’s action. In the absence of a simulator we would need to expose real users to Agents to observe rewards. We utilize a model-based RL approach to avoid letting an untrained Agent interact with real users (with the potential of hurting user satisfaction in the training process).</p>

<p>In this model-based RL formulation the Agent is not trained online against real users. Instead, it makes use of a user model that predicts responses to a list of tracks derived via the Agent’s action. Using this model we optimize actions in such a way as to maximize a (simulated) user satisfaction metric. During the training phase the environment makes use of this user model to return a predicted user response to the action recommended by the Agent.</p> 

<p>We use <a href=\"https://keras.io/\" target=\"_blank\">Keras</a> to design and train our user model. The serialized user model is then unpacked by the simulator and used to calculate rewards during Agent training and evaluation.</p>

<h3>Simulator Design</h3>
  
<p>In the abstract, what we needed to build was clear. We needed a way to simulate user listening sessions for the Agent. Given a simulated user and some content, instantiate a listening session and let the Agent drive recommendations in that session. Allow the simulated user to “react” to these recommendations and let the Agent adjust its strategy based on this result to drive some expected cumulative reward. </p>

<p>The TensorFlow Agents environment design guided us in developing the modular components of our system, each of which was responsible for different parts of the overall simulation.</p> 

<p>In our codebase we define an environment abstraction that requires the following be defined for every concrete instantiation:</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">class<span>&nbsp;</span></span><span class=\"hljs-title class_\">AbstractEnvironment</span>(<span class=\"hljs-title class_ inherited__\">ABC</span>):</span></code><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span>&nbsp;&nbsp; &nbsp;</span>_user_model: AbstractUserModel = <span class=\"hljs-literal\">None</span>
<span>&nbsp;&nbsp; &nbsp;</span>_track_sampler: AbstractTrackSampler = <span class=\"hljs-literal\">None</span>
<span>&nbsp;&nbsp; &nbsp;</span>_episode_tracker: EpisodeTracker = <span class=\"hljs-literal\">None</span>
<span>&nbsp;&nbsp; &nbsp;</span>_episode_sampler: AbstractEpisodeSampler = <span class=\"hljs-literal\">None</span>

<span class=\"hljs-meta\"><span>&nbsp;&nbsp; &nbsp;</span>@abstractmethod</span>
<span class=\"hljs-keyword\"><span>&nbsp;&nbsp; &nbsp;</span>def</span> <span class=\"hljs-title function_\">reset</span>(<span class=\"hljs-params\">self</span>) -&gt; <span class=\"hljs-type\">List</span>[<span class=\"hljs-built_in\">float</span>]:
      <span class=\"hljs-keyword\">pass</span>

<span class=\"hljs-meta\"><span>&nbsp;&nbsp; &nbsp;</span>@abstractmethod</span>
<span class=\"hljs-keyword\"><span>&nbsp;&nbsp; &nbsp;</span>def</span> <span class=\"hljs-title function_\">step</span>(<span class=\"hljs-params\">self, action: <span class=\"hljs-built_in\">float</span></span>) -&gt; (<span class=\"hljs-type\">List</span>[<span class=\"hljs-built_in\">float</span>], <span class=\"hljs-built_in\">float</span>, <span class=\"hljs-built_in\">bool</span>):
      <span class=\"hljs-keyword\">pass</span>

<span class=\"hljs-keyword\"><span>&nbsp;&nbsp; &nbsp;</span>def</span> <span class=\"hljs-title function_\">observation_space</span>(<span class=\"hljs-params\">self</span>) -&gt; <span class=\"hljs-type\">Dict</span>:
      <span class=\"hljs-keyword\">pass</span>

<span class=\"hljs-meta\"><span>&nbsp;&nbsp; &nbsp;</span>@abstractmethod</span>
<span class=\"hljs-keyword\"><span>&nbsp;&nbsp; &nbsp;</span>def</span> <span class=\"hljs-title function_\">action_space</span>(<span class=\"hljs-params\">self</span>) -&gt; <span class=\"hljs-type\">Dict</span>:
      <span class=\"hljs-keyword\">pass</span></span></code></td></tr></tbody></table>

<h4>Set-Up</h4>

<div style=\"text-align: left;\">At the start of Agent training we need to instantiate a simulation environment that has representations of hypothetical users and the content we’re looking to recommend to them. We base these instantiations on both real and hypothetical Spotify listening experiences. The critical information that defines these instantiations is passed to the environment via <code>_episode_sampler</code>. As mentioned, we also need to provide the simulator with a trained user model, in this case via <code>_user_model</code>.</div>

<div style=\"text-align: center;\"><img alt=\"Flow chart of agent training set up\" border=\"0\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMM94Ox1CCXpjzz9O4z922a6nVp89j-1gpPxob5myKJ3oywvkUqy9zhktuRGc0042_Tebmi9sU1rMe_8lbDnrBkulgMzgJ_95X0-rUKPzUmrh8U6QpysOMSaODqc-ZznPq8Api7PSOYfCAH5rKeyy8qqG7iGSQOzOFgOXxS65IQ15K-uhEuZr6OtNVFkU/s1600/TensorFlowSimulated-Spotify-Listening-Experiences-charts-03-V2.png\" width=\"100%\" /></div>
  
<h4>Actions and Observations</h4>

<p>Just like any Agent environment, our simulator requires that we specify the <code>action_spec</code> and <code>observation_spec</code>. Actions in our case may be continuous or discrete depending both on our Agent selection and how we propose to translate an Agent’s action into actual recommendations. We typically recommend ordered lists of items drawn from a pool of potential items. Formulating this action space directly would lead to it being combinatorially complex. We also assume the user will interact with multiple items, and as such previous work in this area that relies on single choice assumptions doesn’t apply.</p>

<div style=\"text-align: left;\">In the absence of a discrete action space consisting of item collections we need to provide the simulator with a method for turning the Agent’s action into actual recommendations. This logic is contained in the via <code>_track_sampler</code>. The “example play modes” proposed by the episode sampler contains information on items that can be presented to the simulated user. The track sampler consumes these and the agent’s action and returns actual item recommendations. </div>

<div style=\"text-align: center;\"><img alt=\"Flow chart of Agent actions_spec and observation_spec combining to create a recommendation\" border=\"0\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTslPiZ4ITRLSNrcBCLxVGyfmbDu0mxRj-iKGfNqnJoNw472CQg0YTdZHF2iE_pYwfUSw8kPtKCdMl4lCLXX0nYffYDMXU2l8wlnUfdlV_z9KTqPA8l6p3DOqAqL-ks2ZcwruY6f0qt5cXOVme9udBpsIsbqgCNlt4bKN71UnTOq4-9i9BHVA3O_xzUUo/s1600/TensorFlowSimulated%20Spotify%20Listening%20Experiences-charts-01.png\" width=\"100%\" /></div>

<h4>Termination and Reset</h4>

<p>We also need to handle the episode termination dynamics. In our simulator, the reset rules are set by the model builder and based on empirical investigations of interaction data relevant to a specific music listening experience. As a hypothetical, we may determine that 92% of listening sessions terminate after 6 sequential track skips and we’d construct our simulation termination logic to match. It also requires that we design abstractions in our simulator that allow us to check if the episode should be terminated after each step. </p>

<p>When the episode is reset the simulator will sample a new hypothetical user listening session pair and begin the next episode. </p>

<h4>Episode Steps</h4>

<p>As with standard TF Agents Environments we need to define the step dynamics for our simulation. We have optional dynamics of the simulation that we need to make sure are enforced at each step. For example, we may desire that the same item cannot be recommended more than once. If the Agent’s action indicates a recommendation of an item that was previously recommended we need to build in the functionality to pick the next best item based on this action.</p> 

<p>We also need to call the termination (and other supporting functions) mentioned above as needed at each step. </p>

<h4>Episode Storage and Replay</h4>
  
<p>The functionality mentioned up until this point collectively created a very complex simulation setup. While the TF Agents <a href=\"https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial\" target=\"_blank\">replay buffer</a> provided us with the functionality required to store episodes for Agent training and evaluation, we quickly realized the need to be able to store more episode data for debugging purposes, and more detailed evaluations specific to our simulation distinct from standard Agent performance measures. </p>

<p>We thus allowed for the inclusion of an expanded <code>_episode_tracker</code> that would store additional information about the user model predictions, information noting the sampled users/content pairs, and more.</p>

<h4>Creating TF-Agent Environments</h4>
  
<p>Our environment abstraction gives us a template that matches that of a standard TF-Agents Environment class. Some inputs to our environment need to be resolved before we can actually create the concrete TF-Agents environment instance. This happens in three steps. </p>

<p>First we define a specific simulation environment that conforms to our abstraction. For example:</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">PlaylistEnvironment</span>(<span class=\"hljs-title class_ inherited__\">AbstractEnvironment</span>):
    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">
        self,
        user_model: AbstractUserModel,
        track_sampler: AbstractTrackSampler,
        episode_tracker: EpisodeTracker,
        episode_sampler: AbstractEpisodeSampler,
	 ....
    </span>):

...</span></code></td></tr></tbody></table>

<p>Next we use an <i>Environment Builder Class</i> that takes as input a user model, track sampler, etc. <i>and</i> an environment class like <code>PlaylistEnvironment</code>. The builder creates a concrete instance of this environment:</p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">self.playlist_env: PlaylistEnvironment = environment_ctor(
            user_model=user_model,
            track_sampler=track_sampler,
            episode_tracker=episode_tracker,
            episode_sampler=self._eps_sampler,
        )</span></code></td></tr></tbody></table>

<p>Lastly, we utilize a conversion class that constructs a TF-Agents <i>Environment</i> from a concrete instance of ours:</p><div><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">TFAgtPyEnvironment</span>(py_environment.PyEnvironment):
  <span>&nbsp;&nbsp; &nbsp;</span><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, environment: AbstractEnvironment</span>):
        <span>&nbsp;&nbsp;</span><span class=\"hljs-built_in\">super</span>().__init__()
        <span>&nbsp;&nbsp;</span>self.env = environment</span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace\">
</span></code></td></tr></tbody></table><p>This is then executed internally to our Environment Builder:</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">EnvironmentBuilder</span>(<span class=\"hljs-title class_ inherited__\">AbstractEnvironmentBuilder</span>):

<span class=\"hljs-keyword\"><span>&nbsp;&nbsp; &nbsp;<span>&nbsp;&nbsp;</span></span>def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, ...</span>):
    <span>&nbsp;&nbsp;<span>&nbsp;&nbsp; &nbsp;</span></span>...

<span class=\"hljs-keyword\"><span>&nbsp;&nbsp; &nbsp;<span>&nbsp;&nbsp;</span></span>def</span> <span class=\"hljs-title function_\">get_tf_env</span>(<span class=\"hljs-params\">self</span>):
   <span>&nbsp;&nbsp; <span>&nbsp;&nbsp; &nbsp;</span></span>...
    <span>&nbsp;&nbsp;<span>&nbsp;&nbsp; &nbsp;</span></span>tf_env: TFAgtPyEnvironment = TFAgtPyEnvironment(
          <span>&nbsp;&nbsp; &nbsp;</span>self.playlist_env
        <span>&nbsp;&nbsp;<span>&nbsp;&nbsp; &nbsp;</span></span>)
    <span>&nbsp;&nbsp; &nbsp;<span>&nbsp;&nbsp;</span></span><span class=\"hljs-keyword\">return</span> tf_env</span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace\">
</span></code></td></tr></tbody></table>
  
  <div style=\"text-align: left;\"><br /></div><div style=\"text-align: left;\">The resulting TensorFlow Agents environment can then be used for Agent training. </div>

<div style=\"text-align: center;\"><img alt=\"Flow chart showing simulator design\" border=\"0\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBJEpC0OzUfjzQTfbHYiqJ_cmQXg6gIxXkQKbZu7a-ER7ciyDcRu4ZTimAzlex7hxRnVwgNGOLDS7kxidsoqObJfYIkA6G3PLZ-01vj5Uw22T7FWxTiaDIzFUROU4X5bzFwPakxVLDhb6v5V6q728Q8xEbs13TYDzpkCw1rqOarUWGn0g4CD9rL6S7iwM/s1600/TensorFlowSimulated%20Spotify%20Listening%20Experiences-charts-04.png\" width=\"100%\" /></div>
  
<div style=\"text-align: left;\">This simulator design allows us to easily create and manage multiple environments with a variety of different configurations as needed. </div>

<p>We next discuss how we used our simulator to train RL Agents to generate Playlists. </p>
  
  <h3>A Customized Agent for Playlist Generation</h3>

<p>As mentioned, Reinforcement Learning provides us with a method set that naturally accommodates the sequential nature of music listening; allowing us to adapt to users’ ever evolving preferences as sessions progress.</p>

<p>One specific problem we can attempt to use RL to solve is that of automatic music playlist generation. Given a (large) set of tracks, we want to learn how to create one optimal playlist to recommend to the user in order to maximize satisfaction metrics. Our use case is different from standard slate recommendation tasks, where usually the target is to select at most one item in the sequence. In our case, we assume we have a user-generated response for multiple items in the slate, making slate recommendation systems not directly applicable. Another complication is that the set of tracks from which recommendations are drawn is ever changing. </p>
  
<div style=\"text-align: left;\">We designed a DQN variant capable of handling these constraints that we called an Action Head DQN (AHDQN). </div>

<div style=\"text-align: center;\"><img alt=\"Moving image of AH-DQN network creating recommendations based on changing variables\" border=\"0\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNP6pB_bKP170hXMVf20NPQjUzkLattEawfZwtUcXrDAl2mkcBT1Bqp1eMcnEdnT78D3m2IQQjOjKoZfje8cXgXh1ZV0BgQ66hS96rM3RRkeFWQfUWnRs8VCtiv0NFU_aqWiye3W5GVFaArVaisiaiU0yJ6tF_KkzBkOg89B4qLzgZ2f0VyYR0x5MQzgk/s1600/image3.gif\" width=\"100%\" /></div>
  

<div style=\"text-align: left;\">The AH-DQN network takes as input the current state and an available action to produce a single Q value for the input action. This process is repeated for every possible item in the input. Finally, the item with the highest Q value is selected and added to the slate, and the process continues until the slate is full.</div>
  
<h4>Experiments In Brief</h4>

<div style=\"text-align: left;\">We tested our approach both offline and online at scale to assess the ability of the Agent to power our real-world recommender systems. In addition to testing the Agent itself we were also keen to assess the extent to which our offline performance estimates for various policies returned by our simulator matched (or at least directionally aligned) with our online results.</div><div class=\"separator\" style=\"clear: both; float: left;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6CsH-cNX-i8iHq9Yif6iFykOfd5YY8Z5vbx9yu4bfhbN4oZdiL_EBC9Fxa9PLXh6JOzFF8FeH5lzuCagEAFymWTtueBD6MVhhCUTTwS4-lgulTRjHeq28lTT0U64cVw88m9zY5UEnNbkqyDzZk4HGWughW7Ju-pekqH59uITWKIhwoDvcHk4YeBRXn9A/s463/image5.png\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;\"><img alt=\"Graph measuring simulated performance assessment by scaled online reward for different policies\" border=\"0\" height=\"270\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6CsH-cNX-i8iHq9Yif6iFykOfd5YY8Z5vbx9yu4bfhbN4oZdiL_EBC9Fxa9PLXh6JOzFF8FeH5lzuCagEAFymWTtueBD6MVhhCUTTwS4-lgulTRjHeq28lTT0U64cVw88m9zY5UEnNbkqyDzZk4HGWughW7Ju-pekqH59uITWKIhwoDvcHk4YeBRXn9A/w400-h270/image5.png\" style=\"width: auto;\" width=\"400\" /></a></div><div style=\"text-align: left;\"><br /></div><p>We observed this directional alignment for numerous naive, heuristic, model driven, and RL policies.</p>

<p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p>Please refer to our KDD paper for more information on the specifics of our model-based RL approach and Agent design.</p><div style=\"text-align: left;\"><br /></div>


<div style=\"text-align: left;\"><a href=\"https://research.atspotify.com/2023/07/automatic-music-playlist-generation-via-simulation-based-reinforcement-learning/\" target=\"_blank\">Automatic Music Playlist Generation via Simulation-based Reinforcement Learning</a></div>
<div style=\"text-align: left;\">Federico Tomasi, Joseph Cauteruccio, Surya Kanoria, Kamil Ciosek, Matteo Rinaldi, and Zhenwen Dai</div>
<div style=\"text-align: left;\">KDD 2023</div><br />

  <h2>Acknowledgements</h2>

<p>We’d like to thank all our Spotify teammates past and present who contributed to this work. Particularly, we’d like to thank Mehdi Ben Ayed for his early work in helping to develop our RL codebase. We’d also like to thank the <a href=\"https://github.com/tensorflow/agents/blob/master/tf_agents/AUTHORS\" target=\"_blank\">TensorFlow Agents team</a> for their support and encouragement throughout this project (and for the <a href=\"https://github.com/tensorflow/agents\" target=\"_blank\">library</a> that made it possible).</p>
</div><p></p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Thu, 19 Oct 2023 19:00:00 +0000"
2108,Building a board game with the TFLite plugin for Flutter,https://blog.tensorflow.org/2023/10/building-board-game-with-tflite-plugin-for-flutter.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOOPfPbEUuINDsxtTqIQnxPMLg_ThHYU0PSK5n12pt0hHAESQem0lIsnv-LO6LFCv8sq68n0dua5wPoXqiKvP8KF-WFF1s1-vpnTD3DMBrbL0IA019h1UWREjSCDEE2CEkImJ71OFBmchJWt8X5jtZkJiVIV06Ogxqhp12hzvvinVDNh1rMTecH9KIjZ8/s1600/SOCIAL-Building-a-board-game-with-the-TFlite-plugin-for-Flutter.png\" style=\"display: none;\" />

<p><em>Posted by Wei Wei, Developer Advocate</em></p><p>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9dGb3glYqGGek7G5cko5_R9dDez1txACGP5qt-dmn7EGgxdjGrt7ZIjNXmeu3KudSTHFs-Euh3xybYplG3arzRIcA8J9jx1gneCUEh_I9QRTMUWkvShjNW9ilbQLxd49Ocvts_w0T-FKCmjFn3lewlxbnkY0QGgzLKGofMwuHRR1bzamk7USpW0LnJmQ/s1600/HEADER-Building-a-board-game-with-the-TFlite-plugin-for-Flutter.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9dGb3glYqGGek7G5cko5_R9dDez1txACGP5qt-dmn7EGgxdjGrt7ZIjNXmeu3KudSTHFs-Euh3xybYplG3arzRIcA8J9jx1gneCUEh_I9QRTMUWkvShjNW9ilbQLxd49Ocvts_w0T-FKCmjFn3lewlxbnkY0QGgzLKGofMwuHRR1bzamk7USpW0LnJmQ/s1600/HEADER-Building-a-board-game-with-the-TFlite-plugin-for-Flutter.png\" /></a>

</p><a name=\"more\"></a><p></p>

<p>In our previous blog posts <a href=\"https://blog.tensorflow.org/2021/10/building-board-game-app-with-tensorflow.html\" target=\"_blank\">Building a board game app with TensorFlow: a new TensorFlow Lite reference app</a> and <a href=\"https://blog.tensorflow.org/2022/09/building-reinforcement-learning-agent-with-JAX-and-deploying-it-on-android-with-tensorflow-lite.html\" target=\"_blank\">Building a reinforcement learning agent with JAX, and deploying it on Android with TensorFlow Lite</a>, we demonstrated how to train a reinforcement learning (RL) agent with TensorFlow, TensorFlow Agents and JAX respectively, and then deploy the converted TFLite model in an Android app using TensorFlow Lite, to play a simple board game ‘Plane Strike’.</p>

<div style=\"text-align: left;\">While these end-to-end tutorials are helpful for Android developers, we have heard from the Flutter developer community that it would be interesting to make the app cross-platform. Inspired by the officially released <a href=\"https://blog.tensorflow.org/2023/08/the-tensorflow-lite-plugin-for-flutter-officially-available.html\" target=\"_blank\">TensorFlow Lite Plugin for Flutter</a> recently, we are going to write one last tutorial and port the app to Flutter.</div>

<div style=\"text-align: center;\"><img alt=\"Flow Chart illustrating training a Reinforncement Learning (RL) Agent with TensorFlow, TensorFlow Agents and JAX, deploying the converted model in an Android app and Flutter using the TensorFlow Lite plugin\" border=\"0\" id=\"imgCaption\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQDl4Xa2yqwJdBabBma9TpiDP6oOt9DApNSnBd0u3dammHHQV5QuMS8WYj0h2xEMPWyG4bZKSDK0pFzjavXn7XscaIItUytQ-6GAtV2PBGyUZg0T7z_DXSHAhDDu8YgDeZxomS0ingTQI2llhqShlFPMVqgAK3UtgbaxKa-AE05RPabKfVFK_ezxyh5pc/s1600/CHART-Building-a-board-game-with-the-TFlite-plugin-for-Flutter.png\" width=\"100%\" /></div>

<p style=\"text-align: left;\">Since we already have the model trained with TensorFlow and converted to TFLite, we can just load the model with TFLite interpreter:</p>

<div style=\"text-align: left;\"><span id=\"docs-internal-guid-01acc8ad-7fff-b6f6-b598-e154539fd1a6\"><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\"><table style=\"border-collapse: collapse; border: none;\"><colgroup><col width=\"718\" /></colgroup><tbody><tr style=\"height: 0pt;\"><td style=\"background-color: #f0f0f0; overflow: hidden; padding: 5pt; vertical-align: top;\"><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span style=\"color: #444444; font-size: 12pt; font-weight: 700; vertical-align: baseline;\">void</span><span style=\"color: #444444; font-size: 12pt; vertical-align: baseline;\"> _loadModel() </span><span style=\"color: #444444; font-size: 12pt; font-weight: 700; vertical-align: baseline;\">async</span><span style=\"color: #444444; font-size: 12pt; vertical-align: baseline;\"> {</span><span style=\"color: #444444; font-size: 12pt; vertical-align: baseline;\"><br /></span><span style=\"color: #444444; font-size: 12pt; vertical-align: baseline;\">&nbsp; </span><span style=\"color: #888888; font-size: 12pt; vertical-align: baseline;\">// Create the interpreter</span><span style=\"color: #444444; font-size: 12pt; vertical-align: baseline;\"><br /></span><span style=\"color: #444444; font-size: 12pt; vertical-align: baseline;\">&nbsp; _interpreter = </span><span style=\"color: #444444; font-size: 12pt; font-weight: 700; vertical-align: baseline;\">await</span><span style=\"color: #444444; font-size: 12pt; vertical-align: baseline;\"> Interpreter.fromAsset(_modelFile);</span><span style=\"color: #444444; font-size: 12pt; vertical-align: baseline;\"><br /></span><span style=\"color: #444444; font-size: 12pt; vertical-align: baseline;\">}</span></span></p></td></tr></tbody></table></div>
  
<p>Then we pass in the user board state and help the game agent identify the most promising position to strike next (please refer to our previous blog posts if you need a refresher on the game rules) by running TFLite inference:</p><p><span id=\"docs-internal-guid-1fd4a4ed-7fff-d43b-dab8-fcc9246ae083\"></span></p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\"><table style=\"border-collapse: collapse; border: none;\"><colgroup><col width=\"718\" /></colgroup><tbody><tr style=\"height: 0pt;\"><td style=\"background-color: #f0f0f0; overflow: hidden; padding: 5pt; vertical-align: top;\"><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: courier;\"><span style=\"background-color: #f0f0f0; color: #397300; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">int</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> predict(</span><span style=\"background-color: #f0f0f0; color: #397300; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">List</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&lt;</span><span style=\"background-color: #f0f0f0; color: #397300; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">List</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&lt;</span><span style=\"background-color: #f0f0f0; color: #397300; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">double</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&gt;&gt; boardState) {</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; </span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">var</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> input = [boardState];</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; </span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">var</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> output = </span><span style=\"background-color: #f0f0f0; color: #397300; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">List</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.filled(_boardSize * _boardSize, </span><span style=\"background-color: #f0f0f0; color: #880000; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">)</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp; .reshape([</span><span style=\"background-color: #f0f0f0; color: #880000; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">, _boardSize * _boardSize]);</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; </span><span style=\"background-color: #f0f0f0; color: #888888; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// Run inference</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; _interpreter.run(input, output);</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; </span><span style=\"background-color: #f0f0f0; color: #888888; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">// Argmax</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; </span><span style=\"background-color: #f0f0f0; color: #397300; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">double</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> max = output[</span><span style=\"background-color: #f0f0f0; color: #880000; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">][</span><span style=\"background-color: #f0f0f0; color: #880000; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0</span>
<span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">];</span>
<span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span>
<span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; </span><span style=\"background-color: #f0f0f0; color: #397300; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">int</span>
<span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> maxIdx = </span><span style=\"background-color: #f0f0f0; color: #880000; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">;</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; </span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">for</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (</span><span style=\"background-color: #f0f0f0; color: #397300; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">int</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> i = </span><span style=\"background-color: #f0f0f0; color: #880000; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">1</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">; i &lt; _boardSize * _boardSize; i++) {</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; </span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">if</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (max &lt; output[</span><span style=\"background-color: #f0f0f0; color: #880000; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">][i]) {</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp; maxIdx = i;</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; &nbsp; max = output[</span><span style=\"background-color: #f0f0f0; color: #880000; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">0</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">][i];</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; &nbsp; }</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; }</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">&nbsp; </span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">return</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> maxIdx;</span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span><span style=\"background-color: #f0f0f0; color: #444444; font-size: 12pt; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">}</span></span></p></td></tr></tbody></table></div></span></div>

<p>That's it! With some <a href=\"https://github.com/tensorflow/flutter-tflite/blob/main/example/reinforcement_learning/lib/main.dart\" target=\"_blank\">additional Flutter frontend code</a> to render the game boards and track game progress, we can immediately run the game on both Android and iOS (currently the plugin only supports these two mobile platforms). You can find the complete code on <a href=\"https://github.com/tensorflow/flutter-tflite/tree/main/example/reinforcement_learning\" target=\"_blank\">GitHub</a>.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"ALT TEXT\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6OW57o43rPY9TavqdgnkbUoNqI1x0Fm57Avpuazd7DD4syTVW3NUWnvc6noNebb56Vm62EojXZBRhyphenhyphenj87JZVXSACv6BfIUAiiKnioK2Iy2wgKInFi0u_41Cnpkv9ZJUkPyfG2EhzxE1ckQuqJmZ1rRbogk0Tkdb-8gEzDDUGUgeQtoz0dy0h_YX0fxyw/s1600/Board-game-TF-Lite-Flutter%20%282%29.gif\" style=\"width: 80%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"></td></tr></tbody></table></div>

<div style=\"text-align: left;\">If you want to dig digger, there are a couple of things you can try:</div>
<ol><blockquote>
<li>Convert the TFAgents-trained model to TFLite and run it with the plugin</li>
<li>Leverage the RL technique we have used and build a new agent for the tic tac toe game in the Flutter <a href=\"https://docs.flutter.dev/resources/games-toolkit\" target=\"_blank\">Casual Games Toolkit</a>. You will need to create a new RL environment and train the model from scratch before deployment, but the core concept and technique are pretty much the same.</li>
  </blockquote></ol>

<p>This concludes this mini-series of blogs on leveraging TensorFlow/JAX to build games for Android and Flutter. And we very much look forward to all the exciting things you build with our tooling, so be sure to share them with <a href=\"https://twitter.com/googledevs\" target=\"_blank\">@googledevs</a>, <a href=\"https://twitter.com/TensorFlow\" target=\"_blank\">@TensorFlow</a>, and your developer communities!</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Wed, 18 Oct 2023 17:00:00 +0000"
2109,People of AI: Season 2,https://blog.tensorflow.org/2023/10/people-of-ai-season-2.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjBAa6kNgbBM4T86yJ0ftJUiLg_Z2cJbcrk_rmCJ_RQwiqwWTurWvIqbZoFgm6u_r1uLV56dTm1FljgiLYYUB33WMtJV8oWLW_i9AF0_DI-lP1CajHpnaobp-OpbrtsVbYfjEbIef0fBVtPqZjIKhuILcMEMOwXSL0w6H3SXIauKVfSTTcGoecfSMzPxOs/s1600/People-With-AI-Season-2-Social.png\" style=\"display: none;\" />

<p><em>Posted by <a href=\"https://www.linkedin.com/in/ashleyoldacre/\" target=\"_blank\">Ashley Oldacre</a></em></p><p>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN5m9hnI-bY5xpmSLfczy02-dG2_82dr1ZR_HAF-XJUhmR1AjFPMDhhYQQPTdilLepav4hB84xTo0RPUj1pDpQKiBiVCyx_fzp1uxsgwXm6X05c99r32nzZQTDSYwIM9-32jOP3ENkII029UTbGhCRDTf3ShuXpgfh8BiiFrbQY0YhGHSmaL_hYBWBOHE/s1600/People-of-AI-Season-2-Banner.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN5m9hnI-bY5xpmSLfczy02-dG2_82dr1ZR_HAF-XJUhmR1AjFPMDhhYQQPTdilLepav4hB84xTo0RPUj1pDpQKiBiVCyx_fzp1uxsgwXm6X05c99r32nzZQTDSYwIM9-32jOP3ENkII029UTbGhCRDTf3ShuXpgfh8BiiFrbQY0YhGHSmaL_hYBWBOHE/s1600/People-of-AI-Season-2-Banner.png\" /></a>

</p><a name=\"more\"></a><p></p>

<p><em>If you are joining us for the first time, you can <a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">binge listen to our amazing 8 episodes</a> from Season 1 wherever you get your podcasts.</em></p> 

<p>We are back for another season of People of AI with a new lineup of incredible guests! I am so excited to introduce my new co-host <a href=\"https://www.linkedin.com/in/gus-martins-64ab5891/\" target=\"_blank\">Luiz Gustavo Martins</a> as we meet inspiring people with interesting stories in the field of Artificial Intelligence.</p>

<p><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Last season</a> we focused on the incredible journeys that our guests took to get into the field of AI. Through our stories, we highlighted that no matter who you are, what your interests are, or what you work on, there is a place for anyone to get into this field. We also explored how much more accessible the technology has become over the years, as well as the importance of building AI-related products responsibly and ethically. It is easier than ever to use tools, platforms and services powered by machine learning to leverage the benefits of AI, and break down the barrier of entry.</p>

<p>For <b>season 2</b>, we will feature amazing conversations, focusing on Generative AI! Specifically, we will be discussing the explosive growth of Generative AI tools and the major technology shift that has happened in recent months. We will dive into various topics to explore areas where Generative AI can contribute tremendous value, as well as boost both productivity and economic growth. We will also continue to explore the personal paths and career development of this season’s guests as they share how their interest in technology was sparked, how they worked hard to get to where they are today, and explore what it is that they are currently working on. </p>

<p>Starting today, we will release one new episode of season 2 per week. Listen to the first episode on the <a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">People of AI</a> site or wherever you get your podcasts. And stay tuned for later in the season when we premiere our first video podcasts as well!</p>
<ul><blockquote>
<li><b><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Episode 1</a></b>: meet your hosts, <b><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Ashley</a></b> and <b><a href=\"https://www.linkedin.com/in/gus-martins-64ab5891/\" target=\"_blank\">Gus</a></b> and learn about Generative AI, Bard and the big shift that has dramatically changed the industry.&nbsp;</li></blockquote><blockquote>
<li><b><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Episode 2</a></b>: meet <b><a href=\"https://www.linkedin.com/in/sunita-verma-1a4491/\" target=\"_blank\">Sunita Verma</a></b>, a long-time Googler, as she shares her personal journey from Engineering to CS, and into Google. As an early pioneer of AI and Google Ads, we will talk about the evolution of AI and how Generative AI will transform the way we work.&nbsp;</li></blockquote><blockquote>
<li><b><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Episode 3</a></b>: meet <b><a href=\"https://sayak.dev/\" target=\"_blank\">Sayak Paul</a></b>, a Google Developer Expert (GDE) as we explore what it means to be a GDE and how to leverage the power of your community through community contributions.&nbsp;</li></blockquote></ul><ul><blockquote>
<li><b><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Episode 4</a></b>: meet <b><a href=\"https://www.linkedin.com/in/crispinvz/\" target=\"_blank\">Crispin Velez</a></b>, the lead for Cloud’s Vertex AI as we dig into his experience in Cloud working with customers and partners on how to integrate and deploy AI. We also learn how he grew his AI developer community in LATAM from scratch.&nbsp;</li></blockquote></ul><ul><blockquote>
<li><b><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Episode 5</a></b>: meet <b><a href=\"https://www.joyceshen.com/about/\" target=\"_blank\">Joyce Shen</a></b>, venture capital/private equity investor. She shares her fascinating career in AI and how she has worked with businesses to spot AI talent, incorporate AI technology into workflows and implement responsible AI into their products.&nbsp;</li></blockquote><blockquote>
<li><b><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Episode 6</a></b>: meet <b><a href=\"https://www.linkedin.com/in/anne-simonds/\" target=\"_blank\">Anne Simonds</a></b> and <b>Brian Gary</b>, founders of Muse https://www.museml.com. Join us as we talk about their recent journeys into AI and their new company which uses the power of Generative AI to spark creativity.&nbsp;</li></blockquote></ul><ul><blockquote>
<li><b><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Episode 7</a></b>: meet <b><a href=\"https://www.linkedin.com/in/tulsee-doshi/\" target=\"_blank\">Tulsee Doshi</a></b>, product lead for <a href=\"https://ai.google/responsibility/responsible-ai-practices/\" target=\"_blank\">Google’s Responsible AI</a> efforts as we discuss the development of Google-wide resources and best practices for developing more inclusive, diverse, and ethical algorithm driven products.&nbsp;</li></blockquote><blockquote>
<li><b><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Episode 8</a></b>: meet<b> <a href=\"https://www.linkedin.com/in/winner/\" target=\"_blank\">Jeanine Banks</a></b>, Vice President and General Manager of Google Developer X and Head of Developer Relations. Join us as we debunk AI and get down to what Generative AI really is, how it has changed over the past few months and will continue to change the developer landscape.&nbsp;</li></blockquote><blockquote>
<li><b><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Episode 9</a></b>: meet <b>Simon Tokumine</b>, Director of Product Management at Google. We will talk about how AI has brought us into the era of task-orientated products and is fueling a new community of makers.</li></blockquote></ul>

<p><a href=\"https://peopleofai.libsyn.com/\" target=\"_blank\">Listen now</a> to the first episode of Season 2. We can’t wait to share the stories of these exceptional People of AI with you!</p>


<p><em><small>This podcast is sponsored by Google. Any remarks made by the speakers are their own and are not endorsed by Google.</small></em></p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Tue, 17 Oct 2023 17:00:00 +0000"
2110,Pre-processing temporal data made easier with TensorFlow Decision Forests and Temporian,https://blog.tensorflow.org/2023/09/forecasting-with-tensorflow-decision-forests-and-temporian.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgO4WDCWDuV4LT0FtXw_qBaJKOFjEkHrFOBQ0z4WZaA6keEQEtFc4ZNwmZTyP2eCPHjN_qIA41VzgJeVCnYT6pTWCic-9gy5hjwknTEqGKfyWrJjtdH20lcxRkI7_N-UEPVgGxvDGvJDeeVm8h4QxVIa2mknX0l2WyG0WAs5Y259erSlTsQpbO8q3umCAI/s1600/TensorFlow-Training-model-on-temporal-data-with-TensorFlow-and-Temporian-header-social-v2.png\" style=\"display: none;\" />

<em>Posted by Google: Mathieu Guillame-Bert, Richard Stotz, Robert Crowe, Luiz GUStavo Martins (Gus), Ashley Oldacre, Kris Tonthat, Glenn Cameron, and Tryolabs: Ian Spektor, Braulio Rios, Guillermo Etchebarne, Diego Marvid, Lucas Micol, Gonzalo Marín, Alan Descoins, Agustina Pizarro, Lucía Aguilar, Martin Alcala Rubi</em>

<a href=\"\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiXMzZM-8kD4Hv3XAwSxdZl1k5jAUCzDkfKdog5XWfPE8l-cfcDLmAPxhG8nd_ZbuGeyeulW9LsbSVsSkAQf3i_-DV6o71xrb57ZfVQ6cUClvMB-h1_rXjVIM4FK9V2GCRkWsIofgZ3hdaoJiYjRyzk-Mrf31-FEPJ6C4VhCoAjiCttPP1Sja53g-Tzz9Y/s1600/TensorFlow-Training-model-on-temporal-data-with-TensorFlow-and-Temporian-header-v2.png\" /></a>

<a name=\"more\"></a><p></p>

<b>Temporal data</b> is <b>omnipresent</b> in applied machine learning applications. Data often changes over time or is only available or valuable at a certain point in time. For example, market prices and weather conditions change constantly. Temporal data is also often <b>highly discriminative</b> in decision-making tasks. For example, the rate of change and interval between two consecutive heartbeats provides valuable insights into a person's physical health, and temporal patterns of network logs are used to detect configuration issues and intrusions. Hence, it is essential to incorporate temporal data and temporal information in ML applications.<div><br /></div><div><span id=\"docs-internal-guid-7da84d0c-7fff-5126-247c-2149c93220d4\"><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\"><table style=\"border-collapse: collapse; border: none;\"><colgroup><col width=\"719\" /></colgroup><tbody><tr style=\"height: 0pt;\"><td style=\"background-color: #fff2cc; border-bottom: solid #f1c232 1pt; border-color: rgb(241, 194, 50); border-left: solid #f1c232 1pt; border-right: solid #f1c232 1pt; border-style: solid; border-top: solid #f1c232 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\"><p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: inherit;\"><span style=\"background-color: transparent; font-weight: 700; vertical-align: baseline;\">INFO:&nbsp; </span><a href=\"https://github.com/google/temporian\"><span style=\"background-color: transparent; color: #1155cc; vertical-align: baseline;\">Temporian</span></a><span style=\"background-color: transparent; vertical-align: baseline;\"> is a new open-source Python library for preprocessing and feature engineering </span><span style=\"background-color: transparent; font-weight: 700; vertical-align: baseline;\">temporal data</span><span style=\"background-color: transparent; vertical-align: baseline;\"> for machine learning applications. It is developed in collaboration between Google and </span><a href=\"https://tryolabs.com/\"><span style=\"background-color: transparent; color: #1155cc; vertical-align: baseline;\">Tryolabs</span></a><span style=\"background-color: transparent; vertical-align: baseline;\">. Check the sister </span><a href=\"https://tryolabs.com/blog/tryolabs-google-introducing-temporian\"><span style=\"background-color: transparent; color: #1155cc; vertical-align: baseline;\">blog post</span></a><span style=\"background-color: transparent; vertical-align: baseline;\"> for more details.</span></span></p></td></tr></tbody></table></div></span></div><div><br /><div>This blog post demonstrates how to train a forecasting model on transactional data. Specifically, we will show how to forecast the total weekly sales from individual sales records. For the modeling part, we will use <a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\">TensorFlow Decision Forests</a> as they are well suited to handle temporal data. To feed the transaction data to our model, and to compute temporal specific features, we will use <a href=\"https://temporian.readthedocs.io/en/latest/\" target=\"_blank\"><b>Temporian</b></a>, <b>a newly released library</b> designed for ingesting and aggregating transactional data from multiple non-synchronized sources.

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"ALT TEXT\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj71gkz0PpC-9zI8sxLx-pU6b3C7ph-A6QXTKDSEpwiiMjSPXiAzYxfw7zvrucntmhqRMkjVke9eKdBTAxR6xSbArO9NT0WN93S8L9RSbTyIsGjX-bXvJ3gvqrwLiMpV0NRWLRQPboGuKRW_f1qu1IMDAXFd4-mfl_o8pM1iVIOJRE_aSjmBdelqtoPpgo/s1600/image6.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>Time series are the most commonly used representation for temporal data. They consist of uniformly sampled values, which can be useful for representing aggregate signals. However, time series are sometimes not sufficient to represent the richness of available data. Instead, multivariate time series can represent multiple signals together, while time sequences or event sets can represent non-uniformly sampled measurements. Multi-index time sequences can be used to represent relations between different time sequences. In this blog post, we will use the <b>multivariate multi-index time sequence</b>, also known as <b>event sets</b>. Don’t worry, they’re not as complex as they sound.</p>

<p><span id=\"docs-internal-guid-1e2295cf-7fff-2efb-0717-597d19036c23\"></span></p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\"><table style=\"border-collapse: collapse; border: none;\"><colgroup><col width=\"719\" /></colgroup><tbody><tr style=\"height: 0pt;\"><td style=\"background-color: #fff2cc; border-bottom: solid #f1c232 1pt; border-color: rgb(241, 194, 50); border-left: solid #f1c232 1pt; border-right: solid #f1c232 1pt; border-style: solid; border-top: solid #f1c232 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\"><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span style=\"font-family: inherit;\">Examples of temporal data include:</span></span></p><ul style=\"margin-bottom: 0px; margin-top: 0px; padding-inline-start: 48px;\"><li dir=\"ltr\" style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;\"><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: inherit;\"><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Weather and other environmental data</span><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> for weather forecasting, soil profile forecasting and crop yield optimization, temperature tracking, and climate change characterization.</span></span></p></li><li dir=\"ltr\" style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;\"><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: inherit;\"><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sensory data </span><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">for</span><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">quality monitoring, and predictive maintenance.</span></span></p></li><li dir=\"ltr\" style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;\"><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: inherit;\"><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Health data</span><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> for early treatment, personalized medicine, and epidemic detection.</span></span></p></li><li dir=\"ltr\" style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;\"><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: inherit;\"><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Retail customer data</span><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> for sales forecasting, sales optimization, and targeted advertising</span><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">.</span></span></p></li><li dir=\"ltr\" style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre;\"><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: inherit;\"><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Banking customer data </span><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">for fraud detection and loan risk analysis.</span></span></p></li><li dir=\"ltr\" style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre;\"><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: inherit;\"><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Economic and financial data</span><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> for risk analysis, budgetary analysis, stock market analysis, and yield projections.</span></span></p></li></ul></td></tr></tbody></table></div>

<h3>A simple example</h3>

<p>Let's start with a simple example. We have collected sales records from a fictitious online shop. Each time a client makes a purchase, we record the following information: time of the purchase, client id, product purchased, and price of the product.</p>

<p>The dataset is stored in a single CSV file, with one transaction per line:</p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"color: #0d904f;\">$ head -n <span class=\"hljs-number\">5</span> sales.csv
timestamp,client,product,price
<span class=\"hljs-number\">2010</span>-<span class=\"hljs-number\">10</span>-05 <span class=\"hljs-number\">11</span>:09:<span class=\"hljs-number\">56</span>,c64,p35,<span class=\"hljs-number\">405.35</span>
<span class=\"hljs-number\">2010</span>-09-<span class=\"hljs-number\">27</span> <span class=\"hljs-number\">15</span>:<span class=\"hljs-number\">00</span>:<span class=\"hljs-number\">49</span>,c87,p29,<span class=\"hljs-number\">605.35</span>
<span class=\"hljs-number\">2010</span>-09-09 <span class=\"hljs-number\">12</span>:<span class=\"hljs-number\">58</span>:<span class=\"hljs-number\">33</span>,c97,p10,<span class=\"hljs-number\">108.99</span>
<span class=\"hljs-number\">2010</span>-09-06 <span class=\"hljs-number\">12</span>:<span class=\"hljs-number\">43</span>:<span class=\"hljs-number\">45</span>,c60,p85,<span class=\"hljs-number\">443.35</span></span>
</code></td></tr></tbody></table><p>Looking at data is crucial to understand the data and spot potential issues. Our first task is to load the sales data into an <a href=\"https://temporian.readthedocs.io/en/latest/3_minutes/#events-and-eventsets\" target=\"_blank\"><code style=\"padding: 0;\">EventSet</code></a> and plot it.</p><p><span id=\"docs-internal-guid-6bf70412-7fff-e7a7-495f-16a548305b1b\"></span></p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\"><table style=\"border-collapse: collapse; border: none;\"><colgroup><col width=\"719\" /></colgroup><tbody><tr style=\"height: 0pt;\"><td style=\"background-color: #fff2cc; border-bottom: solid #f1c232 1pt; border-color: rgb(241, 194, 50); border-left: solid #f1c232 1pt; border-right: solid #f1c232 1pt; border-style: solid; border-top: solid #f1c232 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\"><p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: inherit;\"><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 700; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">INFO: </span><span style=\"background-color: transparent; color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A Temporian </span></span><span style=\"font-family: inherit;\"><a href=\"https://temporian.readthedocs.io/en/latest/3_minutes/#events-and-eventsets\" target=\"_blank\"><code style=\"padding: 0;\">EventSet</code></a><span style=\"background-color: transparent; color: black; font-family: inherit; vertical-align: baseline;\"> is a general-purpose container for temporal data. It can represent multivariate time series, time sequences, and indexed data.</span></span></p></td></tr></tbody></table></div>

<div style=\"text-align: left;\"><br /></div><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span class=\"hljs-comment\"><span style=\"color: #b80672;\"># Import Temporian</span></span>
<span class=\"hljs-keyword\"><span style=\"color: #0b5394;\">import</span></span> temporian <span class=\"hljs-keyword\"><span style=\"color: #0b5394;\">as</span></span> tp

<span class=\"hljs-comment\"><span style=\"color: #b80672;\"># Load the csv dataset</span></span>
sales = tp.from_csv(<span class=\"hljs-string\">\"/tmp/sales.csv\"</span>)

<span class=\"hljs-comment\"><span style=\"color: #b80672;\"># Print details about the EventSet</span></span>
sales

</code></td></tr></tbody></table><p>This code snippet load and print the data:</p>

<div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHQtAZEIVQ0PgkUZHNo_Y2qEjekwVzzFX6O5RrZcNPF0paAqMN4c5X81974WzLIOAtahk9_HQNfTCOmr2QP8Xv9G-wOXjZJU4gcgCPOIYfkL-2Af6F_T1r1TG15Mv-tqODNsUgUDBPemlHw-Lv_ojNyuNYLv9Qt1z7znqJyyadd-JnXqGjc4A9GOjDprE/s366/image9.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgHQtAZEIVQ0PgkUZHNo_Y2qEjekwVzzFX6O5RrZcNPF0paAqMN4c5X81974WzLIOAtahk9_HQNfTCOmr2QP8Xv9G-wOXjZJU4gcgCPOIYfkL-2Af6F_T1r1TG15Mv-tqODNsUgUDBPemlHw-Lv_ojNyuNYLv9Qt1z7znqJyyadd-JnXqGjc4A9GOjDprE/s16000/image9.png\" style=\"margin: auto; width: auto;\" /></a></div><br />

<div>We can also plot the data:</div>

<div><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"color: #b80672;\"><span class=\"hljs-comment\"># Plot \"price\" feature of the EventSet</span>
</span>sales[<span class=\"hljs-string\">\"price\"</span>].plot()
</code></td></tr></tbody></table><br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"ALT TEXT\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg1w0FmzajBeYuBJPG0TqQPcmRHAGSsK1D0RphUZWU00JwG8X_izxgWeZ_5LPUxTzAc_Ysb5pqvWYEYn4sxZ5oPpNeYuMU7rm-tockW5DlB8djAMbDvu3MKjR_BhBYVGYR4uVSDdFXIdfKswprCOjEZU_6KW3FGSPtD8xasGC9ThuMDXoF8rebofAZndTs/s1600/image3.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>We have shown how to load and visualize temporal data in just a few lines of code. However, the resulting plot is very busy, as it shows all transactions for all clients in the same view.</p>

<p>A common operation on temporal data is to calculate the moving sum. Let's calculate and plot the sum of sales for each transaction in the previous seven days. The moving sum can be computed using the <a href=\"https://temporian.readthedocs.io/en/latest/reference/temporian/operators/window/moving_sum/\" target=\"_blank\"><code style=\"padding: 0;\">moving_sum</code></a> operator.</p>

<div><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\">weekly_sales = sales[<span class=\"hljs-string\">\"price\"</span>].moving_sum(tp.duration.days(<span class=\"hljs-number\"><span style=\"color: red;\">7</span></span>))

weekly_sales.plot()
</code></td></tr></tbody></table><br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"ALT TEXT\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhF-PoY7wh6aeTHCn5tb97pGfU85iayXAl2OeRXjJv9VdTMFFg-FUeV5EOarKUJiubKyzDZ3A3-JcU92WONvqTTljQWBe2gCGKUCOwnIqUd4pWQy_kPBmF5YE9Zm8Q0B85t9tQvZebL17HSyNaZ4ttfUMTMNbvNVRwzs1mUUSadsggQea84qhyQQRVy1LU/s1600/image3.png\" style=\"width: 100%;\" /></center></td></tr></tbody></table></div><span id=\"docs-internal-guid-44015182-7fff-1d6d-c040-36d083e29ff0\" style=\"font-weight: normal;\"><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\"><span id=\"docs-internal-guid-32debacd-7fff-b177-6f07-739e46812001\"><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\"><table style=\"border-collapse: collapse; border: none;\"><colgroup><col width=\"719\" /></colgroup><tbody><tr style=\"height: 0pt;\"><td style=\"background-color: #fff2cc; border-bottom: solid #f1c232 1pt; border-color: rgb(241, 194, 50); border-left: solid #f1c232 1pt; border-right: solid #f1c232 1pt; border-style: solid; border-top: solid #f1c232 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\"><p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: inherit;\"><span style=\"background-color: transparent; font-weight: 700; vertical-align: baseline;\">BONUS: </span><span style=\"background-color: transparent; vertical-align: baseline;\">To make the plots interactive,&nbsp;you can add the </span><code style=\"padding: 0;\">interactive=True</code><span style=\"background-color: transparent; font-family: inherit;\"> argument </span></span><span style=\"background-color: transparent; font-family: inherit;\">to the </span><code style=\"padding: 0;\">plot</code><span style=\"background-color: transparent; font-family: inherit;\"> function.&nbsp;</span></p></td></tr></tbody></table></div></span>
  
<h3>Sales per products</h3>

<p>In the previous step, we computed the overall moving sum of sales for the entire shop. However, what if we wanted to calculate the rolling sum of sales for each product or client separately?</p>

<p>For this task, we can use an index.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span class=\"hljs-comment\"><span style=\"color: #b80672;\"># Index the data by \"product\"</span></span>
sales_per_product = sales.add_index(<span class=\"hljs-string\">\"product\"</span>)

<span class=\"hljs-comment\"><span style=\"color: #b80672;\"># Compute the moving sum for each product</span></span>
weekly_sales_per_product = sales_per_product[<span class=\"hljs-string\">\"price\"</span>].moving_sum(
<span>&nbsp;&nbsp; &nbsp;</span><span>&nbsp;&nbsp; &nbsp;</span>tp.duration.days(<span class=\"hljs-number\"><span style=\"color: red;\">7</span></span>)
)

<span class=\"hljs-comment\"><span style=\"color: #b80672;\"># Plot the results</span></span>
weekly_sales_per_product.plot()

</code></td></tr></tbody></table>

<div><br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"ALT TEXT\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6xexNKR1lXwC3dxWvhFUZnVUc7IvQcE_96MsqzwmLt8eh-ETtbYSgJf9X-1NtKSuot1liNjnCyxi4MRjM51IKyByuReZCZCMLk3PgpxhDER40mVjU4Qj6fTa1800GHz2WeNvkFpOQm43PwkSSRmEMfiqncL5MmeT1j-fv-0HJSnBTL3uV7jcCzppo8gI/s1600/image8.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table><br /></div>
  
<span id=\"docs-internal-guid-d345409c-7fff-0df4-caeb-afc399f65ea3\" style=\"font-weight: normal;\"><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\"><span id=\"docs-internal-guid-c921cad8-7fff-2bb7-b624-9c9bfa1b2574\"><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\"><table style=\"border-collapse: collapse; border: none;\"><colgroup><col width=\"719\" /></colgroup><tbody><tr style=\"height: 0pt;\"><td style=\"background-color: #fff2cc; border-bottom: solid #f1c232 1pt; border-color: rgb(241, 194, 50); border-left: solid #f1c232 1pt; border-right: solid #f1c232 1pt; border-style: solid; border-top: solid #f1c232 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\"><p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: inherit;\"><span style=\"background-color: transparent; font-weight: 700; vertical-align: baseline;\">NOTE: </span><span style=\"background-color: transparent; vertical-align: baseline;\">Many operators such as <a href=\"https://temporian.readthedocs.io/en/latest/reference/temporian/operators/window/moving_sum/\" target=\"_blank\"><code style=\"padding: 0;\">moving_sum</code></a><span style=\"background-color: transparent; vertical-align: baseline;\"><span style=\"font-family: inherit;\"> applied independently on each index.</span></span></span></span></p></td></tr></tbody></table></div></span></div><h3 style=\"margin-left: 0pt;\">Aggregate transactions into time series</h3></span>

<p>Our dataset contains individual client transactions. To use this data with a machine learning model, it is often useful to aggregate it into time series, where the data is sampled uniformly over time. For example, we could aggregate the sales weekly, or calculate the total sales in the last week for each day.</p>

<p>However, it is important to note that aggregating transaction data into time series can result in some data loss. For example, the individual transaction timestamps and values would be lost. This is because the aggregated time series would only represent the total sales for each time period.</p>

<p>Let's compute the total sales in the last week for each day for each product individually.</p>


<div><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span class=\"hljs-comment\"><span style=\"color: #b80672;\"># The data is sampled daily</span></span>
daily_sampling = sales_per_product.tick(tp.duration.days(<span class=\"hljs-number\"><span style=\"color: red;\">1</span></span>))

weekly_sales_daily = sales_per_product[<span class=\"hljs-string\">\"price\"</span>].moving_sum(
<span>&nbsp;&nbsp; &nbsp;</span>tp.duration.days(<span class=\"hljs-number\"><span style=\"color: red;\">7</span></span>),
<span>&nbsp;&nbsp; &nbsp;</span>sampling=daily_sampling,  <span class=\"hljs-comment\"><span style=\"color: #b80672;\"># The new bit</span></span>
)

weekly_sales_daily.plot()
</code></td></tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"ALT TEXT\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnXTVdhUOnSrT-z520RCWLb8i_TYuS0pZk6M9Egp9rvVB8Zg_wOHudt53bAZxXUrGKnIqoFtRiZIuMXfMfpByjwMMOEpT8xY07v0wzQOdhPHTJLvjyrpbtwTtrNyaMGSupEm30LX2JX99xbxsb4ABzq77xnSWyYumLztxWgT3GiDt-R45IYSSyr1dPMMw/s1600/image2.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p><span id=\"docs-internal-guid-be009cac-7fff-b376-0084-268a6bca1bba\"></span></p><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\"><span id=\"docs-internal-guid-83754c62-7fff-2032-66bf-ff17d019d5d3\"><div align=\"left\" dir=\"ltr\" style=\"margin-left: 0pt;\"><table style=\"border-collapse: collapse; border: none;\"><colgroup><col width=\"719\" /></colgroup><tbody><tr style=\"height: 0pt;\"><td style=\"background-color: #fff2cc; border-bottom: solid #f1c232 1pt; border-color: rgb(241, 194, 50); border-left: solid #f1c232 1pt; border-right: solid #f1c232 1pt; border-style: solid; border-top: solid #f1c232 1pt; border-width: 1pt; overflow: hidden; padding: 5pt; vertical-align: top;\"><p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"font-family: inherit;\"><span style=\"background-color: transparent; font-weight: 700; vertical-align: baseline;\">NOTE: </span><span style=\"background-color: transparent; vertical-align: baseline;\">The current plot is a continuous line, while the previous plots have markers. This is because Temporian uses continuous lines by default when the data is uniformly sampled, and markers otherwise.</span></span></p></td></tr></tbody></table></div></span></div><p>After the data preparation stage is finished, the data can be exported to a Pandas DataFrame as a final step.</p><div><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\">tp.to_pandas(weekly_sales_daily)
</code></td></tr></tbody></table><br /><br /></div><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgc5UGbsDXrqRgB93gWVQAlVQyTOLOatGAUqpxTHbKwvfgtsxiY_7VDol2M-8ned5Hf1S-AJTskaJst9zSdSQLW5zTI-Pn790thkiMvgAuCJNWIUfyiA3It2IvyKUXg6plk4RQOo3N_eLMHZgZyNCurPgJzlLLVvExzNfHljTux3PYyG2Ud0NMARN2xA4k/s410/image10.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgc5UGbsDXrqRgB93gWVQAlVQyTOLOatGAUqpxTHbKwvfgtsxiY_7VDol2M-8ned5Hf1S-AJTskaJst9zSdSQLW5zTI-Pn790thkiMvgAuCJNWIUfyiA3It2IvyKUXg6plk4RQOo3N_eLMHZgZyNCurPgJzlLLVvExzNfHljTux3PYyG2Ud0NMARN2xA4k/s16000/image10.png\" style=\"margin: auto; width: auto;\" /></a></div>

<h3>Train a forecasting model with TensorFlow model</h3>

<p>A key application of Temporian is to clean data and perform feature engineering for machine learning models. It is well suited for forecasting, anomaly detection, fraud detection, and other tasks where data comes continuously.</p>

<p>In this example, we show how to train a TensorFlow model to predict the next day's sales using past sales for each product individually. We will feed the model various levels of aggregations of sales as well as calendar information.</p>

<p>Let's first augment our dataset and convert it to a dataset compatible with a tabular ML model.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\">sales_per_product = sales.add_index(<span class=\"hljs-string\"><span style=\"color: #0d904f;\">\"product\"</span></span>)

<span class=\"hljs-comment\"><span style=\"color: #b80672;\"># Create one example per day</span></span>
daily_sampling = sales_per_product.tick(tp.duration.days(<span class=\"hljs-number\">1</span>))

<span style=\"color: #b80672;\"><span class=\"hljs-comment\"># Compute moving sums with various window length.</span>
<span class=\"hljs-comment\"># Machine learning models are able to select the ones that matter.</span></span>
features = []
<span class=\"hljs-keyword\"><span style=\"color: #3d85c6;\">for</span></span> w <span class=\"hljs-keyword\"><span style=\"color: #3d85c6;\">in</span></span> [<span class=\"hljs-number\"><span style=\"color: red;\">3</span></span>, <span class=\"hljs-number\"><span style=\"color: red;\">7</span></span>, <span class=\"hljs-number\"><span style=\"color: red;\">14</span></span>, <span class=\"hljs-number\"><span style=\"color: red;\">28</span></span>]:
    features.append(sales_per_product[<span class=\"hljs-string\"><span style=\"color: #0d904f;\">\"price\"</span></span>]
        .moving_sum(
            tp.duration.days(w),
            sampling=daily_sampling)
        .rename(<span class=\"hljs-string\">f\"moving_sum_<span class=\"hljs-subst\">{w}</span><span style=\"color: #0d904f;\">\"</span></span><span style=\"color: #0d904f;\">))

<span class=\"hljs-comment\"># Calendar information such as the day of the week are</span>
<span class=\"hljs-comment\"># very informative of human activities.</span>
features.append(daily_sampling.calendar_day_of_week())

<span class=\"hljs-comment\"># The label is the daly sales shifted / leaked one days in the future.</span>
</span><span>label = (sales_per_product[</span><span class=\"hljs-string\"><span>\"</span>price<span>\"</span></span>]
    .leak(tp.duration.days(<span class=\"hljs-number\"><span style=\"color: red;\">1</span></span>))
    .moving_sum(
        tp.duration.days(<span class=\"hljs-number\"><span style=\"color: red;\">1</span></span>),
        sampling=daily_sampling,
        )
    .rename(<span class=\"hljs-string\"><span style=\"color: #0d904f;\">\"label\"</span></span>))

<span style=\"color: #b80672;\"><span class=\"hljs-comment\"># Collect the features and labels together.</span>
</span>dataset = tp.glue(*features, label)

dataset
</code></td></tr></tbody></table>

<div><br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"ALT TEXT\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFBQzNJ3TQJh1i3MJgyUtVs7u7-AK7kZZNa_m_idEJcVUc2jeZrVQNIN5ixoAp_9pLA-Bpk7EiNRNtFtqwX7GDkM1ue83SU95-OWC8Pp3aoZ_gh7cumkynmG553soJlNdrOCm_ciz9isSZT5hsMmN0DoDlyYpW99-g6om4YayES3NimzR3lw2izMy3d1k/s1600/image1.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>We can then convert the dataset from <code style=\"padding: 0;\">EventSet</code> to TensorFlow Dataset format, and train a Random Forest.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span class=\"hljs-keyword\"><span style=\"color: #3d85c6;\">import</span></span> tensorflow_decision_forests <span class=\"hljs-keyword\"><span style=\"color: #3d85c6;\">as</span></span> tfdf

<span class=\"hljs-keyword\"><span style=\"color: #3d85c6;\">def</span></span> <span class=\"hljs-title function_\">extract_label</span>(<span class=\"hljs-params\">example</span>):
    example.pop(<span class=\"hljs-string\">\"timestamp\"</span>) <span class=\"hljs-comment\"><span style=\"color: #b80672;\"># Don't use use the timestamps as feature</span></span>
    label = example.pop(<span class=\"hljs-string\">\"label\"</span>)
    <span class=\"hljs-keyword\"><span style=\"color: #3d85c6;\">return</span></span> example, label

tf_dataset = tp.to_tensorflow_dataset(dataset).<span class=\"hljs-built_in\"><span style=\"color: #3d85c6;\">map</span></span>(extract_label).batch(<span class=\"hljs-number\"><span style=\"color: red;\">100</span></span>)

model = tfdf.keras.RandomForestModel(task=tfdf.keras.Task.REGRESSION,verbose=<span class=\"hljs-number\"><span style=\"color: red;\">2</span></span>)
model.fit(tf_dataset)
</code></td></tr></tbody></table>

<p>And that’s it, we have a model trained to forecast sales. We now can look at the variable importance of the model to understand what features matter the most.</p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\">model.summary()
</code></td></tr></tbody></table><p>In the summary, we can find the <a href=\"https://ydf.readthedocs.io/en/latest/cli_user_manual.html#variable-importances\" target=\"_blank\"><code style=\"padding: 0;\">INV_MEAN_MIN_DEPTH</code></a> variable importance:</p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"color: #0d904f;\"><span class=\"hljs-type\">Type</span>: <span class=\"hljs-string\">\"RANDOM_FOREST\"</span>
Task: REGRESSION
...
Variable Importance: INV_MEAN_MIN_DEPTH:
    <span class=\"hljs-number\">1.</span>        <span class=\"hljs-string\">\"moving_sum_28\"</span>  <span class=\"hljs-number\">0.342231</span> <span class=\"hljs-comment\">################</span>
    <span class=\"hljs-number\">2.</span>              <span class=\"hljs-string\">\"product\"</span>  <span class=\"hljs-number\">0.294546</span> <span class=\"hljs-comment\">############</span>
    <span class=\"hljs-number\">3.</span> <span class=\"hljs-string\">\"calendar_day_of_week\"</span>  <span class=\"hljs-number\">0.254641</span> <span class=\"hljs-comment\">##########</span>
    <span class=\"hljs-number\">4.</span>        <span class=\"hljs-string\">\"moving_sum_14\"</span>  <span class=\"hljs-number\">0.197038</span> <span class=\"hljs-comment\">######</span>
    <span class=\"hljs-number\">5.</span>         <span class=\"hljs-string\">\"moving_sum_7\"</span>  <span class=\"hljs-number\">0.124693</span> <span class=\"hljs-comment\">#</span>
    <span class=\"hljs-number\">6.</span>         <span class=\"hljs-string\">\"moving_sum_3\"</span>  <span class=\"hljs-number\">0.098542</span></span>
</code></td></tr></tbody></table><p>We see that <code style=\"padding: 0;\">moving_sum_28</code> is the feature with the highest importance (0.342231). This indicates that the sum of sales in the last 28 days is very important to the model. To further improve our model, we should probably add more temporal aggregation features. The <code style=\"padding: 0;\">product</code> feature also matters a lot.</p>

<p>And to get an idea of the model itself, we can plot one of the trees of the Random Forest.</p><div><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\">tfdf.model_plotter.plot_model_in_colab(model, tree_idx=<span class=\"hljs-number\"><span style=\"color: red;\">0</span></span>, max_depth=<span class=\"hljs-number\"><span style=\"color: red;\">2</span></span>)
</code></td></tr></tbody></table>
  
<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"ALT TEXT\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5Hy66U7TQyFZVjyIJo4ZQMf52R8KCfyFgTY6thKhmlpiknmF_MtQoCt_ssAAOczBwUmcm4j6an92WjBP1HW3sBroSkwazzl-uAyCbFl-0mZvtNsE2jxvXHvfEzwHwy2f8mE8QcJqxcdY11CtiI2fYBXKD0rSXZ8p3Bygkw-LIqHRnr0UXYyA9ILC34JQ/s1600/moving_sum%20model%20tree%20%281000%20%C3%97%20550%20px%29.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<h3>More on temporal data preprocessing</h3>

<p>We demonstrated some simple data preprocessing. If you want to see other examples of temporal data preprocessing on different data domains, check the <a href=\"https://temporian.readthedocs.io/en/latest/tutorials/\" target=\"_blank\">Temporian tutorials</a>. Notably:</p>
<ul><blockquote>
<li><b>Heart rate analysis</b> ❤️  detects individual heartbeats and derives heart rate related features on raw ECG signals from Physionet.</li>
<li><b>M5 Competition</b> 🛒 predicts retail sales in the M5 Makridakis Forecasting competition.</li>
<li><b>Loan outcomes prediction</b> 🏦 prepares relational SQL data to predict outcomes for finished loans.</li>
<li><b>Detecting payment card fraud</b> 💳 detects fraudulent payment card transactions in real time.</li>
<li><b>Supervised and unsupervised anomaly detection</b> 🔎 perform data analysis and feature engineering to detect anomalies in a group of server’s resource usage metrics.</li>
</blockquote></ul>

<h3>Next Steps</h3>

<p>We demonstrated how to handle temporal data such as transactions in TensorFlow using the Temporian library. Now you can try it too!</p>
<ul><blockquote>
<li>Join <a href=\"https://discord.gg/Zqa4BY5uV8\" target=\"_blank\">our Discord server</a>, to share your feedback or ask for help.</li>
<li>Read the <a href=\"https://temporian.readthedocs.io/en/latest/3_minutes/\" target=\"_blank\">3 minutes to Temporian guide</a> for a quick introduction.</li>
<li>Check the <a href=\"https://temporian.readthedocs.io/en/latest/user_guide/\" target=\"_blank\">User guide</a>.</li>
<li>Visit the <a href=\"https://github.com/google/temporian\" target=\"_blank\">GitHub repository</a>.</li></blockquote></ul><p>To learn more about model training with TensorFlow Decision Forests:</p>
<ul><blockquote>
<li>Visit the <a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\">official website</a>.</li>
<li>Follow the <a href=\"https://www.tensorflow.org/decision_forests/tutorials/beginner_colab\" target=\"_blank\">beginner notebook</a>.</li>
<li>Check the <a href=\"https://www.tensorflow.org/decision_forests/tutorials\" target=\"_blank\">various guides and tutorials</a>.</li>
<li>Check the <a href=\"https://discuss.tensorflow.org/\" target=\"_blank\">TensorFlow Forum</a>.</li>
</blockquote></ul></div></span></div></div>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Mon, 11 Sep 2023 20:14:00 +0000"
2111,Distributed Fast Fourier Transform in TensorFlow,https://blog.tensorflow.org/2023/08/distributed-fast-fourier-transform-in-tensorflow.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYIMKKNGUkhWyF3gL1qLpn7cQ3iQu5GleP-36OHbTJwn90YdRUk8vGTefc9ctjwPcJhyBbFlprd581nDsWEKfMAeAo9xuX8zKfxti8Fvl2f2v69Qmvt695cCJY1dfVPbMIlfWqMFKMEyBCgIaRLXypYCrHlob-OiAb0mvVbhmBEt65-agfmRWDMuaI/s1600/image1.png\" style=\"display: none;\" />

<em>Posted by <a href=\"https://www.linkedin.com/in/ruijiao-sun\" target=\"_blank\">Ruijiao Sun</a>, Google Intern - DTensor team</em>
  
<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRgwmbW2Tp7Q_Y3GTKSmkfboFNHuUizDaYh1uLrQSCZarfkvs1mK7OodevH9l318Ls8ddEmKNPewlpVhMJzKvtpktP6TeKniEEMAzgRrHq-D-kIEsoQnZyvc7n4pUVsn1RkFF066dnujQZ1htprWST0uSJftVZxQyc2Qm8aijQTMhrtJlj7rrEc6s7/s1600/Tensorflow-septmber-update-header%20%282%29.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRgwmbW2Tp7Q_Y3GTKSmkfboFNHuUizDaYh1uLrQSCZarfkvs1mK7OodevH9l318Ls8ddEmKNPewlpVhMJzKvtpktP6TeKniEEMAzgRrHq-D-kIEsoQnZyvc7n4pUVsn1RkFF066dnujQZ1htprWST0uSJftVZxQyc2Qm8aijQTMhrtJlj7rrEc6s7/s1600/Tensorflow-septmber-update-header%20%282%29.png\" /></a>

<a name=\"more\"></a><p></p>

<p><a href=\"https://en.wikipedia.org/wiki/Fast_Fourier_transform\" target=\"_blank\">Fast Fourier Transform</a> is an important method of signal processing, which is commonly used in a number of ways, including speeding up convolutions, extracting features, and regularizing models. Distributed Fast Fourier Transform (Distributed FFT) offers a way to compute Fourier Transforms in models that work with image-like datasets that are too large to fit into the memory of a single accelerator device.  In a previous Google Research Paper,  “<a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465154\" target=\"_blank\"><i>Large-Scale Discrete Fourier Transform on TPUs</i></a>” by Tianjian Lu, a Distributed FFT algorithm was implemented for TensorFlow v1 as a library. This work presents the newly added native support in TensorFlow v2 for Distributed FFT, through the new TensorFlow distribution API, DTensor.</p>

<h2>About DTensor</h2>

<p><a href=\"https://www.tensorflow.org/guide/dtensor_overview\" target=\"_blank\">DTensor</a> is an extension to TensorFlow for synchronous distributed computing. It distributes the program and tensors through a procedure called <a href=\"https://en.wikipedia.org/wiki/SPMD\" target=\"_blank\">Single program, multiple data (SPMD)</a> extension. DTensor offers an uniform API for traditional data and model parallelism patterns used widely in Machine Learning. </p>

<h2>Example Usage</h2>

<p>The API interface for distributed FFT is the same as the original FFT in TensorFlow. Users just need to pass a sharded tensor as an input to the existing FFT ops in TensorFlow, such as <a href=\"https://www.tensorflow.org/api_docs/python/tf/signal/fft2d\" target=\"_blank\">tf.signal.fft2d</a>. The output of a distributed FFT becomes sharded too.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">import</span> TensorFlow <span class=\"hljs-keyword\">as</span> tf
<span class=\"hljs-keyword\">from</span> TensorFlow.experimental <span class=\"hljs-keyword\">import</span> dtensor


<span style=\"color: #b80672;\"><span class=\"hljs-comment\"># Set up devices</span>
</span>device_type = dtensor.preferred_device_type()
<span class=\"hljs-keyword\">if</span> device_type == <span class=\"hljs-string\">'CPU'</span>:
cpu = tf.config.list_physical_devices(device_type)
tf.config.set_logical_device_configuration(cpu[<span class=\"hljs-number\">0</span>], [tf.config.LogicalDeviceConfiguration()] * <span class=\"hljs-number\">8</span>)
<span class=\"hljs-keyword\">if</span> device_type == <span class=\"hljs-string\">'GPU'</span>:
gpu = tf.config.list_physical_devices(device_type)
tf.config.set_logical_device_configuration(gpu[<span class=\"hljs-number\">0</span>], [tf.config.LogicalDeviceConfiguration(memory_limit=<span class=\"hljs-number\">1000</span>)] * <span class=\"hljs-number\">8</span>)
dtensor.initialize_accelerator_system()


<span style=\"color: #b80672;\"><span class=\"hljs-comment\"># Create a mesh</span>
</span>mesh = dtensor.create_distributed_mesh(mesh_dims=[(<span class=\"hljs-string\">'x'</span>, <span class=\"hljs-number\">1</span>), (<span class=\"hljs-string\">'y'</span>, <span class=\"hljs-number\">2</span>), (<span class=\"hljs-string\">'z'</span>, <span class=\"hljs-number\">4</span>)], device_type=device_type)


<span style=\"color: #b80672;\"><span class=\"hljs-comment\"># Set up a distributed input Tensor</span>
</span><span class=\"hljs-built_in\">input</span> = tf.<span class=\"hljs-built_in\">complex</span>(
tf.random.stateless_normal(shape=(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">4</span>), seed=(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>), dtype=tf.float32),
tf.random.stateless_normal(shape=(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">4</span>), seed=(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">4</span>), dtype=tf.float32))
init_layout = dtensor.Layout([<span class=\"hljs-string\">'x'</span>, <span class=\"hljs-string\">'y'</span>, <span class=\"hljs-string\">'z'</span>], mesh)
d_input = dtensor.relayout(<span class=\"hljs-built_in\">input</span>, layout=init_layout)


<span style=\"color: #b80672;\"><span class=\"hljs-comment\"># Run distributed fft2d. DTensor determines the most efficient</span>
<span class=\"hljs-comment\">#</span></span><span class=\"hljs-comment\"> </span><span class=\"hljs-comment\">layout of of d_output.</span>
d_output = tf.signal.fft2d(d_input)</span></code></td></tr></tbody></table>

<h2>Performance Analysis</h2>

<p>The following experiment demonstrates that the distributed FFT can process more data than the non-distributed one by utilizing memory across multiple devices. The tradeoff is spending additional time on communication and data transposes that slow down the calculation speed. </p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Graph of performance on different machines, measuri8ng wall clock time in seconds by size per dimension across single GPU, Distributed FFT and Undistributed FFT\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigTOJgosBdpn5Wr7djYt3eYpmcxmRy3nn_UQbG1aW5nO96v5fMFFzsx9p-ahhpTuWJPNRQ5IMRvUjdon-CollI87lu6mAEnnW7KcaIHyrZVLK0tUMQJ2jDBVxtYz9zcXBH2FI5kou775n-Yt4iBGVfzO3rAwaCo4vgkXrWU3BWqv-_YPKf2mkU8kihSxc/s1600/image2.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>This phenomenon is shown in detail from the profiling result of the 10K*10K distributed FFT experiment. The current implementation of distributed FFT in TensorFlow follows the simple shuffle+local FFT method, which is also used by other popular distributed FFT libraries such as <a href=\"https://ieeexplore.ieee.org/document/681704\" target=\"_blank\">FFTW</a> and <a href=\"https://epubs.siam.org/doi/abs/10.1137/120885887\" target=\"_blank\">PFFT</a>.  Notably, the two local FFT ops only take 3.6% of the total time (15ms). This is around 1/3 of the time for non-distributed fft2d. Most of the computing time is spent on data shuffling, represented by the ncclAllToAll Operation. Note that these experiments were conducted on an 8xV100 GPU system.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Table of Top 10 TensorFlow operations on GPU highlighting two local FFT ops in the top 3\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhNIgERau2VMVV1Uz5jqsjNow9OVkfhd_WI0Irr424u7sUAEo-iILbMOxhrp_dbjMsKhssjCw6iYfe8m5OCBGsldiqM-6_SjAjY0ZamWkBSJUGuW7Xwubw5fYfrNVY9WSr9B2SXX2m0fQHRS7Gqr-jjKMNxIzTax2xz5ntAGZhIMMb4biWvK98tqd4M7H4/s1600/image1.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<h2>Next steps</h2>

<p>The feature is new and we have adopted a simplest distributed FFT algorithm. A few ideas to fine tune or improve the performance are:</p>
<ul><blockquote>
<li>Switch to a different DFT/FFT algorithm.</li>
<li>Tweaks on the NCCL communication settings for the particular FFT sizes may improve utilization of the network bandwidth and increase the speed.</li>
<li>Reducing the number of collectives to minimize bandwidth requirements.</li>
<li>Use N-d local FFTs, rather than multiple 1-d local FFTs.</li>
</blockquote></ul>

<p>Try the new distributed FFT! We welcome your feedback on the <a href=\"https://discuss.tensorflow.org/\" target=\"_blank\">TensorFlow Forum</a> and look forward to working with you on improving the performance. Your input would be invaluable!</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Thu, 24 Aug 2023 17:00:00 +0000"
2112,The TensorFlow Lite Plugin for Flutter is Officially Available,https://blog.tensorflow.org/2023/08/the-tensorflow-lite-plugin-for-flutter-officially-available.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEir6TpBti1i4y8JWJ0JdadWBI-dV8iziJhCeUDFq7-KzRm_KzbWtUDw2sL7drCRP77A1H1fkgw-N0saaGLtUsxXbUApCiW-A5M7IbKUovycgIknEcLAJq6qut5Ov6ec7QALMFQJC2srGzFMHRvUN856QGju3qR8f71XSRNFYJwBOFS-3s84Gmvz-Q3k2_Q/s1600/Tensorflow-Jax-on-the-Web-02%20%281%29.png\" style=\"display: none;\" />

<em>Posted by <a href=\"https://twitter.com/ptruiz_dev\" target=\"_blank\">Paul Ruiz</a>, Developer Relations Engineer</em><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2kHOZOCF_yXCqaxxvQWHY3Gd3sR7xJ0P5NSRDABUCzAHTDZ69kv6ofmUVY-amjyZnxOmj3Mt66xwnKtqR6ftOOHkCLckkMRlawpniZwCwcqEvExyMHoaMfyQJqvw-vi63OcWqAcLCJKDPHaWrPVJW-U8aTvNHwXWDVa6PLbhAUgYxyjdFuKHnT6ISqwY/s1600/Tensorflow-Jax-on-the-Web-01%20%281%29.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2kHOZOCF_yXCqaxxvQWHY3Gd3sR7xJ0P5NSRDABUCzAHTDZ69kv6ofmUVY-amjyZnxOmj3Mt66xwnKtqR6ftOOHkCLckkMRlawpniZwCwcqEvExyMHoaMfyQJqvw-vi63OcWqAcLCJKDPHaWrPVJW-U8aTvNHwXWDVa6PLbhAUgYxyjdFuKHnT6ISqwY/s1600/Tensorflow-Jax-on-the-Web-01%20%281%29.png\" /></a>

<a name=\"more\"></a><p></p>

<p>We're excited to announce that the TensorFlow Lite plugin for Flutter has been officially migrated to the TensorFlow GitHub account and released!

</p><p>Three years ago, <a href=\"https://github.com/am15h\" target=\"_blank\">Amish Garg</a>, one of our talented Google Summer of Code contributors, wrote a widely used TensorFlow Lite plugin for Flutter. The plugin was so popular that we decided to migrate it to our official repo, making it easier to maintain directly by the Google team. We are grateful to Amish for his contributions to the TensorFlow Lite Flutter plugin.</p>

<p>Through the efforts of developers in the community, the plugin has been updated to the latest version of TensorFlow Lite, and a collection of new features and example apps have been added, such as object detection through a live camera feed.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Moving image of a live camera feed showing several objects on a work desk being detected\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzl-IA2HrigPggyplfe6fsfCxuW8URlGxY_qYI5i9x1JcYjRXUHQX6iRSML-tfojxIYP35nEOgdQ3e0VQevPjlwL0l0rG-obAIgXYqJOU77ZuM-G_ZeB0S66_aoVcU02WQLjrbGdj22zdp3JhEvL08rVOrq3G0jc0NUgRyUu2uDWL-1-RyFwIqj1E7e88/s1600/image2.gif\" style=\"width: 55%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>So what is TensorFlow Lite? TensorFlow Lite is a way to run TensorFlow models on devices locally, supporting mobile, embedded, web, and edge devices. TensorFlow Lite’s cross-platform support and on-device performance optimizations make it a great addition to the Flutter development toolbox. Our goal with this plugin is to make it easy to integrate TensorFlow Lite models into Flutter apps across mobile platforms, with desktop support currently in development through the efforts of our developer community. Find pre-trained TensorFlow Lite models on model repos like <a href=\"https://www.kaggle.com/models?framework=tfLite\" target=\"_blank\">Kaggle Models</a> or <a href=\"https://www.kaggle.com/models?framework=tfLite\" target=\"_blank\">create your own custom TensorFlow Lite models</a>. </p>

<p>Let’s take a look at how you could use the Flutter TensorFlow Lite plugin for image classification:</p>

<h2>TensorFlow Lite Image Classification with Flutter</h2>

<p>First you will need to install the plugin from <a href=\"https://pub.dev/packages/tflite_flutter\" target=\"_blank\">pub.dev</a>. Once the plugin is installed, you can load a TensorFlow Lite model into your Flutter app and define the input and output tensor shapes. If you’re using the MobileNet model, then the input tensor will be a 224 by 224 RGB image, and the output will be a list of confidence scores for the trained labels.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-comment\">// Load model</span>
Future&lt;<span class=\"hljs-keyword\">void</span>&gt; <span class=\"hljs-title function_\">_loadModel</span><span class=\"hljs-params\">()</span> async {
    <span class=\"hljs-type\">final</span> <span class=\"hljs-variable\">options</span> <span class=\"hljs-operator\">=</span> InterpreterOptions();

    <span class=\"hljs-comment\">// Load model from assets</span>
    interpreter = await Interpreter.fromAsset(modelPath, options: options);
    <span class=\"hljs-comment\">// Get tensor input shape [1, 224, 224, 3]</span>
    inputTensor = interpreter.getInputTensors().first;
    <span class=\"hljs-comment\">// Get tensor output shape [1, 1001]</span>
    outputTensor = interpreter.getOutputTensors().first;
}</span></code></td></tr></tbody></table><p style=\"text-align: left;\">To make things a bit more organized, you can also load in the labels for the 1000 items that MobileNet is trained for:</p><div><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-comment\">// Load labels from assets</span>
Future&lt;<span class=\"hljs-keyword\">void</span>&gt; <span class=\"hljs-title function_\">_loadLabels</span><span class=\"hljs-params\">()</span> async {
    <span class=\"hljs-type\">final</span> <span class=\"hljs-variable\">labelTxt</span> <span class=\"hljs-operator\">=</span> await rootBundle.loadString(labelsPath);
    labels = labelTxt.split(<span class=\"hljs-string\">'\\n'</span>);
}</span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace\">
</span></code></td></tr></tbody></table></div><div><p>For the sake of being succinct, let’s go ahead and skip some of the pre-processing steps, though you can find them in the repo’s image classification example here.</p>

<p>When you’re ready to run inference, you can create a new input and output based on the tensor shapes that you defined earlier, then call run on the interpreter to get your final results.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-comment\">// Run inference</span>
 Future&lt;<span class=\"hljs-keyword\">void</span>&gt; <span class=\"hljs-title function_\">runInference</span><span class=\"hljs-params\">(
 </span><span class=\"hljs-params\">List&lt;List&lt;List&lt;</span><span class=\"hljs-params\"><span style=\"color: #792ba1;\">num</span></span><span class=\"hljs-params\">&gt;&gt;&gt; imageMatrix,</span><span class=\"hljs-params\">
 )</span> async {
   <span class=\"hljs-comment\">// Tensor input [1, 224, 224, 3]</span>
   <span class=\"hljs-type\">final</span> <span class=\"hljs-variable\">input</span> <span class=\"hljs-operator\">=</span> [imageMatrix];
   <span class=\"hljs-comment\">// Tensor output [1, 1001]</span>
   <span class=\"hljs-type\">final</span> <span class=\"hljs-variable\">output</span> <span class=\"hljs-operator\">=</span> [List&lt;<span class=\"hljs-type\">int</span>&gt;.filled(<span class=\"hljs-number\">1001</span>, <span class=\"hljs-number\">0</span>)];

<span class=\"hljs-comment\"><span>&nbsp;&nbsp; </span>// Run inference</span>
<span>&nbsp;&nbsp; </span>interpreter.run(input, output);

<span class=\"hljs-comment\"><span>&nbsp;&nbsp; </span>// Get first output tensor</span>
<span class=\"hljs-type\"><span>&nbsp;&nbsp; </span>final</span> <span class=\"hljs-variable\">result</span> <span class=\"hljs-operator\">=</span> output.first;</span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace\">
</span></code></td></tr></tbody></table>

<p>Now that you have your results, you can match them to your labels and use them in your app.</p>
  
<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Moving image of a live camera feed showing several objects on a work desk being correctly identified in the app\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVgsmqQrHHWe-t2Sh9NYURx7PJXkxCxkVfvNGNOc3VrHyHB3bxCkzUegDWqea9twNo62rQzLngtQ4Tq7L-EhyHJv4RXyewL_hp1xH48JVsg8xfaboptEm-DliMpCB6Qj7v6T7WrAIpPtt9RpK1BAmaxkLsImzkkSuj2X-miVDiauiKKeZi-fFQzgJRU4U/s1600/image1.gif\" style=\"width: 55%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

  <h2>What’s next?</h2>

<p>To explore what else you can do with the Flutter TensorFlow Lite plugin, check out the official <a href=\"https://github.com/tensorflow/flutter-tflite\" target=\"_blank\">GitHub repository</a> where you can find examples for text classification, super resolution, style transfer, and more!</p>

<p>Additionally, we are working on a new plugin specifically for <a href=\"https://developers.google.com/mediapipe\" target=\"_blank\">MediaPipe Tasks</a>, a low-code tool for easily performing common on-device machine learning tasks. This includes image classification and object detection, like you’ve just learned about, as well as audio classification, face landmark detection, and gesture recognition, alongside a whole lot more.</p>

<p>We look forward to all the exciting things you make, so be sure to share them with <a href=\"https://twitter.com/googledevs\" target=\"_blank\">@googledevs</a>, <a href=\"https://twitter.com/TensorFlow\" target=\"_blank\">@TensorFlow</a>, and your developer communities!</p>

<p></p><p></p></div>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Fri, 18 Aug 2023 16:00:00 +0000"
2113,Simpleperf case study: Fast initialization of TFLite’s Memory Arena,https://blog.tensorflow.org/2023/08/simpleperf-case-study-fast.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-46_SfAtZJniOxOGJLavcD545oJnNBs3Q-AhPbzy57QVleiDFyY6-GgS8ABhWdCEXdweomDoLJjIn_GjtQx73iZ2zmccrou5jCRy6_fDrM9LpZlmEbd3x9DV8V2pBxFZNks6ytR9VlmK9Cw_Em1u5_tXp-pvhBjHj2SGjDd39mSAsHA0KrTCAHoTY2uI/s1600/tfsocial%20%281%29.png\" style=\"display: none;\" />

<em>Posted by <a href=\"https://www.linkedin.com/in/alanjkelly\" target=\"_blank\">Alan Kelly</a>, Software Engineer</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNa6_GH3xPgUogQjPA_RUBB5vga9TXVtt6i3TjX9Dqpl4iCzRlpfmtehk3P7f15H0pNVTM3HzZKnGOWB5qICy9BHnDxoRjfRvD5TZXqyH9kEfiSzUADxTalQaJRtmI7CdKMfU73Y-rPb7F9uAYslkzPuwDYuaJyvsjdGSawh2pk2CzrAAdqiKw9W3y1r8/s1600/Simpleperf-TF-Header.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNa6_GH3xPgUogQjPA_RUBB5vga9TXVtt6i3TjX9Dqpl4iCzRlpfmtehk3P7f15H0pNVTM3HzZKnGOWB5qICy9BHnDxoRjfRvD5TZXqyH9kEfiSzUADxTalQaJRtmI7CdKMfU73Y-rPb7F9uAYslkzPuwDYuaJyvsjdGSawh2pk2CzrAAdqiKw9W3y1r8/s1600/Simpleperf-TF-Header.png\" /></a>


<a name=\"more\"></a><p></p>

<p>One of our previous articles, <a href=\"https://blog.tensorflow.org/2020/10/optimizing-tensorflow-lite-runtime.html\" target=\"_blank\">Optimizing TensorFlow Lite Runtime Memory</a>, discusses how TFLite’s memory arena minimizes memory usage by sharing buffers between tensors. This means we can run models on even smaller edge devices. In today’s article, I will describe the performance optimization of the memory arena initialization so that our users get the benefit of low memory usage with little additional overhead.</p>

<p>ML is normally deployed on-device as part of a larger pipeline. TFLite is used because it’s fast and lightweight, but the rest of the pipeline must also be fast. Profiling on the target device with representative data lets us identify the slowest parts of the pipeline so that we can optimize the most important part of the code.</p>

<p>In this article, I will describe the profiling and optimization of TFLite’s memory arena with instructions on how to use <a href=\"https://developer.android.com/ndk/guides/simpleperf\" target=\"_blank\">Simpleperf</a> and visualize the results. Sample commands are given. It is assumed that the <a href=\"https://developer.android.com/ndk\" target=\"_blank\">Android NDK</a> is installed and that you have a development device that you can connect to using <a href=\"https://developer.android.com/studio/command-line/adb\" target=\"_blank\">adb</a>.</p>

<h2>Simpleperf</h2>

<p>Simpleperf comes with some scripts to make it easier to use. <a href=\"https://android.googlesource.com/platform/prebuilts/simpleperf/+/d0669f6130721d9a6f0da8e5d95e4114f535f5a6/run_simpleperf_on_device.py\" target=\"_blank\">run_simpleperf_on_device.py</a> pushes simpleperf to the device and runs your binary with the given arguments.</p>

<p><span style=\"color: #0d904f; font-family: courier;\">/usr/lib/android-ndk/simpleperf/run_simpleperf_on_device.py record –call-graph fp /data/local/tmp/my_binary arg0 arg1 …</span></p>

<p>This will generate the output file <span style=\"color: #0d904f; font-family: courier;\">perf.data</span> which you must then copy back to your computer.</p>

<p><span style=\"color: #0d904f; font-family: courier;\">adb pull /data/local/tmp/perf.data</span></p>

<p>You then generate the binary cache which contains all the information needed later to generate a useful profile.</p>

<p><span style=\"color: #0d904f; font-family: courier;\">/usr/lib/android-ndk/simpleperf/binary_cache_builder.py -lib /your/binarys/folder -i perf.data</span></p>

<p>And generate the proto buffer used for visualization:</p>

<p><span style=\"color: #0d904f; font-family: courier;\">/usr/lib/android-ndk/simpleperf/pprof_proto_generator.py --ndk_path=/path/to/android-ndk -i perf.data -o profile.proto</span></p>

<p>You can then display the results this using <a href=\"https://github.com/google/pprof\" target=\"_blank\">pprof</a>:</p>

<p><span style=\"color: #0d904f; font-family: courier;\">pprof -http :8888 profile.proto</span></p>

<p>And open localhost:8888 in your browser to view the profile. I find flame graphs to be the most useful:</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVItxfwNM9arJL4E__71igirLNeD4oEu2Ytfx49k-ty-TXxvS3vd-J53sO2FIPbay0qabdu4CNVSz4olF7xzl6sgAufXhX2Ag5w_8Ak_jt76Dk_GQq7rgmzJ5L-GwS-ZdOVb58EIH9NjC8K74jC1O4BXRWbVhVsUY1yWm1zoAfxJNdOk8z-FYyyKZmhCQ/s1600/image2.png\" target=\"_blank\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVItxfwNM9arJL4E__71igirLNeD4oEu2Ytfx49k-ty-TXxvS3vd-J53sO2FIPbay0qabdu4CNVSz4olF7xzl6sgAufXhX2Ag5w_8Ak_jt76Dk_GQq7rgmzJ5L-GwS-ZdOVb58EIH9NjC8K74jC1O4BXRWbVhVsUY1yWm1zoAfxJNdOk8z-FYyyKZmhCQ/s1600/image2.png\" style=\"width: 100%;\" /></a></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiVItxfwNM9arJL4E__71igirLNeD4oEu2Ytfx49k-ty-TXxvS3vd-J53sO2FIPbay0qabdu4CNVSz4olF7xzl6sgAufXhX2Ag5w_8Ak_jt76Dk_GQq7rgmzJ5L-GwS-ZdOVb58EIH9NjC8K74jC1O4BXRWbVhVsUY1yWm1zoAfxJNdOk8z-FYyyKZmhCQ/s1600/image2.png\" target=\"_blank\"><i></i></a></td></tr></tbody></table></div>

<div style=\"text-align: left;\"><br /></div><h2>Optimizing TFLite’s Memory Arena</h2>

<p><span style=\"color: #0d904f; font-family: courier;\">ArenaPlanner::ExecuteAllocations</span> accounts for 54.3% of the runtime of this model. I was expecting to find that ML operators such as fully connected layers or convolutions to be the bottleneck of this model, and not runtime overhead. This is a particularly bad case, the memory arena overhead isn’t this bad for every model, but improvements here will impact all models. This model has variable input sizes and many dynamic tensors, whose output size isn’t known until operator evaluation, which trigger frequent tensor re-allocations. This really is as bad as it gets. Let’s zoom in on the profile.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIOKkwSLXtGYzdqHThekCF4VjNgo2LePTK13zrWGU4jPTsi8PfReFGD9HUh0p44FDW-T1ZWW_2lxLNP6_YRFJdTGIxv6W08uDb8ehR98uOWmTyfBBHpfUSRN0vf7DCak-BvturTj-LPITLC3OdcevYJM6sNdRlYZXK1NWzr1XBcFplTwQ_iNbeoi4Qcs0/s1600/image5.png\" target=\"_blank\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjIOKkwSLXtGYzdqHThekCF4VjNgo2LePTK13zrWGU4jPTsi8PfReFGD9HUh0p44FDW-T1ZWW_2lxLNP6_YRFJdTGIxv6W08uDb8ehR98uOWmTyfBBHpfUSRN0vf7DCak-BvturTj-LPITLC3OdcevYJM6sNdRlYZXK1NWzr1XBcFplTwQ_iNbeoi4Qcs0/s1600/image5.png\" style=\"width: 100%;\" /></a></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p><span style=\"color: #0d904f; font-family: courier;\">InterpreterInfo::num_tensors()</span> accounts for 10.4% of the runtime. The reason this function is so expensive is because it is a virtual function which calls another function and it is called within a loop. I would never have suspected this.</p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\"><span style=\"color: #9c27b0;\">for</span></span> (<span class=\"hljs-type\"><span style=\"color: #9c27b0;\">int</span></span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-built_in\"><span style=\"color: #9c27b0;\">static_cast</span></span><span style=\"color: #0d904f;\">&lt;<span class=\"hljs-type\">int</span>&gt;</span>(graph_info_-&gt;<span class=\"hljs-built_in\"><span>num_tensors</span></span>()); ++i) {
  …
}</span></code></td></tr></tbody></table>

<p>Arena planner does not create or destroy tensors so the number of tensors is constant. Let’s cache it. </p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span style=\"color: #9c27b0;\"><span class=\"hljs-type\">const</span> <span class=\"hljs-type\">int</span></span> num_tensors = <span class=\"hljs-built_in\"><span style=\"color: #9c27b0;\">static_cast</span></span><span style=\"color: #0d904f;\">&lt;<span class=\"hljs-type\">int</span>&gt;</span>(graph_info_-&gt;<span class=\"hljs-built_in\">num_tensors</span>());
<span class=\"hljs-keyword\"><span style=\"color: #9c27b0;\">for</span></span> (<span class=\"hljs-type\"><span style=\"color: #9c27b0;\">int</span></span> i = <span class=\"hljs-number\">0</span>; i &lt; num_tensors); ++i) {
  …
}</span></code></td></tr></tbody></table>

<p>Our next piece of low hanging fruit is <span style=\"color: #0d904f; font-family: courier;\">InterpreterInfo::tensor(unsigned long)</span> which is another virtual function which does bounds checking and then returns a pointer to a tensor. Tensors are stored in an array so let’s add a function to get a pointer to this array. The commits are <a href=\"https://github.com/tensorflow/tensorflow/commit/7528df84ad0207ec88eb8324dee8e10cf79fda0d\" target=\"_blank\">here</a> and <a href=\"https://github.com/tensorflow/tensorflow/commit/91cc89a28fcfeef7df3f257bd27086343d438610\" target=\"_blank\">here</a>.</p>

<p>After these simple changes the runtime of this model has reduced by 25% and then overhead of the memory allocator by half. Simpleperf made identifying these inefficiencies easy! Time to profile again to measure the impact of these changes.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5M5lS4MnwYZ7kHG_6c9t3nsipbgiR4gzxBTYaLQeyhuKU0lXx9U15K1x5a4oRpVnH622OPNBAQgbA2YN08BTP7SHxZPGdmZe6SsAsEG-6MVZ95_Vgz3ija9aXGRVmQrWwgVsqiolynHpXm9ljLC3siSXrb7R9_50MBcJT9yF6N_wv9MZUaKxiLwdMLr8/s1600/image1.png\" target=\"_blank\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh5M5lS4MnwYZ7kHG_6c9t3nsipbgiR4gzxBTYaLQeyhuKU0lXx9U15K1x5a4oRpVnH622OPNBAQgbA2YN08BTP7SHxZPGdmZe6SsAsEG-6MVZ95_Vgz3ija9aXGRVmQrWwgVsqiolynHpXm9ljLC3siSXrb7R9_50MBcJT9yF6N_wv9MZUaKxiLwdMLr8/s1600/image1.png\" style=\"width: 100%;\" /></a></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p><span style=\"color: #0d904f; font-family: courier;\">ArenaPlanner::CalculateAllocations</span> is now the most expensive function at 12.7%. This calls two functions: <span style=\"color: #0d904f; font-family: courier;\">SimpleMemoryArena::Allocate</span> and <span style=\"color: #0d904f; font-family: courier;\">ArenaPlanner::CreateTensorAllocationVector</span>.</p>

<p>Although <span style=\"color: #0d904f; font-family: courier;\">ArenaPlanner::CreateTensorAllocationVector</span> is the cheaper of the two, the code is far simpler so it might be easier to optimize. This function identifies which tensors are allocated between two nodes in the graph and then sorts them by size as a Greedy by Size allocation algorithm is used where the largest tensors are allocated first. The structure of the graph is constant so we can store a map of tensors allocated at each node. Instead of checking each tensor in the model to see if it is allocated between the two nodes, we can identify the tensors to be allocated by iterating through the map. The cost of <span style=\"color: #0d904f; font-family: courier;\">ArenaPlanner::CreateTensorAllocationVector</span> has gone from 4.8% to 0.8% of the runtime. Code can be seen <a href=\"https://github.com/tensorflow/tensorflow/commit/72c981ec392a4ec8b3e9fcae44c8be3217265437\" target=\"_blank\">here</a>. Sort does not appear in the profile so we ignore it.</p>

<p>The next function to look at is <span style=\"color: #0d904f; font-family: courier;\">ArenaPlanner::ResolveTensorAllocation</span> which is 10.9% of the runtime after the previous optimizations. This function resets each tensor’s data pointer after allocation. However, these pointers don’t always change. How about keeping track of and only updating the ones which change? After this <a href=\"https://github.com/tensorflow/tensorflow/commit/c9b82169ab767a96aa1bf31556c186c9c91a9fe8\" target=\"_blank\">change</a>, <span style=\"color: #0d904f; font-family: courier;\">ArenaPlanner::ResolveTensorAllocation</span> doesn’t appear in the profile anymore.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmAbsVuiceykKRiNA-XlEA9R3BLLfvoYZrtd8c5Hd4k97tlHm9tBuMEag4-UDU5439fWZysh2kDJmkQcmeIU4LVYjlQpN_ukTwQC3mFjofKqc09LKKwXpG5VmSPOjsVlr0WV-C0N2J61hkNi3Xpkd9iopSmtgU8sjdE1aECcXtn8wxchnQkfNbfvO-COM/s1600/image3.png\" target=\"_blank\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjmAbsVuiceykKRiNA-XlEA9R3BLLfvoYZrtd8c5Hd4k97tlHm9tBuMEag4-UDU5439fWZysh2kDJmkQcmeIU4LVYjlQpN_ukTwQC3mFjofKqc09LKKwXpG5VmSPOjsVlr0WV-C0N2J61hkNi3Xpkd9iopSmtgU8sjdE1aECcXtn8wxchnQkfNbfvO-COM/s1600/image3.png\" style=\"width: 100%;\" /></a></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>Let’s now take a look at allocation and deallocation. <span style=\"color: #0d904f; font-family: courier;\">SimpleMemoryArena::Allocate</span> accounts for 7% and <span style=\"color: #0d904f; font-family: courier;\">SimpleMemoryArena::Deallocate</span> accounts for 6.8% of the runtime. A record of all allocations in the arena are stored in a vector ordered by their offsets within the memory arena. Entries in this sorted data structure are inserted, removed and searched. These operations are all <b>O(N)</b> in a vector. Could an <span style=\"color: #0d904f; font-family: courier;\">std::multimap</span> with the offset as the key be better? A multimap is needed because the records are ordered by their offsets and there may be multiple tensors with the same offset. Removal and insertion are <b>O(logN)</b> but search would still be <b>O(N)</b> as we are searching for the tensor id and not the offset. The best way to find out is to test and profile.</p>

<p>Replacing the vector with a multimap actually slows down the arena code: it is almost three times slower than using a vector! While this goes against intuition, this is commonly found when optimizing code. Operations on a set or a map have linear or logarithmic complexities, however, there is also a constant value in the complexity. This value is higher than the constant value for the complexity of a vector. We also iterate through the records, which is much cheaper for a vector than for a list or multimap. A list was also tested, coming in at twice as slow as a vector.</p> 

<p>Deallocation can still be improved though. <span style=\"color: #0d904f; font-family: courier;\">SimpleMemoryArena::Deallocate</span> iterates through the records and when it finds the record to deallocate, it removes it from the vector. This has O(N2) complexity. The <span style=\"color: #0d904f; font-family: courier;\">memcpy</span> seen in the profile above comes from the frequent calls to <span style=\"color: #0d904f; font-family: courier;\">std::vector::erase</span>. It is much more efficient to mark records to be erased and then to erase them in one pass using <span style=\"color: #0d904f; font-family: courier;\">std::remove_if</span>. The second optimization here is to look at how tensors are typically deallocated: <span style=\"color: #0d904f; font-family: courier;\">ArenaPlanner::ResetAllocationsAfter</span> deallocates all tensors from a node until the end of the graph. To address this, <span style=\"color: #0d904f; font-family: courier;\">SimpleMemoryArena::DeallocateAfter(int32_t node)</span> was added which iterates once through all the records, marking those which are allocated after the node. <span style=\"color: #0d904f; font-family: courier;\">SimpleMemoryArena::ResolveDeallocations</span> erases these in one pass making deallocation O(N). After these changes, <span style=\"color: #0d904f; font-family: courier;\">ResetAllocationsAfter</span> no longer appears in the profile! Commits are <a href=\"https://github.com/tensorflow/tensorflow/commit/509b811aba38294f451dd7beea3c27558ce1f7da\" target=\"_blank\">here</a> and <a href=\"https://github.com/tensorflow/tensorflow/commit/9e582c01e1a6f813d4c76a845fe3802fbc3140f8\" target=\"_blank\">here</a>.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcVMdXbVIA-debMqWR-kltkIDR0381nhAGz9No2gHR_tONq4fbSPRrtEr3gO_J0A-hRK2OCiZ7Vb48l7tQjOlixc-Rx1JisMu5cAmxnDoUAAb3UQ_uZswvrwyJrpt2_CmWB7HcrKrzjPCBF9EmEpFMKbwYLNmlnziMxt5ixPu4iFDlpNtRofU2HLHXRA8/s1600/image4.png\" target=\"_blank\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcVMdXbVIA-debMqWR-kltkIDR0381nhAGz9No2gHR_tONq4fbSPRrtEr3gO_J0A-hRK2OCiZ7Vb48l7tQjOlixc-Rx1JisMu5cAmxnDoUAAb3UQ_uZswvrwyJrpt2_CmWB7HcrKrzjPCBF9EmEpFMKbwYLNmlnziMxt5ixPu4iFDlpNtRofU2HLHXRA8/s1600/image4.png\" style=\"width: 100%;\" /></a></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>The profile now looks very different with the overhead of tensor allocation gone from 49.9% of the runtime to 11%. This profile already looks much more reasonable. <span style=\"color: #0d904f; font-family: courier;\">SimpleMemoryArena::Allocate</span> is the last function left to optimize. For each tensor, this function iterates through the vector of records trying to find space for the current allocation. The complexity of this is <b>O(N<sup><small>2</small></sup>)</b>. This is a fundamental limitation of the Greedy By Size algorithm. Efficient use of memory comes at the cost of increased overhead. Although the complexity can’t be reduced, <b>N</b> can. We process nodes in the order in which they are executed. Allocation information for tensors which have been deallocated on already executed nodes is not needed anymore, it is only slowing things down. Records are purged periodically so that only records which are active are considered. On a large model, this significantly reduces <b>N</b>. <span style=\"color: #0d904f; font-family: courier;\">ArenaPlanner::ExecuteAllocations</span> is no longer the most expensive function! It has gone from 11% to 6% and a fully connected operator is now the most expensive function in the profile, which is what we expect when profiling neural network inference.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpwRYUSEqYWG2iBb-jdKpEHB9n7LqmIycVUdbZImZZUX_SsBtcpjnZ4neVsAdzusonvoruTnVMqomuQ7rEo_mnSQTyI8Bp9xN4hMEZibsrVv8LMMu6YMvpWf8AoXeLMNR-txstn66E-hN-QKaGJNjm-bC4wi2gHVx6GTfhcZ2MSoM4VN5ZmkL_ZP2XQ10/s1600/image6.png\" target=\"_blank\"><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhpwRYUSEqYWG2iBb-jdKpEHB9n7LqmIycVUdbZImZZUX_SsBtcpjnZ4neVsAdzusonvoruTnVMqomuQ7rEo_mnSQTyI8Bp9xN4hMEZibsrVv8LMMu6YMvpWf8AoXeLMNR-txstn66E-hN-QKaGJNjm-bC4wi2gHVx6GTfhcZ2MSoM4VN5ZmkL_ZP2XQ10/s1600/image6.png\" style=\"width: 100%;\" /></a></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>This is what a neural network profile should look like. Time should be spent running your model’s operators, not in the inference runtime.</p>

<p>The optimized memory arena is now publicly available as part of TensorFlow 2.13.</p>

<h2>Next Steps</h2>

<p>Today’s post walked you through an example of Simpleperf helping to find easy to fix inefficiencies in TFLite’s memory arena that would never have been found by just looking at the code. Pprof can display annotated source code, disassembly and graphs making it easy to find the bottlenecks in your on-device pipelines. </p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Wed, 09 Aug 2023 16:00:00 +0000"
2114,What's new in TensorFlow 2.13 and Keras 2.13?,https://blog.tensorflow.org/2023/07/whats-new-in-tensorflow-213-and-keras-213.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgYIMKKNGUkhWyF3gL1qLpn7cQ3iQu5GleP-36OHbTJwn90YdRUk8vGTefc9ctjwPcJhyBbFlprd581nDsWEKfMAeAo9xuX8zKfxti8Fvl2f2v69Qmvt695cCJY1dfVPbMIlfWqMFKMEyBCgIaRLXypYCrHlob-OiAb0mvVbhmBEt65-agfmRWDMuaI/s1600/image1.png\" style=\"display: none;\" />

<em>Posted by the TensorFlow and Keras Teams</em>
  
<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRgwmbW2Tp7Q_Y3GTKSmkfboFNHuUizDaYh1uLrQSCZarfkvs1mK7OodevH9l318Ls8ddEmKNPewlpVhMJzKvtpktP6TeKniEEMAzgRrHq-D-kIEsoQnZyvc7n4pUVsn1RkFF066dnujQZ1htprWST0uSJftVZxQyc2Qm8aijQTMhrtJlj7rrEc6s7/s1600/Tensorflow-septmber-update-header%20%282%29.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRgwmbW2Tp7Q_Y3GTKSmkfboFNHuUizDaYh1uLrQSCZarfkvs1mK7OodevH9l318Ls8ddEmKNPewlpVhMJzKvtpktP6TeKniEEMAzgRrHq-D-kIEsoQnZyvc7n4pUVsn1RkFF066dnujQZ1htprWST0uSJftVZxQyc2Qm8aijQTMhrtJlj7rrEc6s7/s1600/Tensorflow-septmber-update-header%20%282%29.png\" /></a>

<a name=\"more\"></a><p></p>

<p>TensorFlow 2.13 and Keras 2.13 have been released! Highlights of this release include publishing Apple Silicon wheels, the new Keras V3 format being default for .keras extension files and many more!</p>

<h2>TensorFlow Core</h2>

<h4>Apple Silicon wheels for TensorFlow</h4>

<p>TensorFlow 2.13 is the first version to provide Apple Silicon wheels, which means when you install TensorFlow on an Apple Silicon Mac, you will be able to use the latest version of TensorFlow. The nightly builds for Apple Silicon wheels were released in March 2023 and this new support will enable more fine-grained testing, thanks to technical collaboration between Apple, MacStadium, and Google.</p>

<h4>tf.lite</h4>
  
<p>The Python TensorFlow Lite Interpreter bindings now have an option to use  <span style=\"color: #0d904f; font-family: courier;\">experimental_disable_delegate_clustering</span> flag to turn-off delegate clustering during delegate graph partitioning phase. You can set this flag in TensorFlow Lite interpreter Python API</p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"color: #444444; font-family: courier;\">interpreter = new Interpreter(file_of_a_tensorflowlite_model, experimental_preserve_all_tensors=<span class=\"hljs-literal\">False</span>)</span></code></td></tr></tbody></table>

<p>The flag is set to <span style=\"color: #0d904f; font-family: courier;\">False</span> by default. This is an advanced feature in experimental that is designed for people who insert explicit control dependencies  via  <code>with tf.control_dependencies()</code>  or need to change graph execution order.</p>

<p>Besides, there are several operator improvements in TensorFlow Lite in 2.13</p>
<ul>
<li><span style=\"color: #0d904f; font-family: courier;\">add</span> operation now supports broadcasting up to 6 dimensions. This will remove explicit broadcast ops from many models. The new implementation is also much faster than the current one which calculates the entire index for both inputs the the input instead of only calculating the part that changes.</li>
  
<li>Improve the coverage for <a href=\"https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8\">16x8 quantization</a> by enabling int16x8 ops for <span style=\"color: #0d904f; font-family: courier;\">exp</span>, <span style=\"color: #0d904f; font-family: courier;\">mirror_pad</span>, <span style=\"color: #0d904f; font-family: courier;\">space_to_batch_nd</span>, <span style=\"color: #0d904f; font-family: courier;\">batch_to_space_nd</span></li> 

<li>Increase the coverage of integer data types</li>
  <ul><li>enabled int16 for <span style=\"color: #0d904f; font-family: courier;\">less</span>, <span style=\"color: #0d904f; font-family: courier;\">greater_than</span>, <span style=\"color: #0d904f; font-family: courier;\">equal</span>, <span style=\"color: #0d904f; font-family: courier;\">bitcast</span>, <span style=\"color: #0d904f; font-family: courier;\">bitwise_xor</span>, <span style=\"color: #0d904f; font-family: courier;\">right_shift</span>, <span style=\"color: #0d904f; font-family: courier;\">top_k</span>, <span style=\"color: #0d904f; font-family: courier;\">mul</span>, and int16 indices for <span style=\"color: #0d904f; font-family: courier;\">gather</span> and <span style=\"color: #0d904f; font-family: courier;\">gather_nd</span></li>
  <li>enabled int8 for <span style=\"color: #0d904f; font-family: courier;\">floor_div</span> and <span style=\"color: #0d904f; font-family: courier;\">floor_mod</span>, <span style=\"color: #0d904f; font-family: courier;\">bitwise_xor</span>, <span style=\"color: #0d904f; font-family: courier;\">bitwise_xor</span></li>
  <li>enabled 32-bit int for <span style=\"color: #0d904f; font-family: courier;\">bitcast</span>, <span style=\"color: #0d904f; font-family: courier;\">bitwise_xor</span>,&nbsp;<span style=\"color: #0d904f; font-family: courier;\">right_shift</span></li></ul>
</ul>

<h4>tf.data</h4>

<p>We have improved usability and added functionality for tf.data APIs.</p>

<p><span style=\"color: #0d904f; font-family: courier;\">tf.data.Dataset.zip</span> now supports Python-style zipping. Previously users were required to provide an extra set of parentheses when zipping datasets as in <span style=\"color: #0d904f; font-family: courier;\">Dataset.zip((a, b, c))</span>. With this change, users can specify the datasets to be zipped simply as <span style=\"color: #0d904f; font-family: courier;\">Dataset.zip(a, b, c)</span> making it more intuitive.</p>

<div>Additionally,&nbsp;<span style=\"color: #0d904f; font-family: courier;\">tf.data.Dataset.shuffle</span>&nbsp;now supports full shuffling. To specify that data should be fully shuffled, use&nbsp;<span style=\"color: #0d904f; font-family: courier;\">dataset = dataset.shuffle(dataset.cardinality())</span>. This will load the full dataset into memory so that it can be shuffled, so make sure to only use this with datasets of filenames or other small datasets.</div>

<p>We have also added a new <span style=\"color: #0d904f; font-family: courier;\">tf.data.experimental.pad_to_cardinality</span> transformation which pads a dataset with zero elements up to a specified cardinality. This is useful for avoiding partial batches while not dropping any data.</p>

<p>Example usage:</p>
<blockquote style=\"border: none; margin: 0px 0px 0px 40px; padding: 0px;\"><span style=\"color: #0d904f; font-family: courier;\">ds = tf.data.Dataset.from_tensor_slices({'a': [1, 2]})<br />ds = ds.apply(tf.data.experimental.pad_to_cardinality(3))<br />list(ds.as_numpy_iterator())<br />[{'a': 1, 'valid': True}, {'a': 2, 'valid': True}, {'a': 0, 'valid': False}] <br /><br /></span><div>This can be useful, e.g. during eval, when partial batches are undesirable but it is also important not to drop any data.</div></blockquote>

<h4>oneDNN BF16 Math Mode on CPU</h4>

<p>oneDNN supports <a href=\"https://oneapi-src.github.io/oneDNN/enum_dnnl_fpmath_mode.html#detailed-documentation\" target=\"_blank\">BF16 math mode</a> where full FP32 tensors are implicitly down-converted to BF16 during computations for faster execution time. TensorFlow CPU users can enable this by setting the environment variable <span style=\"color: #0d904f; font-family: courier;\">TF_SET_ONEDNN_FPMATH_MODE</span> to <span style=\"color: #0d904f; font-family: courier;\">BF16</span>. This mode may negatively impact model accuracy. To go back to full FP32 math mode, unset the variable.</p>

<h2>Keras</h2>
  
<h4>Keras Saving format</h4>

<p>The new Keras V3 saving format, released in <a href=\"https://blog.tensorflow.org/2023/03/whats-new-in-tensorflow-212.html\" target=\"_blank\">TF 2.12</a>, is now the default for all files with the <span style=\"color: #0d904f; font-family: courier;\">.keras</span> extension.</p>
<p>You can start using it now by calling <span style=\"color: #0d904f; font-family: courier;\">model.save(“your_model.keras”)</span>.</p>

<p>It provides richer Python-side model saving and reloading with numerous advantages:</p>
<ul><blockquote>
<li>A <b>lightweight, faster</b> format:</li>
</blockquote></ul>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"ALT TEXT\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXZQ34cVqejCXufc77ohHqz8xlV2B1rLelRk-35axBFELs1gfc1WG8Xs6Xk2KLZ1jzkb-pGqd807UgGTu17ncZ6ioKkogwM8OEh34gH8opFgdFcF0GmaoUtV-R5YBkHFWNoyfqh4kLY69tkiV8dnauiM0FG3cjbnSSIg20Ym50x3Tki20FlAwPEY19PY4/s1600/image1.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<ul><blockquote>
<li><b>Human-readable:</b> The new format is <b>name-based</b>, with a more detailed serialization format that makes debugging much easier. What you load is exactly what you saved, from Python’s perspective.</li>
<li><b>Safer</b>: Unlike SavedModel, there is <b>no reliance on loading via bytecode or pickling</b> – a big advancement for secure ML, as pickle files can be exploited to cause arbitrary code execution at loading time.</li>
<li><b>More general</b>: Support for non-numerical states, such as <b>vocabularies</b> and <b>lookup tables</b>, is included in the new format.</li>
<li><b>Extensible</b>: You can add support for saving and loading exotic state elements in custom layers using&nbsp;<span style=\"color: #0d904f; font-family: courier;\">save_assets()</span>, such as a FIFOQueue – or anything else you want. You have full control of disk I/O for custom assets.</li>
</blockquote></ul>

<p>The legacy formats (h5 and Keras SavedModel) will stay supported in perpetuity. However, we recommend that you consider adopting the new Keras v3 format for saving/reloading in Python runtimes, and using&nbsp;<span style=\"color: #0d904f; font-family: courier;\">model.export()</span> for inference in all other runtimes (such as TF Serving).</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Tue, 25 Jul 2023 16:00:00 +0000"
2115,On-device fetal ultrasound assessment with TensorFlow Lite,https://blog.tensorflow.org/2023/06/on-device-fetal-ultrasound-assessment-with-tensorflow-lite.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi8sF1ZR3G1VqS2QV_ezdQQUzHHhrj2dArTKxiA1AVgsqfIxwj03tGHKyHOp5KXArD73iI0ddC46hewEPwN9NH4ZEB7g8SMvBnMqnESruQXqxj-U2-iMLUdDlAuyYvsRu-j1QImxa0d95UTq5mQJd9cTn94Oz8ZceRwHTnm79BATrj4ZJg6qqJsA7Et/s1600/Social%20-%20TensorFlow%20-%20Fetal%20ultrasound%20assessment%20with%20TensorFlow%20Lite.png\" style=\"display: none;\" />

<em>Posted by Angelica Willis and Akib Uddin, Health AI Team, Google Research</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLP4cZ-jPTE6AJWMnVjy9Md7BLgc-mIuXbLMPzJ2n9mksPyJJpIHw2mrHoANJHAaTsp4yaXcjegQPHXFm1UgLFSB3V7BrnTZRZn_6lPl-d_Htc1Uvg-2lp5RdTiqVk2jaG4pz6_2VKxaEfnKolND_0yi1bLyb9p2iplrBkklM55G9ZnBnlau7aJ9Ae/s1600/Header%20-%20TensorFlow%20-%20Fetal%20ultrasound%20assessment%20with%20TensorFlow%20Lite.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLP4cZ-jPTE6AJWMnVjy9Md7BLgc-mIuXbLMPzJ2n9mksPyJJpIHw2mrHoANJHAaTsp4yaXcjegQPHXFm1UgLFSB3V7BrnTZRZn_6lPl-d_Htc1Uvg-2lp5RdTiqVk2jaG4pz6_2VKxaEfnKolND_0yi1bLyb9p2iplrBkklM55G9ZnBnlau7aJ9Ae/s1600/Header%20-%20TensorFlow%20-%20Fetal%20ultrasound%20assessment%20with%20TensorFlow%20Lite.png\" /></a>

<a name=\"more\"></a><p></p>

<h1>How researchers at Google are working to expand global access to maternal healthcare with the help of AI</h1>

<p>TensorFlow Lite* is an open-source framework to run machine learning models on mobile and edge devices. It’s popular for use cases ranging from image classification, object detection, speech recognition, natural language tasks, and more. From <a href=\"https://www.youtube.com/watch?v=WC9x3jp_nV8\" target=\"_blank\">helping parents of deaf children learn sign language</a>, to <a href=\"https://blog.tensorflow.org/2019/02/air-cognizer-predicting-air-quality.html\" target=\"_blank\">predicting air quality</a>, projects using TensorFlow Lite are demonstrating how on-device ML could directly and positively impact lives by making these socially beneficial applications of AI more accessible, globally.  In this post, we describe how TensorFlow Lite is being used to help develop ultrasound tools in under-resourced settings.</p>

<h2>Motivation</h2>

<p><a href=\"https://www.who.int/news-room/fact-sheets/detail/maternal-mortality\" target=\"_blank\">According to the WHO</a>, complications from pregnancy and childbirth contribute to roughly 287,000 maternal deaths and 2.4 million neonatal deaths worldwide each year. As many as 95% of these deaths occur in under-resourced settings and many are preventable if detected early. Obstetric diagnostics, such as determining gestational age and fetal presentation, are important indicators in planning prenatal care, monitoring the health of the birthing parent and fetus, and determining when intervention is required. Many of these factors are traditionally determined by ultrasound.</p> 

<p>Advancements in sensor technology have made <a href=\"https://pubmed.ncbi.nlm.nih.gov/26683523/\" target=\"_blank\">ultrasound devices more affordable and portable</a>, integrating directly with smartphones. However, ultrasound requires years of training and experience, and, in many rural or underserved regions, there is a shortage of trained ultrasonography experts, making it difficult for people to access care. Due to this global <a href=\"https://www.thelancet.com/journals/langlo/article/PIIS2214-109X%2821%2900442-3\" target=\"_blank\">lack of availability</a>, it has been estimated that as many as <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5704652/\" target=\"_blank\">two-thirds of pregnant people</a> in these settings do not receive ultrasound screening during pregnancy. </p> 

<h2>Expanding access by enabling non-experts</h2>

<p>Google Research is building AI models to help expand access to ultrasound, including models  to predict gestational age and fetal presentation, to allow health workers with no background in ultrasonography to collect clinically useful ultrasound scans. These models make predictions from ultrasound video obtained using an easy-to-teach operating procedure, a <a href=\"https://ieeexplore.ieee.org/document/6103652\" target=\"_blank\">blind sweep protocol</a>, in which a user blindly sweeps the ultrasound probe over the patient's abdomen. In our <a href=\"https://www.nature.com/articles/s43856-022-00194-5\" target=\"_blank\">recent paper</a>, <i>“A mobile-optimized artificial intelligence system for gestational age and fetal malpresentation assessment”</i>, published in Nature Communications Medicine, we demonstrated that, when utilizing blind sweeps, these models enable these non-experts to match standard of care performance in predicting these diagnostics.</p> 

<h2>Blind Sweep Operating Procedure</h2>

<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQRaFpMQ5oRPg5V-yuJFqnRAyPWS_YTAGQLXSESFJOh8gDuBGrXJwVJQ7WtgBfmcGH_nOI1881OruWG3zeAc3rWpEWXLd-Nu8AO7OZifapLEnJUPp1EFjIFI9eaFN6EKwbLqzSi6nxsaqXqAf2cVCelqqLY727PgD6pdQNMBeQ9ImO0zeYCp7K7z1iKHI/s1600/On-device-fetal-ultrasound-side-by-side-1.gif\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;\"><img alt=\"Moving image illustrating the bind sweep method from a lateral view on the left and arial view on the right\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQRaFpMQ5oRPg5V-yuJFqnRAyPWS_YTAGQLXSESFJOh8gDuBGrXJwVJQ7WtgBfmcGH_nOI1881OruWG3zeAc3rWpEWXLd-Nu8AO7OZifapLEnJUPp1EFjIFI9eaFN6EKwbLqzSi6nxsaqXqAf2cVCelqqLY727PgD6pdQNMBeQ9ImO0zeYCp7K7z1iKHI/s1600/On-device-fetal-ultrasound-side-by-side-1.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>This blind-sweep ultrasound acquisition procedure can be performed by non-experts with only a few hours of ultrasound training.</i></td></tr></tbody></table>

<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFKHCUo8RKKY9QL0OeDrMAzDVDFAwUb1LF5MKuQHHoB-Om1k-rMqAHbcvuslm7Bv-cZ_W_aQHjFez-6Eut0VvOW5lDueZP4JU_ugD-Ty2WWLeBUEiB8kj_HUekqipXw10f5mwldasgHZlh6hkGDUUoMAqB9uOvAgn0Z-NauHnd_R8crkjN0yil2qAEZFw/s1600/TF-Model-Performance-charts%20%281%29.png\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;\"><img alt=\"Graphs showing gestational age model performance on left and fetal presentation model performance on right\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhFKHCUo8RKKY9QL0OeDrMAzDVDFAwUb1LF5MKuQHHoB-Om1k-rMqAHbcvuslm7Bv-cZ_W_aQHjFez-6Eut0VvOW5lDueZP4JU_ugD-Ty2WWLeBUEiB8kj_HUekqipXw10f5mwldasgHZlh6hkGDUUoMAqB9uOvAgn0Z-NauHnd_R8crkjN0yil2qAEZFw/s1600/TF-Model-Performance-charts%20%281%29.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i><b>Figure A</b> compares our blind sweep-based gestational age regression model performance with that of the clinical standard of care method for fetal age estimation from fetal biometry measured by expert sonographers. Boxes indicate 25th, 50th, and 75th percentile absolute error in days, and whiskers indicate 5th and 95th percentile absolute error (n = 407 study participants). <b>Figure B</b> shows the Receiver Operating Characteristic (ROC) curves for our blind sweep-based fetal malpresentation classification model, as well as specific performance curves for cases in which blind sweeps were collected by expert sonographers or novices (n = 623 study participants). See our recent <a href=\"https://www.nature.com/articles/s43856-022-00194-5\" target=\"_blank\">paper</a> for further details and additional analysis.</i></td></tr></tbody></table>

<h2>Model development</h2>

<p>Understanding that our target deployment environment is one in which users might not have reliable access to power and internet, we designed these models to be mobile-optimized. Our grouped convolutional LSTM architecture utilizes <a href=\"https://tfhub.dev/s?network-architecture=mobilenet-v2\" target=\"_blank\">MobileNetV2</a> for feature extraction on each video frame as it is received. The final feature layer produces a sequence of image embeddings which are processed by the convolutional LSTM cell state. Since the recurrent connections only operate on the less memory-intensive embeddings, this model can run efficiently in a mobile environment.  </p>

<p>For each subsequence of video frames that make up a sweep, we generate a clip-level diagnostic result, and in the case of gestational age, also produce a model confidence estimate represented as the predicted variance in the detected age. Clip-level gestational age predictions are aggregated via inverse variance weighting to produce a final case-level prediction. </p>

<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgH2p3X83t7oyMYkTrEepL-1cLFz2UzOJuhm1J6iKopYw5Z78LTw1-35u1468TQsrVgPJfb5Kn7q-Tkid4-mceGjTXAMB3Em-BtXekUxgl4KlAsgSaWKZBjjJRtAjefBBLQ177C2E0xYrvX3F2ZayN8cJjFd8lhYDoZFMprBhRJS7KPctBl81wjSwft/s1600/image5.png\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px; text-align: center;\"><img alt=\"Flow chart depicting Gestational Age LSTM Video Model\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgH2p3X83t7oyMYkTrEepL-1cLFz2UzOJuhm1J6iKopYw5Z78LTw1-35u1468TQsrVgPJfb5Kn7q-Tkid4-mceGjTXAMB3Em-BtXekUxgl4KlAsgSaWKZBjjJRtAjefBBLQ177C2E0xYrvX3F2ZayN8cJjFd8lhYDoZFMprBhRJS7KPctBl81wjSwft/s1600/image5.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table>

<h4>Optimization through TensorFlow Lite</h4>

<p>On-device ML has many advantages, including providing enhanced privacy and security by ensuring that sensitive input data never needs to leave the device. Another important advantage of on-device ML, particularly for our use case, is the ability to leverage ML offline in regions with low internet connectivity, including where smartphones serve as a stand-in for more expensive traditional devices. Our prioritization of on-device ML made TensorFlow Lite a natural choice for optimizing and evaluating the memory use and execution speed of our existing models, without significant changes to model structure or prediction performance.</p>

<p>After converting our models to TensorFlow Lite using the <a href=\"https://www.tensorflow.org/lite/models/convert/convert_models#python_api_\" target=\"_blank\">converter API</a>, we explored various optimization strategies, including <a href=\"https://www.tensorflow.org/lite/performance/post_training_quantization\" target=\"_blank\">post-training quantization</a> and alternative delegate configurations. Leveraging a TensorFlow Lite GPU delegate, optimized for sustained inference speed, provided the most significant boost to execution speed. There was a roughly 2x speed improvement with no loss in model accuracy, which equated to real-time inference of more than 30 frames/second with both the gestational age and fetal presentation models running in parallel on Pixel devices. We benchmarked model initialization time, inference time and memory usage for various delegate configurations using TensorFlow Lite <a href=\"https://www.tensorflow.org/lite/performance/measurement\" target=\"_blank\">performance measurement tools</a>, finding the optimal configuration across multiple mobile device manufacturers.</p>

<p>These critical speed improvements allow us to leverage the model confidence estimate to provide sweep-quality feedback to the user immediately after the sweep was captured. When low-quality sweeps are detected, users can be provided with tips on how their sweep can be improved (for example, applying more pressure or ultrasound gel), then prompted to re-do the sweep.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp6xg54KydyzChZ8vhJ8XEyiC2kgHnGeZeuZAkuSkd3CiIqo0odUqcU496DUxtnQN4wE7V213GsSsDgGaWNIQ3V88AEXLIdm6Q-KpXhUufT1wmDL7O2qethVYTDwMvux18tthOIMO-0dSE8yeFQZ6moXR_KIAuG5eJaKBWJUA14muPcB-s0dQ0g1UJDCo/s1600/image4.gif\"><img alt=\"Screen capture of sweep exam being conducted\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp6xg54KydyzChZ8vhJ8XEyiC2kgHnGeZeuZAkuSkd3CiIqo0odUqcU496DUxtnQN4wE7V213GsSsDgGaWNIQ3V88AEXLIdm6Q-KpXhUufT1wmDL7O2qethVYTDwMvux18tthOIMO-0dSE8yeFQZ6moXR_KIAuG5eJaKBWJUA14muPcB-s0dQ0g1UJDCo/s1600/image4.gif\" style=\"width: 50%;\" /></a></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>We developed a mobile application that demonstrates what a potential user experience could look like and allows us to evaluate our TensorFlow Lite models in realistic environments. This app enables ultrasound video frames to be received directly from portable ultrasound devices that support this use case.</i></td></tr></tbody></table></div>

<h2 style=\"text-align: left;\">Looking ahead</h2>

<p>Our vision is to enable safer pregnancy journeys using AI-driven ultrasound that could broaden access globally. We want to be thoughtful and responsible in how we develop our AI to maximize positive benefits and address challenges, guided by our <a href=\"https://ai.google/principles/\" target=\"_blank\">AI Principles</a>. TensorFlow Lite has helped enable our research team to explore, prototype, and de-risk impactful care-delivery strategies designed with the needs of lower-resource communities in mind.</p>


<p>This research is in its early stages and we look forward to opportunities to expand our work. To achieve our goals and scale this technology for wider reach globally, partnerships are critical. We are excited about our partnerships with <a href=\"https://www.nm.org/\" target=\"_blank\">Northwestern Medicine</a> in the US and <a href=\"https://www.jacarandahealth.org/\" target=\"_blank\">Jacaranda Health</a> in Kenya to further develop and evaluate these models. With more automated and accurate evaluations of maternal and fetal health risks, we hope to lower barriers and help people get timely care.</p>


<h3>Acknowledgements</h3>

<p><em><strong>This work was developed by an interdisciplinary team within Google Research:</strong> Ryan G. Gomes, Chace Lee, Angelica Willis, Marcin Sieniek, Christina Chen, James A. Taylor, Scott Mayer McKinney, George E. Dahl, Justin Gilmer, Charles Lau, Terry Spitz, T. Saensuksopa, Kris Liu, Tiya Tiyasirichokchai, Jonny Wong, Rory Pilgrim, Akib Uddin, Greg Corrado, Lily Peng, Katherine Chou, Daniel Tse, &amp; Shravya Shetty.</em></p>

<div style=\"text-align: left;\"><em><strong>This work was developed in collaboration with:</strong></em></div>
<div style=\"text-align: left;\"><em>Department of Obstetrics and Gynaecology, University of Zambia School of Medicine, Lusaka, Zambia</em></div>
<div style=\"text-align: left;\"><em>Department of Obstetrics and Gynecology, University of North Carolina School of Medicine, Chapel Hill, NC, USA</em></div>
<div style=\"text-align: left;\"><em>UNC Global Projects—Zambia, LLC, Lusaka, Zambia</em></div>
  
<p><em><strong>Special thanks to:</strong> Yun Liu, Cameron Chen, Sami Lachgar, Lauren Winer, Annisah Um’rani, and Sachin Kotwani</em></p>



<p><small>*TensorFlow Lite has not been certified or validated for clinical, medical, or diagnostic purposes. TensorFlow Lite users are solely responsible for their use of the framework and independently validating any outputs generated by their project. </small></p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Tue, 20 Jun 2023 16:00:00 +0000"
2116,Augmenting recommendation systems with LLMs,https://blog.tensorflow.org/2023/06/augmenting-recommendation-systems-with.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEidaxPxHcwdaf6A0Obz6XG9olWAQyaEaMEV9G0LemGgy2VOnr6LQnHfZJW5xiIsASZtjh12k2fZQjgyexlArNcXf7VlLbgPTe1Is2aPWGZPC0RdFXJGdN829C99bV_vZT71FNADkIdc3HH_TpcNyHyjdwjBSe_--rJ2OM7fokaYLUvh4Tulg2yLq28d/s1600/Social---TensorFlow---Leveraging-LLMs-in-your-recommendation-systems.png\" style=\"display: none;\" />

<em>Posted by Wei Wei, Developer Advocate</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFwce3FRplW_THDff3QhldCkucOxUMLX-webj6doJnJLHaLVnm9v517Zp6q4TH2pFmoCE4JznzsjKU-6w4NOEyYsEOJgVEvk77HuH8-455k9oyX6dbiSPHO0_7ydYrPSaYql12QN9rnGYA6tJUpGcy0Qdu1Y4nItcaTMuPZpni-UfsjTnXKyFyVLWh/s1600/Header%20-%20TensorFlow%20-%20Leveraging%20LLMs%20in%20your%20recommendation%20systems.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFwce3FRplW_THDff3QhldCkucOxUMLX-webj6doJnJLHaLVnm9v517Zp6q4TH2pFmoCE4JznzsjKU-6w4NOEyYsEOJgVEvk77HuH8-455k9oyX6dbiSPHO0_7ydYrPSaYql12QN9rnGYA6tJUpGcy0Qdu1Y4nItcaTMuPZpni-UfsjTnXKyFyVLWh/s1600/Header%20-%20TensorFlow%20-%20Leveraging%20LLMs%20in%20your%20recommendation%20systems.png\" /></a>

<a name=\"more\"></a><p></p>


<p>Large language models (LLMs) are taking the world by storm, thanks to their powerful ability to generate text, translate languages, and answer questions in a coherent and informative way. At Google I/O 2023, we released the <a href=\"https://developers.generativeai.google/guide\" target=\"_blank\">PaLM API</a> as ‘public preview’ so that many developers can start building apps with it. While PaLM API already has excellent documentation on its <a href=\"https://developers.generativeai.google/guide\" target=\"_blank\">extensive usage and best practices</a>, in this blog we are going to take a more focused approach to explore how to leverage LLMs to augment your ML systems in a practical application: recommendation systems.</p>

<p>As a refresher, modern recommendation systems often follow a retrieval-ranking architecture, which enables them to effectively and efficiently filter and rank relevant items to maximize the utility in production. You can go through this <a href=\"https://codelabs.developers.google.com/tfrecommenders-flutter#0\" target=\"_blank\">codelab</a> to learn about building a fullstack movie recommendation system using TensorFlow and Flutter.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Demonstration of the retrieval-ranking architecture where candidate items move from retrieval to ranking, then post-ranking.\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhT7_z2Se7OnZTdExokN9Zs2_KevXHdPJ-Y5E4uGNz-5g6l_FMJdma-O1jMA-VWE50rgT3slXCLrtYUgNKq_Jq0ljjQgvloRQ1W_MBEBcquRV-XRpp56Hm3MfP7qEV6zwUkONhucwnO4DumAPVB53KKOOWMCUUgrWsgNIxL4LFcRW8E53gd04sAXJZz/s16000/Chart%201%20-%20TensorFlow%20-%20Leveraging%20LLMs%20in%20your%20recommendation%20systems.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>We will discuss how LLMs can be incorporated into this retrieval-ranking pipeline.</p>

<h3>Conversational recommendations</h3>

<p>
If you already have access to <a href=\"https://bard.google.com/\" target=\"_blank\">Bard</a>, you can ask it to create recommendations for you interactively in a dialogue. Here is an example of asking Bard for movie recommendations:</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"A user asks 'I'm in the mood for some drama movies with artistic elements tonight. Could you recommend three? Titles only. No other text' Bard responds 'Sure, here are three drama movies with artistic elements that you might enjoy: The Tree of Life, The Piano Teacher, The Passion of Joan of Arc'\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgSqbn9YARgvggZCnYSqYtgSb-Txl5a7Ii_FH9LDwdwcBAGhnJDOYi5AvYA5cMBIElXKBv7v_8iwG_7SHNZn4gwlmqcTQQZopQ83MvORSttQAVwGOs30PGpj0F7ikeOGX10BsZrs-kb6VGU0OVwd01gx8tpAkDLWaBYl_JXlZTLIiYF_dOPlidVeCDc/s1896/image2.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

  
<p>As a developer, you can build a similar functionality in your own applications, using the <a href=\"https://developers.generativeai.google/guide/palm_api_overview#chat_service\" target=\"_blank\">PaLM API Chat service</a> with minimal effort:</p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">prompt = <span class=\"hljs-string\">\"\"\"You are a movie recommender and your job is to recommend new movies based on user input.
So for user 42, he is in the mood for some drama movies with artistic elements tonight.
Could you recommend three? Output the titles only. Do not include other text.\"\"\"</span>
response = palm.chat(messages=prompt)
<span class=\"hljs-built_in\">print</span>(response.last)

<span class=\"hljs-comment\"># Sure, here are three drama movies with artistic elements that I recommend for user 42:</span>
<span class=\"hljs-comment\">#</span>
<span class=\"hljs-comment\"># 1. The Tree of Life (2011)</span>
<span class=\"hljs-comment\"># 2. 20th Century Women (2016)</span>
<span class=\"hljs-comment\"># 3. The Florida Project (2017)</span>
<span class=\"hljs-comment\">#</span>
<span class=\"hljs-comment\"># I hope you enjoy these movies!</span></span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, Liberation Mono, Courier New, monospace\" style=\"font-size: 12px;\">
</span></code></td></tr></tbody></table>

<p>The PaLM API also allows you to help your user continue the exploration and interactively refine the recommendations (e.g., asking to swap The Florida Project for another one) in a dialogue, which is what <a href=\"https://developers.generativeai.google/guide/palm_api_overview#chat_service\" target=\"_blank\">Chat service</a> is designed for. This kind of conversational recommendation interface (think having a knowledgeable chatbot that guides a customer along the way in your shopping app) provides a fluid and personalized experience for the users, and can sometimes be a very appealing addition to your existing recommendation surfaces.</p>

<h3>Sequential recommendations</h3>

<p>Recommendations would be much more useful if your system knows what your users may like. One way to find out your users’ interest is looking at their historical activities and then extrapolating. This is often called ‘sequential recommendation’ because the recommender looks at the sequence of items that have been interacted with and infers what to recommend. Usually you need to use a ML library (i.e., <a href=\"https://www.tensorflow.org/recommenders/examples/sequential_retrieval\" target=\"_blank\">TensorFlow Recommenders</a>) to achieve this. But now with the power of LLMs, you can also do this with the <a href=\"https://developers.generativeai.google/guide/palm_api_overview#palm_api_for_text_and_chat\" target=\"_blank\">PaLM API Text service</a>:</p>
  
<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">prompt = <span class=\"hljs-string\">\"\"\"You are a movie recommender and your job is to recommend new movies based on the sequence of movies that a user has watched. You pay special attention to the order of movies because it matters.

User 42 has watched the following movies sequentially:

\"Margin Call\",
“The Big Short”,
\"Moneyball\",
\"The Martian\",

Recommend three movies and rank them in terms of priority. Titles only. Do not include any other text.
\"\"\"</span>

response = palm.generate_text(
   model=<span class=\"hljs-string\">\"models/text-bison-001\"</span>, prompt=prompt, temperature=<span class=\"hljs-number\">0</span>
)
<span class=\"hljs-built_in\">print</span>(response.result)

<span class=\"hljs-comment\"># 1. The Wolf of Wall Street</span>
<span class=\"hljs-comment\"># 2. The Social Network</span>
<span class=\"hljs-comment\"># 3. Inside Job</span></span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, Liberation Mono, Courier New, monospace\" style=\"font-size: 12px;\">
</span></code></td></tr></tbody></table>

<p>This example prompts the <a href=\"https://developers.generativeai.google/guide/palm_api_overview#text_service\" target=\"_blank\">Text service</a> with 4 movies that have been watched and asks the PaLM API to generate new recommendations based on the sequence of past movies.</p>

<h3>Rating predictions</h3>

<p>In the ranking phase of modern recommendation engines, a list of candidates needs to be sorted based on certain criteria. This is usually done by using a learning-to-rank library (such as, <a href=\"https://www.tensorflow.org/ranking\" target=\"_blank\">TensorFlow Ranking</a>) to predict the ordering. Now you can do this with the PaLM API. Here is an example of predicting movie ratings:
</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">prompt = <span class=\"hljs-string\">\"\"\"You are a movie recommender and your job is to predict a user's rating (ranging from 1 to 5, with 5 being the highest) on a movie, based on that user's previous ratings.

User 42 has rated the following movies:
\"Moneyball\" 4.5
\"The Martian\" 4
\"Pitch Black\" 3.5
“12 Angry Men” 5

Predict the user's rating on \"The Matrix\". Output the rating score only. Do not include other text.
\"\"\"</span>
response = palm.generate_text(model=<span class=\"hljs-string\">\"models/text-bison-001\"</span>, prompt=prompt)
<span class=\"hljs-built_in\">print</span>(response.result)

<span class=\"hljs-comment\"># 4.5</span></span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, Liberation Mono, Courier New, monospace\" style=\"font-size: 12px;\">
</span></code></td></tr></tbody></table>

<p>The PaLM API predicted a high score for The Matrix. You can ask the PaLM API to predict a rating for a list of candidate movies one by one and then sort them in order before making final recommendations; this process is called ‘<a href=\"https://en.wikipedia.org/wiki/Learning_to_rank#Pointwise_approach\" target=\"_blank\">pointwise ranking</a>’. You can even leverage the PaLM API to do <code><a href=\"https://en.wikipedia.org/wiki/Learning_to_rank#Pairwise_approach\" target=\"_blank\">pairwise ranking</a></code> or <code><a href=\"https://en.wikipedia.org/wiki/Learning_to_rank#Listwise_approach\" target=\"_blank\">listwise ranking</a></code>, if you adjust the prompt accordingly.</p>

<p>For a more comprehensive study on rating prediction with LLMs, you can refer to this <a href=\"https://arxiv.org/pdf/2305.06474.pdf\" target=\"_blank\">paper</a> from Google.</p>

<h3>Text embedding-based recommendations</h3>

<p>At this point you may be asking: all the use cases so far involve well known movies that the LLM is already aware of, so maybe there is a requirement that candidate items need to be captured in LLMs in advance (in the training phase)? What if I have private items not known to LLMs beforehand? How could I use the PaLM API then?</p>

<p>Not to worry. The <a href=\"https://developers.generativeai.google/guide/palm_api_overview#palm_api_for_embeddings\" target=\"_blank\">PaLM API for Embeddings</a> can help you out in this case. The basic idea is to embed text associated with your items (for example, product description, movie plot) into vectors and use <a href=\"https://en.wikipedia.org/wiki/Nearest_neighbor_search#:~:text=Nearest%20neighbor%20search%20(NNS)%2C,the%20larger%20the%20function%20values.\" target=\"_blank\">nearest neighbor search</a> techniques (i.e., using the tf.math.top_k op from TensorFlow for brute force search or <a href=\"https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html\" target=\"_blank\">Google ScaNN</a>/<a href=\"https://developers.generativeai.google/examples/vectordb_with_chroma\" target=\"_blank\">Chroma</a> for approximate search) to identify similar items to recommend, based on a user query. Let’s walk through a simple example.</p>

<p>Suppose you are building a news app and at the bottom of each news article you want to recommend similar news to your users. First you can embed all news articles by calling the PaLM API Embedding service like below:</p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">embedding = palm.generate_embeddings(model=<span class=\"hljs-string\">'embedding-gecko-001'</span>, text=<span class=\"hljs-string\">'example news article text'</span>)[<span class=\"hljs-string\">'embedding'</span>]</span></code></td></tr></tbody></table>

<p>For simplicity, let’s assume you store all the news texts and their embeddings in a simple Pandas DataFrame with 2 columns: <code>news_text</code> and <code>embedding</code>. Then you can recommend interestings news to your users using the following:</p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">recommend_news</span>(<span class=\"hljs-params\">query_text, df, topk=<span class=\"hljs-number\">5</span></span>):
  <span class=\"hljs-string\">\"\"\"
  Recommend news based on user query
  \"\"\"</span>
  query_embedding = palm.generate_embeddings(model=<span class=\"hljs-string\">'embedding-gecko-001'</span>, text=query_text)
  dot_products = np.dot(np.stack(df[<span class=\"hljs-string\">'embedding'</span>]), query_embedding[<span class=\"hljs-string\">'embedding'</span>])
  result = tf.math.top_k(dot_products, k=topk)
  indices = result.indices.numpy()
  <span class=\"hljs-keyword\">return</span> df.loc[indices][<span class=\"hljs-string\">'news_text'</span>]

recommend_news(<span class=\"hljs-string\">'news currently being read'</span>, dataframe, <span class=\"hljs-number\">5</span>)</span></code></td></tr></tbody></table>

<p>The <code>recommend_news</code> function computes the query embedding’s <a href=\"https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity\" target=\"_blank\">dot product similarity</a> with all news articles using the pre-computed embeddings, and then identifies 5 news articles most similar to what your user is reading.</p>

<p>This approach is often a quick and effective way to generate candidates and create recommendations based on item similarities. It may be sufficient for many use cases and can be particularly useful in the <a href=\"https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)#New_item\" target=\"_blank\">item cold start situation</a>.</p>

<p>In practice, the candidate generation phase of modern large scale recommenders often consists of multiple sources. For example, you can use a mixer of text embedding-based retrieval, collaborative filtering, users’ subscriptions (i.e., new uploads from followed accounts on YouTube), real time trending items (i.e., breaking news) and etc. Thus, leveraging the <a href=\"https://developers.generativeai.google/guide/palm_api_overview#palm_api_for_embeddings\" target=\"_blank\">PaLM API Embedding service</a> could be a helpful augment for the retrieval stage in your existing recommendation system.</p>


<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"An example of text embedding-based retrieval.\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjpOkFgUYaj3rXJiHOxDnKyh4OWUHsS0BCD-PhDuV16zwDoRHvR382KmtDLnv622KRfBd6ONt1c-WcV9EMzKJ74TkfLsDp2gj-umEyzhFiW2HltOszgxPd_1IjFI1WPC945EoH1VayAzKey2gbTW0zIfZ2D87RxZPqM-mMKjPjx4JCp1PZ86U-DeKsR/s16000/Chart%202%20-%20TensorFlow%20-%20Leveraging%20LLMs%20in%20your%20recommendation%20systems.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<h3>Text embeddings as side features</h3>

<p>In addition, you could also use the text embeddings as side features in a recommendation model. The text embeddings capture the semantic information of the candidate items via the description text and can potentially help improve the model accuracy. For example, in this <a href=\"https://www.tensorflow.org/recommenders/examples/featurization#movie_model\" target=\"_blank\">TensorFlow Recommenders feature preprocessing tutorial</a>, if you have pre-computed text embeddings for movie plot using LLMs, it’s fairly easy to inject them into the model as side features, when concatenating all the embeddings:</p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">MovieModel</span>(tf.keras.Model):

   <span class=\"hljs-comment\"># ......</span>

   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">call</span>(<span class=\"hljs-params\">self, inputs</span>):
       <span class=\"hljs-keyword\">return</span> tf.concat(
           [
               self.title_embedding(inputs[<span class=\"hljs-string\">\"movie_title\"</span>]),
               self.title_text_embedding(inputs[<span class=\"hljs-string\">\"movie_title\"</span>]),
               inputs[<span class=\"hljs-string\">\"movie_plot_embedding\"</span>], <span class=\"hljs-comment\"># inject movie plot embedding</span>
           ],
           axis=<span class=\"hljs-number\">1</span>,
       )</span></code></td></tr></tbody></table>

<p>The default PaLM Embedding service returns a vector of 768 floating numbers for any text, which may be too much. You can reduce the dimensions by initializing a <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\" target=\"_blank\">tf.keras.layers.Embedding</a> layer with the movie plot embedding matrix and then stacking a fully connected layer on top of it to project it down to fewer dimensions.</p>


<h3>Conclusion</h3>


<p>We have shared several ideas on leveraging LLMs to augment recommenders. Obviously, this is just scratching the surface as there are more not covered here. Also note that there may still be a long way before they can make it into production (i.e., latency and cost issues). But we hope this blog inspires you to start thinking about how you can improve your own recommendation systems with LLMs.</p>

<p>Lastly, we are holding an online Developer Summit on Recommendation Systems on June 9, 2023. If you want to learn more about Google products related to building recommendation systems, feel free to sign up <a href=\"https://rsvp.withgoogle.com/events/recommendation-system-dev-summit\" target=\"_blank\">here</a> to attend.</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Tue, 06 Jun 2023 21:00:00 +0000"
2117,Visualizing and interpreting decision trees,https://blog.tensorflow.org/2023/06/visualizing-and-interpreting-decision.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj4evToPaDZpHLq9vcpFjJ_BTAW2ck5UvZ-awvcQRETQB3omK8PW6KxaZksJz2cX9FQiZAZZ_lGtOh_8eyb4gBmVW6akBpRzn8aBb2y6O8283q_2Qu4xUccI7t5X5oHHKHp37_DuEfc7ccWIvYSJQ3knvaIAN8cXY1bgtQWGeaMyfKBcjGa22CaE0IQ/s1600/Social%20%281%29.png\" style=\"display: none;\" />

<em>Posted by <a href=\"https://explained.ai/\" target=\"_blank\">Terence Parr</a>, Google</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuxlMytt4e5sTGjF2M5LzU9UbC3LczfNPqLQ9_1MaEHstY3SUrK_85c2CnW7PALKZ3k3DQDeE94gsyKd6GY-TD17_gwlT8SihSANzH9P-wz2o2yN1MJKomv3OHz1e_UlJt4AlVHefORRaQM5gEYjXPEML6o2xbAIagq4E6D6IhU1wgK9NT4AKQXDKy/s1600/Header%20%281%29%20%281%29.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjuxlMytt4e5sTGjF2M5LzU9UbC3LczfNPqLQ9_1MaEHstY3SUrK_85c2CnW7PALKZ3k3DQDeE94gsyKd6GY-TD17_gwlT8SihSANzH9P-wz2o2yN1MJKomv3OHz1e_UlJt4AlVHefORRaQM5gEYjXPEML6o2xbAIagq4E6D6IhU1wgK9NT4AKQXDKy/s1600/Header%20%281%29%20%281%29.png\" /></a>

<a name=\"more\"></a><p></p>

<p>Decision trees are the fundamental building block of Gradient Boosted Trees and Random Forests, the two most popular machine learning models for tabular data. To learn how decision trees work and how to interpret your models, visualization is essential.</p>

<p>TensorFlow recently published a new <a href=\"https://www.tensorflow.org/decision_forests/tutorials/dtreeviz_colab\" target=\"_blank\">tutorial</a> that shows how to use <a href=\"https://github.com/parrt/dtreeviz\" target=\"_blank\">dtreeviz</a>, a state-of-the-art visualization library, to visualize and interpret <a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\">TensorFlow Decision Forest Trees</a>.</p>

<p>The dtreeviz library, first released in 2018, is now the most popular visualization library for decision trees. The library is constantly being updated and improved, and there is a large community of users who can <a href=\"https://stackoverflow.com/questions/tagged/dtreeviz\" target=\"_blank\">provide support and answer questions</a>. There is a helpful <a href=\"https://www.youtube.com/watch?v=4FC1D9SuDBc\" target=\"_blank\">YouTube video</a> and article on the <a href=\"https://explained.ai/decision-tree-viz/index.html\" target=\"_blank\">design of dtreeviz</a>.</p>

<p>Let’s demonstrate how to use dtreeviz to interpret decision tree predictions.</p>

<p>At a basic level, a decision tree is a machine learning model that learns the relationship between observations and target values by examining and condensing training data into a binary tree. Each leaf in the decision tree is responsible for making a specific prediction. For regression trees, the prediction is a value, such as price. For classifier trees, the prediction is a target category, such as cancer or not-cancer.</p> 

<p>Any path from the root of the decision tree to a specific leaf predictor passes through a series of (internal) decision nodes. Each decision node compares a single feature's value with a specific split point value learned during training. Making a prediction means walking from the root down the tree, comparing feature values, until we reach a leaf. Consider the following simple decision tree that tries to classify animals based upon two features, the number of legs and the number of eyes.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Illustration of a simple decision tree to select an animal based on number of legs (more than or equal to 4; if no = penguin, and/or number of eyes (more than or equal to three; if yes = spider, if no = dog)\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5flFKVZOKjSFkTHzlB7YwDR5uushqWt0gzo_LlYIVMCFMNZLbkZp0FNmPa1zJuKd1-fFtSH9WH9S9W7uIhHjxc1gXzwfWxgRiaUWeGg_waQwfHuSs6ksqdElf-FDu9kZVFvVNVEfpMLvI_05EIXaVzQENFbz5uYB-lIN8SrxCENDCwdS9G5lZluiR/s1600/image3.png\" style=\"width: 60%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>Let's say that our test animal has four legs and two eyes. To classify the test animal, we start at the root of the tree and compare our test animal’s number of legs to four.  Since the number of legs is equal to four, we move to the left.  Next, we test the number of eyes to three. Since our test animal only has two eyes, we move to the right and arrive at a leaf node, which gives us a prediction of <b>dog</b>. To learn more, check out this <a href=\"https://developers.google.com/machine-learning/decision-forests\" target=\"_blank\">class on decision trees</a>.</p>

<p>To interpret decision tree predictions we use dtreeviz to visualize how each decision node in the tree splits up a specific feature's domain, and to show the distribution of training instances in each leaf. For example, here is the first few levels of a classification tree from a Random Forest trained on the <a href=\"https://allisonhorst.github.io/palmerpenguins/articles/intro.html\" target=\"_blank\">Penguin</a> data set:</p>


<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Illustration of the first few levels of a classification tree from a Random Forest trained on the Penguin data set\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjx5qambLYHZh3C7OIpdMrE1t2PtEMOO7W50CJDrz8QaZeXYVO7v4Dk_v1kV7wzuTjwkzxbG_jEk6eRL7mfteYJ9M8R1VLEmJmfaaoSR7R0K5BezRPUKT6m7Pv8YSc7NDTvv7jD5tNaJChxn5SjbV_2MWNnuDJRv89Co5yTfHniOQA5I1BGspddnEyk/s1600/image4.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>To make a prediction for a test penguin, this decision tree first tests the flipper_length_mm feature and if it's less than 206, it descends to the left and then tests the island feature; otherwise, if the flipper length were &gt;= 206, it would descend to the right and test the bill_length_mm feature. (Check out the <a href=\"https://www.tensorflow.org/decision_forests/tutorials/dtreeviz_colab\" target=\"_blank\">tutorial</a> for a description of the visualization elements.)</p>

<p>The code used to generate that tree is short. Given a classifier model called cmodel, we collect and wrap up all of the information about the data and model then ask dtreeviz to visualize the tree:</p>

<div><span id=\"docs-internal-guid-515c23f6-7fff-60f8-64e0-81ba93fe06ce\"><span style=\"font-family: courier;\"><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"vertical-align: baseline;\">penguin_features = [f.name for f in cmodel.make_inspector().features()]</span></p><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"vertical-align: baseline;\">penguin_label = \"species\" &nbsp; # Name of the classification target label</span></p><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"vertical-align: baseline;\">viz_cmodel = dtreeviz.model(cmodel,</span></p><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tree_index=3, # pick tree from forest</span></p><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X_train=train_ds_pd[penguin_features],</span></p><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y_train=train_ds_pd[penguin_label],</span></p><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;feature_names=penguin_features,</span></p><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;target_name=penguin_label,</span></p><p dir=\"ltr\" style=\"line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"vertical-align: baseline;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class_names=classes)</span></p><span style=\"vertical-align: baseline;\">viz_cmodel.view()</span></span></span><p><span><span style=\"vertical-align: baseline;\"><span style=\"font-family: courier;\"></span></span></span></p></div>

<p>And here are the first few layers of a regressor tree from a Random Forest trained on the <a href=\"https://archive.ics.uci.edu/ml/datasets/abalone\" target=\"_blank\">Abalone</a> data set:</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Illustration of the first few layers of a regressor tree from a Random Forest trained on the Abalone data set\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEghL8W1zK344tdXR2ZSAX8dU97nFwVGc-w6nG7a9dO6uI8nLx6vk8yLkeoTBSEE8dp94sGPlVk_wV4tt87rsTy6I9Rl9Kl3vGtd5Qs8Acw6j4fb2Zc8IPOZFGjwvOZbrcJxlDHE2XAfKp2fNxXCd83Ccmikr9C-oxGca7YcqGwW6zHkafqzQvwwHmkw/s1600/image5.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>Another useful tool for interpretation is to visualize how a specific test instance (feature vector) weaves its way down the tree from the root to a specific leaf. By looking at the path taken by a decision tree when making a prediction, we learn why a test instance was classified in a particular way. We know which features are tested and against what range of values. Imagine being rejected for a bank loan. Looking at the decision tree could tell us exactly why we were rejected (e.g., credit score too low or debt to income ratio too high). Here's an example showing the decisions made by the decision tree for a specific Penguin instance, with the path highlighted in orange boxes and the test instance features shown at the bottom left:</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Illustration of the decisions made by the decision tree for a specific Penguin instance, with the path highlighted in orange boxes and the test instance features\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-Qlam26fG-A9OlFJh8vq_ZvxPMsqYXD0AzHkRTgq_oNfRbO4kEEr_KdLeGtuxO49LLYI3NnmN-MprGn_mvgWX5uIWMxx0X7gI_kTtYKkUfCYrBxVryYiv2Qa79Ktw0lSNJKwmGRCNgqIENri50zgcQxi9iRIjvnzXLMfz4E20ni9iKUoQmMmbFgZh/s1600/image1.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>You can also look at information about the leaf contents by calling  <code>viz_cmodel.ctree_leaf_distributions()</code>. For example, here's a plot showing the leaf ID versus samples-per-class for the Penguin dataset:</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Bar diagram showing the leaf ID versus samples-per-class for the Penguin dataset\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxLXt1mNGVIv-YQMLbPUI9ISVvB5oVpRVdvJNIVsmF9zhE7-xGksFYa6Ki-1XWhrejN5TWfKgQ4zX0Q5t4JGz59qZzHZ8gBc_mB88599UT0yzMRN5JVZ7cAq53d4YQmw5XL-KatWVI51eFyAVn-9DMF8Q00frEfG0xF54HYNfI9DsADHsP1ztkDT8w/s1600/image2.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>For regressors, the leaf plot shows the distribution of the target (predicted) variable for the instances in each leaf, such as in this plot from an Abalone decision tree:</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Plot diagram an Abalone decision tree\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg7f3XLxK6ouT4kC6kkcLx-ccdouO_HU3WDyvZFuUso0dtgYq3rS8DPqkH3HChtgelktlIBUvtCUDlZ-9zLVp7n2zDKBTi58Qx_DKX3LFw1SDo4D3WrYWS02RlKRIYKmgerZ4_YYt_VCP4q7rZ3XOLDgHJ1qnIPqQx-Dqe70GmmnA43Xt2V7H8mSYB9/s1600/image6.png\" style=\"width: 75%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>Each “row” in this plot represents a specific leaf and the blue dots indicate the distribution of the rings prediction values for instances associated with that leaf by the training process.</p>

<p>The library can do lots more; this is just a taste. Your next step is to check out the <a href=\"https://www.tensorflow.org/decision_forests/tutorials/dtreeviz_colab\" target=\"_blank\">tutorial</a>!  Then, try dtreeviz on your own tree models. To dig deeper into how decision trees are built and how they carve up feature space to make predictions, you can watch the <a href=\"https://www.youtube.com/watch?v=4FC1D9SuDBc\" target=\"_blank\">YouTube video</a> or the article on the <a href=\"https://explained.ai/decision-tree-viz/index.html\" target=\"_blank\">design of dtreeviz</a>.  Enjoy!</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Tue, 06 Jun 2023 17:00:00 +0000"
2118,Attend our first Developer Summit on Recommendation Systems,https://blog.tensorflow.org/2023/05/attend-our-first-developer-summit-on-recommendation-systems.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh27ZNP5A5naq_5gKOuhYyHcc7wntjw-LJg0jyefTiLHptYgbehHHNknQ2_1G0QXYoNWBjG_4GI7GugVNYdGbLj_L3wrwXZrQEHbaTXwIetpBt4pvBnR_Jsmf2boUBOBQTCMJOuqNXPOlJ6Tyep7rxewA92TqovAMJYH-fEK9f78LkS_RfbFerN0s4W/s1600/TF-Eco-Social.png\" style=\"display: none;\" />

<em>Posted by Wei Wei, Developer Advocate</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihLoVASHytpardLVhZuO9imYDYnIEKx3JF6EXHcSCxhEJN_EepJ4S4UbhePH7bZ8d8lVRvqJ17j9GkCPuUJa01P1beFpePzBdEv5Z7crj4POOmpARFRYodhS5FjEuC6pjOh4UN8lJcBN8ujPf4Xvd8Vcj8JDdT0SZle-91HLGMtlgjzvgw6H0Yyd82/s1600/TF%20BLOG%20TF%20eco.jpeg\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihLoVASHytpardLVhZuO9imYDYnIEKx3JF6EXHcSCxhEJN_EepJ4S4UbhePH7bZ8d8lVRvqJ17j9GkCPuUJa01P1beFpePzBdEv5Z7crj4POOmpARFRYodhS5FjEuC6pjOh4UN8lJcBN8ujPf4Xvd8Vcj8JDdT0SZle-91HLGMtlgjzvgw6H0Yyd82/s1600/TF%20BLOG%20TF%20eco.jpeg\" /></a>

<a name=\"more\"></a><p></p>

<p>Register for the Summit <a href=\"https://rsvp.withgoogle.com/events/recommendation-system-dev-summit\" target=\"_blank\">here</a>!</p>

<p>Recommendation systems are everywhere. They power our favorite websites, apps, and services, helping us find the things we enjoy. But how do modern recommenders work? What are the key components and how do they fit together? How can we make them even better?</p>

<p>Since we launched our <a href=\"https://www.tensorflow.org/resources/recommendation-systems\" target=\"_blank\">recommendation system landing page</a> last year, we have heard many positive feedback from our developer community. While many developers find the new consolidated page very useful to get started with our suite of products, they are also eager to learn more about how to best leverage them to build powerful in-house recommenders for their own business needs.</p>

<p>This is why we are very excited to announce our first-ever Developer Summit on Recommendation Systems (<a href=\"https://rsvp.withgoogle.com/events/recommendation-system-dev-summit\" target=\"_blank\">registration</a> is open now). This event will be held online on June 9, 2023 10AM - 12:15PM US Pacific Time and it will bring together many Google engineers who authored our suite of products to share their insights and expertise in recommendation systems. At this summit, we will not only cover specific products (such as <a href=\"https://www.tensorflow.org/recommenders\" target=\"_blank\">TensorFlow Recommenders</a>, <a href=\"https://www.tensorflow.org/ranking\" target=\"_blank\">TensorFlow Ranking</a>, and <a href=\"https://www.tensorflow.org/agents\" target=\"_blank\">TensorFlow Agents</a>), share ideas on augmenting recommenders with Large Language Models (LLMs), but also discuss Google’s cutting edge recommendation system research (e.g., <a href=\"https://shashankrajput.github.io/Generative.pdf\" target=\"_blank\">generative retrieval</a> using generative AI techniques). </p>

<p>This Developer Summit is the perfect event for anyone who wants to learn more about recommendation systems. Whether you're just getting started or a seasoned practitioner in this exciting domain, you're sure to find something valuable at this event. </p>

<p>We look forward to (virtually) meeting you there!</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Fri, 26 May 2023 14:30:00 +0000"
2119,American Sign Language Fingerspelling Recognition,https://blog.tensorflow.org/2023/05/american-sign-language-fingerspelling-recognition.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWi5-x1EvY3iq6DTWLttZ7-Ym0fLpXEuSR5Ctu_jPxHyI8PL7axqyt6iCajI7QuKgVqX15gaiLP_RJr-A6GNINxsRv3noZkyDBaT2pZsoW2BNdxppejifHQRZwdCExYlZLIzIPYyJ_PDJ9z_-HqGse1T_gW8f4sFW4NF5T2mI_UOjT7HCrMXMr7Vor/s1600/GDS_ASL_FingerspellingCompetition_Banners_Metacard.png\" style=\"display: none;\" />

<em>Posted by Thad Starner (Professor, Georgia Tech and Staff Research Scientist, Google), Sam Sepah (ML Research Program Manager), Manfred Georg (Software Engineer, Google), Mark Sherwood (Senior Product Manager, Google), Glenn Cameron (Product Marketing Manager, Google)</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaHD3yzznEbLuhZQ0u9xCJfu6X7CZtDgJy3Fdd3oIAt0uXXrs6wa3fFVp8kLRxFLgfKH0joBLgHi-_ykI7An2gmjLFnjwOEzjMmzX5NDUReJgv4EUnbjBqKsJXHe8TD7gylvW7qSSt58hwUFS9KicowMUo8yKGCaBJG2sFS1-Ol-uPn92JyYQYCmk6/s1600/GDS_ASL_FingerspellingCompetition_Banners_BlogImage.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaHD3yzznEbLuhZQ0u9xCJfu6X7CZtDgJy3Fdd3oIAt0uXXrs6wa3fFVp8kLRxFLgfKH0joBLgHi-_ykI7An2gmjLFnjwOEzjMmzX5NDUReJgv4EUnbjBqKsJXHe8TD7gylvW7qSSt58hwUFS9KicowMUo8yKGCaBJG2sFS1-Ol-uPn92JyYQYCmk6/s1600/GDS_ASL_FingerspellingCompetition_Banners_BlogImage.png\" /></a>

<a name=\"more\"></a><p></p>

<p>
Over <a href=\"https://www.un.org/en/observances/sign-languages-day\" target=\"_blank\">70 million deaf people</a> around the world use sign language to communicate. Collectively, they use more than 300 different sign languages worldwide. And over 1.5 billion people are affected by hearing loss globally. Most Deaf and Hard of Hearing people cannot use their voice to initiate a search or perform actions due to speech limitations.  Additionally, the interfaces used by smart home devices and mobile platforms to respond to speech are generally audio based.
</p>

<p>
Signed languages are sophisticated systems of communication, each with a complete set of language features.  On a surface level, handshapes along with four other \"parameters\" form the basis of signed communication.  An open hand or a closed hand while making the same motion can completely change the meaning of a sign. Likewise, palm orientation, motion/contact, location, and non-manual markers (typically mouth movements and facial expressions) define individual signs.  A number of grammatical constructs, some of which have no analog in spoken languages, allow a signer to produce complex phrases.
</p>

<p>
As we develop translation systems for American Sign Language (ASL) and other sign languages, it is natural to break apart various aspects of the language and attempt to perform tasks using those parts. </p> 
 
<p>To that end, we’re excited to announce the release of one of the largest datasets of ASL fingerspelling and a <a href=\"https://www.kaggle.com/competitions/asl-fingerspelling\" target=\"_blank\">Kaggle ML competition</a> that will award $200k in prizes to ML engineers who develop the most accurate ASL fingerspelling recognition models using MediaPipe and TensorFlow Lite. The winning models will be open sourced to help developers add support for fingerspelling to their apps.
</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><div style=\"text-align: left;\"></div></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Watch These Hands (Kaggle remix) <br />Performed by Sean Forbes, Co-Founder, Deaf Professional Arts Network</i></td></tr></tbody></table></div>

<p>
Fingerspelling communicates words using hand shapes that represent individual letters. While fingerspelling is only a part of sign languages, it is often used for communicating names, addresses, phone numbers, names, and other information that is commonly entered on a mobile phone.  Many Deaf smartphone users can fingerspell words faster than they can type on mobile keyboards. In fact, in our dataset, ASL fingerspelling of phrases averages 57 words per minute, which is substantially faster than the US average of 36 words per minute for an on screen keyboard. But, sign language recognition AI for text entry lags far behind voice-to-text or even gesture-based typing, as robust datasets didn't previously exist.</p>

<p>
Although fingerspelling is just a small part of sign languages, there are many reasons to produce systems which specifically focus on it, even while maintaining an ultimate goal of full translation.  While fingerspelling at full speed (which can peak over 80 words per minute) the handshapes in the fingerspelling co-articulate together and entire words can become lexicalized into different shapes from their slowed down version.  The resulting movements are visually among the fastest used in ASL, and thus stretch particular aspects of any visual recognition system which seeks to perform full translation.</p>

<h3>Big Steps Forward</h3>


<p>
Google Research and the Deaf Professional Arts Network have worked together to create a massive fingerspelling dataset that we will release for this competition to help move sign language recognition forward. The dataset includes over 3 million fingerspelled characters produced by over 100 Deaf signers in the form of continuous phrases, names, addresses, phone numbers, and URLs. This signing was captured using the selfie camera of a smartphone with a variety of backgrounds and lighting conditions and is the largest dataset collection of its kind to date.</p>

<p>
Large language models show increasing promise in a variety of language and speech tasks.  Everything from chat agents to assistant technology is progressing at breathtaking speed.  It is time to ensure that gesture and visual based systems also produce usable interfaces.  Fingerspelling recognition models are part of this larger solution, which will address the widening gap in accessibility for Deaf and Hard of Hearing individuals.
</p>

<h3>How to Get Involved</h3>

<p>
<a href=\"https://www.kaggle.com/competitions/asl-fingerspelling\" target=\"_blank\">Join the Kaggle competition today</a> to help us make AI more accessible for the Deaf and hard of hearing community.
</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Thu, 11 May 2023 18:00:00 +0000"
2120,Google I/O 2023: What’s new in TensorFlow and Keras?,https://blog.tensorflow.org/2023/05/google-io-2023-whats-new-in-tensorflow-and-keras.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAdcrkW0Y7yDr044bRNHL8KNuJzBU5t8AxBSvSTeNa_k26-s2sYnVWNtCM1vJ2AocvbWNwUBr4KUuThHYWdIMmN_nAZr_WcHSS6Toloyb5IGoW4FFCr_LSriSEf7N777wpxajhOFPOcWt7cEjIwxszyARPRtIhbVLDzo-2sTa5-ViMbevKR50YnSaH/s1600/Social%20-%20TensorFlow%20-%20What%27s%20new%20in%20TensorFlow%20and%20Keras%20%281%29.png\" style=\"display: none;\" />

<em>Posted by Ayush Jain, Carlos Araya, and Mani Varadarajan for the TensorFlow team</em>

<a name=\"more\"></a><p></p>



<p>Welcome to TensorFlow and Keras at Google I/O!</p> 

<p>The world of machine learning is changing, faster than ever. The rise of Large Language Models (LLMs) is sparking the imagination of developers worldwide, with new generative AI applications reaching hundreds of millions of people around the world. These models are trained on massive datasets, and used to solve a variety of tasks, from natural language processing to image generation. </p> 

<p>Powering all these new capabilities requires new levels of model efficiency and performance, as well as support for seamless deployment across a growing number of devices – be it on a server, the web, mobile devices, or beyond. As stewards of one of the largest machine learning communities in the world, the TensorFlow team is continually asking how we can better serve you.</p> 

<p>To that end, this post covers a few of the many improvements and additions coming this year to the TensorFlow ecosystem. Let's dive in!</p> 

<h2><strong>A Growing Ecosystem</strong></h2>

<p>New functionality we’re covering today:</p> 

<p><strong>KerasCV and KerasNLP</strong> allows you to access pre-trained, state-of-the-art models in just a few lines of code.</p> 

<p><strong>DTensor</strong> helps you scale up your models and train them efficiently by combining different parallelism techniques.</p> 

<p>With <strong>JAX2TF</strong>, models written with the <a href=\"https://jax.readthedocs.io/en/latest/\" target=\"_blank\">JAX numerical library</a> can be used in the TensorFlow ecosystem.</p> 

<p>We also preview the <strong>TF Quantization API</strong>, which enables you to make your models more cost and resource-efficient without compromising on accuracy.</p> 


<h2><strong>Applied ML with KerasCV &amp; KerasNLP</strong></h2>

<p>KerasCV and KerasNLP are powerful, modularized libraries that give you direct access to the state-of-the-art in computer vision and natural language processing.</p> 

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"The KerasCV + KerasNLP suite, at a glance.\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg9WdV2Oh8zdilguksOGVZSjjfbknPoj9g38Iswe0PHlaBxIlo6TX_0xWODuHJnQkp8FzgjBpcqd6Y-fJFEjfbYd1bNNKAdWqhOYQTPKrOv7uLiFv8c1FB5kZqSCoM37MI5yq7EBhVTsVpN78jNiZqSZMUlCOQdvKKcxbxBRyVI0-TzpE5MK8S2TmAf/s1600/image5.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>The KerasCV + KerasNLP suite, at a glance.</i></td></tr></tbody></table></div>

<p>Whether you want to classify images, auto-generate text from prompts like with <a href=\"http://bard.google.com\" target=\"_blank\">Bard</a> or anything in between, KerasCV and KerasNLP make it easy with just a few lines of code.  And since it’s a part of Keras, it’s fully integrated with the TensorFlow Ecosystem.</p>

<p> Let's look at some code for image generation. KerasCV is designed to support many models, and in this case we'll use a diffusion model. Despite the complexity of the underlying architecture, you can get it up and running with just a few lines of code.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">from</span> keras_cv.models <span class=\"hljs-keyword\">import</span> (
    StableDiffusion,
)
      
model = StableDiffusion(
    img_width=<span class=\"hljs-number\">512</span>,
    img_height=<span class=\"hljs-number\">512</span>,
)</span></code></td></tr></tbody></table>

<p>With one line to import and another to initialize the model, you can generate completely new images:</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">images = model.text_to_image(
    <span class=\"hljs-string\">\"photograph of an astronaut \"</span>
    <span class=\"hljs-string\">\"riding a horse\"</span>,
    batch_size=<span class=\"hljs-number\">3</span>,
)</span></code></td></tr></tbody></table>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"KerasCV-generated images of an astronaut riding a horse\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEggkjw30TnovVUBFIvPZWiM23AaMQGJaompUuEBSSs2GpzIqU8PBDCoCnGVTO6vJG9WGZmfvhItCCIflq16wC08LdEliEPG8LJsxgENZOFzSEjcP65zps47TKMd8EbSWh6zgkeY-m9VW5kIMuS4rOyoVMhQTPUADiFK2MoEx7tpu-w3QLA36Nt--lcb/s1600/image3.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>KerasCV-generated images of an astronaut riding a horse!</i></td></tr></tbody></table></div>

<p>This is just one of many examples. To learn more, check out our <a href=\"https://youtu.be/K2PKZS1fPlY\" target=\"_blank\">full talk on KerasCV and KerasNLP</a> or in-depth toolkit guides at <a href=\"https://keras.io/keras_cv/\" target=\"_blank\">keras.io/keras_cv</a> and <a href=\"https://keras.io/keras_nlp/\" target=\"_blank\">keras.io/keras_nlp</a>.</p>


<h2><strong>Machine Learning at Scale with DTensor</strong></h2>

<p>DTensor enables larger and more performant model training by giving developers the flexibility to combine and fine-tune multiple parallelism techniques.</p>

<p>Traditionally, ML developers have scaled up models through data parallelism, which splits up your data and feeds it to horizontally-scaled model instances. This scales up training but has an important limitation: it requires that the model fits within a single hardware device. </p> 

<p>As models get bigger, fitting into a single device is no longer a guarantee — developers need to be able to scale their models across hardware devices. This is where model parallelism becomes important, allowing for the model to be split up into shards that can be trained in parallel.</p>

<p>With DTensor, data and model parallelism are not only supported, but also can be directly combined to scale models even more efficiently. And it’s completely accelerator agnostic — whether you use TPUs, GPUs, or something else.</p>


<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Diagram illustrating mixed (data + model) parallelism, with DTensor.\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg7n5JkTotBstwiwVjeqej5EaK7PMiVWbcJctFg9eEm2wLRWj5UJWZiJ1MRg5HrUDH2VqXz6L4jv6IgYt-ybxtf-LrLPo4iVR_HtJQFidDr4Yw1ZxmRUAhfLksUVbajtnhHDzPY8WA5IZkPiArj_8uAoGaeT3uaKMty2bQmcARJtoyiaZ8MOfaQQrn3/s1600/image2.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Mixed (data + model) parallelism, with DTensor.</i></td></tr></tbody></table></div>

<p>Let’s go through an example. Let’s say that you are building with a transformer model, like the Open&nbsp;Pre-trained Transformer (OPT) available through KerasNLP, and training it with some input dataset:</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">opt_lm = keras_nlp.models.OPTCasualLM.from_preset(<span class=\"hljs-string\">\"opt_6.7b_en\"</span>)
opt_lm.<span class=\"hljs-built_in\">compile</span>(...)
opt_lm.fit(wiki_text_dataset)</span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace\" style=\"font-size: 12px;\">
</span></code></td></tr></tbody></table>


<p>But here’s the thing about OPT — it’s <em>big</em>. With variations up to 175 billion parameters, if we tried traditional data parallelism, it would have errored outright — there’s just too many weights to reasonably replicate within a single hardware device. That’s where DTensor comes in.</p>

<p>To work with DTensor, we need to define two things:</p>

<p>First is a <strong>mesh</strong>, where you define (a) a set of hardware devices and (b) a topology, here the batch and model dimensions.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">mesh_dims = [(<span class=\"hljs-string\">\"batch\"</span>, <span class=\"hljs-number\">2</span>), (<span class=\"hljs-string\">\"model\"</span>, <span class=\"hljs-number\">4</span>)]
mesh = dtensor.create_distributed_mesh(mesh_dims, device_type=<span class=\"hljs-string\">\"GPU\"</span>)
dtensor.initialize_accelerator_system(<span class=\"hljs-string\">\"GPU\"</span>)</span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace\" style=\"font-size: 12px;\">
</span></code></td></tr></tbody></table>

<p>Second is a <strong>layout</strong>, which defines how to shard the Tensor dimension on your defined mesh. Through our Keras domain package integrations, you can do this in just one line.</p><p><span id=\"docs-internal-guid-1dd998c1-7fff-68b6-d1cc-0f854cc22024\"></span></p><p dir=\"ltr\" style=\"line-height: 1.44; margin-bottom: 0pt; margin-top: 0pt;\"><span style=\"background-color: #eeeeee;\"><span style=\"color: black; font-style: normal; font-variant: normal; font-weight: 400; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><span style=\"font-family: courier;\">layout_map = keras_nlp.models.OPTCausalLM.create_layout_map(mesh</span></span><span>)</span></span></p><p dir=\"ltr\" style=\"line-height: 1.44; margin-bottom: 0pt; margin-top: 0pt;\"><br /></p>From there, you create the DTensor layout’s context and include your model creation code within it. Note that at no point did we have to make any changes to the model itself!

<div><br /></div><div><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">with</span> layout_map.scope():
    opt_lm = keras_nlp.models.OPTCasualLM.from_preset(<span class=\"hljs-string\">\"opt_6.7b_en\"</span>)
opt_lm.<span class=\"hljs-built_in\">compile</span>(...)
opt_lm.fit(wiki_text_dataset)</span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace\" style=\"font-size: 12px;\">
</span></code></td></tr></tbody></table></div>

<p>Performance for DTensor today is already on par with industry benchmarks, nearly matching the gold-standard implementation of model parallelism offered by NVIDIA’s Megatron for GPUs. Further improvements are in the works to raise the bar even further, across hardware devices.</p>

<p>In the future, DTensor will be fully integrated with key interfaces like <code>tf.distribute</code> and Keras as a whole, with one entry point regardless of hardware and a number of other quality of life features. If you want to learn more, check out the <a href=\"https://www.tensorflow.org/guide/dtensor_overview\" target=\"_blank\">DTensor overview</a> or the <a href=\"https://www.tensorflow.org/tutorials/distribute/dtensor_keras_tutorial\" target=\"_blank\">Keras integration guide</a>!</p>


<h2><strong>Bringing Research to Production with JAX2TF</strong></h2>

<p>Many of the ML advancements that are now household names had their beginnings in research. For example, the Transformer architecture, created and published by Google AI, underpins the fantastic advances in language models.</p>

<p>JAX has emerged as a trusted tool for much of this kind of discovery, but productionizing it is hard. To that end, we’ve been thinking about how to bring research more easily into TensorFlow, giving innovations built on JAX the full strength of TensorFlow’s uniquely robust and diverse production ecosystem.</p>

<p>That’s why we’ve built JAX2TF, a lightweight API that provides a pathway from the JAX ecosystem to the TensorFlow ecosystem. There are many examples of how this can be useful - here’s just a few:</p>
<ul><blockquote>
  <li><strong>Inference:</strong> Taking a model written for JAX and deploying it either on a server using TF Serving or on-device using TFLite.</li>
<li><strong>Fine Tuning:</strong> Taking a model that was trained using JAX, we can bring its components to TF using JAX2TF, and continue training it in TensorFlow with your existing training data and setup.</li>
<li><strong>Fusion:</strong> Combining parts of models that were trained using JAX with those trained using TensorFlow for maximum flexibility.</li>
  </blockquote></ul>

<p>The key to enabling this kind of interoperation between JAX and TensorFlow is baked into <code>jax2tf.convert</code>, which takes in model components created on top of JAX (e.g. your loss function, prediction function, etc.) and creates equivalent representations of them as <a href=\"https://www.tensorflow.org/api_docs/python/tf/function\" target=\"_blank\">TensorFlow functions</a>, which can then be exported as a <a href=\"https://www.tensorflow.org/guide/saved_model\" target=\"_blank\">TensorFlow SavedModel</a>.</p>

<p>We’ve created a code walkthrough for one of the examples above: a quick fine-tuning setup, creating a simple model using modeling libraries in the JAX ecosystem (like <a href=\"https://flax.readthedocs.io/en/latest/\" target=\"_blank\">Flax</a> and <a href=\"https://optax.readthedocs.io/en/latest/\" target=\"_blank\">Optax</a>) and bringing it into TF to finish training. Check it out <a href=\"https://www.tensorflow.org/guide/jax2tf\">here.</a></p>

<p>JAX2TF is already baked into various tools in the TensorFlow ecosystem, under the hood. For example, here are code guides for simple conversion <a href=\"https://optax.readthedocs.io/en/latest/\" target=\"_blank\">from JAX to TFLite</a> for mobile devices and <a href=\"https://blog.tensorflow.org/2022/08/jax-on-web-with-tensorflowjs.html\" target=\"_blank\">from JAX to TF.js</a> for web deployment!</p>

<h2><strong>Coming Soon: The TensorFlow Quantization API</strong></h2>

<p>ML developers today face a wide variety of real-world constraints introduced by the settings they’re working in, like the size of a model or where it gets deployed.</p>

<p>With TensorFlow, we want developers to be able to quickly adjust and accommodate for these kinds of constraints, and to do so without sacrificing model quality. To do this,  we’re building the TF Quantization API, a native quantization toolkit for TF2 which will be available publicly later in 2023. </p>

<p>Briefly, quantization is a group of techniques designed to make models faster, smaller, and generally less resource- and infrastructure-intensive to train and serve. </p>

<p>Quantization does this by reducing the precision of a model’s parameters, just like reducing pixel depth in an image like the one of Albert Einstein below. Note that even with reduced precision, we can still make out the key details:</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Eight renderings of a photograph of Albert Einstein with increasingly reduced bit precision from 8-bit to 1-bit.\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjrfUr_P4Nc_S0OqyRFXHlJ-nP5Fxm6fsbbnj9t7ufsopqF6KTPAGz6x64bvccLI6af5Z8HE2Itne1rs7hh1cf0jJWCYmdgsL23A2fJnaOcSN0rUDfJBXbb4aSuA_78xxSD1Pwk8gHdoCyd2rou3hkEJ66FxzLXb3uRV9b0PrHUkH0MyxF0TCfCFNGd/s1600/image4.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Renderings of a photograph of Albert Einstein with increasingly reduced bit precision.</i></td></tr></tbody></table></div>

<p>At a high level, this works by taking a range of values in your starting precision, and mapping that range to a single bucket in your ending precision. Let’s illustrate this with an example:</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Graph showing quantizing float representation to 4-bit integers\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhp94S4Oi7x4ViL0Se5FvPICRqMALH8h19M3XwNKcLWFOlFIVs2u4dRQ__gAxMeP8IyOh94q5kcBBybjtBwH5czfmzTBMJEkExAXWzktaMBT_VbH-_-2bBVK5VCKr09UCkp60BQ2We9Ap7dLZjRZU98VcvJzwKJVuVIH70RlnsBSnUo4uRDAWnHN8dP/s1600/image1.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Quantizing float representation to 4-bit integers.</i></td></tr></tbody></table></div>

<p>Take a look at the range [0.27, 0.49] on the x-axis: for float32, the blue line actually represents 7381976 unique numbers! The red line represents the int4 quantization of this range, condensing all of those numbers into a single bucket: 1001 (the number 9 in decimal). </p>

<p>By lowering precision through quantization, we can store model weights in a much more efficient, compressed form.</p>

<p>There’s a few different ways to quantize.</p>
<ul><blockquote>
  <li><strong>Post-Training Quantization (PTQ):</strong> Convert to a quantized model after training. This is as simple as it gets and most readily accessible, but there can be a small quality drop.</li>
<li><strong>Quantization-Aware Training (QAT):</strong> Simulate quantization during just the forward pass, providing for maximal flexibility with a minimal quality tradeoff.</li>
<li><strong>Quantized Training:</strong> Quantize all computations while training. This is still nascent, and needs a lot more testing, but is a powerful tool we want to make sure TensorFlow users have access to.</li>
</blockquote></ul>

<p>TensorFlow previously has had a few tools for developers to quantize their models, like <a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/post_training\" target=\"_blank\">this guide for PTQ</a> and <a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide\" target=\"_blank\">this one for QAT</a>. However, these have been limited - with PTQ depending on conversion to TFLite for mobile deployment and QAT requiring you to rewrite your model.</p>

<p>The TF Quantization API is different – it’s designed to work regardless of where you’re deploying, and without you having to rewrite a single line of existing modeling code. We’re building it with flexibility and fidelity in mind, so you get the benefits of a smaller quantized model with new levels of fine-grained control and without any concerns about how it’ll all fit into your stack.</p>

<p>Since you’ve made it this far into the blog, here’s a sneak peek at how it’ll look. We’ll start with a typical setup for a TensorFlow model, just a few layers in Keras. From there, we can load in a predefined quantization schema to apply as a config map to our model.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-comment\"># Step 1: Define your model, just like always.</span>
model = tf.keras.models.Sequential([
 tf.keras.layers.Conv2D(<span class=\"hljs-number\">32</span>, <span class=\"hljs-number\">3</span>, strides=<span class=\"hljs-number\">1</span>, padding=<span class=\"hljs-string\">'same'</span>, activation=<span class=\"hljs-string\">'relu'</span>),
 … …])

<span class=\"hljs-comment\"># Step 2: Set up the quantization config, using a predefined schema.</span>
scheme = scheme_registry.get_scheme(<span class=\"hljs-string\">'pixel_8bit_qat'</span>)
config_map = QuantizationConfigurationMap(scheme)</span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace\" style=\"font-size: 12px;\"></span></code></td></tr></tbody></table>

<p>But if you need more flexibility, TF Quantization API will also let you fully customize how you quantize. There’s built-in support for you to curate your schema to apply different behaviors for every layer, operation, or tensor!</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-comment\"># ...or just as easily configure your own, whether per-layer:</span>
layer_config = LayerConfiguration(
   weight_config=..., activation_config=...)
config_map.set_config(model.layers[<span class=\"hljs-number\">0</span>], layer_config)

<span class=\"hljs-comment\"># per-op:</span>
config_map.set_config(model.layers[<span class=\"hljs-number\">0</span>], op_type=<span class=\"hljs-string\">'matmul'</span>, config={
   <span class=\"hljs-string\">'a'</span>: ..., <span class=\"hljs-string\">'b'</span>: ...,
   <span class=\"hljs-string\">'return'</span>: ...
})

<span class=\"hljs-comment\"># even per-tensor:</span>
_8bit_moving_average = QuantizationConfiguration(...)
per_tensor_config = LayerConfiguration(
   weight_config=..., activation_config=_8bit_moving_average)
config_map.set_config(model.layers[<span class=\"hljs-number\">0</span>], per_tensor_config)</span><span face=\"ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace\" style=\"font-size: 12px;\"></span></code></td></tr></tbody></table>

<p>With that, we can directly apply quantization and train or save within a quantization context. Our model still has natural compatibility with the rest of the TF ecosystem, where quantization truly bears fruit.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-comment\"># Now you can generate a quantization-aware model!</span>
tf.quantization.apply_quantization_on_model(model, config_map, …)

<span class=\"hljs-comment\"># From here, you can train and save just as always.</span>
<span class=\"hljs-keyword\">with</span> tf.quantization.scope(config_map):
  model.fit()
  model.save()

<span class=\"hljs-comment\"># You can also export to TFLite, without any changes!</span>
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()</span><span></span></code></td></tr></tbody></table>

<p>We ran a bunch of tests using the <a href=\"https://arxiv.org/abs/1801.04381\" target=\"_blank\">MobileNetV2</a> model on the Pixel 7, and saw up to 16.7x gains in serving throughput versus the non-quantized baseline. This gain comes without any noticeable detriment to quality: both the float32 baseline and the int8 quantized model reported 73% accuracy. </p>

<p>The TF Quantization API isn’t public just yet, but will be available very soon and will continue to evolve to provide even more benefits.</p>

<h2><strong>That’s a wrap!</strong></h2>

<p>Today, we’ve shown you just a few of the key things we’ve been working on, and there’s a lot more to come.  

<p>We can’t wait to see what you’ll build, and we're always inspired by our community’s enduring enthusiasm and continued partnership. Thanks for stopping by!

<h3><strong>Acknowledgements</strong></h3>

<p><em>Special thanks to George Necula, Francois Chollet, Jonathan Bischof, Scott Zhu, Martin Gorner, Dong Li, Adam Koch, Bruce Fontaine, Laurence Moroney, Josh Gordon, Lauren Usui, and numerous others for their contributions to this post.</em></p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Wed, 10 May 2023 20:30:00 +0000"
2121,AI and Machine Learning @ I/O Recap,https://blog.tensorflow.org/2023/05/ai-and-machine-learning-io-recap.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2XCTScVBPnjR48vwMirVfh0rWWKQMxu0V2iGggyIFvPptS4chSE0QJrUKevGGgRz3M4eOzaGwGwrgSy0LeRK7xcFioKpvDGvVirIWLmVePo-SIv37_hcgJCVPgVLviehjUHFMrt0Z1z36IBH4X67yiMFiH0agsCMBjXtPAJVwvZcxD1wxOV8f14W9/s1600/IO23_Blog_metacard_AI@2x.png\" style=\"display: none;\" />

<p><em>Posted by Lauren Usui and Joe Fernandez</em><p>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWop09VTaoPdAEM9iWURzlhJ1rJXlZP9UOhrUokGUTfMx1XfNYRVXn5zggPqiXTFioP6YUGXpM0UUjKaNB_LOuES7MJCRFUJNbDfwn2odKJNNJzwchZ93_Y9B19dQjSixGOBznklMcJoAuTexEfBEHBnWTHfMovcKVot8dqiQ1GzKCTwR2bYP24YWr/s1600/IO23_Blog_Banner_Ai@2x.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWop09VTaoPdAEM9iWURzlhJ1rJXlZP9UOhrUokGUTfMx1XfNYRVXn5zggPqiXTFioP6YUGXpM0UUjKaNB_LOuES7MJCRFUJNbDfwn2odKJNNJzwchZ93_Y9B19dQjSixGOBznklMcJoAuTexEfBEHBnWTHfMovcKVot8dqiQ1GzKCTwR2bYP24YWr/s1600/IO23_Blog_Banner_Ai@2x.png\" /></a>

<a name=\"more\"></a><p></p>

<p>Artificial intelligence is a topic of kitchen table conversations around the world today, and as AI becomes more accessible for users and developers, we want to make it easier and more useful for everyone. This year at Google I/O, we highlighted how we are helping developers like you build with generative AI, use machine learning in spreadsheets and applications, create ML models from the ground up, and scale them up to serve millions of users.</p>

<p>While AI technology is advancing rapidly, we must continue to ensure it is used responsibly. So we also took some time to explain how Google is taking a <a href=\"https://youtu.be/oAc0ZhbCfi8\" target=\"_blank\">principled approach to applying generative AI</a> and how you can apply our guidelines and tools to make sure your AI-powered products and projects are built responsibly to serve all your users. </p>

<p>If you are new to AI and want to get a quick overview of the technology, check out the <a href=\"https://youtu.be/3K1414RwNDU\" target=\"_blank\">getting started video</a> from Google's AI advocate lead, Laurence Moroney.</p>

<h2><strong>Develop generative AI apps with PaLM 2</strong></h2>

<p>Everyone seems to be chatting with—or about—generative AI recently, and we want you to be able to use Google’s latest large language model, PaLM 2, to power new and helpful experiences for your users with the <a href=\"https://developers.generativeai.google/\" target=\"_blank\">PaLM API</a>. Our session on Generative AI reveals more about how you can easily prompt models with MakerSuite to quickly prototype generative AI applications. We demonstrate how you can use the PaLM API for prompting using examples, conversational chat interactions, and using embedding functionality to compress and compare text data in useful ways. We also showed off how to use the PaLM API in Google Colab notebooks with a simple, magical syntax. Check out this talk and sign up to request access to the PaLM API and <a href=\"https://makersuite.google.com/\" target=\"_blank\">MakerSuite</a>!</p>

 

<h2><strong>Crunch numbers with AI-powered spreadsheets</strong></h2>

<p>Hundreds of millions of people use spreadsheets to organize, manage, and analyze data for everything from business transactions, to inventory accounting, to family budgets. We’re making it easy for everyone to bring the power of AI into spreadsheets with <a href=\"https://simplemlforsheets.com/\" target=\"_blank\">Simple ML for Sheets</a>, a Google Sheets add-on. We recently updated this tool to include anomaly detection and forecasting features. Check out the demonstration of how to predict missing data values and  forecast sales with the tool. No coding required!</p>

 

<h2><strong>Simplify on-device ML applications with MediaPipe</strong></h2>

<p>AI is finding its way into applications across multiple platforms and <a href=\"https://developers.google.com/mediapipe\" target=\"_blank\">MediaPipe</a> makes it easy to build, customize, and deploy on-device ML solutions. We upgraded MediaPipe Solutions this year, improving existing solutions and adding new ones, including interactive segmentation to blur the background behind a selected subject and face stylization to render that selfie in your favorite graphic style.</p>




<h2><strong>Do more with Web ML</strong></h2>

<p>Every week, hundreds of thousands of developers build AI-powered applications to run in the browser or Node.js using JavaScript and web technologies. Web ML has advanced in multiple areas, and we provide a round up of the top updates in this year’s I/O talk. We announced Visual Blocks for ML, an open JavaScript framework for quickly and interactively building custom ML pipelines. You can now run machine learning models even faster with improved WebGL performance and the release of WebGPU in Chrome. More tools and resources are also now available for web ML developers, including TensorFlow Decision Forest support, a visual debugger for models, JAX to JS conversion support, and a new Zero to Hero training course to grow your skills in Web ML.</p>
  

  
  
<h2><strong>Find pre-trained models fast with Kaggle Models</strong></h2>

<p>Building machine learning models can take a huge amount of time and effort: collecting data, training, evaluating, and optimizing. Kaggle is making it a whole lot easier for developers to discover and use pretrained models. With <a href=\"https://www.kaggle.com/models\" target=\"_blank\">Kaggle Models</a>, you can search thousands of open-licensed models from leading ML researchers for multiple ML platforms. Find the model you need quickly with filters for tasks, supported data types, model architecture, and more. Combine this new feature with Kaggle's huge repository of over 200K datasets and accelerate your next ML project.</p>

 

<h2><strong>Apply ML to vision and text with Keras</strong></h2>

<p>Lots of developers are exploring AI technologies and many of you are interested in working on computer vision and natural language processing applications. Keras released new, easy-to-use libraries for computer vision and natural language processing with <a href=\"https://keras.io/keras_cv/\" target=\"_blank\">KerasCV</a> and <a href=\"https://keras.io/keras_nlp/\" target=\"_blank\">KerasNLP</a>. Using just a few lines of code, you can apply the latest techniques and models for data augmentation, object detection, image and text generation, and text classification. These new libraries provide modular implementations that are easy to customize and are tightly integrated with the broader TensorFlow ecosystem including TensorFlow Lite, TPUs, and DTensor.</p>



<h2><strong>Build ML flexibly and scalably with TensorFlow</strong></h2>

<p>With one of the largest ML development communities in the world, the <a href=\"https://www.tensorflow.org/\" target=\"_blank\">TensorFlow</a> ecosystem helps hundreds of thousands of developers like you build, train, deploy, and manage machine learning models. ML technology is rapidly evolving, and we’re upgrading TensorFlow with new tools to give you more flexibility, scalability, and efficiency. If you’re using JAX, you can now bring your model components into the TensorFlow ecosystem with JAX2TF. We also improved DTensor support for model parallelization, allowing you to scale up execution of larger models by running portions of a single model, or shards, across multiple machines. We also announced a toolkit for applying quantization techniques to practically any TensorFlow model, helping you gain substantial efficiency improvements for your AI applications. The quantization toolkit will be available later this year.</p>



<h2><strong>Scale large language models with Google Cloud</strong></h2>

<p>When it's time to deploy your AI-powered applications to your business, enterprise, or the world, you need reliable tools and services that scale with you. Google Cloud's <a href=\"https://cloud.google.com/vertex-ai\" target=\"_blank\">Vertex AI</a> is an end-to-end ML platform that helps you develop ML models quickly and easily, and deploy them at any scale. To help you build generative AI technology for your product or business, we've introduced Model Garden and the Generative AI Studio as part of the Vertex AI platform. Model Garden gives you quick access to the latest foundation models such as Google PaLM 2, and many more to build AI-powered applications for text processing, imagery, and code. Generative AI Studio lets you quickly prototype generative AI applications right in your browser, and when you are ready to deploy, Vertex AI and Google Cloud services enable you to scale up to hundreds, thousands, or millions of users. </p>



<h2><strong>Explore new resources to build with Google AI</strong></h2>

<p>As tools, technology, and techniques for AI development rapidly advance, finding what you need to get started or take the next step with your project can be challenging. We're making it easier to find the right resources to accelerate your AI development at <a href=\"https://ai.google/build/machinelearning/\" target=\"_blank\">Build with Google AI</a>. This new site brings together tools, guidance, and community for building, deploying, and managing ML. Whether you are creating AI for on-device apps or deploying AI at scale, we help you navigate the options and find your path. Check out our latest toolkits on <a href=\"https://ai.google/build/machine-learning/toolkits/llm-android\" target=\"_blank\">Building an LLM on Android</a> and <a href=\"https://ai.google/build/machine-learning/toolkits/applied-ml-keras\" target=\"_blank\">Text Classification with Keras</a>.</p>


<h2><strong>Making Generative AI safe and responsible</strong></h2>

<p>AI is a powerful tool, and it's up to all of us to ensure that it is used responsibly and for the benefit of all. We’re committed to ensuring Google's AI systems are developed according to our <a href=\"https://ai.google/principles/\" target=\"_blank\">AI principles</a>. This year at Google I/O, we shared how we've created guidelines and tools for building generative AI safely and responsibly, and how you can apply those same guidelines and tools for your own projects.</p>




<p>Aaannnd that's a wrap! Check out the <a href=\"https://goo.gle/IO23_ai_ml\" target=\"_blank\">full playlist</a> of all the AI-related sessions we mentioned above. We are excited to share these new tools, resources, and technologies with you, and we can't wait to see what you build with them!</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Wed, 10 May 2023 20:30:00 +0000"
2122,Scaling deep retrieval with TensorFlow Recommenders and Vertex AI Matching Engine,https://blog.tensorflow.org/2023/05/scaling-deep-retrieval-with-tensorflow-recommenders-and-vertex-ai-matching-engine.html,"<img src=\"https://blogger.googleusercontent.com/img/a/AVvXsEioxLwPYbbp4dtHj1gVVgUJ3PkqQ7GtRr0VS-DLtuwoUnOWf99F8Cl8lQ2cZpTVLnY-fd6M22hltTa9kzgrlE0tLmt0JVpdNEcz3nv9wNBLEcyPakq0nscIkCimcwi2HwYo0osTOltdpe0bJA1VlQVIgiUz1cJLqfLybKBykoOxQVGhlC9FqYwtVdRB\" style=\"display: none;\" />

<p><em>Posted by Jeremy Wortz, ML specialist, Google Cloud &amp; Jordan Totten, Machine Learning Specialist</em></p><p>

<a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEioxLwPYbbp4dtHj1gVVgUJ3PkqQ7GtRr0VS-DLtuwoUnOWf99F8Cl8lQ2cZpTVLnY-fd6M22hltTa9kzgrlE0tLmt0JVpdNEcz3nv9wNBLEcyPakq0nscIkCimcwi2HwYo0osTOltdpe0bJA1VlQVIgiUz1cJLqfLybKBykoOxQVGhlC9FqYwtVdRB\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEioxLwPYbbp4dtHj1gVVgUJ3PkqQ7GtRr0VS-DLtuwoUnOWf99F8Cl8lQ2cZpTVLnY-fd6M22hltTa9kzgrlE0tLmt0JVpdNEcz3nv9wNBLEcyPakq0nscIkCimcwi2HwYo0osTOltdpe0bJA1VlQVIgiUz1cJLqfLybKBykoOxQVGhlC9FqYwtVdRB\" /></a>

</p><a name=\"more\"></a><p></p>

<p><em> Cross posted from <a href=\"https://cloud.google.com/blog/products/ai-machine-learning/scaling-deep-retrieval-tensorflow-two-towers-architecture\" target=\"_blank\">Google Cloud AI &amp; Machine Learning</a></em></p>

<p>In a previous <a href=\"https://cloud.google.com/blog/topics/developers-practitioners/looking-build-recommendation-system-google-cloud-leverage-following-guidelines-identify-right-solution-you-part-i\" target=\"_blank\">blog</a>, we outlined three approaches for implementing recommendation systems on Google Cloud, including (1) a fully managed solution with <a href=\"https://cloud.google.com/retail/docs\" target=\"_blank\">Recommendations AI</a>, (2) matrix factorization from <a href=\"https://cloud.google.com/bigquery/docs\" target=\"_blank\">BigQuery ML</a>, and (3) custom deep retrieval techniques using two-tower encoders and <a href=\"https://cloud.google.com/vertex-ai/docs/matching-engine/overview\" target=\"_blank\">Vertex AI Matching Engine</a>. In this blog, we dive deep into option (3) and demonstrate how to build a playlist recommendation system by implementing an end-to-end candidate retrieval workflow from scratch with Vertex AI. Specifically, we will cover: </p>
<ul><blockquote>
<li>The evolution of retrieval modeling and why two-tower encoders are popular for deep retrieval tasks</li>
<li>Framing a playlist-continuation use-case using the <a href=\"https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge\" target=\"_blank\">Spotify Million Playlist Dataset</a> (MPD)</li>
<li>Developing custom two-tower encoders with the <a href=\"https://www.tensorflow.org/recommenders\" target=\"_blank\">TensorFlow Recommenders</a> (TFRS) library</li>
<li>Serving candidate embeddings in an approximate nearest neighbors (ANN) index with <a href=\"https://cloud.google.com/vertex-ai/docs/matching-engine/overview\" target=\"_blank\">Vertex AI Matching Engine</a></li></blockquote></ul>

<p>All related code can be found in this <a href=\"https://github.com/jswortz/spotify_mpd_two_tower\" target=\"_blank\">GitHub repository</a>.</p>

<h2>Background</h2>
  
<p>To meet low latency serving requirements, large-scale recommenders are often deployed to production as <a href=\"https://medium.com/nvidia-merlin/recommender-systems-not-just-recommender-models-485c161c755e\" target=\"_blank\">multi-stage systems</a>. The goal of the first stage (candidate retrieval) is to sift through a large (&gt;100M elements) corpus of candidate items and retrieve a relevant subset (~hundreds) of items for downstream ranking and filtering tasks. To optimize this retrieval task, we consider two core objectives:</p><p>
</p><ol><blockquote>
<li>During model training, find the best way to compile all knowledge into <code>query, candidate</code> embeddings.</li>
<li>During model serving, retrieve relevant items fast enough to meet latency requirements</li></blockquote></ol>
  
<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Conceptual components of multi-stage recommendation systems; the focus of this blog is the first stage, candidate retrieval.\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEihZXbuNoFq-aES9SNyrUqp4pVxkADmpB5sNKWeoa4k5cAokDvc9iBbhyYsDL-OMqP0vxlyWigBOfpgzifz0rlEr6EbsFYh3F0rqJwph714O0QSVZV5o8Aji9KSy1Bl8375DYKPP4ntlKrYkmnRHy-arYFUY_SZBaOntL4YV1_h9yY8AgIIQJtJyv_L/s1600/figure-01_a3ezgV3.max-700x700.jpg\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 1: Conceptual components of multi-stage recommendation systems; the focus of this blog is the first stage, candidate retrieval.</i></td></tr></tbody></table></div>
  
<p>Two-tower architectures are popular for retrieval tasks because they capture the semantics of query and candidate entities, and map these to a shared <a href=\"https://developers.google.com/machine-learning/recommendation/overview/candidate-generation#embedding-space\" target=\"_blank\">embedding space</a> such that semantically similar entities cluster closer together. This means, if we compute the <a href=\"https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings\" target=\"_blank\">vector embeddings</a> of a given query, we can search the embedding space for the closest (most similar) candidates. Because these neural network-based retrieval models take advantage of metadata, context, and feature interactions, they can produce highly informative embeddings and offer flexibility to adjust for various business objectives.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Moving image illustrating how a two tower encoder model trains, calculates, and retrieves data from the embedding space\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0OkYOgUiGD1QxMxyGlJPM0VZDOQFhXwW_SwRBeZoRwHsUsI6cFzaB7VDAy5Lgi67O0dnxqoO4TZwyTQ6-SQ2UsEMKcehpgLgntl6mmNkO_CaQp0EThgtIfFd_g1bZNdTiJwcaDE_OOxhmyC1hkjINQMKVrMEIWDSTkLwWm1sQJCmEfS9YIim7XquA/s1600/Figure-02.gif\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 2: The two-tower encoder model is a specific type of embedding-based search where one deep neural network tower produces the query embedding and a second tower computes the candidate embedding. Calculating the <a href=\"https://developers.google.com/machine-learning/recommendation/overview/candidate-generation#dot-product\" target=\"_blank\">dot product</a> between the two embedding vectors determines how close (similar) the candidate is to the query. Source: <a href=\"https://medium.com/mlearning-ai/building-a-multi-stage-recommendation-system-part-1-1-95961ccf3dd8\" target=\"_blank\">Announcing ScaNN: Efficient Vector Similarity Search</a>.</i></td></tr></tbody></table></div>

<p>While these capabilities help achieve useful <code>query, candidate</code> embeddings, we still need to resolve the retrieval latency requirements. To this end, the two-tower architecture offers one more advantage: the ability to decouple inference of query and candidate items. This decoupling means all candidate item embeddings can be precomputed, reducing the serving computation to (1) converting queries to embedding vectors and (2) searching for similar vectors (among the precomputed candidates).</p>

<p>As candidate datasets scale to millions (or billions) of vectors, the similarity search often becomes a computational bottleneck for model serving. Relaxing the search to approximate distance calculations can lead to significant latency improvements, but we need to minimize negatively impacting search accuracy (i.e., relevance, recall).</p>

<p>In the paper <a href=\"https://arxiv.org/abs/1908.10396\" target=\"_blank\">Accelerating Large-Scale Inference with Anisotropic Vector Quantization</a>, Google Researchers address this speed-accuracy tradeoff with a novel compression algorithm that, compared to <a href=\"https://github.com/erikbern/ann-benchmarks#evaluated\" target=\"_blank\">previous state-of-the-art methods</a>, improves both the relevance and speed of retrieval. At Google, this technique is widely-adopted to support deep retrieval use cases across Search, YouTube, Ads, Lens, and others. And while it’s available in an open-sourced library (<a href=\"https://github.com/google-research/google-research/tree/master/scann\" target=\"_blank\">ScaNN</a>), it can still be challenging to implement, tune, and scale. To help teams take advantage of this technology without the operational overhead, Google Cloud offers these capabilities (and more) as a managed service with <a href=\"https://cloud.google.com/vertex-ai/docs/matching-engine/overview\" target=\"_blank\">Vertex AI Matching Engine</a>. </p>

<p>The goal of this post is to demonstrate how to implement these deep retrieval techniques using Vertex AI and discuss the decisions and trade-offs teams will need to evaluate for their use cases.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Reference architecture for two-tower training and deployment on Vertex AI.\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiPqPFlV3qId7XO2AAP6fcCawuaCBmkHwV44zj7OCiB4unh7BJYbp27c0qpWY4AMxM62ONYEfrMfG8sZnUMCYX7brI7DRh_ulbpV1pLDP5c1Z3KFiWpQ7mgYmyoszyl8PSvxUiCC7Xd0NrEduCEaePwO9Am3OZw2pT0zVOTYtOxUBWMRa9QV_dRCy4A/s1600/figure-03_7QHFOAX.max-800x800.jpg\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 3: Figure 3: A reference architecture for two-tower training and deployment on Vertex AI.</i></td></tr></tbody></table></div>

<h2>Two-towers for deep retrieval</h2>

<p>To better understand the benefits of two-tower architectures, let’s review three key modeling milestones in candidate retrieval.</p>

<h4>Evolution of retrieval modeling</h4>

<p>Traditional information retrieval systems rely heavily on token-based matching, where candidates are retrieved using an inverted index of n-grams. These systems are interpretable, easy to maintain (e.g., no training data), and are capable of achieving high precision. However, they typically suffer poor recall (i.e., trouble finding all relevant candidates for a given query) because they look for candidates having exact matches of key words. While they are still used for select Search use cases, many retrieval tasks today are either adapted with or replaced by embedding-based techniques.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Flow chart illustrating token based retrieval\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhuMjrVtSOfyMbeC5IXiP0dC17TU5UU40IQxORVdafanmIHWmmQDM9McLWjoboCpJVah6fECBnbNplRBlMgcKliKj8NWph6GCKprcs8FJnqgX2s0_XoYTv-1KA3BGH_6l4wFY_t6t5VYLNUsL7UjxGurKaTOIz6OG21vpGHVT9UyGIG-BqZ_LzivtxE/s1600/figure-04_pJByae8.max-1400x1400.jpg\" style=\"width: 50%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 4: Token-based matching selects candidate items by matching key words found in both query and candidate items.</i></td></tr></tbody></table></div>

<p>Factorization-based retrieval introduces a simple embedding-based model that offers much better <a href=\"https://developers.google.com/machine-learning/crash-course/generalization/video-lecture\" target=\"_blank\">generalization</a> by capturing the similarity between <code>query, candidate</code> pairs and mapping them to a shared embedding space. One of the major benefits to this <a href=\"https://developers.google.com/machine-learning/recommendation/collaborative/basics\" target=\"_blank\">collaborative filtering</a> technique is that embeddings are learned automatically from implicit query-candidate interactions. Fundamentally, these models factorize the full query-candidate interaction (co-occurrence) matrix to produce smaller, dense embedding representations of queries and candidates, where the product of these embedding vectors is a good approximation of the interaction matrix. The idea is that by compacting the full matrix into k dimensions the model learns the top k latent factors describing <code>query, candidate</code> pairs with respect to the modeling task.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Illustration of how a factorization based model factoizes a query-candidate interaction matrix intothe product of two lower rank matrices\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi9dC-U02ISI-b2s5cHvYdfCvV_gyNisW5D2h-MqgKCGKhe0fxyACGKoA8r9YCcox8OSTBJErdSeUQYtM6rD1n8gtXf185TH05LRrBt41Ju6cdZhzFHJf65RiEVwcLvi5c1P3oAnJ9QcYdCKH_UqJwIdyg0IEhwhonSh-kcvy_vH9RnkaTMy11mbPfM/s1600/figure-05_gXUdrZt.max-1000x1000.jpg\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 5: Factorization-based models factorize a query-candidate interaction matrix into the product of two lower-rank matrices that capture the query-candidate interactions.</i></td></tr></tbody></table></div>

<p>The latest modeling paradigm for retrieval, commonly referred to as neural deep retrieval (NDR), produces the same embedding representations, but uses deep learning to create them. NDR models like two-tower encoders apply deep learning by processing input features with successive network layers to learn layered representations of the data. Effectively, this results in a neural network that acts as an information distillation pipeline, where raw, multi-modal features are repeatedly transformed such that useful information is magnified and irrelevant information is filtered. This results in a highly expressive model capable of learning non-linear relationships and more complex feature interactions.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Side-by-side illustrations showing the differences between factorization based retrieval and neural deep retreival\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNsBwfCGqFh9_rm9BcJvE50BI7l4GjfaoboECQtcRtCH5ITjgPW7C-xyDSVDYrA1pzNkr_xZtwNXzRqdrMk3UhNq2ONC1ERi1HPDIPIWCbAnFPspW_mXWubNh15MLI1hcyx1YQkx4tHa_OS9voCxo5cgz34dFrgrfjTPah1qQ-TcstRo3HdnOJ7oJ2/s1600/figure-6_uA0W9dA.max-1100x1100.jpg\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 6: NDR architectures like two-tower encoders are conceptually similar to factorization models. Both are embedding-based retrieval techniques computing lower-dimensional vector representations of query and candidates, where the similarity between these two vectors is determined by computing their <a href=\"https://developers.google.com/machine-learning/recommendation/overview/candidate-generation#dot-product\" target=\"_blank\">dot product</a>.</i></td></tr></tbody></table></div>

<p>In a two-tower architecture, each tower is a neural network that processes either query or candidate input features to produce an embedding representation of those features. Because the embedding representations are simply vectors of the same length, we can compute the <a href=\"https://developers.google.com/machine-learning/recommendation/overview/candidate-generation#dot-product\" target=\"_blank\">dot product</a> between these two vectors to determine how close they are. This means the orientation of the embedding space is determined by the dot product of each <code>query, candidate</code> pair in the training examples.</p>

<h2>Decoupled inference for optimal serving</h2>

<p>In addition to increased expressivity and generalization, this kind of architecture offers optimization opportunities for serving. Because each tower only uses its respective input features to produce a vector, the trained towers can be operationalized separately. Decoupling inference of the towers for retrieval means we can precompute what we want to find when we encounter its pair in the wild. It also means we can optimize each inference task differently:</p>
<ul><blockquote>
<li>Run a batch prediction job with a trained candidate tower to precompute embedding vectors for all candidates, attach NVIDIA GPU to accelerate computation</li>
<li>Compress precomputed candidate embeddings to an ANN index optimized for low-latency retrieval; deploy index to an endpoint for serving</li>
<li>Deploy trained query tower to an endpoint for converting queries to embeddings in real time, attach NVIDIA GPU to accelerate computation</li>
</blockquote></ul>
<p>Training two-tower models and serving them with an ANN index is different from training and serving traditional machine learning (ML) models. To make this clear, let’s review the key steps to operationalize this technique.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Side-by-side illustrations showing the differences between factorization based retrieval and neural deep retreival\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgnDMV4PhibCuk5h_whxRUp-Keg8pItXANLSJ50S7yxB_R3l7ZSJcOO1X0N5B0aYNUNQgdtH44pTTdHYSiSYKknx47ZPquC-tqV9uKlAQ5MtCmLh1NR0pAC913UlAS8anpJImwa6o7mmbjJPzNjM7eIzKx3vVTzKN-1CmI7NVEr7Ip-o1oE4Ciiu44M/s1600/figure-07_eejzduW.max-1000x1000.jpg\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 7: A reference architecture for two-tower training and deployment on Vertex AI.</i></td></tr></tbody></table></div>

<ol><blockquote>
<li>Train combined model (two-towers) offline; each tower is saved separately for different tasks</li>
<li>Upload the query tower to Vertex AI Model Registry and deploy to an online endpoint</li>
<li>Upload the candidate tower to Vertex AI Model Registry</li>
<li>Request candidate tower to predict embeddings for each candidate track, save embeddings in JSON file</li>
<li>Create ANN serving index from embeddings JSON, deploy to online index endpoint</li>
<li>User application calls endpoint.predict() with playlist data, model returns the embedding vector representing that playlist</li>
<li>Use the playlist embedding vector to search for N nearest neighbors (candidate tracks)</li>
<li>Matching Engine returns the product IDs for the N nearest neighbors</li>
  </blockquote></ol>

<h2>Problem Framing</h2>

<p>In this example, we use MPD to construct a recommendation use case, playlist continuation, where candidate tracks are recommended for a given playlist (query). This dataset is publicly available and offers several benefits for this demonstration:</p>
<ul><blockquote>
<li>Includes real relationships between entities (e.g., playlists, tracks, artists) which can be difficult to replicate</li>
<li>Large enough to replicate scalability issues likely to occur in production</li>
<li>Variety of feature representations and data types (e.g., playlist and track IDs, raw text, numerical, datetime); ability to enrich dataset with additional metadata from the <a href=\"https://developer.spotify.com/documentation/web-api\" target=\"_blank\">Spotify Web Developer API</a></li>
<li>Teams can analyze the impact of modeling decisions by listening to retrieved candidate tracks (e.g., generate recommendations for your own Spotify playlists)</li>  
  </blockquote></ul>


<h4>Training examples</h4>

<p>Creating training examples for recommendation systems is a non-trivial task. Like any ML use case, training data should accurately represent the underlying problem we are trying to solve. Failure to do this can lead to poor model performance and unintended consequences for the user experience. One such lesson from the <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf\" target=\"_blank\">Deep Neural Networks for YouTube Recommendations</a> paper highlights that relying heavily on features such as ‘click-through rate’ can result in recommending clickbait (i.e., videos users rarely complete), as compared to features like ‘watch time’ which better capture a user’s engagement.</p>

<p>Training examples should represent a semantic match in the data. For playlist-continuation, we can think of a semantic match as pairing playlists (i.e., a set of tracks, metadata, etc.) with tracks similar enough to keep the user engaged with their listening session. How does the structure of our training examples influence this? </p>
<ul><blockquote>
<li>Training data is sourced from positive <code>query, candidate</code> pairs</li>
<li>During training, we forward propagate query and candidate features through their respective towers to produce the two vector representations, from which we compute the dot product representing their similarity </li>
<li>After training, and before serving, the candidate tower is called to predict (precompute) embeddings for all candidate items</li>
<li>At serving time, the model processes features for a given playlist and produces a vector embedding</li>
<li>The playlist’s vector embedding is used in a search to find the most similar vectors in the precomputed candidate index</li>
<li>The placement of candidate and playlist vectors in the embedding space, and the distance between them, is defined by the semantic relationships reflected in the training examples</li>
</blockquote></ul>
  
<p>The last point is important. Because the quality of our embedding space dictates the success of our retrieval, the model creating this embedding space needs to learn from training examples that best illustrate the relationship between a given playlist and <code>similar</code> tracks to retrieve.</p>

<p>This notion of similarity being highly dependent on the choice of paired data highlights the importance of preparing features that describe semantic matches. A model trained on <code>playlist title, track title</code> pairs will orient candidate tracks differently than a model trained on <code>aggregated playlist audio features, track audio features</code> pairs. </p>

<p>Conceptually, training examples consisting of <code>playlist title, track title</code> pairs would create an embedding space in which all tracks belonging to playlists of the same or similar titles (e.g., <code>beach vibes</code> and <code>beach tunes</code>) would be closer together than tracks belonging to different playlist titles (e.g., <code>beach vibes</code> vs  <code>workout tunes</code>); and examples consisting of <code>aggregated playlist audio features, track audio features</code> pairs would create an embedding space in which all tracks belonging to playlists with similar audio profiles (e.g., <code>live recordings of instrumental jams</code> and <code>high energy instrumentals</code>) would be closer together than tracks belonging to playlists with different audio profiles (e.g., <code>live recordings of instrumental jams</code> vs <code>acoustic tracks with lots of lyrics</code>). </p>

<p>The intuition for these examples is that when we structure the rich track-playlist features in a format that describes how tracks show up on certain playlists, we can feed this data to a two tower model that learns all of the niche relationships between parent playlist and child tracks. Modern deep retrieval systems often consider user profiles, historical engagements, and context. While we don’t have user and context data in this example, they can easily be added to the query tower.</p>

<h2>Implementing deep retrieval with TFRS</h2>

<p>When building retrieval models with TFRS, the two towers are implemented with <a href=\"https://www.tensorflow.org/guide/keras/custom_layers_and_models\" target=\"_blank\">model subclassing</a>. Each tower is built separately as a callable to process input feature values, pass them through feature layers, and concatenate the results. This means the tower is simply producing one concatenated vector (i.e., the representation of the query or candidate; whatever the tower represents).

First, we define the basic structure of a tower and implement it as a subclassed Keras model:
  
</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">Playlist_Tower</span>(tf.keras.Model):
   <span class=\"hljs-string\">'''
   produced embedding represents the features
   of a Playlist known at query time
   '''</span>
   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, layer_sizes, vocab_dict</span>):
       <span class=\"hljs-built_in\">super</span>().__init__()

       <span class=\"hljs-comment\"># <span class=\"hljs-doctag\">TODO:</span> build sequential model for each feature here</span>

   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">call</span>(<span class=\"hljs-params\">self, data</span>):
       <span class=\"hljs-string\">'''
       defines what happens when the model is called
       '''</span>
       all_embs = tf.concat(
           [
               <span class=\"hljs-comment\"># <span class=\"hljs-doctag\">TODO:</span> concatenate output of all features defined above</span>
                           
           ], axis=<span class=\"hljs-number\">1</span>)
      
       <span class=\"hljs-comment\"># pass output to dense/cross layers</span>
       <span class=\"hljs-keyword\">if</span> self._cross_layer <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:
           cross_embs = self._cross_layer(all_embs)
           <span class=\"hljs-keyword\">return</span> self.dense_layers(cross_embs)
       <span class=\"hljs-keyword\">else</span>:
           <span class=\"hljs-keyword\">return</span> self.dense_layers(all_embs)</span></code></td></tr></tbody></table>

<p>We further define the subclassed towers by creating Keras sequential models for each feature being processed by that tower:</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-comment\"># Feature: pl_name_src</span>
self.pl_name_src_text_embedding = tf.keras.Sequential(
   [
       tf.keras.layers.TextVectorization(
           vocabulary=vocab_dict[<span class=\"hljs-string\">'pl_name_src'</span>],
           ngrams=<span class=\"hljs-number\">2</span>,
           name=<span class=\"hljs-string\">\"pl_name_src_textvectorizor\"</span>
       ),
       tf.keras.layers.Embedding(
           input_dim=MAX_TOKENS,
           output_dim=EMBEDDING_DIM,
           name=<span class=\"hljs-string\">\"pl_name_src_emb_layer\"</span>,
           mask_zero=<span class=\"hljs-literal\">False</span>
       ),
       tf.keras.layers.GlobalAveragePooling1D(name=<span class=\"hljs-string\">\"pl_name_src_1d\"</span>),
   ], name=<span class=\"hljs-string\">\"pl_name_src_text_embedding\"</span>
)</span></code></td></tr></tbody></table>

<p>Because the features represented in the playlist’s <code>STRUCT</code> are sequence features (lists), we need to reshape the embedding layer output and use 2D pooling (as opposed to the 1D pooling applied for non-sequence features):</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-comment\"># Feature: artist_genres_pl</span>
self.artist_genres_pl_embedding = tf.keras.Sequential(
   [
       tf.keras.layers.TextVectorization(
           ngrams=<span class=\"hljs-number\">2</span>,
           vocabulary=vocab_dict[<span class=\"hljs-string\">'artist_genres_pl'</span>],
           name=<span class=\"hljs-string\">\"artist_genres_pl_textvectorizor\"</span>
       ),
       tf.keras.layers.Embedding(
           input_dim=MAX_TOKENS, 
           output_dim=EMBED_DIM,
           name=<span class=\"hljs-string\">\"artist_genres_pl_emb_layer\"</span>,
           mask_zero=<span class=\"hljs-literal\">False</span>
       ),
       tf.keras.layers.Reshape([-<span class=\"hljs-number\">1</span>, MAX_PL_LENGTH, EMBED_DIM]),
       tf.keras.layers.GlobalAveragePooling2D(name=<span class=\"hljs-string\">\"artist_genres_pl_2d\"</span>),
   ], name=<span class=\"hljs-string\">\"artist_genres_pl_emb_model\"</span>
)</span></code></td></tr></tbody></table>

<p>Once both towers are built, we use the TFRS base model class (<a href=\"https://www.tensorflow.org/recommenders/api_docs/python/tfrs/models/Model\" target=\"_blank\">tfrs.models.Model</a>) to streamline building the combined model. We include each tower in the class <code>__init__</code> and define the <code>compute_loss</code> method:</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">TheTwoTowers</span>(tfrs.models.Model):

   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, layer_sizes, vocab_dict, parsed_candidate_dataset</span>):
       <span class=\"hljs-built_in\">super</span>().__init__()
      
       self.query_tower = Playlist_Tower(layer_sizes, vocab_dict)

       self.candidate_tower = Candidate_Track_Tower(layer_sizes, vocab_dict)
      
       self.task = tfrs.tasks.Retrieval(
           metrics=tfrs.metrics.FactorizedTopK(
               candidates=parsed_candidate_dataset.batch(<span class=\"hljs-number\">128</span>).<span class=\"hljs-built_in\">map</span>(
                   self.candidate_tower,
                   num_parallel_calls=tf.data.AUTOTUNE
               ).prefetch(tf.data.AUTOTUNE)
           )
       )

   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">compute_loss</span>(<span class=\"hljs-params\">self, data, training=<span class=\"hljs-literal\">False</span></span>):
      
       query_embeddings = self.query_tower(data)
       candidate_embeddings = self.candidate_tower(data)
      
       <span class=\"hljs-keyword\">return</span> self.task(
           query_embeddings,
           candidate_embeddings,
           compute_metrics=<span class=\"hljs-keyword\">not</span> training,
           candidate_ids=data[<span class=\"hljs-string\">'track_uri_can'</span>],
           compute_batch_metrics=<span class=\"hljs-literal\">True</span>
       )</span></code></td></tr></tbody></table>

<h2>Dense and cross layers</h2>

<p>We can increase the depth of each tower by adding dense layers after the concatenated embedding layer. As this will emphasize learning successive layers of feature representations, this can improve the expressive power of our model. </p>

<p>Similarly, we can add deep and cross layers after our embedding layer to better model feature interactions. Cross layers model explicit feature interactions before combining with deep layers that model implicit feature interactions. These parameters often lead to better performance, but can significantly increase the computational complexity of the model. We recommend evaluating different deep and cross layer implementations (e.g., parallel vs stacked). See the TFRS <a href=\"https://www.tensorflow.org/recommenders/examples/dcn\" target=\"_blank\">Deep and Cross Networks</a> guide for more details.</p>

<h2>Feature engineering</h2>

<p>As the factorization-based models offer a pure collaborative filtering approach, the advanced feature processing with NDR architectures allow us to extend this to also incorporate aspects of <a href=\"https://developers.google.com/machine-learning/recommendation/content-based/basics\" target=\"_blank\">content-based filtering</a>. By including additional features describing playlists and tracks, we give NDR models the opportunity to learn semantic concepts about <code>playlist, track</code> pairs. The ability to include label features (i.e., features about candidate tracks) also means our trained candidate tower can compute an embedding vector for candidate tracks not observed during training (i.e., cold-start). Conceptually, we can think of such a new candidate track embedding compiling all the content-based and collaborative filtering information learned from candidate tracks with the same or similar feature values.</p>

<p>With this flexibility to add multi-modal features, we just need to process them to produce embedding vectors with the same dimensions so they can be concatenated and fed to subsequent deep and cross layers. This means if we use pre-trained embeddings as an input feature, we would pass these through to the concatenation layer (see Figure 8).</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Illustration of feature processing from input to concatenated output.\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVmlvZLD8Z3qomGp6PtbDkLVuhVuj-qXJJGiT-Qwo7zXz1-g-kJdcUD0_O-PeTC6MwCDDgZfqrxgBtkzZ9BZwBMqiO3qLzGoFCPXVxoMVkyg2phPmbdpG_XNl6YiAYjXizaOexxkgasIRLkrwZqELfLsL7IC_9tVAuDFPw0Dh3mIJePWgxxX8NzDL0/s1600/figure-08_tNj3ZJ9.max-1400x1400.jpg\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 8: Illustration of feature processing from input to concatenated output. Text features are generated via n-grams. Integer indexes of n-grams are passed to an embedding layer. Hashing produces unique integers up to 1,000,000; values passed to an embedding layer. If using pre-trained embeddings, these are passed through the tower without transformation and concatenated with the other embedding representations.</i></td></tr></tbody></table></div>

<strong>Hashing vs StringLookup() layers</strong>

<p>Hashing is generally recommended when fast performance is needed and is preferred over string lookups because it skips the need for a lookup table. Setting the proper bin size for the hashing layer is critical. When there are more unique values than hashing bins, values start getting placed into the same bins, and this can negatively impact our recommendations. This is commonly referred to as a hashing collision, and can be avoided when building the model by allocating enough bins for the unique values. See <a href=\"https://www.tensorflow.org/recommenders/examples/featurization#turning_categorical_features_into_embeddings\" target=\"_blank\">turning categorical features into embeddings</a> for more details. </p>

<div><strong>TextVectorization() layers</strong></div>

<p>The key to text features is to understand if creating additional NLP features with the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\">TextVectorization</a> layer is helpful. If additional context derived from the text feature is minimal, it may not be worth the cost to model training. This layer needs to be adapted from the source dataset, meaning the layer requires a scan of the training data to create lookup dictionaries for the top N n-grams (set by <code>max_tokens</code>). </p>


<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Decision tree to guide feature engineering strategy\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhN6GYCkrSvtDI5cUl-bCWe4ibcJAQnvXsYWHsNeN_GEJ4O7XaNNsN2uCADDN5vvz-Dn_8ofE0CSBMZ-xsdHAG1U5si5cQZjRi02fgqQrcq6O0OQD15ov7BAoO64IQx5AkxXuQdejbr1075mwxwjsRzmuNXLKCW8hMEcLKyAuPuVjC_Qg1mwJCU5pci/s1600/figure-09.max-800x800.jpg\" style=\"width: 80%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 9: Decision tree to guide feature engineering strategy.</i></td></tr></tbody></table></div>

<h2>Efficient retrieval with Matching Engine</h2>

<p>So far we’ve discussed how to map queries and candidates to the shared embedding space. Now let’s discuss how to best use this shared embedding space for efficient serving. </p>

<p>Recall at serving time, we will use the trained query tower to compute the embeddings for a query (playlist) and use this embedding vector in a nearest neighbor search for the most similar candidate (track) embeddings. And, because the candidate dataset can grow to millions or billions of vectors, this nearest neighbor search often becomes a computational bottleneck for low-latency inference. </p>

<p>Many state-of-the-art techniques address the computational bottleneck by compressing the candidate vectors such that ANN calculations can be performed in a fraction of the time needed for an exhaustive search. The novel compression algorithm proposed by Google Research modifies these techniques to also optimize for the nearest neighbor search accuracy. The details of their proposed technique are described <a href=\"https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html\" target=\"_blank\">here</a>, but fundamentally their approach seeks to compress the candidate vectors such that the original distances between vectors are preserved. Compared to previous solutions, this results in a more accurate relative ranking of a vector and its nearest neighbors, i.e., it minimizes distorting the vector similarities our model learned from the training data.</p>

<h4>Fully managed vector database and ANN service </h4>

<p>Matching Engine is a managed solution utilizing these techniques for efficient vector similarity search. It offers customers a highly scalable vector database and ANN service while alleviating the operational overhead of developing and maintaining similar solutions, such as the open sourced <a href=\"https://github.com/google-research/google-research/tree/master/scann\" target=\"_blank\">ScaNN</a> library. It includes several capabilities that simplify production deployments, including:</p>
<ul><blockquote>
<li>Large-scale: supports large embedding datasets with up to 1 billion embedding vectors</li>
<li>Incremental updates: depending on the number of vectors, complete index rebuilds can take hours. With incremental updates, customers can make small changes without building a new index (see <a href=\"https://cloud.google.com/vertex-ai/docs/matching-engine/update-rebuild-index\" target=\"_blank\">Update and rebuild an active index</a> for more details)</li>
<li>Dynamic rebuilds: when an index grows beyond its original configuration, Matching Engine periodically re-organizes the index and serving structure to ensure optimal performance</li>
<li>Autoscaling: underlying infrastructure is autoscaled to ensure consistent performance at scale</li>
<li>Filtering and diversity: ability to include multiple restrict and crowding tags per vector. At query inference time, use boolean predicates to filter and diversify retrieved candidates (see <a href=\"https://cloud.google.com/vertex-ai/docs/matching-engine/filtering\" target=\"_blank\">Filter vector matches</a> for more details)</li>
  </blockquote></ul>

<p>When creating an ANN index, Matching Engine uses the <a href=\"https://arxiv.org/abs/1908.10396\" target=\"_blank\">Tree-AH</a> strategy to build a distributed implementation of our candidate index. It combines two algorithms:</p>
<ul><blockquote>
<li>Distributed search tree for hierarchically organizing the embedding space. Each level of this tree is a clustering of the nodes at the next level down, where the final leaf-level is a clustering of our candidate embedding vectors</li>
<li>Asymmetric hashing (AH) for fast dot product approximation algorithm used to score similarity between a query vector and the search tree nodes</li>
  </blockquote></ul>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Illustration showing the partitioned candidate vector dataset.\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhgSVYoP9wxnLqHpIwdRjmbDK_C8LnnbyZ0b98Vrm1tSxtzLjYeyR6-V-QOF7VKj8OeTgimX79rkPu-d2OjvZ-rKu8oWTD1G_zFjLd5QFctJIQ19i2aHfy_AiXJj6FmIovekEOxeEZc-1pjv73KKUK9U3TYLNjXYAcDQemu5OUg1EEEY0QJlGBjcT1/s1600/figure-10.max-1000x1000.jpg\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 10: conceptual representation of the partitioned candidate vector dataset. During query inference, all partition centroids are scored. In the centroids most similar to the query vector, all candidate vectors are scored. The scored candidate vectors are aggregated and re-scored, returning the top N candidate vectors.</i></td></tr></tbody></table></div>

<p>This strategy shards our embedding vectors into partitions, where each partition is represented by the centroid of the vectors it contains. The aggregate of these partition centroids form a smaller dataset summarizing the larger, distributed vector dataset. At inference time, Matching Engine scores all the partitioned centroids, then scores the vectors within the partitions whose centroids are most similar to the query vector.</p>

<h2>Conclusion</h2>

<p>In this blog we took a deep dive into understanding critical components of a candidate retrieval workflow using <a href=\"https://www.tensorflow.org/recommenders\" target=\"_blank\">TensorFlow Recommenders</a> and <a href=\"https://cloud.google.com/vertex-ai/docs/matching-engine/overview\" target=\"_blank\">Vertex AI Matching Engine</a>. We took a closer look at the foundational concepts of two-tower architectures, explored the semantics of query and candidate entities, and discussed how things like the structure of training examples can impact the success of candidate retrieval. </p> 

<p>In a subsequent post we will demonstrate how to use Vertex AI and other Google Cloud services to implement these techniques at scale. We’ll show how to leverage BigQuery and Dataflow to structure training examples and convert them to <a href=\"https://www.tensorflow.org/tutorials/load_data/tfrecord\" target=\"_blank\">TFRecords</a> for model training. We’ll outline how to structure a Python application for training two-tower models with the Vertex AI Training service. And we’ll detail the steps for operationalizing the trained towers.</p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Tue, 02 May 2023 17:00:00 +0000"
2123,Serving With TF and GKE: Stable Diffusion,https://blog.tensorflow.org/2023/04/serving-with-tf-and-gke-stable-diffusion.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEguJhniX2FvspHjYxJweJYNTFf1DAveLJuABYaoET7jnheNVQpnXN_twr3CuvTjMUZprSwoqF9QNipAjD985qwRfNpIg1HkPU2iUUfGeGFRkKTJOxpv9l1BWGd11bsrr6spltyp93eiW6XxGtl3foLcZ6zplnnGL617JyimdhzVxxwAhoJuDKtqpsV8/s1600/TF%20Stable%20Diffusion%20%282%29.png\" style=\"display: none;\" />

<p><em>Posted by <a href=\"https://github.com/deep-diver\" target=\"_blank\">Chansung Park</a> and <a href=\"https://github.com/sayakpaul\" target=\"_blank\">Sayak Paul</a> (ML and Cloud GDEs)</em></p><p>

</p><a name=\"more\"></a><p></p>

<p>Generative AI models like <a href=\"https://stability.ai/blog/stable-diffusion-public-release\" target=\"_blank\">Stable Diffusion</a><sup><a href=\"https://blog.tensorflow.org/feeds/posts/default?alt=rss#fn-1-ref\">1</a></sup> that lets anyone generate high-quality images from natural language text prompts enable different use cases across different industries. These types of models allow people to generate these images not only from images but also condition them with other inputs such as segmentation maps, other images, depth maps, etc. In many ways, an end Stable Diffusion system (such as <a href=\"https://huggingface.co/spaces/stabilityai/stable-diffusion\" target=\"_blank\">this</a>) is often very complete. One gives a free-form text prompt to start the generation process, and in the end, an image (or any data in the continuous modality) gets generated.</p>

<p>In this post, we discuss how TensorFlow Serving (TF Serving) and Google Kubernetes Engine (GKE) can serve such a system with online deployment. Stable Diffusion is just one example of many such systems that TF and GKE can serve with online deployment. We start by breaking down Stable Diffusion into main components and how they influence the subsequent consideration for deployment. Then we dive deep into the deployment-specific bits such as TF Serving deployment and k8s cluster configuration. Our code is open-sourced in <a href=\"https://github.com/deep-diver/keras-sd-serving\" target=\"_blank\">this repository</a>. </p> 

<p>Let’s dive in. </p>

<strong><h2>Stable Diffusion in a nutshell</h2></strong>

<p>Stable Diffusion, is comprised of three sub-models:</p>
<ul><blockquote>
<li><a href=\"https://openai.com/blog/clip/\" target=\"_blank\">CLIP’s</a> text tower as the Text Encoder,</li>
<li>Diffusion Model (UNet), and </li>
<li>Decoder of a Variational Autoencoder</li>
  </blockquote></ul>

<p>When generating images from an input text prompt, the prompt is first embedded into a latent space with the text encoder. Then an initial noise is sampled, which is fed to the Diffusion model along with the text embeddings. This noise is then denoised using the Diffusion model in a continuous manner – the so-called “diffusion” process. The output of this step is a denoise latent, and it is fed to the Decoder for final image generation.  Figure 1 provides an overview. </p>

<p>(For a more complete overview of Stable Diffusion, refer to <a href=\"https://jalammar.github.io/illustrated-stable-diffusion/\" target=\"_blank\">this post</a>.)</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Flow chart illustrating stable diffusion architecture\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiQ1OsQLLUKIwVt9yPv0pEuJ7PVJ-QWWUV2AvGkBX5gtHksVNkzlFITMLvV7VmrNTsAP6WL80Tn3X6Z3zdkBs4m1mjqUHletya9b0Ef5wgAvNqVndqKONvmwjWkwPQZsu7Q7OlQRpEKkefJ1ZveKCvVAKzDh4xyzVYIhqgw7SCIsjeqf1Bj4OOmlxwJ/s1600/image3.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 1. Stable Diffusion Architecture</i></td></tr></tbody></table></div>

<p>As mentioned above, three sub-models of Stable Diffusion work in a sequential manner. It’s common to run all three models on a single server (which constructs the end Stable Diffusion system) and serve the system as a whole. </p>

<p>However, because each component is a standalone deep learning model, each one could be served independently. This is particularly useful because each component has different hardware requirements. This can also have potentially improved resource utilization. The text encoder can still be run on moderate CPUs, whereas the other two should be run on GPUs, especially the UNet should be served with larger size GPUs (~3.4 GBs in size). </p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Flow chart illustrating decomposing stable diffusion in three parts\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1YVyDjjKWeQYsnna9cWkty6buXuJql2SA3xjLqji0teVuA_748WQl67s3--TJiusFbApgIW6z_FYMV3WoGCia0RijpaprQfD3pMBRMsVcZBPOB8xhfVdNAR9eWGHkYVGMNpU51S5myEFlp_HOFaM7V_xpwBy0pWLUXSR68LUQmtqOmeEi9Wv50vmz/s1600/image4.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 2. Decomposing Stable Diffusion in three parts</i></td></tr></tbody></table></div>

<p>Figure 2 shows the Stable Diffusion serving architecture that packages each component into a separate container with TensorFlow Serving, which runs on the GKE cluster. This separation brings more control when we think about local compute power and the nature of fine-tuning of Stable Diffusion as shown in Figure 3. </p>

<p><em>NOTE: TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments,  which is widely adopted in industry. The benefits of using it include GPU serving support, dynamic batching, model versioning, RESTful and gRPC APIs, to name but a few.</em></p>

<p>In modern personal devices such as desktops and mobile phones, it is common that they are equipped with moderate CPUs and sometimes GPU/NPUs. In this case, we could selectively run the UNet and/or Decoder in the cloud using high capacity GPUs while running the text encoder locally on the user’s device. In general, this approach allows us to flexibly architect the Stable Diffusion system in a way to maximize the resource utilization.</p>


<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Flow chart illustrating flexible serving structure of stable diffusion\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEitUhiVzFClwCI5lUlN6wcIi3d-OkA8mogPW6B5WTSjuq535ELkDR3ziPZHySFk9tNq05Ps_kn7l3gFFP1LL8GAqg4t8_M9fP7ht4SXhX8smPO0k3PTq2DJ3EvJ6wSMHP912xM642eIfr0oiH_FXMBeV5MW8hHHOLrmpcVLVyUKlepKJqMaZz_mmTAE/s1600/image2.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 3. Flexible serving structure of Stable Diffusion</i></td></tr></tbody></table></div>

<p>One more scenario to consider is fine-tuned Stable Diffusion. Many variations such as DreamBooth, Textual Inversion, or style transfer have shown that modifying only one or two components (usually Text Encoder and UNet) can generate images with new concepts or different styles. In this case, we could selectively deploy more of certain fine-tuned models on separate instances or replace existing models without touching other parts.</p>

<strong><h2>Wrapping Stable Diffusion in SavedModels</h2></strong>

<p>In order to serve a TensorFlow/Keras model with TF Serving, it should be saved in the  <a href=\"https://www.tensorflow.org/guide/saved_model\" target=\"_blank\">SavedModel format</a>. After that, the model can be served by TF Serving, a high-performance serving system for machine learning models, specially designed for production environments. The potentially non-trivial parts of making a <code>SavedModel</code> could be divided into three parts: </p>
<ol><blockquote>
<li>defining an appropriate input signature specification of the underlying model, </li>
<li>performing computations with the underlying model so that everything can be compiled in native TensorFlow, and </li>
<li>including most of the pre and post-processing operations within the <code>SavedModel</code> graph itself to reduce training/serving skew (this is optional, but highly recommended). </li>
</blockquote></ol>
  
<p>To make the <a href=\"https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/stable_diffusion.py\" target=\"_blank\">Stable Diffusion class</a> shipped in KerasCV compatible with TF Serving, we need to first isolate the sub-networks (as mentioned above) of the class. Recall that we have got three sub-networks here: text encoder, diffusion model, and a decoder. We then have to serialize these networks as <code>SavedModel</code>s. </p>

<p>A diffusion system also involves iterative sampling where a noise vector is gradually turned into an image. KerasCV’s Stable Diffusion class <a href=\"https://github.com/keras-team/keras-cv/blob/421acf209a0ef15808c8c359edb324fd120aca07/keras_cv/models/stable_diffusion/stable_diffusion.py#L206\" target=\"_blank\">implements</a> the sampling process with non-TensorFlow operations.  So, we need to eliminate those operations and ensure that it’s implemented in pure TensorFlow so that there is end-to-end compatibility. This was the single most challenging aspect for us in the whole project. </p>

<p>Since the serialization of the text encoder and the decoder is straightforward, we’ll skip that in this post and instead, focus on the serialization of the diffusion model, including the sampling process. You can find an end-to-end notebook <a href=\"https://colab.research.google.com/github/deep-diver/keras-sd-serving/blob/main/notebooks/tfs_saved_models.ipynb\" target=\"_blank\">here</a>. </p>

<strong><h4>Diffusion Model and Iterative Sampling </h4></strong>

<p>We start by defining an input signature dictionary for the <code>SavedModel</code> to be serialized. In this case, the inputs consist:</p>
<ul><blockquote>
<li><code>context</code>, that denotes embeddings of the input text prompt extracted with the text encoder  </li>
<li><code>unconditional_context</code>, that denotes the embeddings of a so-called “null prompt” (see <a href=\"https://arxiv.org/abs/2207.12598\" target=\"_blank\">classifier-free guidance</a>) </li>
<li><code>num_steps</code>, that denotes the number of sampling steps for the reverse diffusion process </li>
<li><code>batch_size</code>, that denotes the number of images to be returned</li></blockquote></ul><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">from keras_cv.models.stable_diffusion.constants import ALPHAS_CUMPROD_TF
import tensorflow as tf

IMG_HEIGHT = 512
IMG_WIDTH = 512
MAX_PROMPT_LENGTH = 77
ALPHAS_CUMPROD_TF = tf.constant(ALPHAS_CUMPROD_TF)
UNCONDITIONAL_GUIDANCE_SCALE = 7.5
HIDDEN_DIM = 768
SEED = None


signature_dict = {
    <span class=\"hljs-string\">\"context\"</span>: tf.TensorSpec(shape=[None, MAX_PROMPT_LENGTH, HIDDEN_DIM], dtype=tf.float32, name=<span class=\"hljs-string\">\"context\"</span>),
    <span class=\"hljs-string\">\"unconditional_context\"</span>: tf.TensorSpec(
        shape=[None, MAX_PROMPT_LENGTH, HIDDEN_DIM], dtype=tf.float32, name=<span class=\"hljs-string\">\"unconditional_context\"</span>
    ),
    <span class=\"hljs-string\">\"num_steps\"</span>: tf.TensorSpec(shape=[], dtype=tf.int32, name=<span class=\"hljs-string\">\"num_steps\"</span>),
    <span class=\"hljs-string\">\"batch_size\"</span>: tf.TensorSpec(shape=[], dtype=tf.int32, name=<span class=\"hljs-string\">\"batch_size\"</span>),
}</span></code></td></tr></tbody></table>

<p>Next up, we implement the iterative reverse diffusion process that involves the pre-trained diffusion model. <code>diffusion_model_exporter()</code> takes this model as an argument. <code>serving_fn()</code> is the function we use for exporting the final <code>SavedModel</code>. Most of this code is taken from the original KerasCV implementation <a href=\"https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/stable_diffusion.py\" target=\"_blank\">here</a>, except it has got all the operations implemented in native TensorFlow. </p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">def diffusion_model_exporter(model: tf.keras.Model):
    @tf.function
    def <span class=\"hljs-built_in\">get_timestep_embedding</span>(timestep, batch_size, dim=<span class=\"hljs-number\">320</span>, max_period=<span class=\"hljs-number\">10000</span>):
        ...
    @tf.<span class=\"hljs-built_in\">function</span>(input_signature=[signature_dict])
    def <span class=\"hljs-built_in\">serving_fn</span>(inputs):
        img_height = tf.<span class=\"hljs-built_in\">cast</span>(tf.math.<span class=\"hljs-built_in\">round</span>(IMG_HEIGHT / <span class=\"hljs-number\">128</span>) * <span class=\"hljs-number\">128</span>, tf.int32)
        img_width = tf.<span class=\"hljs-built_in\">cast</span>(tf.math.<span class=\"hljs-built_in\">round</span>(IMG_WIDTH / <span class=\"hljs-number\">128</span>) * <span class=\"hljs-number\">128</span>, tf.int32)

        batch_size = inputs[<span class=\"hljs-string\">\"batch_size\"</span>]
        num_steps = inputs[<span class=\"hljs-string\">\"num_steps\"</span>]

        context = inputs[<span class=\"hljs-string\">\"context\"</span>]
        unconditional_context = inputs[<span class=\"hljs-string\">\"unconditional_context\"</span>]

        latent = tf.random.<span class=\"hljs-built_in\">normal</span>((batch_size, img_height // <span class=\"hljs-number\">8</span>, img_width // <span class=\"hljs-number\">8</span>, <span class=\"hljs-number\">4</span>))

        timesteps = tf.<span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1000</span>, <span class=\"hljs-number\">1000</span> // num_steps)
        alphas = tf.<span class=\"hljs-built_in\">map_fn</span>(lambda t: ALPHAS_CUMPROD_TF[t], timesteps, dtype=tf.float32)
        alphas_prev = tf.<span class=\"hljs-built_in\">concat</span>([[<span class=\"hljs-number\">1.0</span>], alphas[:-<span class=\"hljs-number\">1</span>]], <span class=\"hljs-number\">0</span>)

        index = num_steps - <span class=\"hljs-number\">1</span>
        latent_prev = None
        for timestep in timesteps[::-<span class=\"hljs-number\">1</span>]:
            latent_prev = latent
            t_emb = <span class=\"hljs-built_in\">get_timestep_embedding</span>(timestep, batch_size)
            unconditional_latent = <span class=\"hljs-built_in\">model</span>(
                [latent, t_emb, unconditional_context], training=False
            )
            latent = <span class=\"hljs-built_in\">model</span>([latent, t_emb, context], training=False)
            latent = unconditional_latent + UNCONDITIONAL_GUIDANCE_SCALE * (
                latent - unconditional_latent
            )
            a_t, a_prev = alphas[index], alphas_prev[index]
            pred_x0 = (latent_prev - tf.math.<span class=\"hljs-built_in\">sqrt</span>(<span class=\"hljs-number\">1</span> - a_t) * latent) / tf.math.<span class=\"hljs-built_in\">sqrt</span>(a_t)
            latent = (
                latent * tf.math.<span class=\"hljs-built_in\">sqrt</span>(<span class=\"hljs-number\">1.0</span> - a_prev) + tf.math.<span class=\"hljs-built_in\">sqrt</span>(a_prev) * pred_x0
            )
            index = index - <span class=\"hljs-number\">1</span>

        return {\"latent\": latent}

    return serving_fn</span></code></td></tr></tbody></table>

<p>Then, we can serialize the diffusion model as a <code>SavedModel</code> like so:</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">tf.saved_model.save(
    diffusion_model,
    path_to_serialize_the_model,
    signatures={<span class=\"hljs-string\">\"serving_default\"</span>: diffusion_model_exporter(diffusion_model)},
)</span></code></td></tr></tbody></table>

<p>Here, <code>diffusion_model</code> is the pre-trained diffusion model initialized like so:</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">from keras_cv.models.stable_diffusion.diffusion_model <span class=\"hljs-keyword\">import</span> <span class=\"hljs-type\">DiffusionModel</span>
<span class=\"hljs-variable\">diffusion_model</span> <span class=\"hljs-operator\">=</span> DiffusionModel(IMG_HEIGHT, IMG_WIDTH, MAX_PROMPT_LENGTH)</span></code></td></tr></tbody></table>

<strong><h2>Deploy Stable Diffusion to GKE</h2></strong>

<p>Once you have successfully created TensorFlow <code>SavedModel</code>s, it is quite straightforward to deploy them with TensorFlow Serving to a GKE cluster in the following steps.</p>
<ol><blockquote>
<li>Write Dockerfiles which are based on the TensorFlow Serving base image </li>
<li>Create a GKE cluster with accelerators attached</li>
<li>Apply NVIDIA GPU driver installation daemon to install the driver on each node</li>
<li>Write deployment manifests with GPU allocation </li>
<li>Write service manifests to expose the deployments</li>
<li>Apply all the manifests</li>
  </blockquote></ol>

<p>The easiest way to wrap a <code>SavedModel</code> in TensorFlow Serving is to leverage the pre-built <a href=\"https://hub.docker.com/r/tensorflow/serving\" target=\"_blank\">TensorFlow Serving Docker images</a>. Depending on the configuration of the machine that you’re deploying to, you should choose either <code>tensorflow/serving:latest</code> or <code>tensorflow/serving:latest-gpu</code>. Because all the steps besides GPU-specific configuration are the same, we will explain this section with an example of the Diffusion Model part only.</p>

<p>By default, TensorFlow Serving recognizes embedded models under <code><a href=\"https://www.tensorflow.org/tfx/serving/docker#serving_with_docker\" target=\"_blank\">/models</a></code>, so the entire <code>SavedModel</code> folder tree should be placed inside <code>/models/{model_name}/{version_num}</code>. A  single TensorFlow Serving instance can serve multiple versions of multiple models, so that is why we need such a <code>{model_name}/{version_num}</code> folder structure. A <code>SavedModel</code> can be exposed as an API by setting a special environment variable <code><a href=\"https://www.tensorflow.org/tfx/serving/docker#serving_with_docker\" target=\"_blank\">MODEL_NAME</a></code>, which is used for TensorFlow Serving to look for which model to serve.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">FROM tensorflow/serving:latest-gpu
...
RUN <span class=\"hljs-built_in\">mkdir</span> -p /models/text-encoder/1
RUN <span class=\"hljs-built_in\">cp</span> -r tfs-diffusion-model/* /models/diffusion-model/1/
ENV MODEL_NAME=diffusion-model
...</span></code></td></tr></tbody></table>

<p>Next step is to create a GKE cluster. You can do this by using either Google Cloud Console or <code>gcloud</code> container CLI as below. If you want accelerators available on each node, you can specify how many of which GPUs to be attached with <code>--accelerator=type={ACCEL_TYPE}, count={ACCEL_NUM}</code> option.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-variable\">$</span> gcloud container clusters create <span class=\"hljs-punctuation\">{</span>CLUSTER_NAME<span class=\"hljs-punctuation\">}</span> \\
  --machine-<span class=\"hljs-keyword\">type</span><span class=\"hljs-punctuation\">=</span><span class=\"hljs-punctuation\">{</span>MACHINE_TYPE<span class=\"hljs-punctuation\">}</span> \\                     <span class=\"hljs-comment\"># n1-standard-4</span>
  --accelerator<span class=\"hljs-punctuation\">=</span><span class=\"hljs-keyword\">type</span><span class=\"hljs-punctuation\">=</span><span class=\"hljs-punctuation\">{</span>GPU_TYPE<span class=\"hljs-punctuation\">}</span>,count<span class=\"hljs-punctuation\">=</span><span class=\"hljs-punctuation\">{</span>GPU_NUM<span class=\"hljs-punctuation\">}</span> \\     <span class=\"hljs-comment\"># nvidia-tesla-v100, 1</span>
  <span class=\"hljs-punctuation\">...</span></span></code></td></tr></tbody></table>

<p>Once the cluster is successfully created, and if the nodes in the cluster have accelerators attached, an appropriate driver for them should be installed correctly. This is done by running a special DaemonSet, which tries to install the driver on each node. If the driver has not been successfully installed, and if you try to apply Deployment manifests requiring accelerators, the status of the pod remains as Pending. </p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-meta prompt_\">$ </span><span class=\"language-bash\">DRIVER_URL = https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml</span>
<span class=\"hljs-meta prompt_\">
$ </span><span class=\"language-bash\">kubectl apply -f <span class=\"hljs-variable\">$DRIVER_URL</span></span></span></code></td></tr></tbody></table>

<p>Make sure all the pods are up and running with <code>kubectl get pods -A</code> command. Then, we are ready to apply prepared Deployment manifests. Below is an example of the Deployment manifest for the Diffusion Model. The only consideration you need to take is to specify which resource the pods of the Deployment should consume. Because the Diffusion Model needs to be run on accelerators, <code>resources:limits:nvidia.com/gpu: {ACCEL_NUM}</code> should be set. </p>


<p>Furthermore, if you want to expose gRPC and RestAPI at the same time, you need to set containerPort for both. TensorFlow Serving exposes the two endpoints via 8500 and 8501, respectively, by default, so both ports should be specified. </p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-attr\">apiVersion:</span> <span class=\"hljs-string\">apps/v1</span>
<span class=\"hljs-attr\">kind:</span> <span class=\"hljs-string\">Deployment</span>
<span class=\"hljs-string\">...</span>
    <span class=\"hljs-attr\">spec:</span>
      <span class=\"hljs-attr\">containers:</span>
      <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">image:</span> {<span class=\"hljs-string\">IMAGE_URI</span>}
<span class=\"hljs-string\">...</span>
        <span class=\"hljs-attr\">args:</span> [<span class=\"hljs-string\">\"--rest_api_timeout_in_ms=1200000\"</span>]
        <span class=\"hljs-attr\">ports:</span>
        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">containerPort:</span> <span class=\"hljs-number\">8500</span>
          <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">grpc</span>
        <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">containerPort:</span> <span class=\"hljs-number\">8501</span>
          <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">restapi</span>
        <span class=\"hljs-attr\">resources:</span>
          <span class=\"hljs-attr\">limits:</span>
            <span class=\"hljs-attr\">nvidia.com/gpu:</span> <span class=\"hljs-number\">1</span></span></code></td></tr></tbody></table>

<p>One more thing to note is that <code>--rest_api_timeout_in_ms</code> flag is set in args with a huge number. It takes a long time for heavy models to run inference. Since the flag is set to 5,000ms by default which is 5 seconds, sometimes timeout occurs before the inference is done. You can experimentally find out the right number, but we simply set this with a high enough number to demonstrate this project smoothly. </p>


<p>The final step is to apply prepared manifest files to the provisioned GKE cluster. This could be easily done with the <code>kubectl apply -f</code> command. Also, you could apply Service and Ingress depending on your needs. Because we simply used vanilla LoadBalancer type of Service for demonstration purposes, it is not listed in this blog. You can find all the Dockerfiles, and the Deployment and Service manifests in the accompanying <a href=\"https://github.com/deep-diver/keras-sd-serving/tree/main/tfserving\" target=\"_blank\">GitHub repository</a>.</p>

<strong><h2>Let’s generate images!</h2></strong>

<p>Once all the TensorFlow Serving instances are deployed, we could generate images by calling their endpoints. We will show how to do it through RestAPI, but you could do the same with the gRPC channel as well. The image generation process could be done in the following steps:</p>
<ol><blockquote>
<li>Prepare tokens for the prompt of your choice</li>
<li>Send the tokens to the Text Encoder endpoint</li>
<li>Send context and unconditional context obtained from the Text Encoder to the Diffusion Model endpoint</li>
<li>Send latent obtained from the Diffusion Model to the Decoder endpoint</li>
<li>Plot the generated images</li>
  </blockquote></ol>

<p>Since it is non-trivial to embed a tokenizer into the Text Encoder itself, we need to prepare the tokens for the prompt of your choice. KerasCV library provides <code>SimpleTokenizer</code> in the <code>keras_cv.models.stable_diffusion.clip_tokenizer</code> module, so you could simply pass the prompt to it. Since the Diffusion Model is designed to accept <code>77</code> tokens, the tokens are padded with <code>MAX_PROMPT_LENGTH</code> up to <code>77</code> long.</p>
  
<p><em>NOTE: Since KerasCV comes with lots of modules that we don’t need for tokenization, it is not recommended to import the entire library. Instead, you could simply copy the codes for the SimpleTokenizer in your environment. Due to incompatibility issues, the current tokenizer cannot be shipped as a part of the Text Encoder <code>SavedModel</code>. </em></p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">from keras_cv.models.stable_diffusion.clip_tokenizer import SimpleTokenizer

MAX_PROMPT_LENGTH = 77
PADDING_TOKEN = 49407

tokenizer = SimpleTokenizer()

prompt = <span class=\"hljs-string\">\"photograph of an astronaut riding a horse in a green desert\"</span>
tokens = tokenizer.encode(prompt)
tokens = tokens + [PADDING_TOKEN] * (MAX_PROMPT_LENGTH - len(tokens))</span></code></td></tr></tbody></table>

<p>Once the tokens are prepared, we could simply pass it to the Diffusion Model’s endpoint. The headers and the way to call the all endpoints are identical as below, so we will omit it in the following steps. Just keep in mind you set the <code>ADDRESS</code> and the <code>MODEL_NAME</code> correctly, which is identical to the one we set in each Dockerfile. </p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">import</span> <span class=\"hljs-type\">requests</span>

<span class=\"hljs-variable\">ADDRESS</span> <span class=\"hljs-operator\">=</span> <span class=\"hljs-type\">ENDPOINT_IP_ADDRESS</span>

<span class=\"hljs-variable\">headers</span> <span class=\"hljs-operator\">=</span> {<span class=\"hljs-string\">\"content-type\"</span>: <span class=\"hljs-string\">\"application/json\"</span>}
payload = <span class=\"hljs-type\">ENDPOINT_SPECIFIC</span>

<span class=\"hljs-variable\">response</span> <span class=\"hljs-operator\">=</span> requests.post(
    f<span class=\"hljs-string\">\"http://{ADDRESS}:8501/v1/models/{MODEL_NAME}:predict\"</span>, 
    data=payload, headers=headers
)</span></code></td></tr></tbody></table>

<p>As you see, each payload is dependent on the upstream tasks. For instance, we pass tokens to the Text Encoder’s endpoint, context and <code>unconditional_context</code> retrieved from the Text Encoder to the Diffusion Model’s endpoint, and latent retrieved from the Diffusion Model to Decoder’s endpoint. The <code>signature_name</code> should be the same as when we created <code>SavedModel</code> with the signatures argument.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">import</span> json


BATCH_SIZE = <span class=\"hljs-number\">4</span>

payload_to_text_encoder = json.dumps(
    {
        <span class=\"hljs-string\">\"signature_name\"</span>: <span class=\"hljs-string\">\"serving_default\"</span>, 
        <span class=\"hljs-string\">\"inputs\"</span>: {
            <span class=\"hljs-string\">\"tokens\"</span>: tokens,
            <span class=\"hljs-string\">\"batch_size\"</span>: BATCH_SIZE
        }
})

<span class=\"hljs-comment\"># json_response is from the text_encoder's response</span>
<span class=\"hljs-comment\"># json_response = json.loads(response.text)</span>
payload_to_diffusion_model = json.dumps(
    {
        <span class=\"hljs-string\">\"signature_name\"</span>: <span class=\"hljs-string\">\"serving_default\"</span>, 
        <span class=\"hljs-string\">\"inputs\"</span>: {
            <span class=\"hljs-string\">\"batch_size\"</span>: BATCH_SIZE,
            <span class=\"hljs-string\">\"context\"</span>: json_response[<span class=\"hljs-string\">'outputs'</span>][<span class=\"hljs-string\">'context'</span>],
            <span class=\"hljs-string\">\"num_steps\"</span>: num_steps,
            <span class=\"hljs-string\">\"unconditional_context\"</span>: json_response[<span class=\"hljs-string\">'outputs'</span>][<span class=\"hljs-string\">'unconditional_context'</span>]
        }
})

<span class=\"hljs-comment\"># json_response is from the diffusion_model's response</span>
<span class=\"hljs-comment\"># json_response = json.loads(response.text)</span>
payload_to_decoder = json.dumps(
    {
        <span class=\"hljs-string\">\"signature_name\"</span>: <span class=\"hljs-string\">\"serving_default\"</span>, 
        <span class=\"hljs-string\">\"inputs\"</span>: {
            <span class=\"hljs-string\">\"latent\"</span>: json_response[<span class=\"hljs-string\">'outputs'</span>],
        }
})</span></code></td></tr></tbody></table>

<p>The final response from the Decoder’s endpoint contains a full of pixel values in a list, so we need to convert those into a format that the environment of your choice could understand as images. For demonstration purposes, we used the <code>tf.convert_to_tensor()</code> utility function that turns the Python list into TensorFlow’s Tensor. However, you could plot the images in different languages, too, with your most familiar methods.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">import matplotlib<span class=\"hljs-selector-class\">.pyplot</span> as plt

def <span class=\"hljs-built_in\">plot_images</span>(images):
    plt.<span class=\"hljs-built_in\">figure</span>(figsize=(<span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">20</span>))
    for i in <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-built_in\">len</span>(images)):
        ax = plt.<span class=\"hljs-built_in\">subplot</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-built_in\">len</span>(images), i + <span class=\"hljs-number\">1</span>)
        plt.<span class=\"hljs-built_in\">imshow</span>(images[i])
        plt.<span class=\"hljs-built_in\">axis</span>(<span class=\"hljs-string\">\"off\"</span>)

<span class=\"hljs-built_in\">plot_images</span>(
    tf.<span class=\"hljs-built_in\">convert_to_tensor</span>(json_response[<span class=\"hljs-string\">'outputs'</span>]).<span class=\"hljs-built_in\">numpy</span>()
)</span></code></td></tr></tbody></table>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"Four AI generated images of an astronaut riding a horse\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8I4T52Q1T_QQBucX3djNEXFlkSWYXp1nvoQDzH61M3eDD54Z9AlUvxNTsnJheY0CSxX_W6zddB0vHBiXzsO4eF2RbD2mcjRb2fZanp3hR2lTXUMuZ_JB5N7pkJV1bCWk2fGNHvbepj0j4Q05kDj8gud3B1cHWL3oPBFQIkOnv9uPDGW-D_-pZ70zL/s1600/image1.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i>Figure 4. Generated images with three TensorFlow Serving endpoints</i></td></tr></tbody></table></div>

<strong><h2>Note on XLA compilation</h2></strong>

<p>We can obtain a speed-up of 17 - 25% by incorporating compiling the <code>SavedModel</code>s to be <a href=\"https://www.tensorflow.org/xla\" target=\"_blank\">XLA compatible</a>. Note that the individual sub-networks of the Stable Diffusion class are fully XLA compatible. But in our case, the <code>SavedModel</code>s also contain important operations that are in native TensorFlow, such as the reverse diffusion process. </p>

<p>For deployment purposes, this speed-up could be impactful. To know more, check out the following repository: <a href=\"https://github.com/sayakpaul/xla-benchmark-sd\" target=\"_blank\">https://github.com/sayakpaul/xla-benchmark-sd</a>. </p>
  
<strong><h2>Conclusion</h2></strong>

<p>In this blog post, we explored what Stable Diffusion is, how it could be decomposed into the Text Encoder, Diffusion Model, and Decoder, and why it might be beneficial for better resource utilization. Also, we touched upon the concrete demonstration about the deployment of the decomposed Stable Diffusion by creating <code>SavedModel</code>s, containerizing them in TensorFlow Serving, deploying them on the GKE cluster, and running image generations. We used the vanilla Stable Diffusion, but feel free to try out replacing the only Diffusion Model with <a href=\"https://github.com/deep-diver/keras-sd-serving/blob/main/notebooks/inpainting_without_endpoint.ipynb\" target=\"_blank\">in-painting</a> or <a href=\"https://github.com/sayakpaul/stable-diffusion-keras-ft\" target=\"_blank\">pokemon fine-tuned diffusion models</a>.</p>

<strong><h2>References</h2></strong>

<p>CLIP: Connecting text and images, OpenAI, <a href=\"https://openai.com/research/clip\" target=\"_blank\">https://openai.com/research/clip</a>. </p>

<p>The Illustrated Stable Diffusion, Jay Alammar, <a href=\"https://jalammar.github.io/illustrated-stable-diffusion/\" target=\"_blank\">https://jalammar.github.io/illustrated-stable-diffusion/</a>. </p>

<p>Stable Diffusion, Stability AI, <a href=\"https://stability.ai/stable-diffusion\" target=\"_blank\">https://stability.ai/stable-diffusion</a>. </p>

<strong><h2>Acknowledgements</h2></strong>

<p><em>We are grateful to the <a href=\"https://developers.google.com/community/experts\" target=\"_blank\">ML Developer Programs team</a> that provided Google Cloud credits to support our experiments. We thank <a href=\"https://www.linkedin.com/in/robert-crowe\" target=\"_blank\">Robert Crowe</a> for providing us with helpful feedback and guidance. </em></p>

<p><strong>___________</strong></p>
<small><em><sup><a id=\"fn-1-ref\">1</a></sup> Stable Diffusion is not owned or operated by Google. It is made available by Stability AI. Please see their site for more information: <a href=\"https://stability.ai/blog/stable-diffusion-public-release\" target=\"_blank\">https://stability.ai/blog/stable-diffusion-public-release.</a></em></small>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Fri, 28 Apr 2023 17:00:00 +0000"
2124,Get ready for Google I/O,https://blog.tensorflow.org/2023/04/get-ready-for-google-io.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEizjsHSPpRYoieEo3e820PnUGyL1A4AAAFlLNQDFhgPMoyjOP76apEK60exJ4Qu1IXqLfMvMmiEMVzuND8FnoE-8rVpi2_0K4AAERvnMgK2TRcbFYiyVosE2U6QtkiBcMLg3_wIHLvzbW8kCiicgwWkVQjhSb5ZB8gxlcfLa06ann-HKJ9rghUGZxz3/s1600/278944847__42532861__715727.png\" style=\"display: none;\" />

<p><em>Posted by Timothy Jordan, Director, Developer Relations & Open Source</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFVMIlCP4As_DCgFpzemOv1Zs19s40uVLFgOV1n7k3UD7yxlit1lTDPaZLaTyJzf0Wrd9T4lx3KE09cM8bnVGkn0xvvR80KlyMe0Ri2ymrNYOWzmQmYnvegpiZIrPJ9vXH4ZeRvR-zz8-SIDQwz3D5DoVmbkv5pkzKzIr5aJgrRpsKmwF8Ohyi4745/s1600/278944847__42532860__715727.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgFVMIlCP4As_DCgFpzemOv1Zs19s40uVLFgOV1n7k3UD7yxlit1lTDPaZLaTyJzf0Wrd9T4lx3KE09cM8bnVGkn0xvvR80KlyMe0Ri2ymrNYOWzmQmYnvegpiZIrPJ9vXH4ZeRvR-zz8-SIDQwz3D5DoVmbkv5pkzKzIr5aJgrRpsKmwF8Ohyi4745/s1600/278944847__42532860__715727.png\" /></a>
  
</p><a name=\"more\"></a><p></p>

<p>I/O is just a few days away and we couldn’t be more excited to share the latest updates across Google’s developer products, solutions, and technologies. From keynotes to technical sessions and hands-on workshops, these announcements aim to help you build smarter and ship faster. </p>

<p>Here are some helpful tips to maximize your experience online.</p>

<h2>Start building your personal I/O agenda</h2>

<p>Starting now, you can save the Google and developer keynotes to your calendar and explore the <a href=\"https://io.google/2023/program/\" target=\"_blank\">program</a> to preview content. Here are just a few noteworthy examples of what you’ll find this year:</p>
<blockquote>

  <strong>What's new in Android</strong>
<div style=\"text-align: left;\">Get the latest news in Android development: Android 14, form factors, Jetpack + Compose libraries, Android Studio, and performance.</div></blockquote><blockquote>

 <strong>What’s new in Web</strong>
<div style=\"text-align: left;\">Explore new features and APIs that became stable across browsers on the Web Platform this year.</div></blockquote><blockquote>

 <strong>What’s new in Generative AI</strong>
<div style=\"text-align: left;\">Discover a new suite of tools that make it easy for developers to leverage and build on top of Google's  large language models.</div></blockquote><blockquote>

 <strong>What’s new in Google Cloud</strong>
<div style=\"text-align: left;\">Learn how Google Cloud and generative AI will help you develop faster and more efficiently.</div></blockquote>
  
<p>For the best experience, create or connect a <a href=\"https://developers.google.com/profile/u/me\" target=\"_blank\">developer profile</a> and start saving content to My I/O to build your personal agenda. With over 200 sessions and other learning material, there’s a lot to cover, so we hope this will help you get organized. </p>

<p>This year we’ve introduced development focus filters to help you navigate content faster across mobile, web, AI, and cloud technologies. You can also peruse content by topic, type, or experience level so you can find what you’re interested in, faster. </p>

<h2>Connect with the community</h2>

<p>After the keynotes, you can talk to Google experts and other developers online in I/O Adventure chat. Here you can ask questions about new releases and learn best practices from the global developer community. </p>

<p>If you’re craving community now, visit the <a href=\"https://io.google/2023/community/\" target=\"_blank\">Community page</a> to meet people with similar interests in your area or find a watch party to attend.</p>

<p>We hope these updates are useful, and we can’t wait to connect online in May! </p>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Thu, 27 Apr 2023 16:30:00 +0000"
2125,Training a recommendation model with dynamic embeddings,https://blog.tensorflow.org/2023/04/training-recommendation-model-with-dynamic-embeddings.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdc76CVXCePD_2qIelWfi3dw_4P4d8tq8X6er753xrvUUGEDYtAqZiPkU1v_RitRm8bxXevsE54jARVORGnTXuC29DWDPIiN9dBr3nEHHVHbDo40EUJnqDvSu2k2nhwMoNuwQ4EEPiWWXIKc12j59vaSp_e_e7V1XL2iAaVNk-seCsNe5NuxYpY-G-/s1600/social-TensorFlow%20-%20Training%20a%20recommendation%20model%20with%20dynamic%20embeddings.png\" style=\"display: none;\" />

<em>Posted by <a href=\"https://www.linkedin.com/in/thushanganegedara/\" target=\"_blank\">Thushan Ganegedara</a> (<a href=\"https://developers.google.com/community/experts\" target=\"_blank\">GDE</a>), Haidong Rong (Nvidia), Wei Wei (Google)</em>

<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdW4kHBd4n_Tz0ygjDJqqwoKpxgpu5Q9l-OmgfvAbpTo8smw0uBemAHobAN7sYt3U7v5slXFNudpyzY7JF7rxLuDbRB_v2S8UPDZQoP_edPjXl_U3UFMOyIusJK_H_DcYTlXhow7SjDozxoKpWO9z87SfF7on4WTaQgMOWWwtz0BQ-crja1DTHfCge/s1600/header-TensorFlow%20-%20Training%20a%20recommendation%20model%20with%20dynamic%20embeddings.png\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdW4kHBd4n_Tz0ygjDJqqwoKpxgpu5Q9l-OmgfvAbpTo8smw0uBemAHobAN7sYt3U7v5slXFNudpyzY7JF7rxLuDbRB_v2S8UPDZQoP_edPjXl_U3UFMOyIusJK_H_DcYTlXhow7SjDozxoKpWO9z87SfF7on4WTaQgMOWWwtz0BQ-crja1DTHfCge/s1600/header-TensorFlow%20-%20Training%20a%20recommendation%20model%20with%20dynamic%20embeddings.png\" /></a>

<a name=\"more\"></a><p></p>

<p>Modern recommenders heavily leverage embeddings to create vector representations of each user and candidate item. These embedding can then be used to calculate the similarity between users and items, so that users are recommended candidate items that are more interesting and relevant. But when working with data at scale, particularly in an <a href=\"https://en.wikipedia.org/wiki/Online_machine_learning\" target=\"_blank\">online machine learning</a> setting, embedding tables can grow in size dramatically, accumulating millions (and sometimes billions) of items. At this scale, it becomes impossible to store these embedding tables in memory. Furthermore, a large portion of the items might be rarely seen, so it does not make sense to keep dedicated embeddings for such rarely occurring items. A better solution would be to represent those items with one common embedding. This can dramatically reduce the size of the embedding table at a very small fraction of the performance cost. This is the main motivation behind dynamic embedding tables.</p>

<p>TensorFlow's built-in <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\" target=\"_blank\">tf.keras.layers.Embedding</a> layer has a fixed size at creation time, so we need another approach. Fortunately, there is a <a href=\"https://www.tensorflow.org/community/sig_playbook\" target=\"_blank\">TensorFlow SIG</a> project exactly for this purpose: <a href=\"https://github.com/tensorflow/recommenders-addons\" target=\"_blank\">TensorFlow Recommenders Addons</a> (TFRA). You can learn more from its repository, but at a high level TFRA leverages dynamic embedding technology to dynamically change embedding size and achieve better recommendation results than static embeddings. TFRA is fully TF2.0-compatible and works smoothly with the familiar Keras API interfaces, so it can be easily integrated with other TensorFlow products, such as <a href=\"https://www.tensorflow.org/recommenders\" target=\"_blank\">TensorFlow Recommenders</a> (TFRS). </p>

<p>In this tutorial we will build a movie recommender model by leveraging both TFRS and TFRA. We will use the <a href=\"https://grouplens.org/datasets/movielens/\" target=\"_blank\">MovieLens</a> dataset, which contains anonymized data showing ratings given to movies by users. Our primary focus is to show how the dynamic embeddings provided in the <a href=\"https://github.com/tensorflow/recommenders-addons\" target=\"_blank\">TensorFlow Recommenders Addons</a> library can be used to dynamically grow and shrink the size of the embedding tables in the recommendation setting. You can find the full implementation <a href=\"https://github.com/tensorflow/recommenders-addons/blob/master/demo/dynamic_embedding/movielens-1m-keras-notebook/movielens_dynamic_embedding_tutorial.ipynb\" target=\"_blank\">here</a> and a walkthrough <a href=\"https://docs.google.com/presentation/d/1Q8tPMK-zTTXW4nw5vbrZ8bhDKkBx0SQLLpZ61V_wqFo/edit?usp=sharing\" target=\"_blank\">here</a>.
  
    <strong></strong></p><h2><strong>Import Libraries</strong></h2>

<p>We will first import the required libraries.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">import</span> tensorflow <span class=\"hljs-keyword\">as</span> tf
<span class=\"hljs-keyword\">import</span> tensorflow_datasets <span class=\"hljs-keyword\">as</span> tfds

<span class=\"hljs-comment\">#TFRA does some patching on TensorFlow so it MUST be imported after importing TF</span>
<span class=\"hljs-keyword\">import</span> tensorflow_recommenders <span class=\"hljs-keyword\">as</span> tfrs
<span class=\"hljs-keyword\">import</span> tensorflow_recommenders_addons <span class=\"hljs-keyword\">as</span> tfra
<span class=\"hljs-keyword\">import</span> tensorflow_recommenders_addons.dynamic_embedding <span class=\"hljs-keyword\">as</span> de</span></code></td></tr></tbody></table>

<p>Note how we are importing the TFRA library after importing TensorFlow. It is recommended to follow this ordering as the TFRA library will be applying some patches on TensorFlow.</p>

  <strong></strong><h2><strong>Processing the data</strong></h2>

<p>Let’s first build a baseline model with TensorFlow Recommenders. We will follow the pattern of <a href=\"https://www.tensorflow.org/recommenders/examples/basic_retrieval\" target=\"_blank\">this TFRS retrieval tutorial</a> to build a two-tower retrieval model. The user tower will take the user ID as the input, but the item tower will use the tokenized movie title as the input. 

</p><p>To handle the movie titles, we define a helper function that converts the movie titles to lowercase, removes any punctuation in a given movie title, and splits using spaces to generate a list of tokens. Finally we take only the up to <code>max_token_length</code> tokens (from the start) from the movie title. If a movie title has fewer tokens, all the tokens will be taken. This number is chosen based on some analysis and represents the 90th percentile in the title lengths in the dataset.
  
  
</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">max_token_length = <span class=\"hljs-number\">6</span>
pad_token = <span class=\"hljs-string\">\"[PAD]\"</span>
punctuation_regex = <span class=\"hljs-string\">\"[\\!\\\"#\\$%&amp;\\(\\)\\*\\+,-\\.\\/\\:;\\&lt;\\=\\&gt;\\?@\\[\\]\\\\\\^_`\\{\\|\\}~\\\\t\\\\n]\"</span>

<span class=\"hljs-comment\">#First we’ll define a helper function that will process the movie titles for us.</span>

<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">process_text</span>(<span class=\"hljs-params\">x: tf.Tensor, max_token_length: <span class=\"hljs-built_in\">int</span>, punctuation_regex: <span class=\"hljs-built_in\">str</span></span>) -&gt; tf.Tensor:
    
	<span class=\"hljs-keyword\">return</span> tf.strings.split(
    		tf.strings.regex_replace(
        			tf.strings.lower(x[<span class=\"hljs-string\">\"movie_title\"</span>]), punctuation_regex, <span class=\"hljs-string\">\"\"</span>
    		)
	)[:max_token_length]</span></code></td></tr></tbody></table><p></p><p></p>

<p>We also pad the tokenized movie titles to a fixed length and split the dataset using the same random seed so that we get consistent validation results across training epochs. You can find detailed code in the ‘Processing datasets’ section of the <a href=\"https://github.com/tensorflow/recommenders-addons/blob/master/demo/dynamic_embedding/movielens-1m-keras-notebook/movielens_dynamic_embedding_tutorial.ipynb\" target=\"_blank\">notebook</a>.</p>

<strong><h2>Building the two tower model</h2></strong>

<p>Our user tower is pretty much the same as in the <a href=\"https://www.tensorflow.org/recommenders/examples/basic_retrieval\" target=\"_blank\">TFRS retrieval tutorial</a> (except it’s deeper), but for the movie tower there is a <code>GlobalAveragePooling1D</code> layer after the embedding lookup, which averages the embedding of movie title tokens to a single embedding. </p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">get_movie_title_lookup_layer</span>(<span class=\"hljs-params\">dataset: tf.data.Dataset</span>) -&gt; tf.keras.layers.Layer:
	movie_title_lookup_layer = tf.keras.layers.StringLookup(mask_token=pad_token)
	movie_title_lookup_layer.adapt(dataset.<span class=\"hljs-built_in\">map</span>(<span class=\"hljs-keyword\">lambda</span> x: x[<span class=\"hljs-string\">\"movie_title\"</span>]))
	<span class=\"hljs-keyword\">return</span> movie_title_lookup_layer

<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">build_item_model</span>(<span class=\"hljs-params\">movie_title_lookup_layer: tf.keras.layers.StringLookup</span>):
	vocab_size = movie_title_lookup_layer.vocabulary_size()
	<span class=\"hljs-keyword\">return</span> tf.keras.models.Sequential([
    	tf.keras.layers.InputLayer(input_shape=(max_token_length), dtype=tf.string),
    	movie_title_lookup_layer,
    	tf.keras.layers.Embedding(vocab_size, <span class=\"hljs-number\">64</span>),
    	tf.keras.layers.GlobalAveragePooling1D(),
    	tf.keras.layers.Dense(<span class=\"hljs-number\">64</span>, activation=<span class=\"hljs-string\">\"gelu\"</span>),
    	tf.keras.layers.Dense(<span class=\"hljs-number\">32</span>),
    	tf.keras.layers.Lambda(<span class=\"hljs-keyword\">lambda</span> x: tf.math.l2_normalize(x, axis=<span class=\"hljs-number\">1</span>))
	])</span></code></td></tr></tbody></table>

<p>Next we are going to train the model.</p>

<strong><h2>Training the model</h2></strong>

<p>Training the model is simply calling <code>fit()</code> on the model with the required arguments. We will be using our validation dataset <code>validation_ds</code> to measure the performance of our model.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">history = model.fit(datasets.training_datasets.train_ds, epochs=<span class=\"hljs-number\">3</span>, validation_data=datasets.training_datasets.validation_ds)</span></code></td></tr></tbody></table>

<p>At the end, the output looks like below:</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">Epoch 3/3
220/220 [==============================] - 146s 633ms/step  
......
val_factorized_top_k/top_10_categorical_accuracy: 0.0179 - val_factorized_top_k/top_50_categorical_accuracy: 0.0766 - val_factorized_top_k/top_100_categorical_accuracy: 0.1338 - val_loss: 12359.0557 - val_regularization_loss: 0.0000e+00 - val_total_loss: 12359.0557</span></code></td></tr></tbody></table>

<p>We have achieved a top 100 categorical accuracy of 13.38% on the validation dataset. </p>

<strong><h2>Building the model with dynamic embeddings</h2></strong>
<h4 style=\"text-align: left;\">Overview</h4>
<p>We will now learn how we can use the dynamic embedding in the <a href=\"https://github.com/tensorflow/recommenders-addons\" target=\"_blank\">TensorFlow Recommenders Addons</a> (TFRA) library, rather than a static embedding table. As the name suggests, as opposed to creating embeddings for all the items in the vocabulary up front, dynamic embedding would only grow the size of the embedding table on demand. This behavior really shines when dealing with millions and billions of items and users as some companies do. For these companies, it’s not surprising to find static embedding tables that would not fit in memory. Static embedding tables can grow up to hundreds of Gigabytes or even Terabytes, incapacitating even the highest memory instances available in cloud environments.</p>

<p>When you have an embedding table with large cardinality, the accessing weights will be quite sparse. Therefore, a hash-table based data structure is used to hold the weights and required weights for each iteration are retrieved from the underlying table structure. Here, to focus on the core functionality of the library, we will focus on a non-distributed setting. In this case, TFRA will choose cuckoo hashtable by default. But there are <a href=\"https://github.com/tensorflow/recommenders-addons#main-features\" target=\"_blank\">other solutions</a> such as Redis, nvhash available.</p>


<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"A chart showing the various embedding solutions across distruted and non-distributed settings in the TFRA library\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdLeEjNdYb4EiUhh3EvNOf9xpno3w87zdjNHKZjbTORI3Wf5FhNcLj6qTUt0eXKrlTT-ey1DXV7nkFRdy2V1W4EQBXyyuH8jNtbdt-FbyK4BXaHuaAaTasVJVzAa78xdlHajnM9mYPe1WKXo7i_L5e8d8nLNim3iXLoAymx2SMcJkWCyKD3uFbnW2Z/s1600/image2.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>When using the dynamic embedding, we initialize the table with some initial capacity and the table will grow in size on demand as it sees more IDs during model training. For more information about motivation and inner mechanics, please refer to the <a href=\"https://github.com/tensorflow/recommenders-addons/blob/master/rfcs/20200424-sparse-domain-isolation.md\" target=\"_blank\">RFC</a>.</p>

<h4 style=\"text-align: left;\">Types of embedding</h4>
<p>Currently in the TFRA <code>dynamic_embedding</code> module, there are three types of embedding available:</p>
<ul>
<li>Embedding - The most basic form of embeddings. This expects a 1D ([batch_size]) or 2D ([batch_size, time_steps]) tensor of IDs and outputs a [batch_size, embedding_dim] or [batch_size, time_steps, embedding_dim] sized tensor respectively.</li>
<li>SquashedEmbedding - This layer squashes the time step dimension based on some reduction operation (e.g. mean/sum) to transform a [batch_size, time_steps] sized  tensor of IDs to a [batch_size, embedding_dim] tensor.</li>
<li>FieldwiseEmbedding - This type can handle multiple features (i.e. fields) at once. The layer takes <code>n_slots</code> as an argument and IDs are mapped to a slot within the layer. The layer would return a tensor of size [batch_size, n_slots, embedding_dim]. </li>
</ul>

  <h4 style=\"text-align: left;\">Defining the embedding layers</h4>
<p>We will be using the <code>Embedding</code> to represent the user IDs and SquashedEmbedding to represent token IDs. Remember that each movie title has multiple tokens, therefore, we need a way to reduce the resulting token embeddings to a single representative embedding.</p>

<p><strong>Note:</strong> The behavior of Embedding has changed from version 0.5 to 0.6. Please make sure to use version 0.6 for this tutorial.</p>

<p>With that, we can define the two towers as we did in the standard model. However, this time we’ll be using the dynamic embedding layers instead of static embedding layers.</p>

<table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">build_de_user_model</span>(<span class=\"hljs-params\">user_id_lookup_layer: tf.keras.layers.StringLookup</span>) -&gt; tf.keras.layers.Layer:
	vocab_size = user_id_lookup_layer.vocabulary_size()
	<span class=\"hljs-keyword\">return</span> tf.keras.Sequential([
    	tf.keras.layers.InputLayer(input_shape=(), dtype=tf.string),
    	user_id_lookup_layer,
    	de.keras.layers.Embedding(
        	embedding_size=<span class=\"hljs-number\">64</span>,
        	initializer=tf.random_uniform_initializer(),
        	init_capacity=<span class=\"hljs-built_in\">int</span>(vocab_size*<span class=\"hljs-number\">0.8</span>),
        	restrict_policy=de.FrequencyRestrictPolicy,
        	name=<span class=\"hljs-string\">\"UserDynamicEmbeddingLayer\"</span>
    	),
    	tf.keras.layers.Dense(<span class=\"hljs-number\">64</span>, activation=<span class=\"hljs-string\">\"gelu\"</span>),
    	tf.keras.layers.Dense(<span class=\"hljs-number\">32</span>),
    	tf.keras.layers.Lambda(<span class=\"hljs-keyword\">lambda</span> x: tf.math.l2_normalize(x, axis=<span class=\"hljs-number\">1</span>))
	], name=<span class=\"hljs-string\">'user_model'</span>)

<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">build_de_item_model</span>(<span class=\"hljs-params\">movie_title_lookup_layer: tf.keras.layers.StringLookup</span>) -&gt; tf.keras.layers.Layer:
	vocab_size = movie_title_lookup_layer.vocabulary_size()
	<span class=\"hljs-keyword\">return</span> tf.keras.models.Sequential([
    		tf.keras.layers.InputLayer(input_shape=(max_token_length), dtype=tf.string),
    	movie_title_lookup_layer,
    	de.keras.layers.SquashedEmbedding(
        	embedding_size=<span class=\"hljs-number\">64</span>,
        	initializer=tf.random_uniform_initializer(),
        	init_capacity=<span class=\"hljs-built_in\">int</span>(vocab_size*<span class=\"hljs-number\">0.8</span>),
        	restrict_policy=de.FrequencyRestrictPolicy,
        	combiner=<span class=\"hljs-string\">\"mean\"</span>,
        	name=<span class=\"hljs-string\">\"ItemDynamicEmbeddingLayer\"</span>
    	),
    	tf.keras.layers.Dense(<span class=\"hljs-number\">64</span>, activation=<span class=\"hljs-string\">\"gelu\"</span>),
    	tf.keras.layers.Dense(<span class=\"hljs-number\">32</span>),
    	tf.keras.layers.Lambda(<span class=\"hljs-keyword\">lambda</span> x: tf.math.l2_normalize(x, axis=<span class=\"hljs-number\">1</span>))
	])</span></code></td></tr></tbody></table>

<p>With the user tower and movie tower models defined, we can define the retrieval model as usual.</p>

<h4 style=\"text-align: left;\">Creating and compiling the final model</h4>
<p>As a final step in model building, we’ll create the model and compile it.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">create_de_two_tower_model</span>(<span class=\"hljs-params\">dataset: tf.data.Dataset, candidate_dataset: tf.data.Dataset</span>) -&gt; tf.keras.Model:
    
	user_id_lookup_layer = get_user_id_lookup_layer(dataset)
	movie_title_lookup_layer = get_movie_title_lookup_layer(dataset)
	user_model = build_de_user_model(user_id_lookup_layer)
	item_model = build_de_item_model(movie_title_lookup_layer)
	task = tfrs.tasks.Retrieval(
    	metrics=tfrs.metrics.FactorizedTopK(
        	candidate_dataset.<span class=\"hljs-built_in\">map</span>(item_model)
    	),
	)

	model = DynamicEmbeddingTwoTowerModel(user_model, item_model, task)
	optimizer = de.DynamicEmbeddingOptimizer(tf.keras.optimizers.Adam())
	model.<span class=\"hljs-built_in\">compile</span>(optimizer=optimizer)
    
	<span class=\"hljs-keyword\">return</span> model

datasets = create_datasets()
de_model = create_de_two_tower_model(datasets.training_datasets.train_ds, datasets.candidate_dataset)</span></code></td></tr></tbody></table>

<p>Note the usage of the <code>DynamicEmbeddingOptimizer</code> wrapper around the standard TensorFlow optimizer. It is mandatory to wrap the standard optimizer in a <code>DynamicEmbeddingOpitmizer</code> as it will provide specialized functionality needed to train the weights stored in a hashtable. We can now train our model.</p>

<strong><h2>Training the model</h2></strong>
<p>Training the model is quite straightforward, but will involve a bit more extra effort as we’d like to log some extra information. We will perform the logging through a <code>tf.keras.callbacks.Callback</code> object. We’ll name this <code>DynamicEmbeddingCallback</code>.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">epochs = <span class=\"hljs-number\">3</span>
history_de = {}
history_de_size = {}
de_callback = DynamicEmbeddingCallback(de_model, steps_per_logging=<span class=\"hljs-number\">20</span>)

<span class=\"hljs-keyword\">for</span> epoch <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(epochs):

	datasets = create_datasets()
	train_steps = <span class=\"hljs-built_in\">len</span>(datasets.training_datasets.train_ds)
    
	hist = de_model.fit(
    	datasets.training_datasets.train_ds,
    	epochs=<span class=\"hljs-number\">1</span>,
    	validation_data=datasets.training_datasets.validation_ds,
    	callbacks=[de_callback]
	)
    
	<span class=\"hljs-keyword\">for</span> k,v <span class=\"hljs-keyword\">in</span> de_model.dynamic_embedding_history.items():
    	<span class=\"hljs-keyword\">if</span> k==<span class=\"hljs-string\">\"step\"</span>:
        		v = [vv+(epoch*train_steps) <span class=\"hljs-keyword\">for</span> vv <span class=\"hljs-keyword\">in</span> v]
    	history_de_size.setdefault(k, []).extend(v)
   	 
	<span class=\"hljs-keyword\">for</span> k,v <span class=\"hljs-keyword\">in</span> hist.history.items():
    		history_de.setdefault(k, []).extend(v)</span></code></td></tr></tbody></table>

<p>We have taken the loop that goes through the epochs out of the <code>fit()</code> function. Then in every epoch we re-create the dataset, as that will provide a different shuffling of the training dataset. We will train the model for a single epoch within the loop. Finally we accumulate the logged embedding sizes in <code>history_de_size</code> (this is provided by our custom callback) and performance metrics in <code>history_de</code>.</p>

<p>The callback is implemented as follows.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">DynamicEmbeddingCallback</span>(tf.keras.callbacks.Callback):
    
    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, model, steps_per_logging, steps_per_restrict=<span class=\"hljs-literal\">None</span>, restrict=<span class=\"hljs-literal\">False</span></span>):
        self.model = model
        self.steps_per_logging = steps_per_logging
        self.steps_per_restrict = steps_per_restrict
        self.restrict = restrict
    
    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">on_train_begin</span>(<span class=\"hljs-params\">self, logs=<span class=\"hljs-literal\">None</span></span>):
        self.model.dynamic_embedding_history = {}
        
    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">on_train_batch_end</span>(<span class=\"hljs-params\">self, batch, logs=<span class=\"hljs-literal\">None</span></span>):
                
        <span class=\"hljs-keyword\">if</span> self.restrict <span class=\"hljs-keyword\">and</span> self.steps_per_restrict <span class=\"hljs-keyword\">and</span> (batch+<span class=\"hljs-number\">1</span>) % self.steps_per_restrict == <span class=\"hljs-number\">0</span>:
            
            [
                self.model.embedding_layers[k].params.restrict(
                    num_reserved=<span class=\"hljs-built_in\">int</span>(self.model.lookup_vocab_sizes[k]*<span class=\"hljs-number\">0.8</span>), 
                    trigger=self.model.lookup_vocab_sizes[k]-<span class=\"hljs-number\">2</span> <span class=\"hljs-comment\"># UNK &amp; PAD tokens</span>
                ) <span class=\"hljs-keyword\">for</span> k <span class=\"hljs-keyword\">in</span> self.model.embedding_layers.keys()
            ] 
        
        <span class=\"hljs-keyword\">if</span> (batch+<span class=\"hljs-number\">1</span>) % self.steps_per_logging == <span class=\"hljs-number\">0</span>:
            
            embedding_size_dict = {
                k:self.model.embedding_layers[k].params.size().numpy() 
                <span class=\"hljs-keyword\">for</span> k <span class=\"hljs-keyword\">in</span> self.model.embedding_layers.keys()
            }

            <span class=\"hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span> embedding_size_dict.items():
                self.model.dynamic_embedding_history.setdefault(<span class=\"hljs-string\">f\"embedding_size_<span class=\"hljs-subst\">{k}</span>\"</span>, []).append(v)
            self.model.dynamic_embedding_history.setdefault(<span class=\"hljs-string\">f\"step\"</span>, []).append(batch+<span class=\"hljs-number\">1</span>)</span></code></td></tr></tbody></table><br /><div>

              <p>The callback does two things:</p>
<ul>
<li>Logs the sizes of the embedding layers every <code>steps_per_logging</code> iterations</li>
<li>Reduces the size of the embedding table to an 80% size of the total vocabulary size if <code>restrict=True</code>(This is set to <code>False</code> by default)</li>
</ul>

              <p>Let’s understand what reducing the size means and why it is important.</p>

<h4 style=\"text-align: left;\">Reducing the size of the embedding table</h4>
<p>An important topic we still haven’t discussed is how to reduce the size of the embedding table, should it grow over some predefined threshold. This is a powerful functionality as it allows us to define a threshold over which the embedding table should not grow. This will allow us to work with large vocabularies while keeping the memory requirement under the memory limitations we may have. We achieve this by calling <code>restrict()</code> on the underlying variables of the embedding layer as shown in the <code>DynamicEmbeddingCallback</code>. <code>restrict()</code> takes two arguments in: <code>num_reserved</code> (the size after the reduction) and <code>trigger</code> (size at which the reduction should be triggered). The policy that governs how the reduction is performed is defined using the <code>restrict_policy</code> argument in the layer construct. You can see that we are using the <code>FrequencyRestrictPolicy</code>. This means the least frequent items will be removed from the embedding table. The callback enables a user to set how frequently the reduction should get triggered by setting the <code>steps_per_restrict</code> and <code>restrict</code> arguments in the <code>DynamicEmbeddingCallback</code>.</p>

<p>Reducing the size of the embedding table makes more sense when you have streaming data. Think about an online learning setting, where you are training the model every day (or even every hour) on some incoming data. You can think of the outer for loop (i.e. epochs) representing days. Each day you receive a dataset (containing user interactions from the previous day for example) and you train the model from the previous checkpoint. In this case, you can use the <code>DynamicEmbeddingCallback</code> to trigger a restrict if the embedding table grows over the size defined in the trigger argument. 

  <strong></strong></p><h2><strong>Analyzing performance</strong></h2>
<p>Here we analyze the performance of three variants.</p>
<ul>
<li>The standard retrieval model (which uses a static embedding table)</li>
<li>Retrieval model using dynamic embedding but no restrict performed</li>
<li>Retrieval model using dynamic embedding with restrict performed</li>
              </ul> 
  <div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"A graph showing Model accuracy with and without dynamic embeddings\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEje2YQ6vDZwTnU2zZdELInx02rk28bCVFyZzJD_HwwmoyEwludk9wpikmB3JXgxtjoSVcgKknZgse0qkytkqTb_cS7-VEdE4i4tfQahblY4sbvWAR5WRkspleur7pYMULRcmsRh59bLKQKsbHkd4Y-RaBhAimi6pN6fzcCXPhSHVFBcqMnXSftGisfv/s1600/image1.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>
  
<p>You can see that the model using dynamic embeddings (solid green line) has comparative validation performance to the baseline (solid red line). You can see a similar trend in the training accuracy as well. In practice, <strong>dynamic embeddings can often be seen to improve accuracy in a large-scale online learning setup. </strong></p>

<p>Finally, we can see that <code>restrict</code> has a somewhat detrimental effect on the validation accuracy, which is understandable. Since we’re working with a relatively small dataset with a small number of items, the reduction could be getting rid of embeddings that are best kept in the table. For example, you can increase the <code>num_reserved</code> argument (e.g. set it to <code>int(self.model.lookup_vocab_sizes[k]*0.95)</code>)  in the <code>restrict</code> function which would yield performance that improves towards the performance of without <code>restrict</code>.</p>

  <p>Next we look at how <code>dynamic</code> the embedding tables really are over time.</p>

  
  <div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"A graph showing changes in the embedding size over time\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjX6J6cCq6gc3DzlSKWdPUCz8vOo5IPFMhjdMUckDz2K3TwfSxlODzLX8vIcfDDQ_WjRxtSix4jsEAm71cQU27tFMyqXKeZgpiqk9OlFCRp76Vd39hovyqegVFpbHFEHk6Tne9UwVpKXSSw6JghuxkMZB2qVItCIIswpyfkLzTTsvEl4-tOePwLgRqe/s1600/image3.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>
  
<p>We can see that when restrict is not used, the embedding table grows to the full size of the vocabulary (dashed line) and stays there. However when <code>restrict</code> is triggered (dotted line), the size drops and grows in size again as it encounters new IDs.</p>

<p>It is also important to note that constructing a proper validation is not a trivial task. There are considerations such as out-of-sample validation, out-of-time validation, stratification, etc. that needs to be taken into account carefully. However for this exercise, we have not focused on such factors and created a validation set by sampling randomly from the existing dataset.</p>
  
  <strong><h2>Conclusion</h2></strong>
  
<p>Using dynamic embedding tables is a powerful way to perform representation learning when working with large sets of items containing millions or billions of entities. In this tutorial, we learnt how to use the <code>dynamic_embedding</code> module provided in the TensorFlow Recommender Addons library to achieve this. We first explored the data and constructed <code>tf.data.Dataset</code> objects by extracting the features we’ll be using for our model training and evaluation. Next we defined a model that uses static embedding tables to use as an evaluation baseline. We then created a model that uses dynamic embedding and trained it on the data. We saw that using dynamic embeddings, the embedding tables grow only on demand and still achieve comparable performance with the baseline. We also discussed how the <code>restrict</code> functionality can be used to shrink the embedding table if it grows past a pre-defined threshold.</p>

<p>We hope this tutorial gives you a good conceptual introduction to TFRA and dynamic embeddings, and helps you think about how you can leverage it to enhance your own recommenders. If you would like to have a more in-depth discussion, please visit the TFRA repository.</p></div>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Wed, 19 Apr 2023 17:00:00 +0000"
2126,Counterfactual Logit Pairing,https://blog.tensorflow.org/2023/04/counterfactual-logit-pairing.html,"<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi4JTW926mMONbwuuLuLfYR7Nug8wj5HBewNfjabbklWVBH2C0EaM87BTEl38yLdRahrvlkPnYw6ePjg6g1lEEBAqW2dKkSApQSEJkbEiOXuqu6VEr23YzUsdV-OQLUQIqUGIR3V6YweRAoDv1KsRfPkNJsQfGvWXVr1lrVZ8fTpGQC8Th1z4BSSD41/s1600/image2.png\" style=\"display: none;\" />

<p><em>Posted by Bhaktipriya Radharapu, Software Engineer</em></p><p>

</p><a name=\"more\"></a><p></p>

<p><a href=\"https://www.tensorflow.org/responsible_ai/model_remediation\" target=\"_blank\">TensorFlow Model Remediation</a> is an open source toolkit that showcases solutions to help mitigate unfair bias in Machine Learning models. The toolkit  offers resources to build fairer models for everyone  – in line with <a href=\"https://ai.google/principles/\" target=\"_blank\">Google’s AI Principles</a>. Today, we’re excited to announce a new technique within the TensorFlow Model Remediation Library called <a href=\"https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_overview\" target=\"_blank\">Counterfactual Logit Pairing</a> (CLP)  to address unintended bias in ML models.</p>

<p>ML models are prone to making incorrect predictions when a sensitive attribute in an input is removed or replaced, leading to unintended bias. For instance, the <a href=\"https://www.perspectiveapi.com/#/home\" target=\"_blank\">Perspective API</a>, used to identify offensive or toxic text in comments, revealed a positive correlation between identity terms referencing race or sexual orientation and the predicted toxicity score. For instance, the phrase \"I am a lesbian\" received a toxicity score of 0.51, while “I am a man” received a lower toxicity score of 0.2. This correlation resulted in higher toxicity scores for some identity terms, even when used non-pejoratively. For more information on the Perspective API, see the blog post on <a href=\"https://medium.com/jigsaw/unintended-bias-and-names-of-frequently-targeted-groups-8e0b81f80a23\" target=\"_blank\">unintended bias and identity terms</a>. </p>

<p>Counterfactual Logit Pairing (CLP) is a technique that addresses such issues to ensure that a model’s prediction doesn’t change when a sensitive attribute referenced in an example is either removed or replaced. It improves a model’s robustness to such perturbations, and can positively influence a model’s stability, fairness, and safety.</p> 


<p>CLP mitigates such counterfactual fairness issues at training time. It does so by adding an additional loss to the model’s training loss, which penalizes the difference in the model’s outputs between training examples and their counterfactuals.</p>

<p>Another advantage of using CLP is that you can use this even on unlabelled data. As long as the model treats the counterfactual examples similarly you can validate that your model is adhering to counterfactual fairness.</p>

<p>For an in-depth discussion on this topic, see research on <a href=\"https://arxiv.org/abs/1703.06856\" target=\"_blank\">counterfactual fairness</a>, <a href=\"https://arxiv.org/abs/1803.06373\" target=\"_blank\">adversarial logit pairing</a>, and <a href=\"https://arxiv.org/abs/1809.10610\" target=\"_blank\">counterfactual logit pairing</a>.</p>

<h3><strong>Counterfactual Logit Pairing Walkthrough:</strong></h3>

<p>The <a href=\"https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_keras\" target=\"_blank\">CLP with Keras</a> codelab provides an end-to-end example. In this overview, we'll emphasize key points from the notebook, while providing additional context.</p>

<p>The notebook trains a text classifier to identify toxic content. This type of model attempts to identify content that is rude, disrespectful or otherwise likely to make someone leave a discussion, and assigns the content a toxicity score. For this task, our baseline model will be a simple Keras sequential model pre-trained on the <a href=\"https://www.tensorflow.org/datasets/catalog/civil_comments\" target=\"_blank\">Civil Comments dataset</a>.</p>

<p>We will use CLP to avoid having identity terms unfairly skew what is classified as offensive. We consider a narrow class of counterfactuals that involves removing gender and sexual orientation related identity tokens in the input, such as removing “gay” in the input “I’m a gay person” to create the counterfactual example “I’m a person.”</p>

<p>The high-level steps will be to:</p>
<ol><blockquote>
<li>Calculate <a href=\"https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_usage_steps#measure_flip_count_and_flip_rate\" target=\"_blank\">flip rate and flip count</a> of the classifier on original and counterfactual examples. </li>
  <li>Build a counterfactual dataset using <code>CounterfactualPackedInputs</code> by performing a naive ablation based on term matching.</li> 
<li>Improve performance on flip rate and flip count by training with CLP.</li>
<li>Evaluate the new model’s performance on flip rate and flip count.</li>
  </blockquote></ol>

<p>Be aware that this is a minimal workflow to demonstrate usage of the CLP technique, and not a complete approach to fairness in machine learning. CLP addresses one specific challenge that may impact fairness in machine learning. See the <a href=\"https://www.tensorflow.org/responsible_ai\" target=\"_blank\">Responsible AI toolkit</a> for additional information on responsible AI and tools that can be used to complement CLP.</p>

<p>In a production setting, you would want to approach each of these steps with more rigor. For example:</p>
<ul><blockquote>
<li>Consider the fairness goals of your model. What qualifies as “fair” for your model? Which <a href=\"https://fairmlbook.org/tutorial2.html\" target=\"_blank\">definitions of fairness</a> are you trying to achieve?</li>
<li>Consider when counterfactual pairs should have the same prediction. Many syntactic counterfactuals generated by token substitution may not require identical output. Consider the application space and the potential societal impact of your model and understand <a href=\"https://arxiv.org/abs/2102.05085\" target=\"_blank\">when the outputs should be the same and when they shouldn’t be.</a></li>
<li>Consider using semantically and grammatically grounded counterfactuals instead of heuristic based ablations.</li>
<li>Experiment with the configuration of CLP by tuning hyperparameters to get optimal performance.</li>
  </blockquote></ul>

<p>Let’s begin by examining the flip count and flip rate of the original model on the counterfactual examples. The flip count measures the number of times the classifier gives a different decision if the identity term in a given example is changed. The flip rate measures the total number of times that the classifier incorrectly provides an incorrect decision over the total count. </p>

<p>Let’s use the \"Fairness Indicators widget\" in the notebook to measure the flip rate and counts. Select <code>flip_rate/overall</code> in the widget. Notice that the overall flip rate for females is about 13% and male is about 14%, which are both higher than the overall dataset of 8%. This means that the model is likely to change the classification based on the presence of gender related terms.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhvQkzbndgfpbxHTvmQtc3AHpr_eg5gBTuc6K5-zBqnZGPGYjBpQ5cwEMi7uZbcvRERbvY0CO8N-QouyfrtrTJCs9pIphpPyt4YjA17HSc2ownJAKegDnTW1XmfhPhxSdQAstqZfaJHWZaYhCLC9gBro-a8Mn29KjXHl_uy9ExwSDawgS617_Lx4H-H/s1600/image1.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

We’ll now use CLP to try to reduce the model's flip rate and flip count for gender-related terms in our dataset. We start by creating an instance of <code>CounterfactualPackedInputs</code>, which packs the original_input and counterfactual_data. 


<p></p><p></p><div><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\"><span class=\"hljs-built_in\">CounterfactualPackedInputs</span>(
  original_input=(x, y, sample_weight),
 counterfactual_data:(original_x, counterfactual_x,   
                 counterfactual_sample_weight)
)</span></code></td></tr></tbody></table></div>

We next remove instances of gender specific terms using the helper function, <code>build_counterfactual_data</code>. Note that we only include non-pejorative terms, as pejorative terms should have a different toxicity score. Requiring equal predictions across examples with pejorative terms would both weaken the model’s ability to perform its task and potentially increase harm to vulnerable groups.<div>&nbsp;
<div><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">sensitive_terms_to_remove = [
 <span class=\"hljs-string\">'aunt'</span>, <span class=\"hljs-string\">'boy'</span>, <span class=\"hljs-string\">'brother'</span>, <span class=\"hljs-string\">'dad'</span>, <span class=\"hljs-string\">'daughter'</span>, <span class=\"hljs-string\">'father'</span>, <span class=\"hljs-string\">'female'</span>, <span class=\"hljs-string\">'gay'</span>,
 <span class=\"hljs-string\">'girl'</span>, <span class=\"hljs-string\">'grandma'</span>, <span class=\"hljs-string\">'grandpa'</span>, <span class=\"hljs-string\">'grandson'</span>, <span class=\"hljs-string\">'grannie'</span>, <span class=\"hljs-string\">'granny'</span>, <span class=\"hljs-string\">'he'</span>,
 <span class=\"hljs-string\">'heir'</span>, <span class=\"hljs-string\">'her'</span>, <span class=\"hljs-string\">'him'</span>, <span class=\"hljs-string\">'his'</span>, <span class=\"hljs-string\">'hubbies'</span>, <span class=\"hljs-string\">'hubby'</span>, <span class=\"hljs-string\">'husband'</span>, <span class=\"hljs-string\">'king'</span>,
 <span class=\"hljs-string\">'knight'</span>, <span class=\"hljs-string\">'lad'</span>, <span class=\"hljs-string\">'ladies'</span>, <span class=\"hljs-string\">'lady'</span>, <span class=\"hljs-string\">'lesbian'</span>, <span class=\"hljs-string\">'lord'</span>, <span class=\"hljs-string\">'man'</span>, <span class=\"hljs-string\">'male'</span>,
 <span class=\"hljs-string\">'mom'</span>, <span class=\"hljs-string\">'mother'</span>, <span class=\"hljs-string\">'mum'</span>, <span class=\"hljs-string\">'nephew'</span>, <span class=\"hljs-string\">'niece'</span>, <span class=\"hljs-string\">'prince'</span>, <span class=\"hljs-string\">'princess'</span>,
 <span class=\"hljs-string\">'queen'</span>, <span class=\"hljs-string\">'queens'</span>, <span class=\"hljs-string\">'she'</span>, <span class=\"hljs-string\">'sister'</span>, <span class=\"hljs-string\">'son'</span>, <span class=\"hljs-string\">'uncle'</span>, <span class=\"hljs-string\">'waiter'</span>,
 <span class=\"hljs-string\">'waitress'</span>, <span class=\"hljs-string\">'wife'</span>, <span class=\"hljs-string\">'wives'</span>, <span class=\"hljs-string\">'woman'</span>, <span class=\"hljs-string\">'women'</span>
]


<span class=\"hljs-comment\"># Convert the Pandas DataFrame to a TF Dataset</span>
dataset_train_main = tf.data.Dataset.from_tensor_slices(
   (data_train[TEXT_FEATURE].values, labels_train)).batch(BATCH_SIZE)


counterfactual_data = counterfactual.keras.utils.build_counterfactual_dataset(
   original_dataset=dataset_train_main,
   sensitive_terms_to_remove=sensitive_terms_to_remove)


counterfactual_packed_input = counterfactual.keras.utils.pack_counterfactual_data(
 dataset_train_main,
 counterfactual_data)</span></code></td></tr></tbody></table></div></div>

<p>To train with a Counterfactual model, simply take the original model and wrap it in a <code>CounterfactualModel</code> with a corresponding <code>loss</code> and <code>loss_weight</code>. This will co-train the model on the main classification task and on the debiasing task using the CLP loss.</p>

<p>We are using 1.0 as the default <code>loss_weight</code>, but this is a parameter that can be tuned for your use case, since it depends on your model and product requirements.  You should experiment with changing the value to see how it impacts the model, noting that increasing it would cause the model to penalize the counterfactual examples more heavily. You can test a range of values to explore the trade off between the task performance and the flip rate.</p>

<p>Here, we use the <a href=\"https://www.tensorflow.org/responsible_ai/model_remediation/api_docs/python/model_remediation/counterfactual/losses/PairwiseMSELoss\" target=\"_blank\">Pairwise Mean Squared Error Loss</a>. You can try experimenting with other metrics in the suite to know which options offer the best results.</p><table class=\"leading-snug\"><colgroup></colgroup><tbody><tr><td><code class=\"m-0 p-0 whitespace-pre-wrap font-monospace\" id=\"code-output\"><span style=\"font-family: courier;\">counterfactual_weight = 1.0

counterfactual_model = counterfactual.keras.CounterfactualModel(
    baseline_model,
    loss=counterfactual.losses.PairwiseMSELoss(),
    loss_weight=counterfactual_weight)

<span class=\"hljs-comment\"># Compile the model normally after wrapping the original model.</span>
<span class=\"hljs-comment\"># Note that this means we use the baseline's model's loss here.</span>
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
loss = tf.keras.losses.BinaryCrossentropy()
counterfactual_model.compile(optimizer=optimizer, loss=loss,
                             metrics=['accuracy'])

counterfactual_model.fit(counterfactual_packed_input,
                         epochs=1)</span></code></td></tr></tbody></table>

<p>Once again, we evaluate the results by looking at the flip count and flip rate. Select “flip_rate/overall” within Fairness Indicators and compare the results for female and male between the two models. You should notice that the flip rate for overall, female, and male have all decreased by about 90%, which leaves the final flip rate for female at approximately 1.3% and male at approximately 1.4%.</p>

<div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><center><img alt=\"\" border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhegZBC4CDN7nr-osy1tvTMyciRBdCbvUcx3AhMv484xBMJV3iyvQKZSWuLytpQ-M24qzcWYL4qVX6DORs6ms3AjYqGa2J0dIvHe1hhebxAck74nwyBwRnAnWUig0zMuj_yFqsjphULBvmjqZnkYkbjX8fEKDwBowMyXi_g_bujiF37nb-2Tk5frRYP/s1600/image3.png\" style=\"width: 100%;\" /></center></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><i></i></td></tr></tbody></table></div>

<p>You can get started with Counterfactual by visiting <a href=\"https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_overview\" target=\"_blank\">TensorFlow Responsible AI</a> and learn more about evaluation fairness with <a href=\"https://www.tensorflow.org/responsible_ai/fairness_indicators/guide/guidance\" target=\"_blank\">Fairness Indicators</a>.</p>


<p></p><h4>Acknowledgements</h4><p></p>

<div style=\"text-align: left;\">The Counterfactual framework was developed in collaboration with
</div><ul><li>Amy Wang, Ben Packer, Bhaktipriya Radharapu, Christina Greer, Nick Blumm, Parker Barnes, Piyush Kumar, Sean O’Keefe, Shivam Jindal, Shivani Poddar, Summer Misherghi, Thomas Greenspan.</li></ul>

<div style=\"text-align: left;\">This research effort was jointly led by
</div><ul><li>Alex Beutel, Jilin Chen, Tulsee Doshi in collaboration with Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed H. Chi.</li></ul>

<div style=\"text-align: left;\"><em>Further, this work was pursued in collaboration with</em></div><ul><li>Andrew Smart, Francois Chollet, Molly FitzMorris, Tomer Kaftan, Mark Daoust, Daniel 'Wolff' Dobson, Soo Sung.</li></ul>","[{'name': 'TensorFlow Blog', 'email': 'noreply@blogger.com'}]","Thu, 06 Apr 2023 17:00:00 +0000"
2127,How Many Days Would February Have if the Earth Rotated Backward?  Exploring Leap Years with Wolfram Language,https://blog.wolfram.com/2024/02/29/how-many-days-would-february-have-if-the-earth-rotated-backward-exploring-leap-years-with-wolfram-language/,"Happy Leap Day 2024! A leap day is an extra day (February 29) that is added to the Gregorian calendar (the calendar most of us use day to day) in leap years. While leap years most commonly come in four-year intervals, they sometimes come every eight years. This is because a traditional leap day every [&#8230;]",[{'name': 'José Martín-García'}],"Thu, 29 Feb 2024 15:59:09 +0000"
2128,Your Invitation to Take a Quantum Leap in Education,https://blog.wolfram.com/2024/02/27/your-invitation-to-take-a-quantum-leap-in-education/,"Learning quantum theory requires dedication and a willingness to challenge classical assumptions. Quantum interference, particularly for massive particles, is a pivotal example in this journey. The Schrödinger equation, inspired by de Broglie’s hypothesis, revolutionized our understanding by revealing the wavelike nature of even massive particles. This phenomenon not only deepens our grasp of nature but [&#8230;]",[{'name': 'Mads Bahrami'}],"Tue, 27 Feb 2024 23:02:19 +0000"
2129,Reduce Quantum Noise with Wolfram Language and Fire Opal,https://blog.wolfram.com/2024/02/27/reduce-quantum-noise-with-wolfram-language-and-fire-opal/,"Practical quantum computers have not entered the mainstream, but that has not stopped researchers and developers from innovating. Simulating quantum results on classical hardware and getting meaningful results from noisy quantum hardware are two important areas with lots of recent innovations. The Wolfram Quantum Framework is a toolkit for Wolfram Language that offers quantum simulations. [&#8230;]",[{'name': 'Mads Bahrami'}],"Tue, 27 Feb 2024 16:42:16 +0000"
2130,Hypergeometric Functions: From Euler to Appell and Beyond,https://blog.wolfram.com/2024/01/25/hypergeometric-functions-from-euler-to-appell-and-beyond/,"Hypergeometric series appeared in the mid-seventeenth century; since then, they have played an important role in the development of mathematical and physical theories. Most of the elementary and special functions are members of the large hypergeometric class. Hypergeometric functions have been a part of Wolfram Language since Version 1.0. The following plot shows the implementation [&#8230;]",[{'name': 'Tigran Ishkhanyan'}],"Thu, 25 Jan 2024 17:35:34 +0000"
2131,Leveling Up in Life Sciences: Unleashing the Power of Computational Biology with Wolfram Language,https://blog.wolfram.com/2024/01/18/leveling-up-in-life-sciences-unleashing-the-power-of-computational-biology-with-wolfram-language/,"In days past, life sciences was reserved for those who had access to the proper equipment to observe and experiment with the organisms of the physical world. For today’s scientist, exploration doesn’t end with access to physical encounters. Whether you’re classifying an animal for the first time or using a protein visualizer to develop medication, [&#8230;]",[{'name': 'Bailey Long'}],"Thu, 18 Jan 2024 19:40:37 +0000"
2132,The Story Continues: Announcing Version 14 of Wolfram Language and Mathematica,https://blog.wolfram.com/2024/01/09/the-story-continues-announcing-version-14-of-wolfram-language-and-mathematica/,<p>Today we celebrate a new waypoint on our journey of nearly four decades with the release of Version 14.0 of <a href=\"https://www.wolfram.com/language/\">Wolfram Language</a> and <a href=\"https://www.wolfram.com/mathematica/\">Mathematica</a>. Over the two years since we released <a href=\"https://reference.wolfram.com/legacy/language/v13/\">Version 13.0</a> we’ve been steadily delivering the fruits of our research and development in .1 releases every six months. Today we’re aggregating these—and more—into Version 14.0.</p>,[{'name': 'Stephen Wolfram'}],"Tue, 09 Jan 2024 22:42:48 +0000"
2133,Get Down to Business with Finite Mathematics in Wolfram Language,https://blog.wolfram.com/2023/12/22/get-down-to-business-with-finite-mathematics-in-wolfram-language/,"“There is every reason to expect that the various social sciences will serve as incentives for the development of great new branches of mathematics and that some day the theoretical social scientist will have to know more mathematics than the physicist needs to know today.” —John G. Kemeny, first author of the original textbook on [&#8230;]",[{'name': 'John McNally'}],"Fri, 22 Dec 2023 15:41:26 +0000"
2134,A Year in Review: New from Wolfram Media in 2023,https://blog.wolfram.com/2023/12/15/a-year-in-review-new-from-wolfram-media-in-2023/,"With seven newly published titles and deals penned for translations into 13 languages, 2023 has been Wolfram Media’s most productive year yet. As our list continues to grow, we are proud to cover more and more Wolfram Language functionality in conceptual and practical ways for our readers. As we near the end of the year, [&#8230;]",[{'name': 'Paige Vigliarolo'}],"Fri, 15 Dec 2023 15:29:53 +0000"
2135,Journey from the Wolfram Data Science Boot Camp to Creative Data Analytics,https://blog.wolfram.com/2023/12/06/journey-from-the-wolfram-data-science-boot-camp-to-creative-data-analytics/,"When I attended the 2019 Wolfram Data Science Boot Camp as an instructor, I had the pleasure of meeting Professor Fei Du and Davis Mo, the youngest Boot Camp alum to date. I don’t think any of us expected our conversations from the camp to grow into our collaboration on the book Creative Data Analytics: [&#8230;]",[{'name': 'Kelvin Mischo'}],"Wed, 06 Dec 2023 16:39:04 +0000"
2136,Efficient learning of many-body systems,https://www.nature.com/articles/s41567-024-02393-4,,[],"Wed, 6 Mar 2024 00:00:00 +0000"
2137,Protecting entanglement between logical qubits via quantum error correction,https://www.nature.com/articles/s41567-024-02446-8,,[],"Wed, 6 Mar 2024 00:00:00 +0000"
2138,Quantum approximate optimization via learning-based adaptive optimization,https://www.nature.com/articles/s42005-024-01577-x,,[],"Wed, 6 Mar 2024 00:00:00 +0000"
2139,Learning quantum Hamiltonians from high-temperature Gibbs states and real-time evolutions,https://www.nature.com/articles/s41567-023-02376-x,,[],"Wed, 6 Mar 2024 00:00:00 +0000"
2140,Mapping a 50-spin-qubit network through correlated sensing,https://www.nature.com/articles/s41467-024-46075-4,,[],"Tue, 5 Mar 2024 00:00:00 +0000"
2141,Robust parallel laser driving of quantum dots for multiplexing of quantum light sources,https://www.nature.com/articles/s41598-024-55634-0,,[],"Mon, 4 Mar 2024 00:00:00 +0000"
2142,Graph states of atomic ensembles engineered by photon-mediated entanglement,https://www.nature.com/articles/s41567-024-02407-1,,[],"Fri, 1 Mar 2024 00:00:00 +0000"
2143,QAL-BP: an augmented Lagrangian quantum approach for bin packing,https://www.nature.com/articles/s41598-023-50540-3,,[],"Fri, 1 Mar 2024 00:00:00 +0000"
2144,Deep learning the hierarchy of steering measurement settings of qubit-pair states,https://www.nature.com/articles/s42005-024-01563-3,,[],"Fri, 1 Mar 2024 00:00:00 +0000"
2145,Analyzing variational quantum landscapes with information content,https://www.nature.com/articles/s41534-024-00819-8,,[],"Thu, 29 Feb 2024 00:00:00 +0000"
2146,Deterministic generation of multidimensional photonic cluster states with a single quantum emitter,https://www.nature.com/articles/s41567-024-02408-0,,[],"Wed, 28 Feb 2024 00:00:00 +0000"
2147,A framework for demonstrating practical quantum advantage: comparing quantum against classical generative models,https://www.nature.com/articles/s42005-024-01552-6,,[],"Wed, 28 Feb 2024 00:00:00 +0000"
2148,Markovian noise modelling and parameter extraction framework for quantum devices,https://www.nature.com/articles/s41598-024-54598-5,,[],"Tue, 27 Feb 2024 00:00:00 +0000"
2149,Author Correction: Improved machine learning algorithm for predicting ground state properties,https://www.nature.com/articles/s41467-024-46164-4,,[],"Mon, 26 Feb 2024 00:00:00 +0000"
2150,Oscillating photonic Bell state from a semiconductor quantum dot for quantum key distribution,https://www.nature.com/articles/s42005-024-01547-3,,[],"Sat, 24 Feb 2024 00:00:00 +0000"
2151,Efficiency optimization in quantum computing: balancing thermodynamics and computational performance,https://www.nature.com/articles/s41598-024-55314-z,,[],"Sat, 24 Feb 2024 00:00:00 +0000"
2152,Practical high-dimensional quantum key distribution protocol over deployed multicore fiber,https://www.nature.com/articles/s41467-024-45876-x,,[],"Fri, 23 Feb 2024 00:00:00 +0000"
2153,Efficient bosonic nonlinear phase gates,https://www.nature.com/articles/s41534-024-00816-x,,[],"Fri, 23 Feb 2024 00:00:00 +0000"
2154,Autonomous error correction of a single logical qubit using two transmons,https://www.nature.com/articles/s41467-024-45858-z,,[],"Fri, 23 Feb 2024 00:00:00 +0000"
2155,Wide-field Fourier magnetic imaging with electron spins in diamond,https://www.nature.com/articles/s41534-024-00818-9,,[],"Wed, 21 Feb 2024 00:00:00 +0000"
2156,Better-than-classical Grover search via quantum error detection and suppression,https://www.nature.com/articles/s41534-023-00794-6,,[],"Tue, 20 Feb 2024 00:00:00 +0000"
2157,Enhancing detection of topological order by local error correction,https://www.nature.com/articles/s41467-024-45584-6,,[],"Tue, 20 Feb 2024 00:00:00 +0000"
2158,A series of fast-paced advances in Quantum Error Correction,https://www.nature.com/articles/s42254-024-00706-3,,[],"Fri, 16 Feb 2024 00:00:00 +0000"
2159,Topological matter created on a quantum chip produces quasiparticles with computing power,https://www.nature.com/articles/d41586-023-04126-8,,[],"Wed, 14 Feb 2024 00:00:00 +0000"
2160,Navigating the 16-dimensional Hilbert space of a high-spin donor qudit with electric and magnetic fields,https://www.nature.com/articles/s41467-024-45368-y,,[],"Wed, 14 Feb 2024 00:00:00 +0000"
2161,Non-Abelian topological order and anyons on a trapped-ion processor,https://www.nature.com/articles/s41586-023-06934-4,,[],"Wed, 14 Feb 2024 00:00:00 +0000"
2162,On the (relation between) efficiency and secret key rate of QKD,https://www.nature.com/articles/s41598-024-54246-y,,[],"Tue, 13 Feb 2024 00:00:00 +0000"
2163,Rapid single-shot parity spin readout in a silicon double quantum dot with fidelity exceeding 99%,https://www.nature.com/articles/s41534-024-00813-0,,[],"Tue, 13 Feb 2024 00:00:00 +0000"
2164,Spin-EPR-pair separation by conveyor-mode single electron shuttling in Si/SiGe,https://www.nature.com/articles/s41467-024-45583-7,,[],"Tue, 13 Feb 2024 00:00:00 +0000"
2165,Publisher Correction: The complexity of NISQ,https://www.nature.com/articles/s41467-024-45799-7,,[],"Mon, 12 Feb 2024 00:00:00 +0000"
2166,"Enabling state-of-the-art quantum algorithms with Qedma’s error mitigation and IonQ, using Braket Direct",https://aws.amazon.com/blogs/quantum-computing/enabling-state-of-the-art-quantum-algorithms-with-qedmas-error-mitigation-and-ionq-using-braket-direct/,The story of how Qedma used Amazon Braket Direct for dedicated access to IonQ hardware to execute milestone VQE circuits. This post details leveraging reservations and collaborating directly with experts. An exciting look at accelerating innovation in quantum computing.,[{'name': 'Eyal Leviatan'}],"Wed, 06 Mar 2024 14:26:10 +0000"
2167,Explainable AI using expressive Boolean formulas,https://aws.amazon.com/blogs/quantum-computing/explainable-ai-using-expressive-boolean-formulas/,ML models driving high-stakes decisions need interpretability. See how the Amazon QSL and Fidelity FCAT developed interpretable models based on Boolean logic.,[{'name': 'Gili Rosenberg'}],"Wed, 21 Feb 2024 16:01:26 +0000"
2168,Citi and Classiq advance quantum solutions for portfolio optimization using Amazon Braket,https://aws.amazon.com/blogs/quantum-computing/citi-and-classiq-advance-quantum-solutions-for-portfolio-optimization/,Today we look at how Citi Innovation Labs is exploring quantum computing for portfolio optimization in partnership with Classiq and AWS. Their research examines how adjustments to the QAOA algorithm's penalty factor impact performance.,[{'name': 'Yoram Avidan'}],"Wed, 07 Feb 2024 15:38:57 +0000"
2169,Exploring industrial use cases in the Airbus-BMW Group Quantum Computing Challenge,https://aws.amazon.com/blogs/quantum-computing/exploring-industrial-use-cases-in-the-airbus-bmw-group-quantum-computing-challenge/,Discover how Airbus and BMW Group are harnessing quantum computing to tackle industry challenges. Join the Airbus-BMW Group Quantum Mobility Quest and help shape the future of transportation.,[{'name': 'Martin Schuetz'}],"Thu, 25 Jan 2024 14:54:04 +0000"
2170,Introducing the Amazon Braket Learning Plan and Digital Badge,https://aws.amazon.com/blogs/quantum-computing/introducing-the-amazon-braket-learning-plan-and-digital-badge/,"Available today, quantum computing developers, educators, and enthusiasts can&nbsp;learn the foundations of quantum computing on Amazon Web Services (AWS) with the Amazon Braket Digital Learning Plan and earn their own Digital badge – at no additional cost. You earn the badge after completing a series of learning courses and scoring at least 80% on an […]",[{'name': 'James Whitfield'}],"Mon, 27 Nov 2023 14:09:35 +0000"
2171,"A detailed, end-to-end assessment of a quantum algorithm for portfolio optimization, released by Goldman Sachs and AWS",https://aws.amazon.com/blogs/quantum-computing/a-detailed-end-to-end-assessment-of-a-quantum-algorithm-for-portfolio-optimization-released-by-goldman-sachs-and-aws/,In this post we’ll walk you through some key takeaways from a paper published today by scientists from Goldman Sachs and AWS describing a quantum algorithm for portfolio optimization.,[{'name': 'Alexander Dalzell'}],"Mon, 13 Nov 2023 19:43:37 +0000"
2172,Towards practical molecular electronic structure simulations on NISQ devices with Amazon Braket and Kvantify’s FAST-VQE algorithm,https://aws.amazon.com/blogs/quantum-computing/towards-practical-molecular-electronic-structure-simulations-on-nisq-devices-with-amazon-braket-and-kvantifys-fast-vqe-algorithm/,"Quantum computing's potential for computational chemistry is immense, but there are practical limitations. We show how Kvantify’s FAST-VQE algorithm can deliver great accuracy, performance, superior cost-effectiveness, driving us closer to transformative applications in drug discovery.",[{'name': 'Patrick Ettenhuber'}],"Wed, 08 Nov 2023 13:47:08 +0000"
2173,Analog Hamiltonian simulation with PennyLane,https://aws.amazon.com/blogs/quantum-computing/analog-hamiltonian-simulation-with-pennylane/,"In this post, we'll describe how the PennyLane-Braket SDK plugin to study the ground state of the anti-ferromagnetic Ising spin-chain on a 1D lattice on the Aquila quantum processor, a neutral-atom quantum computer available on-demand via the AWS Cloud.",[{'name': 'Mao Lin'}],"Wed, 01 Nov 2023 18:09:31 +0000"
2174,Explore quantum algorithms faster by running your local Python code as an Amazon Braket Hybrid Job with minimal code changes,https://aws.amazon.com/blogs/quantum-computing/explore-quantum-algorithms-faster-by-running-your-local-python-code-as-an-amazon-braket-hybrid-job-with-minimal-code-changes/,Today we'll show you how to use a new python decorator from the Amazon Braket SDK to help algorithm researchers seamlessly execute local Python functions as an Amazon Braket Hybrid Job with just one extra line of code.,[{'name': 'Stefan Natu'}],"Tue, 17 Oct 2023 14:58:22 +0000"
2175,Introducing a new temperature-resistant packaging technique for optical devices,https://aws.amazon.com/blogs/quantum-computing/introducing-a-new-temperature-resistant-packaging-technique-for-optical-devices/,"Today, we’re announcing a first-of-its-kind advancement in photonic interconnection - a fiber-device interface that can withstand multiple cycles of cooling to cryogenic temperatures - and back - without introducing additional losses.",[{'name': 'Denis Sukachev'}],"Mon, 16 Oct 2023 14:18:04 +0000"
2176,"Quantum computers – out of the lab, into the world",https://www.dlr.de/en/latest/news/2023/04/quantum-computers-out-of-the-lab-into-the-world,"The German Aerospace Center (DLR) has commissioned a special mini quantum computer to be used for training courses; for future users, students and companies to really touch and try out.",[],
2177,International Gaia-X expert talks,https://www.dlr.de/en/ki/latest/news/international-gaia-x-expert-talks,"As part of the Gaia-X 4 Future Mobility and Catena-X projects, colleagues from the DLR Institute for AI Safety and Security attended various conferences in Seoul in November and held in-depth discussions on the topic of secure data ecosystems and innovative data-driven mobility concepts.",[],
2178,Meeting of the Family of Projecs Gaia-X 4 Future Mobility,https://www.dlr.de/en/ki/latest/news/meeting-of-the-family-of-projetcs-gaia-x-4-fture-mobilityrojets,"In early December, the second major exchange of the Gaia-X 4 Future Mobility project family took place in Cologne, Germany, with participants from all around 80 participating companies and research institutes. The project family is developing numerous applications and foundations for future data-based mobility.",[],
2179,Actively shaping regional transformation,https://www.dlr.de/en/ki/latest/news/actively-shaping-regional-transformation,"Under this inspiring motto, an event was held in Leipzig on 4 and 5 December 2023 to discuss the challenges and solutions for the future of the automotive and supplier industry.",[],
2180,ELSA co-operation workshop,https://www.dlr.de/en/ki/latest/news/elsa-co-operation-workshop,"Last week, the two ELSA teams (Ethical, Legal and Social Aspects) of the DLR Institute for the Protection of Maritime Infrastructure and the Institute for AI Safety and Security met for a workshop to discuss current projects and possible future cooperation.",[],
2181,Focus on machine learning,https://www.dlr.de/en/ki/latest/news/focus-on-machine-learning,"Last week, 120 colleagues from various disciplines met at the DLR site in Ulm for a four-day internal workshop. Machine learning and artificial intelligence play an important role in many DLR projects.",[],
2182,Talents sought & hopefully found,https://www.dlr.de/en/ki/latest/news/talents-sought-hopefully-found,"At the Career Day at the University of Ulm, our team from the Institute of AI Safety and Security, together with our neighbouring institutes presented our work and tangible results of our respective research.",[],
2183,AI Safety Summit 2023,https://www.dlr.de/en/ki/latest/news/ai-safety-summit-2023,"A high level of safety is essential for artificial intelligence: \"Safe AI is important because we will find AI-based systems in all areas of life in the future. This also includes areas in which high safety requirements must be met,\" says Prof Frank Köster, Founding Director of the Institute of AI Safety and Security at the German Aerospace Center (DLR). Last week, representatives from almost 30 countries and technology companies met in England to discuss the safety of artificial intelligence (AI). In a joint declaration, they pledged to work together, among other things. The establishment of an \"AI Safety Institute\" was also announced.",[],
2184,i2b meet-up: Generative AI – What‘s next?,https://www.dlr.de/en/ki/latest/news/i2b-meet-up-ai-chatgpt-is-just-the-beginning,"Music is generated by AI and you can't figure it out? Nils Bäumer from DenkSinn GmbH gives a keynote at the i2b meet-up, partly in Japanese, without knowing the language...! The associated i2b talk round, moderated by Knut Bensen in Oldenburg in mid-October, agrees: ChatGPT is just the beginning.",[],
2185,DLR improves the prediction of flight characteristics for future aircraft,https://www.dlr.de/en/latest/news/2023/04/dlr-improves-the-prediction-of-flight-characteristics-of-future-aircraft,"In wind-tunnel tests, the German Aerospace Center (DLR) has investigated the lift generated by commercial aircraft using state-of-the-art measurement and simulation technologies.",[],
2186,Institute for AI Safety and Security at the Quantum Effects trade fair,https://www.dlr.de/en/ki/latest/news/institute-for-ai-safety-and-security-at-the-quantum-effects-trade-fair,"At the Quantum Effects exhibition in Stuttgart, we presented the interim result of a quantum computing project: A 40 x 60 cm 3D print of the optimisation landscape of a classification task solved with quantum machine learning.",[],
2187,Physics-informed AI methods for predicting complex systems.,https://www.dlr.de/en/ki/latest/news/physics-informed-ai-methods-for-predicting-complex-systems,"In the Innovative Computational Methods research group, Dennis Duncan and Christoph Raeth at the Institute for AI Safety and Security are developing and optimising methods for hybrid reservoir computing. They combine data-driven and model-based predictions to better forecast complex physical-technical systems. These methods can be used, for example, to predict instabilities in turbines or frequency fluctuations in power grids.",[],
2188,Transforming the automotive industry,https://www.dlr.de/en/ki/latest/news/transforming-the-automotive-industry,"Last week, the Federal Minister Dr. Robert Habeck and the expert group \"Transformation of the Automotive Industry\" (ETA) met for an exchange. The topic was Germany as an automotive location and its successful transformation. The focus was on setting the course for a resilient supply of raw materials and semiconductors as well as the use of vehicle data.",[],
2189,ELSA in AI,https://www.dlr.de/en/ki/latest/news/elsa-in-ai,Colleagues from the Institute for AI Safety and Security also participated in the Responsible AI Forum 2023. The discussions clearly showed that a responsible approach to AI is indispensable.,[],
2190,MdL Martin Rivoir visits DLR in Ulm,https://www.dlr.de/en/ki/latest/news/member-of-the-state-parliament-martin-rivoir-visits-dlr-in-ulm,"Martin Rivoir (SPD), Member of the Baden-Württemberg State Parliament, visited the Ulm site of the German Aerospace Center (DLR) on 18 September 2023.",[],
2191,Cutting-edge tech to fight hunger,https://www.dlr.de/en/latest/news/2024/cutting-edge-tech-to-fight-hunger,"Humanitarian aid is urgently required as 333 million people face acute hunger due to conflict, natural disasters, climate change and high food prices. DLR has been providing technological support in the fight against hunger for about 20 years, in particular alongside the United Nations World Food Programme (WFP), the world's largest humanitarian organisation.",[],
2192,Better coordination of emergency services in buildings,https://www.dlr.de/en/latest/news/2023/04/better-coordination-of-emergency-services-in-buildings,"Emergency responders operating in buildings or industrial plants should not only know their exact location. In order for a mission to be coordinated, it is also important that the location of each individual team member can be tracked.",[],
2193,Institute becomes part of the green campus,https://www.dlr.de/en/pi/latest/news/institute-becomes-part-of-the-green-campus,"The Institute for the Protection of Terrestrial Infrastructures has been based in Sankt Augustin since 2020. In a few years, the institute will move into its own building, including a test hall, on the site „Auf dem Butterberg“.",[],
2194,EURIALO – Safe aviation ensured from space,https://www.dlr.de/en/latest/news/2023/03/eurialo-safe-aviation-ensured-from-space,The goal of the future EURIALO space system is to increase international air traffic safety. Small satellites will be used to monitor all commercial flights continuously and globally.,[],
2195,Network for responsive satellite deployment,https://www.dlr.de/en/latest/news/2023/03/network-for-responsive-satellite-deployment,"The security and functionality of a modern society depend on unhindered access to space-based information and communications channels. Under DLR's leadership, 39 European institutions and numerous companies are working together on a network for the rapid deployment of satellites.",[],
2196,DLR at the Paris Air Show 2023 in France,https://www.dlr.de/en/latest/news/2023/02/dlr-at-the-pas-2023-in-france,"Satellites for environmental monitoring, high-altitude platforms and climate-friendly propulsion systems for air transport – these are just three examples of current research work that DLR will be showcasing at the Paris Air Show from 19 to 25 June 2023.",[],
2197,"New re­search air­craft for cli­mate-friend­ly flight, new wind en­er­gy re­search farm and green light for the Start­up Fac­to­ry",https://www.dlr.de/en/latest/news/2023/01/dlr-provides-research-impetus-for-sustainable-society-of-future-in-2023,"In 2023, the German Aerospace Center (Deutsches Zentrum für Luft- und Raumfahrt; DLR) will provide significant research support for the development and rapid application of sustainable technologies.",[],
2198,Land sur­face has been dis­placed by up to six me­tres,https://www.dlr.de/en/latest/news/2023/01/land-surface-has-been-displaced-by-up-to-six-metres,"Satellite images have revealed how the land has shifted as a result of the severe earthquakes in Turkey and Syria. According to an analysis by the German Aerospace Center (Deutsches Zentrum für Luft- und Raumfahrt; DLR), this shift amounts to up to six metres in some places.",[],
2199,How the institute and the NRW police cooperate in security research,https://www.dlr.de/en/pi/latest/news/how-the-institute-and-the-nrw-police-cooperate-in-security-research,"Together with the DLR board chairwoman Prof. Dr. Anke Kaysser-Pyzalla visited the NRW Police Innovation Lab, head of the institute Michael Langerbein.",[],
2200,DLR sup­ports emer­gen­cy re­spon­ders in Turkey,https://www.dlr.de/en/latest/news/2023/01/dlr-supports-emergency-responders-in-turkey,"The German Aerospace Center (Deutsches Zentrum für Luft- und Raumfahrt; DLR) is supporting rescue teams in areas affected by the recent earthquakes in Turkey. Two researchers from the DLR Institute of Optical Sensor Systems arrived in the area on 7 February 2023, together with a team from International Search And Rescue (I.S.A.R.) Germany.",[],
2201,Cooperation in research and development,https://www.dlr.de/en/pi/latest/news/cooperation-in-research-and-development,The German Aerospace Center (DLR) and Henkel intend to intensify their existing cooperation in research and development for civil security applications. The collaboration pursues the goal of digitizing buildings and facilities in an automated way and making them usable for security applications.,[],
2202,Pro­vid­ing as­sis­tance world­wide with aeri­al maps,https://www.dlr.de/en/latest/news/2022/04/providing-assistance-worldwide-with-aerial-maps,"Since 2016, the German Aerospace Center (Deutsches Zentrum für Luft- und Raumfahrt; DLR) and the non-profit aid organisation I.S.A.R. Germany have been cooperation partners. Together, they test and deploy new technical developments for crisis and disaster management.",[],
2203,Re­search for re­li­able and se­cure ar­ti­fi­cial in­tel­li­gence,https://www.dlr.de/en/latest/news/2022/04/research-for-reliable-and-secure-artificial-intelligence,"Artificial intelligence (AI) is used, for instance, in transport systems, in aerospace and in energy supply. It has been part of everyday life in modern society for a long time - and as digitalisation advances, AI becomes all the more important: it must be both secure and transparent.",[],
2204,DL­R­magazine 171 – Awak­en­ing the sleep­ing gi­ant,https://www.dlr.de/en/latest/news/2022/04/dlrmagazine-171-awakening-the-sleeping-giant,"Heat pumps, thermal wind energy systems and the use of lime for heat storage, DLR scientists are conducting research in various areas for the energy transition. The cover story in this issue of the DLRmagazine gives an overview of the different aspects and potential of heat. This DLRmagazine also features a 'League of extraordinary robots' and the question of why the Arctic is warming up much faster than the rest of the planet.",[],
2205,DLR tests tech­nolo­gies to pro­tect mar­itime in­fras­truc­ture,https://www.dlr.de/en/latest/news/2022/04/dlr-tests-technologies-to-protect-maritime-infrastructure,Public attention is currently focused on the security of maritime infrastructure. The German Aerospace Center (Deutsches Zentrum für Luft- und Raumfahrt; DLR) is conducting research into the protection of port facilities as well as their functionality.,[],
2206,Delegation from Fukushima visits Jülich,https://www.dlr.de/en/ff/latest/news/2024/delegation-from-fukushima-visits-julich,"A Japanese business delegation visited us at the Jülich site. The talks focussed on the synergy potential between companies in the Fukushima region, DLR and the Brainergy Park Jülich, which is currently under construction. Guided tours of the large-scale research facilities Synlight and Solar Towers Jülich provided the framework.",[],
2207,Springschool points the way to a sustainable future with hydrogen,https://www.dlr.de/en/ve/latest/news/archive/springschool-points-the-way-to-a-sustainable-future-with-hydrogen,The Springschool Hydrogen Technology offers a scientific exchange on hydrogen-related technologies and their applications from 17 to 22 March 2024 in Heide / Schleswig-Holstein. Around 40 experts from research and industry will provide young researchers with practical insights into the entire spectrum of the hydrogen economy.,[],
2208,Workshop \"Zulassung - Zertifizierung- Normung\" (Admission – Certification – Standardisation) on 20th/21st February 2024,https://www.dlr.de/en/ve/latest/news/archive/workshop-admission-certification-standardisation-on-20th-21st-feb-2024,The 13th edition of the workshop takes place in Freiburg. Registration is posible until 10th February.,[],
2209,Project ENaQ receives ODH-Award 2023,https://www.dlr.de/en/ve/latest/news/archive/project-enaq-receives-odh-award-2023,The energetical neighbourhood project Fliegerhorst has received the award in the category \"Integration\".,[],
2210,Institute Director Carsten Agert stresses the importance of flexibilities in the energy system,https://www.dlr.de/en/ve/latest/news/archive/agert-in-parliamentary-group,The importance of sector integration in electricity market design was the subject of a keynote speech by Institute Director Prof. Carsten Agert on 30th November 2023 in the German Bundestag's parliamentary group on sector integration.,[],
2211,Building hydrogen infrastructure: experts recommend rapid development of a storage concept,https://www.dlr.de/en/ve/latest/news/archive/building-hydrogen-infrastructure-experts-recommend-rapid-development-of-a-storage-concept,"At its meeting on 27 and 28 November 2023 in Oldenburg, the \"Infrastructure and System Integration\" cluster from the Hydrogen Research Network recommended placing a greater focus on the integration of large-scale storage capacities in the preparations for the market ramp-up of a German hydrogen infrastructure.",[],
2212,Presenting the roadmap for the use of hydrogen at Hamburg Airport,https://www.dlr.de/en/latest/news/2023/04/presenting-the-roadmap-for-the-use-of-hydrogen-at-hamburg-airport,The mobility of people and goods clearly characterises today's cities. Transforming this mobility offers the opportunity to make cities and their transport hubs more attractive while reducing emissions.,[],
2213,Drones to transport personnel and materials to offshore wind farms,https://www.dlr.de/en/latest/news/2023/04/drone-to-transport-personnel-and-material-to-offshore-wind-farm,Maintenance teams and materials have to travel long distances to reach offshore wind turbines. Can drones take over transport tasks and relieve maintenance personnel?,[],
2214,Special exhibition on fertiliser: Solar Fuels exhibit shows sustainable production methods,https://www.dlr.de/en/ff/latest/news/2023/special-exhibition-on-fertiliser-solar-fuels-exhibit-shows-sustainable-production-methods,"Solar Fuels in the agriculture of tomorrow? No more dreams of the future with CO₂-neutral fertilizers! On the ecologically important topic of fertilizer, the special exhibition \"Elixir of life. Fertilizer between magic power and explosive\" has started at the Museum Brot und Kunst in Ulm. We are pleased to be there with our exhibit \"Solar Fuels\"!",[],
2215,"Temperature record in the MOPUW, the hottest thermal oil plant in the world",https://www.dlr.de/en/ff/latest/news/2023/temperature-record-in-the-mopuw-the-hottest-thermal-oil-plant-in-the-world,"The Mobile Pump and Heat Transfer Fluid Test Stand (MOPUW) enables the demonstration and research of new types of heat transfer fluids for solar thermal power plants and industrial plants on a pilot plant scale. In the completed first system demonstration with a high-temperature silicone oil, DLR, in cooperation with the participating industrial partners, demonstrated the functionality of the entire system at temperatures of up to 470 degrees Celsius in continuous operation.",[],
2216,DLR improves the prediction of flight characteristics for future aircraft,https://www.dlr.de/en/latest/news/2023/04/dlr-improves-the-prediction-of-flight-characteristics-of-future-aircraft,"In wind-tunnel tests, the German Aerospace Center (DLR) has investigated the lift generated by commercial aircraft using state-of-the-art measurement and simulation technologies.",[],
2217,PtL technology platform – looking to the future,https://www.dlr.de/en/latest/news/2023/04/ptl-technology-platform-looking-to-the-future,"With the Technology Platform Power-to-Liquid Fuels (TPP), DLR is building a research and demonstration facility in Leuna, Saxony-Anhalt, that is unique in form and size, using funding from the German Federal Ministry for Digital and Transport (BMDV).",[],
2218,Impressions from the hydrogen fair,https://www.dlr.de/en/ff/latest/news/2023/impressions-from-the-hydrogen-fair,"The solar production of hydrogen, solely from solar energy and water. The thermochemical process we presented at this year's Düren District Hydrogen Fair in Jülich makes use of this simple formula.",[],
2219,German trade magazine reports on conference \"Energy Systems Stability 2040 and beyond\",https://www.dlr.de/en/ve/latest/news/archive/german-trade-magazine-reports-on-energy-systems-stability-2040,"In its Jul/Aug 2023 issue, the specialist magazine \"Energiewirtschaftliche Tagesfragen\" devotes four pages to the international conference \"Energy Systems Stability 2040 and beyond\", which the Institute of Networked Energy Systems played a key role in organising.",[],
2220,DLR opens wind energy research farm in Krummendeich,https://www.dlr.de/en/latest/news/2023/03/dlr-opens-wind-energy-research-farm-in-krummendeich,"With its wind energy research farm, DLR has a globally unique large-scale research facility in Krummendeich, near the mouth of the River Elbe. The Wind Validation (WiValdi) facility enables full-scale research with an unprecedented level of detail under real environmental conditions. The goal is to better understand wind energy with all its influencing factors.",[],
2221,Olympia Planum – between dunes and steep ice cliffs,https://www.dlr.de/en/latest/news/2024/olympia-planum-between-dunes-and-steep-ice-cliffs,"These images, created using data acquired by DLR's High Resolution Stereo Camera (HRSC), show a section of Mars' north pole region. Several steep scarps mark the boundary between the north polar ice cap and the lower-lying surrounding terrain with large dune fields. These vertical ice walls are up to 1000 metres high and, with their layers, serve as a martian climate archive.",[],
2222,Four hundred kilograms of research in microgravity,https://www.dlr.de/en/latest/news/2024/four-hundred-kilograms-of-research-in-microgravity,"On 27 February 2024 at 8:27 CET the MAPHEUS 14 high-altitude research rocket, operated by the German Aerospace Center (Deutsches Zentrum für Luft- und Raumfahrt; DLR), successfully launched from the Esrange Space Center near Kiruna, Sweden.",[],
2223,Interview with ESA astronaut Marcus Wandt part II,https://www.dlr.de/en/latest/news/2024/interview-with-esa-astronaut-marcus-wandt-part-ii,"On 9 February 2024, the Swedish ESA astronaut Marcus Wandt returned to Earth after spending 21 days on the International Space Station ISS.",[],
2224,Cutting-edge tech to fight hunger,https://www.dlr.de/en/latest/news/2024/cutting-edge-tech-to-fight-hunger,"Humanitarian aid is urgently required as 333 million people face acute hunger due to conflict, natural disasters, climate change and high food prices. DLR has been providing technological support in the fight against hunger for about 20 years, in particular alongside the United Nations World Food Programme (WFP), the world's largest humanitarian organisation.",[],
2225,TanDEM-X – German radar satellites show volcanic eruption in Iceland,https://www.dlr.de/en/latest/news/2024/tandem-x-german-radar-satellites-show-volcanic-eruption-in-iceland,"On 18 December 2023, a volcanic eruption occurred in the area of the Sundhnúkur 'crater row' between Grindavík and Fagradalsfjall.",[],
2226,GJ 367b - reloaded,https://www.dlr.de/en/pf/about-us/departments/extrasolar-planets-and-atmospheres/news-from-our-department-epa/gj-367-reloaded,"Among the exoplanets, GJ 367 b is a flyweight, but with a relatively large iron core. Measurements with JWST show further characteristics: a hot, dark planet without an air atmosphere.",[],
2227,Tracking interactions in Earth's atmosphere,https://www.dlr.de/en/latest/news/2024/tracking-interactions-in-earth-s-atmosphere,"Whether facing drought and heat in southern Europe or coping with heavy precipitation events in Germany, solar radiation is the decisive factor influencing climate events and weather dynamics on Earth, as it drives atmospheric circulation.",[],
2228,Milestones in space exploration,https://www.dlr.de/en/latest/news/2024/milestones-in-space-exploration,The LISA observatory will detect low-frequency gravitational waves from space and determine the nature of their sources with great accuracy once it goes into operation at the end of 2035.,[],
2229,Europe is on its way to Venus,https://www.dlr.de/en/latest/news/2024/europe-is-on-its-way-to-venus,"Venus, Earth's inner neighbouring planet, is a mysterious celestial body. Almost the same size as Earth, with only a slightly smaller mass and largely made up of the same materials as Earth, Venus has developed in a completely different way.",[],
2230,DLR develops mobile station for Satellite Laser Ranging,https://www.dlr.de/en/latest/news/2024/dlr-develops-mobile-station-for-satellite-laser-ranging,"Satellite Laser Ranging (SLR) is a very precise, laser-based method for determining the distance between a satellite and Earth with an accuracy of a few millimetres.",[],
2231,Four-legged friends and other robots: ESA astronaut tests avatar control between ISS and Earth,https://www.dlr.de/en/latest/news/2024/four-legged-friends-and-other-robots-esa-astronaut-tests-avatar-control-between-iss-and-earth,Concentrated silence reigns in the Mars laboratory of the German Aerospace Center (Deutsches Zentrum für Luft- und Raumfahrt; DLR) in Oberpfaffenhofen – the connection to the International Space Station ISS has been established.,[],
2232,Eighteen million euros for small satellite platforms and launch services,https://www.dlr.de/en/latest/news/2024/eighteen-million-euros-for-small-satellite-platforms-and-launch-services,"Small satellites are versatile and flexible – for telecommunications services, Earth observation and climate research, or for testing new technologies in space.",[],
2233,DLR's Mars camera – 20 years of operations,https://www.dlr.de/en/latest/news/2024/dlr-s-mars-camera-20-years-of-operations,"An extraordinary success story, and a long-lasting one at that. For 20 years now, DLR's HRSC on board the ESA Mars Express orbiter has been providing the science community and the public with fascinating high-resolution images of Earth's neighbouring planet.",[],
2234,Mission to the ISS with plenty of German science,https://www.dlr.de/en/latest/news/2024/mission-to-the-iss-with-plenty-of-german-science,"On 17 January 2024, Marcus Wandt, the Swedish project astronaut from the European Space Agency (ESA), will embark on his Muninn mission aboard a Dragon capsule provided by the US space company SpaceX from NASA's Kennedy Space Center in Florida, USA.",[],
2235,The journey to the martian moon Phobos begins,https://www.dlr.de/en/latest/news/2024/the-journey-to-the-martian-moon-phobos-begins,"The German-French rover IDEFIX travels to Japan and is expected to arrive there at the end of January. It is part of the MMX mission, which will explore both martian moons under the leadership of JAXA. The aim is to resolve the mystery surrounding the formation of Phobos and Deimos.",[],
2236,Cutting-edge tech to fight hunger,https://www.dlr.de/en/latest/news/2024/cutting-edge-tech-to-fight-hunger,"Humanitarian aid is urgently required as 333 million people face acute hunger due to conflict, natural disasters, climate change and high food prices. DLR has been providing technological support in the fight against hunger for about 20 years, in particular alongside the United Nations World Food Programme (WFP), the world's largest humanitarian organisation.",[],
2237,DLR is in the Top 10 of German employers,https://www.dlr.de/en/sl/latest/news/dlr-is-in-the-top-10-of-german-employers,"The stern magazine is reporting on a survey about the best employers in the country. As part of the Top 10 employer DLR, the Institute of System Architectures in Aeronautics was interviewed as representative.",[],
2238,Changing flight altitudes avoids the formation of climate-damaging contrails,https://www.dlr.de/en/latest/news/2023/04/changing-flight-altitudes-avoids-the-formation-of-climate-damaging-contrails,The German Aerospace Center (DLR) and the EUROCONTROL Maastricht Upper Area Control Centre (MUAC) have jointly demonstrated that long-lasting contrails can be avoided by slightly changing flight altitudes.,[],
2239,Presenting the roadmap for the use of hydrogen at Hamburg Airport,https://www.dlr.de/en/latest/news/2023/04/presenting-the-roadmap-for-the-use-of-hydrogen-at-hamburg-airport,The mobility of people and goods clearly characterises today's cities. Transforming this mobility offers the opportunity to make cities and their transport hubs more attractive while reducing emissions.,[],
2240,Drones to transport personnel and materials to offshore wind farms,https://www.dlr.de/en/latest/news/2023/04/drone-to-transport-personnel-and-material-to-offshore-wind-farm,Maintenance teams and materials have to travel long distances to reach offshore wind turbines. Can drones take over transport tasks and relieve maintenance personnel?,[],
2241,Technology cooperation for aviation fuels of the future,https://www.dlr.de/en/latest/news/2023/04/technology-cooperation-for-aviation-fuels-of-the-future,"Working together for a more sustainable aviation: Representatives of Lufthansa Airlines, the German Aerospace Center (DLR), Airbus, Flughafen München GmbH and MTU Aero Engines signed a Letter of Intent (LoI) in Berlin for a broad-based research collaboration on Power-to-Liquid (PtL) aviation fuels.",[],
2242,DLR improves the prediction of flight characteristics for future aircraft,https://www.dlr.de/en/latest/news/2023/04/dlr-improves-the-prediction-of-flight-characteristics-of-future-aircraft,"In wind-tunnel tests, the German Aerospace Center (DLR) has investigated the lift generated by commercial aircraft using state-of-the-art measurement and simulation technologies.",[],
2243,PtL technology platform – looking to the future,https://www.dlr.de/en/latest/news/2023/04/ptl-technology-platform-looking-to-the-future,"With the Technology Platform Power-to-Liquid Fuels (TPP), DLR is building a research and demonstration facility in Leuna, Saxony-Anhalt, that is unique in form and size, using funding from the German Federal Ministry for Digital and Transport (BMDV).",[],
2244,"Boeing, NASA, United Airlines and DLR to test SAF benefits with air-to-air flights",https://www.dlr.de/en/latest/news/2023/04/boeing-nasa-united-airlines-and-dlr-to-test-saf-benefits-with-air-to-air-flights,"In a collaboration to strengthen sustainability in aviation, Boeing is partnering with NASA and United Airlines for in-flight testing to measure how sustainable aviation fuel (SAF) affects contrails and non-carbon emissions, in addition to reducing the fuel's life cycle climate impact.",[],
2245,DLR and Embraer extend cooperation in aviation research,https://www.dlr.de/en/latest/news/2023/04/dlr-and-embraer-extend-cooperation-in-aviation-research,"São José dos Campos – Brazil, 9 October 2023 – Embraer and the German Aerospace Center (DLR) today announced a Memorandum of Understanding (MoU) for collaboration regarding precompetitive technology research and development activities.",[],
2246,Milestone for hydrogen combustion in aircraft turbines,https://www.dlr.de/en/latest/news/2023/03/milestone-for-hydrogen-combustion-in-aircraft-turbines,"The combustion of hydrogen in aircraft engines is a crucial core technology for climate-friendly air transport. In a joint project led by Rolls-Royce and its partner easyJet, 100 percent hydrogen was successfully tested for the first time under aircraft take-off conditions at DLR in Cologne.",[],
2247,DLR acquires flying hydrogen laboratory and demonstrates whole-system solutions,https://www.dlr.de/en/latest/news/2023/03/dlr-acquires-flying-hydrogen-laboratory-and-demonstrates-integrated-solutions,"On 25 September 2023, the German Aerospace Center (DLR) took delivery of the new D328 UpLift research aircraft during the National Aviation Conference and in the presence of the Federal Minister for Economic Affairs and Climate Action, Robert Habeck.",[],
2248,Joint efforts by research and industry for climate-friendly air transport,https://www.dlr.de/en/latest/news/2023/03/joint-efforts-by-research-and-industry-for-climate-friendly-air-transport,"Climate-friendly air transport is one of the great technological challenges at present. In this short interview, Anke Kaysser-Pyzalla, Chair of the DLR Executive Board, talks about the role of air transport in global mobility, the various aspects of the climate impact of aviation and which steps research, industry and governments should consistently take together to significantly reduce the climate impact of air transport on the basis of existing technologies.",[],
2249,Long-haul flights – small changes with a big climate impact,https://www.dlr.de/en/latest/news/2023/03/long-haul-flights-small-changes-with-a-big-climate-impact,Long-haul flights carry only around 10 percent of all passengers each year but generate approximately 40 percent of the carbon dioxide emissions due to air transport. This is due to the long distances and flight times involved.,[],
2250,Successful demonstration of active flutter suppression,https://www.dlr.de/en/latest/news/2023/03/scientists-tame-dreaded-aviation-phenomenon,A team of researchers in Europe has succeeded in overcoming an important challenge in aeroelasticity – the suppression of the phenomenon of flutter in an aircraft structure by means of an active control system.,[],
2251,KoTAM Workshop: Safety and Security in the Context of C-ITS services,https://www.dlr.de/en/ts/latest/news/2023/kotam-workshop-safety-and-security-in-the-context-of-c-its-services,"\"Safety and security in the context of C-ITS services\" - more than 65 representatives from industry, research and public authorities met in Hamburg on 30 November 2023 to discuss this topic. The workshop was organised by the German Aerospace Center (DLR) together with ITS mobility.",[],
2252,Parliamentary evening in Berlin presented research \"Made in Braunschweig\",https://www.dlr.de/en/ts/latest/news/2023/parliamentary-evening-in-berlin-presented-research-made-in-braunschweig,"What will mobility look like in the future? What do technological developments mean for our social coexistence and how will this transformation change the world of work? These were the questions addressed at the joint Parliamentary Evening of the City of Braunschweig with leading national research institutions from Braunschweig on Thursday, 30 November, at the State Representation of Lower Saxony in Berlin.",[],
2253,Remote Operation: An important building block from Braunschweig for the mobility of the     future,https://www.dlr.de/en/ts/latest/news/2023/remote-operation-an-important-building-block-from-braunschweig-for-the-mobility-of-the-future,Braunschweig is supplying an important building block for the mobility of the future: The Institute of Transportation Systems at the German Aerospace Centre is conducting intensive research into how the concept of remote operation of automated and networked vehicles can be implemented and how the workplace for remote operation can be designed.,[],
2254,New development standard for safe automated driving - result of the joint project Verification and Validation Methods,https://www.dlr.de/en/ts/latest/news/2023/new-development-standard-for-safe-automated-driving-result-of-the-joint-project-verification-and-validation-methods,The German Aerospace Centre (DLR) has joined forces with the German automotive industry in a joint project with 21 partners and developed the world's first structures to make safety standards for automated vehicles in urban environments verifiable.,[],
2255,Presenting the roadmap for the use of hydrogen at Hamburg Airport,https://www.dlr.de/en/latest/news/2023/04/presenting-the-roadmap-for-the-use-of-hydrogen-at-hamburg-airport,The mobility of people and goods clearly characterises today's cities. Transforming this mobility offers the opportunity to make cities and their transport hubs more attractive while reducing emissions.,[],
2256,Reflecting on 178 days of research operations – U-Shift at the German National Garden Show,https://www.dlr.de/en/latest/news/2023/04/reflecting-on-178-days-of-research-operations-u-shift-at-the-german-national-garden-show,"For six months, from 14 April to 8 October 2023, the German Aerospace Center (DLR) offered visitor a first-hand experience of future urban transport at the BUGA 23 garden show in Mannheim.",[],
2257,VRIEDRICH project - 2nd measurement for traffic monitoring on the B179,https://www.dlr.de/en/ts/latest/news/2023/vriedrich-project-2nd-measurement-for-traffic-monitoring-on-the-b179,"In the joint project \"Strengthening infrastructure management: rural areas - accident prevention through digitalisation and auditing of the road - example of the B179 in the Dahme-Spreewald/Brandenburg district\", the German Road Safety Council (DVR) and the German Aerospace Center (DLR) are monitoring and analysing traffic on the B179 federal road with a second measurement at the end of September.",[],
2258,Transforming the automotive industry,https://www.dlr.de/en/ki/latest/news/transforming-the-automotive-industry,"Last week, the Federal Minister Dr. Robert Habeck and the expert group \"Transformation of the Automotive Industry\" (ETA) met for an exchange. The topic was Germany as an automotive location and its successful transformation. The focus was on setting the course for a resilient supply of raw materials and semiconductors as well as the use of vehicle data.",[],
2259,DLR is a member of the German Road Safety Technology Association (BVST),https://www.dlr.de/en/ts/latest/news/2023/dlr-is-a-member-of-the-german-road-safety-technology-association-bvst,"The German Aerospace Centre (DLR) is a new member of the German Road Safety Technology Association (BVST). The BVST is the neutral dialogue partner in the technology-based road safety sector for politicians, authorities, associations and the public. It is a strong driver of technical innovations and an interface between technology, science and authorities. It sees itself as a promoter and supporter in the fields of traffic, safety, environment and technology: Transport, safety, environment and technology.",[],
2260,Utilising the potential of AI for climate protection - AIAMO project receives funding certificate,https://www.dlr.de/en/ts/latest/news/2023/utilising-the-potential-of-ai-for-climate-protection-aiamo-project-receives-funding-certificate,"Traffic in Germany will increase significantly across all modes of transport - this was recently confirmed by the BMDV's long-term traffic forecast up to 2051. In order to support local authorities in managing the increasing demand for mobility in an environmentally friendly way, the German Aerospace Centre (DLR) is relying on artificial intelligence with the AIAMO (\"Artificial Intelligence and Mobility\") project.",[],
2261,DLR at IAA Mobility 2023,https://www.dlr.de/en/latest/news/2023/03/dlr-at-iaa-mobility-2023,"The world’s most environmentally friendly car in operation, unique test infrastructure for the development of autonomous driving and new ideas for connected mobility will be displayed by the German Aerospace Center (DLR) at IAA Mobility 2023 in Munich.",[],
2262,EURIALO – Safe aviation ensured from space,https://www.dlr.de/en/latest/news/2023/03/eurialo-safe-aviation-ensured-from-space,The goal of the future EURIALO space system is to increase international air traffic safety. Small satellites will be used to monitor all commercial flights continuously and globally.,[],
2263,DLR at the Paris Air Show 2023 in France,https://www.dlr.de/en/latest/news/2023/02/dlr-at-the-pas-2023-in-france,"Satellites for environmental monitoring, high-altitude platforms and climate-friendly propulsion systems for air transport – these are just three examples of current research work that DLR will be showcasing at the Paris Air Show from 19 to 25 June 2023.",[],
2264,DLR appoints new Divisional Board Member for Energy and Transport,https://www.dlr.de/en/latest/news/2023/02/dlr-appoints-new-divisional-board-member-for-energy-and-transport,"On 15 June 2023, the DLR Senate appointed Professor Meike Jipp as the new Divisional Board Member for Energy and Transport. The psychology graduate is a professor of Transport Demand and Transport Impacts at the Technical University of Berlin (TU Berlin) and was previously Director of the Berlin Institute of Transport Research at DLR.",[],
2265,DLR at the GREENTECH FESTIVAL,https://www.dlr.de/en/latest/news/2023/02/dlr-at-the-greentech-festival,"From 14 to 16 June 2023, the German Aerospace Center (DLR) will be presenting futuristic technologies and current research projects from the fields of energy, mobility and robotics at the GREENTECH FESTIVAL in Berlin.",[],
